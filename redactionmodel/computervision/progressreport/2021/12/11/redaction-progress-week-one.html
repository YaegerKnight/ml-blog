<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>73% accuracy for redaction object detection | mlops.systems</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="73% accuracy for redaction object detection" />
<meta name="author" content="Alex Strick van Linschoten" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Last time I wrote about my redaction model training project, I explained how I used Prodigy to annotate and label a bunch of images. I subsequently spent a long evening going through the process, getting to know my data. I managed to make 663 annotations, though quite a few of those were negative annotations: I was stating that a certain document contained no redactions at all." />
<meta property="og:description" content="Last time I wrote about my redaction model training project, I explained how I used Prodigy to annotate and label a bunch of images. I subsequently spent a long evening going through the process, getting to know my data. I managed to make 663 annotations, though quite a few of those were negative annotations: I was stating that a certain document contained no redactions at all." />
<link rel="canonical" href="https://www.mlops.systems/redactionmodel/computervision/progressreport/2021/12/11/redaction-progress-week-one.html" />
<meta property="og:url" content="https://www.mlops.systems/redactionmodel/computervision/progressreport/2021/12/11/redaction-progress-week-one.html" />
<meta property="og:site_name" content="mlops.systems" />
<meta property="og:image" content="https://www.mlops.systems/images/redaction-progress-week-one/redaction_sample_3-small.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-12-11T00:00:00-06:00" />
<script type="application/ld+json">
{"datePublished":"2021-12-11T00:00:00-06:00","url":"https://www.mlops.systems/redactionmodel/computervision/progressreport/2021/12/11/redaction-progress-week-one.html","@type":"BlogPosting","dateModified":"2021-12-11T00:00:00-06:00","image":"https://www.mlops.systems/images/redaction-progress-week-one/redaction_sample_3-small.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.mlops.systems/redactionmodel/computervision/progressreport/2021/12/11/redaction-progress-week-one.html"},"author":{"@type":"Person","name":"Alex Strick van Linschoten"},"headline":"73% accuracy for redaction object detection","description":"Last time I wrote about my redaction model training project, I explained how I used Prodigy to annotate and label a bunch of images. I subsequently spent a long evening going through the process, getting to know my data. I managed to make 663 annotations, though quite a few of those were negative annotations: I was stating that a certain document contained no redactions at all.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://www.mlops.systems/feed.xml" title="mlops.systems" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script
  defer
  data-domain="mlops.systems"
  src="https://plausible.io/js/plausible.js"
></script>


<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">mlops.systems</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">73% accuracy for redaction object detection</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-12-11T00:00:00-06:00" itemprop="datePublished">
        Dec 11, 2021
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Alex Strick van Linschoten</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      3 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#redactionmodel">redactionmodel</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#computervision">computervision</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#progressreport">progressreport</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
</ul><p>Last time I wrote about my redaction model training project, <a href="https://mlops.systems/redactionmodel/computervision/datalabelling/2021/11/29/prodigy-object-detection-training.html">I explained how I used Prodigy</a> to annotate and label a bunch of images. I subsequently spent a long evening going through the process, getting to know my data. I managed to make 663 annotations, though quite a few of those were negative annotations: I was stating that a certain document contained no redactions at all.</p>

<p>Once I had my redactions, I needed to convert the files from a Prodigy format into a <code class="language-plaintext highlighter-rouge">.coco</code> annotation format. I am using <a href="https://airctic.com/">IceVision</a>, a really useful computer vision library, for which it is easier if I pass in the annotations in the <code class="language-plaintext highlighter-rouge">.coco</code> format.</p>

<p>From that point, it was fairly easy to follow the steps of <a href="https://airctic.com/0.11.0/getting_started_object_detection/">the object detection tutorial</a> outlined in the IceVision documentation. I ran into some problems with <a href="https://gradient.run/">Paperspace Gradient</a> not easily installing and importing IceVision. For some reason files don’t get unzipped on Paperspace, but it’s possible to just do this manually:</p>

<ul>
  <li>Do the basic install, including the import of <code class="language-plaintext highlighter-rouge">icevision.all</code>. Wait for the error to get raised, then open up a terminal and enter:</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> /root/.icevision/mmdetection_configs/
<span class="nb">rm </span>v2.16.0.zip
wget https://github.com/airctic/mmdetection_configs/archive/refs/tags/v2.16.0.zip
unzip v2.16.0.zip
</code></pre></div></div>

<p>Then run it again as normal. Later on, another error will get raised. Fix it with this (again in the terminal):</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>jupyter nbextension <span class="nb">enable</span> <span class="nt">--py</span> widgetsnbextension
</code></pre></div></div>

<p>This enables ipywidgets in the notebook, I think.</p>

<p>Once through all of that, I was able to fine-tune a model based on the annotations which I currently have. I selected <a href="https://mlops.systems/redactionmodel/computervision/2021/11/30/vfnet-basics.html">VFNet</a> as the model I wanted to use as the pertained model. After training for 40 epochs, I reached an accuracy of 73%:</p>

<p><img src="/images/redaction-progress-week-one/first-training.png" alt="" title="Metrics from the last few epochs"></p>

<p>If we look at some of the results (using <code class="language-plaintext highlighter-rouge">model_type.show_results()</code>) we can get a sense of the parts it found easy and the parts which it found hard. (All the boxes below are what it as predicted, not the ground truth annotations.) Some identification of boxes went as you might expect:</p>

<p><img src="/images/redaction-progress-week-one/redaction_sample_1.png" alt="" title="It was good at identifying solid and clear redactions"></p>

<p>I was surprised that something like this worked as well as it did:</p>

<p><img src="/images/redaction-progress-week-one/redaction_white_boxes.png" alt="" title="It even made a decent effort at recognising opaque boxes on a white background"></p>

<p>It wasn’t perfect, but I don’t remember having annotated too many of this specific redaction type, so I’m fairly happy with how it worked out. You can see it still makes a number of mistakes and isn’t always precise about where the boxes should go. I hope that’ll improve as I add more examples of this type of redaction.</p>

<p>My next steps for this project include the following:</p>

<ul>
  <li>create synthetic data. The redactions are probably easy enough to mimic where we’ll get a lot of value from the use of synthetic data (fake redactions on not-real document backgrounds). It’ll be an easy way to boost my training data set by a good amount, hopefully leading to big improvements in my model accuracy.</li>
  <li>potentially add in either active learning (to help speed up my annotation process) or self-training (using the model to make annotation suggestions on unlabelled data and using only the suggestions with really high confidence estimates).</li>
  <li>think through the augmentations that I use as part of my workflow. I basically want augmentations that are similar to however the production use case will be: i.e. the kinds of redacted images that it might see when being given real-world data at inference time post-training.</li>
  <li>add in experiment tracking. I’ve never used something like Weights &amp; Biases, so I’m excited to try that out and have a real process for tracking my progress throughout this project.</li>
  <li>cleaning up and refactoring (a bit) my repository where the code lives for processing the input data. It’s starting to get a bit unwieldy and I’m worried I’ll start to forget the order things were done and some of those small details.</li>
</ul>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="strickvl/ml-blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/redactionmodel/computervision/progressreport/2021/12/11/redaction-progress-week-one.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A place to share my technical learnings.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/strickvl" title="strickvl"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/strickvl" title="strickvl"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
