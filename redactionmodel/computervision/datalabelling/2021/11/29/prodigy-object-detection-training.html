<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>How to annotate image data for object detection with Prodigy | mlops.systems</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="How to annotate image data for object detection with Prodigy" />
<meta name="author" content="Alex Strick van Linschoten" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How I used Prodigy to annotate my data ahead of training an object detection model" />
<meta property="og:description" content="How I used Prodigy to annotate my data ahead of training an object detection model" />
<link rel="canonical" href="https://mlops.systems/redactionmodel/computervision/datalabelling/2021/11/29/prodigy-object-detection-training.html" />
<meta property="og:url" content="https://mlops.systems/redactionmodel/computervision/datalabelling/2021/11/29/prodigy-object-detection-training.html" />
<meta property="og:site_name" content="mlops.systems" />
<meta property="og:image" content="https://mlops.systems/images/2021-11-29-prodigy-object-detection-training/prodigy-redaction.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-11-29T00:00:00-06:00" />
<script type="application/ld+json">
{"datePublished":"2021-11-29T00:00:00-06:00","url":"https://mlops.systems/redactionmodel/computervision/datalabelling/2021/11/29/prodigy-object-detection-training.html","@type":"BlogPosting","dateModified":"2021-11-29T00:00:00-06:00","image":"https://mlops.systems/images/2021-11-29-prodigy-object-detection-training/prodigy-redaction.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlops.systems/redactionmodel/computervision/datalabelling/2021/11/29/prodigy-object-detection-training.html"},"author":{"@type":"Person","name":"Alex Strick van Linschoten"},"headline":"How to annotate image data for object detection with Prodigy","description":"How I used Prodigy to annotate my data ahead of training an object detection model","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://mlops.systems/feed.xml" title="mlops.systems" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script
  defer
  data-domain="mlops.systems"
  src="https://plausible.io/js/plausible.js"
></script>


<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">mlops.systems</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">How to annotate image data for object detection with Prodigy</h1><p class="page-description">How I used Prodigy to annotate my data ahead of training an object detection model</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-11-29T00:00:00-06:00" itemprop="datePublished">
        Nov 29, 2021
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Alex Strick van Linschoten</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      5 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#redactionmodel">redactionmodel</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#computervision">computervision</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#datalabelling">datalabelling</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
</ul><p>I’m back to working on <a href="https://mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html">the redaction model</a>, though this time with a slightly more focused objective: object detection.</p>

<p>Object detection is when you put bounding boxes around the specific object that you are trying to locate within an image. The end goal for my project is to be able to identify — for an arbitrary image — which parts of the image are redacted, and then to be able to calculate what proportion of the image is redacted.</p>

<p>For this, I need annotations. Annotations are the data that I will use as the fuel for the model I hope to train. We need a lot of annotations of specific redactions in order for the computer to be able to learn to detect what is a redaction and what is just an empty box, for example.</p>

<p>I showed in <a href="https://mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html">an earlier post</a> how I trained a model to detect whether there was any kind of redaction inside an image (to around 95% accuracy). For this next stage, it isn’t enough to offer a binary ‘yes’ or ‘no’ for whether it has been redacted. I need to specify the coordinates of a bounding box which encompasses each redaction.</p>

<p>In terms of the final output of the annotations, there are two main ways that this could go. I could either:</p>

<ol>
  <li>get <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code> coordinates for the centre of the bounding box, and then a height and a width of the box around this centre point</li>
  <li>get the four coordinates for each of the corners of the bounding box.</li>
</ol>

<p>The <a href="https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch">COCO dataset format</a> will eventually want datasets in the second format, but <a href="https://prodi.gy">Prodigy</a> has its own way of storing the data which I just left for now. Once I have a better handle on the annotation flow I will write a <a href="https://prodi.gy/docs/custom-recipes">custom recipe</a> which will save the data in exactly the format that I want. For now, it’s good enough.</p>

<p>Installing <a href="https://prodi.gy">Prodigy</a> into your development environment is a breeze now that you can do it with <code class="language-plaintext highlighter-rouge">pip</code>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>prodigy <span class="nt">-f</span> https://XXXX-XXXX-XXXX-XXXX@download.prodi.gy <span class="c"># where the XXXs are your license code</span>
</code></pre></div></div>

<p>Getting going with the image training was as easy as the following CLI command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>prodigy image.manual redaction-object-detection /path/to/image/data <span class="nt">--label</span> CONTENT,REDACTION <span class="nt">--remove-base64</span>
</code></pre></div></div>

<p>Note that the <code class="language-plaintext highlighter-rouge">--remove-base64</code> is to ensure that Prodigy doesn’t store the raw binary image data inside the database alongside the annotations. <a href="https://prodi.gy">Prodigy</a> (and their sister tool <a href="https://spacy.io">Spacy</a>) is a little more focused on textual data, where storing the original data alongside the annotation doesn’t pose too much of an issue, but for image files this probably is a bit of an anti-pattern and could lead to a very large database.</p>

<p>You get a local URL to go visit and you see an interface where you can make the necessary annotations:</p>

<p><img src="/images/2021-11-29-prodigy-object-detection-training/prodigy-object-interface.png" alt="" title="This is the basic interface for annotating object detection data in Prodigy"></p>

<p>You can see that I am distinguishing between two different classes: redactions and content. Redactions are what we’ve been talking about above. Content, however, is a bounding box for the content on a page. Remember that at the end of all of this we want a percentage of the page that has been redacted. Some images have reduced sized images, where the actual content which could have been redacted only takes up half of the A4 page. If that whole section was redacted, I’d want a final amount closer to 100% for that image rather than the 50% I’d get if I just went with the total percentage of redacted pixels on the whole image file.</p>

<p>Doing a few annotations, I ran into a couple of issues almost immediately. What do I do with a page like this:</p>

<p><img src="/images/2021-11-29-prodigy-object-detection-training/full-square-annotation.png" alt="" title="The whole page is redacted... or is it?"></p>

<p>The whole text of the page is annotated, but the text only extended half-way down the page. There was only 50% of the page that could have been redacted, but should the content boundary box encompass more of the page, or just the only full-section redaction?</p>

<p>And for the following image, what is the right way to think about how to make the annotation?</p>

<p><img src="/images/2021-11-29-prodigy-object-detection-training/polygon-annotation.png" alt="" title="Should this be a single annotation, or two overlapping or adjoining boundary boxes?"></p>

<p>This redaction encompasses multiple lines, so to some extent it doesn’t make a difference whether we have overlapping annotations or two adjoining boundary boxes. But for the purposes of training our model, will this contribute to a less accurate model? Should I be using polygon boundaries (which Prodigy can also use for annotations)?</p>

<div class="Toast">
   <span class="Toast-icon"><svg class="octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg></span>
   <span class="Toast-content">As an aside, this is why annotating your own data is so valuable. You get to see the limits of the annotations, and you get to really own the decisions that are being made. It is a bit early for me to know which approach is the best solution to these two problems, but being aware of them is important.</span>
</div>

<p>Once we’re done with our annotations, we can easily export our data to a <code class="language-plaintext highlighter-rouge">jsonl</code> file with the following CLI command:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>prodigy db-out redaction-object-detection <span class="o">&gt;</span> ./redaction-object-detection-annotations.jsonl
</code></pre></div></div>

<p>This gives us a file containing all our annotations. A sample for one image gives the idea:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"image"</span><span class="p">:</span><span class="w"> </span><span class="s2">"sample/path/redacted/04-F-0269_Global_Screening_Guidance-03.jpg"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"text"</span><span class="p">:</span><span class="w"> </span><span class="s2">"04-F-0269_Global_Screening_Guidance-03"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"meta"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"file"</span><span class="p">:</span><span class="w"> </span><span class="s2">"04-F-0269_Global_Screening_Guidance-03.jpg"</span><span class="w"> </span><span class="p">},</span><span class="w">
  </span><span class="nl">"path"</span><span class="p">:</span><span class="w"> </span><span class="s2">"sample/path/redacted/04-F-0269_Global_Screening_Guidance-03.jpg"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"_is_binary"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span><span class="w">
  </span><span class="nl">"_input_hash"</span><span class="p">:</span><span class="w"> </span><span class="mi">1413334570</span><span class="p">,</span><span class="w">
  </span><span class="nl">"_task_hash"</span><span class="p">:</span><span class="w"> </span><span class="mi">1588323116</span><span class="p">,</span><span class="w">
  </span><span class="nl">"_view_id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"image_manual"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"width"</span><span class="p">:</span><span class="w"> </span><span class="mi">800</span><span class="p">,</span><span class="w">
  </span><span class="nl">"height"</span><span class="p">:</span><span class="w"> </span><span class="mi">1035</span><span class="p">,</span><span class="w">
  </span><span class="nl">"spans"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
    </span><span class="p">{</span><span class="w">
      </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"0ef6ccd0-4a79-471d-9aa1-9c903c83801e"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"CONTENT"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"color"</span><span class="p">:</span><span class="w"> </span><span class="s2">"yellow"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"x"</span><span class="p">:</span><span class="w"> </span><span class="mf">76.5</span><span class="p">,</span><span class="w">
      </span><span class="nl">"y"</span><span class="p">:</span><span class="w"> </span><span class="mf">112.5</span><span class="p">,</span><span class="w">
      </span><span class="nl">"height"</span><span class="p">:</span><span class="w"> </span><span class="mf">786.1</span><span class="p">,</span><span class="w">
      </span><span class="nl">"width"</span><span class="p">:</span><span class="w"> </span><span class="mf">587.6</span><span class="p">,</span><span class="w">
      </span><span class="nl">"center"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">370.3</span><span class="p">,</span><span class="w"> </span><span class="mf">505.55</span><span class="p">],</span><span class="w">
      </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rect"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"points"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="p">[</span><span class="mf">76.5</span><span class="p">,</span><span class="w"> </span><span class="mf">112.5</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mf">76.5</span><span class="p">,</span><span class="w"> </span><span class="mf">898.6</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mf">664.1</span><span class="p">,</span><span class="w"> </span><span class="mf">898.6</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mf">664.1</span><span class="p">,</span><span class="w"> </span><span class="mf">112.5</span><span class="p">]</span><span class="w">
      </span><span class="p">]</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="w">
      </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"cd05d521-8efb-416b-87df-4624f16ca7f3"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"REDACTION"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"color"</span><span class="p">:</span><span class="w"> </span><span class="s2">"cyan"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"x"</span><span class="p">:</span><span class="w"> </span><span class="mf">80.3</span><span class="p">,</span><span class="w">
      </span><span class="nl">"y"</span><span class="p">:</span><span class="w"> </span><span class="mf">786.2</span><span class="p">,</span><span class="w">
      </span><span class="nl">"height"</span><span class="p">:</span><span class="w"> </span><span class="mf">20.2</span><span class="p">,</span><span class="w">
      </span><span class="nl">"width"</span><span class="p">:</span><span class="w"> </span><span class="mf">428.4</span><span class="p">,</span><span class="w">
      </span><span class="nl">"center"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">294.5</span><span class="p">,</span><span class="w"> </span><span class="mf">796.3</span><span class="p">],</span><span class="w">
      </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rect"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"points"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="p">[</span><span class="mf">80.3</span><span class="p">,</span><span class="w"> </span><span class="mf">786.2</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mf">80.3</span><span class="p">,</span><span class="w"> </span><span class="mf">806.4</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mf">508.7</span><span class="p">,</span><span class="w"> </span><span class="mf">806.4</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mf">508.7</span><span class="p">,</span><span class="w"> </span><span class="mf">786.2</span><span class="p">]</span><span class="w">
      </span><span class="p">]</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="p">{</span><span class="w">
      </span><span class="nl">"id"</span><span class="p">:</span><span class="w"> </span><span class="s2">"3e268e33-4eba-457d-8d17-8271a79ee589"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"label"</span><span class="p">:</span><span class="w"> </span><span class="s2">"REDACTION"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"color"</span><span class="p">:</span><span class="w"> </span><span class="s2">"magenta"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"x"</span><span class="p">:</span><span class="w"> </span><span class="mf">108.1</span><span class="p">,</span><span class="w">
      </span><span class="nl">"y"</span><span class="p">:</span><span class="w"> </span><span class="mf">772.3</span><span class="p">,</span><span class="w">
      </span><span class="nl">"height"</span><span class="p">:</span><span class="w"> </span><span class="mf">15.1</span><span class="p">,</span><span class="w">
      </span><span class="nl">"width"</span><span class="p">:</span><span class="w"> </span><span class="mf">400.6</span><span class="p">,</span><span class="w">
      </span><span class="nl">"center"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mf">308.4</span><span class="p">,</span><span class="w"> </span><span class="mf">779.85</span><span class="p">],</span><span class="w">
      </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rect"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"points"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
        </span><span class="p">[</span><span class="mf">108.1</span><span class="p">,</span><span class="w"> </span><span class="mf">772.3</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mf">108.1</span><span class="p">,</span><span class="w"> </span><span class="mf">787.4</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mf">508.7</span><span class="p">,</span><span class="w"> </span><span class="mf">787.4</span><span class="p">],</span><span class="w">
        </span><span class="p">[</span><span class="mf">508.7</span><span class="p">,</span><span class="w"> </span><span class="mf">772.3</span><span class="p">]</span><span class="w">
      </span><span class="p">]</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">],</span><span class="w">
  </span><span class="nl">"answer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"accept"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"_timestamp"</span><span class="p">:</span><span class="w"> </span><span class="mi">1638214078</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>Everything we’re interested in is inside the <code class="language-plaintext highlighter-rouge">spans</code> attribute, and it actually contains both kinds of the annotation that I mentioned above.</p>

<p>As you can see, annotating images in this way is fairly painless, and it brings
you in closer contact with your raw data which is an added bonus.</p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="strickvl/ml-blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/redactionmodel/computervision/datalabelling/2021/11/29/prodigy-object-detection-training.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A place to share my technical learnings.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/strickvl" target="_blank" title="strickvl"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/strickvl" target="_blank" title="strickvl"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
