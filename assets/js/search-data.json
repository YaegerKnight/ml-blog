{
  
    
        "post0": {
            "title": "It's raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
            "content": ". This blog outlines my process (and a few false starts) for generating a series of synthetic images (and corresponding annotations) to supplement training data used in a machine learning project. This problem is one for which there aren’t many (any?) pre-existing data sets that I can repurpose so I’ve been trying to find ways to bootstrap and improve the performance of the model I’m training. . Before I dive into the details, I wanted to include a little context on the wider project and what I’m seeking to accomplish. It is a relatively common practice for documents released as part of FOIA requests to contain redactions. With so many documents being released — and perhaps in specific cases where legal teams are dealing with huge numbers of those redacted documents — it can be useful to identify which documents are redacted and/or to get a sense of just how much has been redacted. If you have 10 or 20 documents you can fairly easily get that overview, but if you have 10,000 or a million documents? That’s where my project comes in: I want to train a model to make it easy to detect redactions in a document and to generate statistics on what proportion of a document or documents have been redacted. . You can read more about the problem domain, about my initial forays into annotating a dataset for this problem, as well as view some examples of these redactions (and perhaps why they’re not as easy to identify as you might think). You can even try out a demo showing some of what the model can identify here. Note that this isn’t the latest version of the model so it’s not the absolute best performance. . What’s the deal with synthetic images? . It’s a truism that in computer vision projects you probably need or want a lot of data to get good results. For the Facebooks and Bytedances of the world this perhaps isn’t an issue: they have access to a ton of data, for better or for worse. But for me, I don’t have teams of data annotators or millions of users generating all this data. This is probably the norm for small- to medium-sized computer vision problems being solved out in the world, especially with more non-traditional entrants into the field who are just trying to do things with the skills instead of generating research and so on. . Instead of using huge amounts of data, we need to be smarter about how we work, levelling ourselves up with whatever tricks of the trade we can muster. The fastai course contains a great number of these best practices, perhaps unsurprisingly since it is in some way targeted at individuals seeking to solve their domain-specific problems. One of the key insights I took away from earlier parts of the fastai book was the benefits of using pre-trained models. With a wealth of these models available and accessible, you don’t need to start your work from scratch. Instead, fine-tune your model and benefit from the expertise and hard work of others. . You do need some data to get started with fine-tuning a pre-trained model, however. That’s why I took a bit of time to make some initial annotations. I currently have annotated 2097 images, labelling where I have found redactions on the images as well as a box to show which parts of the image contain text or content. That approach has done pretty well so far, with in the low to mid seventies in terms of a % COCO score. (This is a commonly-used metric to assess the performance for object detection problems.) I want to go further, though, which is where synthetic images come in. . The big bottleneck in the annotation process is, of course, me. Depending on how many redactions any particular image contains, it could take me 5-10 minutes for a single image’s worth of annotations. This does not scale. Part of the speedup for this process is to use self-training, but I’ll write about that separately. Another option that has is often used is to generate images which approximate (to a greater or lesser degree) the actual real images. The useful thing about generating the images yourself is that you know where you placed the redactions, so you have the annotations at the same time. . My overall goal here was to boost my model’s performance. I didn’t know how how well these synthetic images would contribute, or even if they’d contribute to any boost at all. I was also quite conscious of the fact that you could probably spend a year generating pixel-perfect synthetic redacted documents. I didn’t want to waste too much time doing that, so at various points I had to make decisions as to whether a particular stage was good enough. . Phase 1: Get a Baseline / Naive Trial . When I started this, I didn’t know how hard or easy it was going to be, so I set myself a low bar. I knew it was theoretically possible to create images with Python, but I’d never done it before so didn’t have a sense of the range of possibilities. . In situations like this, I find Jupyter notebooks really reveal their strengths. Experimentation is easy and the pace of iteration can be really high. A few minutes of searching around and it seemed like Pillow (aka ‘PIL’) was probably the best option to go with. I noted that you could edit, resize, copy and paste images. For my basic version of a synthetic image generator, that’s most of what I needed to do: . Take an image that we know contains no redactions. | Get a separate image file that is of a redaction box / squiggle or shape. | Randomly resize the redaction shape. | Paste the redaction shape at a random location on top of the base unredacted image. | . And voila! Finding unredacted images was easy since I had previously used fastai to build a model that could detect to ~95% accuracy whether an image contained a redaction or not. For the redactions, it took me about an hour with Pixelmator Pro and its ‘quick selection’ tool to extract 100 examples of various kinds of redaction that I knew were commonly found in the data set. You can see some of this variety in the illustration that follows, though note that each individual redaction snippet was its own separate image for the purposes of my synthetic generation. . . I found that it was pretty trivial to generate images of the kind I proposed above. The placement of the redactions didn’t always make sense, and sometimes the random resize that the redaction underwent meant that it was either far too small or far too large. I also hadn’t included any steps to capture the annotation in this prototype, but I knew it was possible so continued onwards. . Detour: Get Stuck Pretty Quickly, Experience bbox Sprawl . Buoyed by my success in the prototype stage, I immediately added a bunch of improvements and features to what I wanted to achieve. I knew I wanted to make sure that the redaction stayed within the boundaries of the original base image. I also wanted to ensure that it stayed within the boundaries of the content of the base image — i.e. redactions generally tend to be made on top of content which tends not to be right on the outer margins. . I rushed into things too fast without thinking the problem through and quite quickly got into deep waters as all the various pieces started to overlap. I was somehow still in notebook mode, passing various objects through various other custom libraries, not sure what I was passing where. In short: it was a mess. . One thing that tripped me up really fast was bboxes. (A bbox, in case this means nothing to you, is a data structure or type that allows you to represent where a box is positioned if you were to paste it on top of a base image (for example). It seems that there are different conventions about how to represent this concept of the location of a box on top of some other larger space. Some people represented it with pairs of coordinates, such that for each of the four corners of the box you’d have an [x, y] pair to represent each point. Others took this bbox type to contain references to the xmin, ymin, xmax, and ymax values of the box. In this way you could reconstruct the various corners since you had two opposite corners specified. Another option was that used by COCO, which was [xmin, ymin, width, height]. And yet another option was to represent a bounding box by [x_center, y_center, width, height]. (This is a useful article that details some of these representation differences.) . I’m sure there are people who are really good at keeping multiple types of x and y coordinates, each with slightly different nuances, in their heads. I am not such a person and after an hour or two of struggling in these deep waters I realised I needed to regroup. . My notebook experiments had been good for uncovering the range of possibility, but now that I had a better sense of the edges of the problem — and the twists and turns of dealing with bounding boxes — I had to take a more systematic approach. I spent some time with pen and paper thinking through the flow that this synthetic generation process would have to include. I thought through what the various independent parts of this could be, and how data would flow through this set of steps. . Phase 2: Generate My Own Base Images . The first part of this process was to generate my own base images. In general, the types of base unredacted images in the core data set were relatively unremarkable. These were mostly letters, reports or some kind of form / table. I figured I could approximate this pretty quickly. By chance, that very weekend I happened to listen to an episode of the Real Python podcast which interviewed the creator of borb, a Python package for creating and manipulating PDFs. I knew I wanted images in the end, but I had already created a tool to extract images from PDFs and I figured borb would probably save me time, even if it meant I had to do some converting back and forth between images and PDF files. . The great thing about borb is that it offers an easy abstraction with which to reason about creating PDF documents. Have some text and want it to be displayed on a page? Done. Want that text to be displayed in three columns? Done. Want do insert some images and have the text flow round it? Done. Have styling requirements? Done. And on and on. I figured that this was just the level of abstraction I needed — rather than staying in the world of pixel primitives like lines and boxes. . Once I got going it was easy to generate base images with multi-column text and some random coloured shapes thrown in here and there. (I used lorem-text to generate random Latin texts.) After I created the PDF I then had to convert it into an image format for use elsewhere in the generator pipeline but I think that speed hit was a price worth paying. . Phase 3: Generate My Own Redactions . The redactions weren’t quite as easy as the base images. The easiest version of a redaction box was literally that: a black box that sits on top of the base image. That much was easy to create. Pillow had some useful interfaces that I could use to quickly create randomly sized boxes. I could even add text to them in the upper left corner as I’d noticed that many of the real redactions did that. . It was less clear to me how I’d go about generating the other kinds of redactions, particularly ones that resembled a handwritten mark in thick black marker over the top of a document. In the end, I decided not to go any further with anything that wasn’t a box, but I did make the redaction boxes more varied. I set it such that the box would be filled with a random colour. If the colour was dark enough, I made sure that the text was in a light (contrasting) colour. And ensure that there wasn’t always a text on the box. . Not perfect, but still it gave me a way to move forward. . The Big Picture: Bringing It All Together . With these pieces complete, I had the basics of the next version of my synthetic image generation. You can see the flow and progression of my script in the following diagram: . . You’ll note that there were a number of other steps that supported the image creation. I did again descend into bbox hell when calculating exactly where to paste the redaction image, but with a much more modularised approach to my code I didn’t get lost. Type hints also kept me honest about what variables I was passing in and out of the functions I’d created. . I ended up using the initial model I’d trained so far in the step that figured out where the content of the image was. You’ll recall that this was one of the annotations I’d already been generating when I annotated my data, and since it’s a fairly simple computer vision task I was already seeing excellent performance from that specific class in object detection. IceVision, a library that I’m using for the computer vision and deep learning parts of this project, allowed me to fairly easily make this inference on the images and extract the bbox coordinates for the content box. . I made sure to include a lot of random variation in the first two steps where the base and redaction images were created. I didn’t remove the original naive approach completely. Instead, I made it 50% likely that we’d generate an image versus just picking one of the unredacted images from our store. Then I gave the same chance for the redaction as to whether we’d use an actual redaction snippet or one of the computer-generated boxes. There was lots of resizing and colouring and various other randomisation that was also included. . Phase 5: Make The Images Look Old and Worn . Only one step remained. I realised that when I generated the images completely from scratch, not using any of the real base images or redaction snippets, that they looked very new and unrealistic. A significant proportion of the documents in the collection looked like they’d been photocopied a thousand times and in general had seen better days. Sometimes the quality was such to make them unreadable. I realised if I was going to get good results with the overall goal (i.e. improve my model’s performance) I’d have to make the synthetic creations look old somehow. . After some exploration I settled on augraphy as how I’d process the newly generated images to look old and worn. Luckily for me, this package seems to have been created explicitly to support machine learning workflows for synthetic data creation, and it seemed to be (somewhat) actively maintained. There was a default set of so-called ‘augmentations’ that Augraphy suggested I apply to my image. Unfortunately it was simply too aggressive. I guess for some workflows it would have been great, but the page ended up looking somewhat unintelligible by the end. Compare these two examples: . . Not only did the default Augraphy transforms often make the redaction indistinguishable, it shifted parts of the image around on the page for these crinkle and scrunch effects, which would have rendered my annotations inaccurate. . That said, as you can see from the left image, it was pretty easy to switch out the default for a set of random transforms to be applied that wasn’t quite so aggressive. I’m thankful that tools like this exist out in the open-source space and that allow me to get on with the work of solving the actual problem I’m interested in working on. . Final Results: 2097 Synthetic Images . . This gif gives you a brief sense of some of the images I generated as a result of the process I’ve detailed above. They’re not perfect, and as I write I currently don’t know how well they will perform when training my model. . I have 2097 real annotated images, so I’m going to combine them with a maximum of an equal number of synthetic images. I’ll try out different proportions of real to synthetic, but that’s also a topic for another blogpost to follow. Stay tuned! . It took about three and a half hours to create these 2000+ images on my laptop. There are LOTS of places where I could have made speed improvements, notably all the conversion between PDF and image objects, the inference for the content box and also the fact that the pipeline wasn’t performed in parallel on all my CPU cores. I spent about 30 minutes exploring Ray as a means to getting this process to be executed in parallel but it ended up being not as simple as I’d initially thought so I’ve left that to one side for now. In any case, I won’t be creating so many synthetic images at once so often, so it wasn’t a real blocking point for my work. . Note, too, that the annotations get created as part of the same script. I append them to a synthetic annotations file at the same time as the synthetic images is generated, and the file is subject to being combined with the real annotations at a later stage. . There are obviously lots of ways this synthetic data creation process could be optimised, but I was recently reminded that it’s also important not to lose momentum and not to let the perfect be the enemy of the good. . The next step is to carry out an experiment to see the effect of adding in the synthetic annotations on model performance. There are a bunch of really tricky aspects to this (most notably finding ways to make sure not to allow my training data to leak into the validation data) but I’ll save all that for my next blogpost. . (If you got all the way to the end, well done!) .",
            "url": "https://mlops.systems/redactionmodel/computervision/python/tools/2022/02/10/synthetic-image-data.html",
            "relUrl": "/redactionmodel/computervision/python/tools/2022/02/10/synthetic-image-data.html",
            "date": " • Feb 10, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "What are invariants and how can they help make your Python classes more robust?",
            "content": "We’ve read about enumerations and we’ve read about data classes. Now it’s the turn of classes. Chapter 10 of Patrick Viafore’s excellent book, ‘Robust Python’, is the last of the user-defined types to be covered. Early on he makes a good point that classes are often taught really early to those new to Python and/or programming, and that maybe the story is a bit more complicated. As I’ve mentioned before, things like enums and data classes are more or less unmentioned in such educational materials and as such I found this book really helped me fill in the conceptual gaps. . First off, for someone who has just learned about data classes, how would you explain what is new or distinct when it comes to classes? They’re slightly different syntactically, with classes requiring you to write a bit more boilerplate. Compare the following: . from dataclasses import dataclass import datetime from typing import Literal # data class definition @dataclass class Cat: name: str breed: CatBreed birth_date: datetime.date gender: Literal[&#39;male&#39;, &#39;female&#39;] # class definition class Dog: def __init__(self, name: str, breed: CatBreed, birth_date: datetime.date, gender: Literal[&#39;male&#39;, &#39;female&#39;]): self.name = name self.breed = breed self.birth_date = birth_date self.gender = gender . You can note how it seems like the data class version is much more readable and involves less boilerplate to achieve the same effect, and for a simple example like this you’re probably right. The difference, and where classes make sense and shine, is when you have a conceptual grouping or type that includes some notion of invariants. . What is an invariant? . Most of this chapter is about invariants and how they relate to classes, and I’ll admit I had never heard of the concept before reading in this book. An invariant is defined as “a property of an entity that remains unchanged throughout the lifetime of that entity.” You can think of it as some kind of context or a property about that particular type that you need to encode and that won’t change. . The book gives a pizza example (where a Pizza object could encode that in its list of toppings, the cheese could only be the final topping (i.e. on top) of the pizza). An alternative might be some kind of rule relating to an ID number, where either it must be unique to some kind of specification, or where the ID must conform to some kind of specification. . Even with this rudimentary definition, you can see how there might be some advantages to being able to account for these rules and properties of the object type. (With data classes, you don’t have as much flexibility to specify all these nuances.) So what happens when you’re instantiating a class and you hit one of those scenarios where your contextual rules dictate that something can’t happen? (i.e. someone tries to create a Pizza object that has cheese as the bottom-layer topping) The book offers up two options: . Throw an exception — this will break you out of the code flow and prevent the object from being constructed | Do something to make the data fit — you can perform some kind of transformation which sees the cheese ingredient as being forced onto the top layer of the pizza toppings (or whatever is the equivalent for your specific scenario) | Note that the kinds of restrictions posed by these invariants are things that can’t fully be captured by the typing system. We’ve covered type hints and how they can help make your code more robust, but types don’t help much when it comes to the order of a list, for example. . Why code around invariants? . So why go to all of this trouble in the first place? How does it benefit to code with the invariants in mind? To start with, it’ll probably help you think through edge cases and exceptions that you could do well to be wary of. The invariants alert you to the fact that arguments passed into functions and methods will not always be in the form that you would ideally like. (As a side note, this might also encourage you to add unit tests.) . It will help you keep the code that handles the invariants together instead of mixing it in with the code that instantiates the objects. In general, it will enhance your ability to reason about the code and the concepts that your code reflects. This is important not only for the implementation in code, but for how you think about any particular part and how it relates to the rest of your code base. . The goal for all of this: fewer bugs and a more robust system. Yes, it takes a bit more effort to think whether there are implicit or explicit invariants, but doing so makes your code and your system more reliable. In Viafore’s words: . “You’re making an easier API for people to think about, and you reduce the risk of people using your objects incorrectly. […] You never want someone to be surprised when using your code.” (p. 141) . Invariants and class consumers . The rest of the chapter is about the implementation consequences of thinking about classes in this invariants-first way. For consumers of the class, how should you ensure that the invariants handled are clear? Aside from the implementation itself (in the constructor), docstrings and code comments are suggested as a means to this end. Of course, README files and documentation in general can serve the same purpose, but it’s best if the context and information about invariants is as close to the code as possible. . Invariants and class maintainers . For (future) maintainers of the class, unit tests are the way to go. Make sure that the relevant scenarios and invariants are covered by testing code and you will have extra confidence that your object instantiation really does do what you intend. Your code should already be doing the checking for invariants on the instantiation side, but unit tests are a way of ensuring that this is actually the case (and also that these invariants remain covered as the code base continues to evolve. . (The book offers one way of doing such tests for invariants with contextlib.contextmanager on page 145.) . Encapsulation and classes . As the final chunk of the chapter, we learn about private, protected and public access to the properties and methods of a class, and how they relate to the maintenance of invariants. . This is an important part of the story. As users interface with your class and API, encapsulation is a way to ensure that they update and interact with the these properties in a way that is under your control. For example, even if at instantiation you enforce the Pizza object having cheese as the top-layer topping, what do we have in place to ensure that the user doesn’t just amend the toppings property such that the cheese is the bottom-layer topping (i.e. AFTER instantiation)? Encapsulation — having an entity hide or restrict access to certain properties and actions — is how you handle that. . The book goes into a fair amount of detail on the uses of these different levels of access, and introduces the idea of ‘accessors’ and ‘mutators’ as an alternative to the more commonly-used ‘getters’ and ‘setters’. . Remember, “you use invariants to allow users to reason about your objects and reduce cognitive load.” (p. 151) . So what am I supposed to use? . . The end of the chapter offers this really helpful flowchart diagram which summarises the choices that we’ve covered during the previous three chapters. I really want to highlight that this chapter helped me think about classes in a way I hadn’t, despite having been through courses, having read numerous articles and of course coded in this class-oriented fashion for several years. . The next few chapters continue onwards by thinking about how to design your interfaces such that they make sense for your users and allow your code base to grow with as few headaches as possible. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/02/08/robust-python-10.html",
            "relUrl": "/robustpython/python/books-i-read/2022/02/08/robust-python-10.html",
            "date": " • Feb 8, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Upgrade your Python dicts with data classes",
            "content": "I’ve been curious about data classes since more or less my first day at work when someone mentioned to me that Pydantic was built on the shoulders of data classes. I hadn’t taken the opportunity to dive into all the details of what data classes do until now, prompted by their being part of Patrick Viafore’s book, ‘Robust Python’, specifically chapter nine. . An example upfront might help ground the conversation. Here is a data class in action: . import datetime from dataclasses import dataclass from typing import Literal @dataclass class CatPassport: name: str breed: CatBreed issue_date: datetime.date expiry_date: datetime.date gender: Literal[&#39;male&#39;, &#39;female&#39;] aria = CatPassport(&quot;Aria&quot;, CatBreed(&#39;bengal&#39;), datetime.date(2022, 01, 05), datetime.date(2025, 01, 04), &#39;female&#39;) print(aria.name) # prints &#39;Aria&#39; . From this you can see that it’s an easy way to represent structured data made up of different types. Where it excels over simply using a dict or a class you write yourself is the fact that it auto-generates a number of __ dunder helper methods. You get __str__ and __repr__ to handle what this object looks like when you try to print() it. It also creates an __eq__ method which allows you to check for equality between two objects of the same type with the == comparison operator. . (If you want to add a way to compare between your data class objects, you can add arguments to the @dataclass decorator like @dataclass(eq=True, order=True) which will handle the creation of the relevant dunder methods. . The fact that data classes are just classes at heart mean that you can also add behaviours to these collections of values, something that isn’t possible with a plain dict. . You can specify that your data class should be frozen (@dataclass(frozen=True)) which effectively makes it an immutable data store, though taking note that objects stored as values on the data class’ properties might themselves not be immutable (think lists and dicts). . After reading the chapter in ‘Robust Python’, I read around a little to get a sense of this concept. I read the official docs which were fairly helpful, but in fact it was the PEP document (557) that I found most interesting. I haven’t previously taken the time to dive into the specifics of PEP specifications before, but I discovered that they are pretty readable and you get a real sense of what problem a particular feature or addition to the language was trying to solve. . PEP 557 explains some of the alternatives and why it might be useful to include this new feature. I also learned about the attrs package and how data classes are actually just a subset of what attrs offers. (As a side note, I was surprised that attrs seems to have been mentioned nowhere in ‘Robust Python’, even in the context of the upcoming Pydantic chapter. Perhaps it was just too confusing to have all these things alongside one another.) . Other options to consider alongside data classes when dealing with heterogenous data inside a single object or structure include TypedDict and namedtuple, but it seems like the default for this kind of scenario should probably just be a data class, though I should add that it is only part of the standard library for Python 3.7 and above. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/02/05/robust-python-9.html",
            "relUrl": "/robustpython/python/books-i-read/2022/02/05/robust-python-9.html",
            "date": " • Feb 5, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "How and where to use enums in Python",
            "content": "The second part of Viafore’s ‘Robust Python’ is all about user-created types. We start simple in chapter eight and consider the Enum type as a way of defining a particular restricted set of values. An example might help get us started: . from enum import Enum class TrafficLightsState(Enum): RED = &quot;red&quot; YELLOW = &quot;yellow&quot; GREEN = &quot;green&quot; OFF = &quot;off&quot; current_state = TrafficLightsState.GREEN print(current_state.value) # prints &#39;green&#39; . We subclass off Enum and define the pairings of values that belong together. I hope you can see already that this is a readable way to define these values and show that they are part of the same semantic grouping. . If we’re using these definitions not because we care about the values themselves but because we want to be able to evaluate whether the state of one particular traffic light is the same as a different traffic light, we can use auto to automatically assign values (ascending integers, by default) in the following way: . from enum import Enum, auto class TrafficLightsState(Enum): RED = auto() YELLOW = auto() GREEN = auto() OFF = auto() current_state = TrafficLightsState.GREEN print(current_state.value) # prints 3 . You can iterate through your enums or get their length just as if it was a list, too. . While writing the above text, I realised that I was getting confused about the difference between types and classes in Python. It turns out that whatever differences once existed, they aren’t much of a thing any more and to all intents and purposes they’re practically the same thing. . A lot of the enum-related definitions at work are defined in this file. You can see that we tend not to use auto, though I’m not really sure why. (We don’t ever seem to compare against actual values.) . If you want to make sure that the actual values assigned to these grouped constants are unique, you can add the @unique decorator which will enforce that you aren’t duplicating values. . Better still for the readability of your code, you can use this collective type in your type annotations. For sure the difference between these two options should be clear: . def get_status(some_input: str) -&gt; str: # code goes here def get_status(some_input: str) -&gt; TrafficLightsState: # code goes here . In the first case, it is far less clear what’s going on. . Note that if you’re purely looking for a way to restrict the assignation to a particular variable, you can also use the Literal type, introduced in Python 3.8, though remember that it doesn’t help with iteration, runtime checking or map values from name to value. For all that, you’ll want to be using Enum.” . If you want a way to combine Enums together, you can subclass from enum.Flag. Consider the case of when you have a list of enums for days of the week, but you want to represent the weekend as a pairing of Saturday and Sunday (if you were in Europe, e.g.). You could do the following: . from enum import Flag, auto class Weekday(Flag): MONDAY = auto() TUESDAY = auto() WEDNESDAY = auto() THURSDAY = auto() FRIDAY = auto() SATURDAY = auto() SUNDAY = auto() weekend = Weekday.SATURDAY | Weekday.SUNDAY . You can perform bitwise operations on these combined groupings, but note that the values must support bitwise operations. (Strings don’t support them, while integers do.) . Finally, the chapter covers the special case of IntEnum and IntFlag which allows for the conversion of integer values. This can be confusing and lead to non-robust behaviours, so the book discourages this particular usage. . Next up is Data Classes, something I’m extremely interested in getting to grips with as it comes up in our codebase at work a decent amount. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/01/30/robust-python-8.html",
            "relUrl": "/robustpython/python/books-i-read/2022/01/30/robust-python-8.html",
            "date": " • Jan 30, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Using mypy for Python type checking",
            "content": "The final two chapters of part one of Patrick Viafore’s ‘Robust Python’ cover more practical advice on how to actually use and implement type checking in either a new project or a legacy codebase. . mypy is the most commonly used option for type checking in Python and it does most of what you probably need it for. You can run it via the command line, inline as part of your IDE, or as part of a CI/CD pipeline. At work we do all three. . You can configure mypy to your heart’s desire either with inline comments in your code, or via a configuration file. A configuration file is probably the way to go, particularly if you’re versioning your code and sharing these kinds of settings across a team. . Chapter 6 goes into detail about some of the specific options or settings you can tweak to make mypy more or less sensitive to certain kinds of errors. For example, in a previous post we mentioned how you can implicitly accept None as a type with the Optional type annotation wrapper. But maybe you don’t want to allow this behaviour because it’s generally not a great idea: if so, you can use the —strict-optional flag to get notified whenever you’re using that particular construction. . mypy also allows for the export of its results to html and xml, and you can run it in the background as a daemon which (particularly for large code bases) might speed it up. . We also learn about some alternatives to mypy, namely Pyre and Pyright. Pyre runs as a daemon in the background and allows you to run queries relating to type usage in your codebase. It also includes a static code analyser called Pysa that runs a kind of security analysis on your code called ‘taint analysis’. A quick summary of this would be to say that you can specify specific kinds of security flaws that you want to address and/or prevent being part of your codebase. . Pyright is interesting since it has a useful VS Code integration (via the Pylance extension). You get all sorts of autocompletion and tooltip goodness by using Pyright/Pylance. . Finally, chapter 7 thinks through how you might want to approach actually using type checking and type hints in a larger codebase, perhaps one that already exists. It’s useful this was included as I imagine these sorts of practicalities are much more of a blocker to adoption than any technical issues. After a brief discussion of tradeoffs, we learn about some different options for where you might want to start with introducing types to a legacy codebase. . Focusing on the pain points — i.e. where the lack of type hints has already seen bugs emerge in the past | or perhaps adding them to new code only | or perhaps type annotating the pieces of the codebase that actually drive the product or business’ profits | or maybe whatever is complex to understand | . All of these are options and it will definitely depend on your particular situation. . We also learn about two tools that might help get you started with type annotation: MonkeyType and Pytype. Both auto-generate type hints for your codebase. MonkeyType does so dynamically, so it only generates type hints for parts of your code that it accesses while running the code. Pytype does so by static analysis. Both deliver some kind of output that you can then use (perhaps) as the basis of some annotations of your codebase. My instinct is that these two tools feel like they might lead to some faulty assumptions or errors if you rely on them too much and that in fact it would be better to just methodically go through your code and incrementally add type hints as suggested above. . This concludes the type hints part of the book. I feel like I really got a solid overview of why type hints are used in large or complex Python codebases as well as how to implement this practically. I will be writing separately about how we use mypy and type hinting at ZenML as I think it offers an interesting case study on some of the benefits and tradeoffs that we’ve observed on a day-to-day basis. . Next up in Robust Python: defining your own types with Enums, data classes, classes and how this fits into libraries like Pydantic. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/01/22/robust-python-6.html",
            "relUrl": "/robustpython/python/books-i-read/2022/01/22/robust-python-6.html",
            "date": " • Jan 22, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Using type annotation with collections in Python",
            "content": "The fifth chapter of ‘Robust Python’ continues on from where we left off last time. We saw how to apply type annotations when simple things like strings, integers and floats were involved. This chapter deals with the different ways you annotate your types when collections get involved. . We start with the context for why this is even something that requires a separate chapter to deal with. This involves the difference between homogenous and heterogeneous types. For a Python list, we could say it had homogenous types if all the items were of the same type (strings, e.g.). If this list contains multiple different types (a mix of strings and integers, e.g.) then we’d have to say it contained heterogenous types. This is of importance given that the presence of multiple types in a single list is going to require you to handle the types differently. Even in the most trivial of examples (as with strings and integers being together), the interfaces for both are different. Try adding a string to an integer in Python and see what happens. . So it’s actually not quite true to say that a collection of homogenous types have to all be exactly the same type, but they must share common interfaces and ideally be handled using the same logic. If you think about it, in the real world heterogenous types are pretty common occurrences. There are often situations where, for example, you have to handle the output of API calls or data that doesn’t derive from code that’s in yous control and then you’ll perhaps be dealing with a dictionary that contains all sorts of types. . In Python we do have the typing.Any annotation, but it’s pretty clear — and the book emphasises this — that isn’t really useful in the vast majority of cases. You might as well not bother with type annotations if you’re going to liberally be using Any. . The first of our collection type helpers: TypedDict . TypedDict was introduced in Python 3.8 and allows you to communicate intent when it comes to the types that are being passed through your code. Note that, as with a lot of what we’re talking about here, this is all information that’s useful for a type checker and isn’t something that is dynamically checked. . You can use TypedDict to define structures that specify the types of fields of your dictionary in a way that is easier to parse as a human reader than just using dict. See this example, adapted from one in the book: . from typing import TypedDict class Range(TypedDict): min: float max: float class Stats(TypedDict): value: int unit: str confidenceRange: Range our_stats = Stats(value=3, unit=&quot;some_name&quot;, confidenceRange=Range(min=1.3, max=5.5)) print(our_stats) # returns {&#39;value&#39;: 3, &#39;unit&#39;: &#39;some_name&#39;, &#39;confidenceRange&#39;: {&#39;min&#39;: 1.3, &#39;max&#39;: 5.5}} . If TypedDict doesn’t do everything you need it to, we have some other options. . Custom Collections with TypeVar . TypeVar in Python is how you can implement generics. Generics, as I learned while reading, are ways of representing things that are the same, like when you don’t care what specific type is being used. Take this example from the book, where you want to reverse items in a list, but only if the items are all of the same type. You could write the following: . from typing import TypeVar T = TypeVar(&#39;T&#39;) def reverse(coll: list[T]) -&gt; list[T]: return coll[::-1] . You can use generics in other ways to create new kinds of collections or groupings. For example, again this one is adapted from the book, if you were writing a series of methods that returned either something useful or a particular error message: . def get_weather_data(location: str) -&gt; Union[WeatherData, APIError]: # … def get_financial_data(transaction: str) -&gt; Union[FinancialData, APIError]: # … . …and so on, you could use generics as a way of simplifying how this gets presented: . T = TypeVar(&#39;T&#39;) APIResponse = Union[T, APIError] def get_weather_data(location: str) -&gt; APIResponse[WeatherData]: # … def get_financial_data(transaction: str) -&gt; APIResponse[FinancialData]: # … . That looks and feels so much cleaner! . Tweaking existing functionality with collections . If you’re just making slight changes to the behaviour of collections, instead of subclassing dictionaries or lists or whatever, it’s better to override the methods of collections.UserDict, collections.UserString and/or collections.UserList. . You’ll run into fewer problems when you actually implement this. Of course, there is a slight performance cost to importing these collections, so it’s worth making sure this cost isn’t too high. . You’ll maybe have noticed that there isn’t a collections.UserSet in the list above. For sets we’ll have to use abstract base classes which are found in collections.abc. The big difference between the User* pattern of classes, there is no built-in storage for the abc classes. You have to provide your own storage if you need it. So for sets, we’d use collections.abc.Set and then implement whatever group of methods are required for that particular class. . In the set example, we have to implement __contains__, __iter__ and __len__, and then the other set operations will automatically work. There are currently (as of Python 3.10.2) 25 different ABCs available to use. I definitely will be exploring those as they seem really useful. . Even though this chapter got into the weeds of collections a little, I learned a lot and I’m already finding places in the ZenML codebase where all of this is being used. . Typeguard . Before I leave, since we’re still thinking about types, I wanted to share this little package I discovered the other day: typeguard. You can use it in a bunch of different ways, but a useful short video from calmcode.io showed how a simple decorator can simplify code and catch type errors. . Consider the following example code: . def calculate_risk(risk_factor: float) -&gt; str: &quot;&quot;&quot;Calculates how much risk you took&quot;&quot;&quot; return risk_factor * 3 # arbitrary return value :) . What if someone passes in a wrong type into this function? It’ll fail. So maybe we want to handle that particular situation: . def calculate_risk(risk_factor: float) -&gt; str: &quot;&quot;&quot;Calculates how much risk you took&quot;&quot;&quot; if not isinstance(risk_factor, float): raise ValueError(&quot;Wrong type for risk_factor&quot;) return risk_factor * 3 . If you have lots of parameters in your function and you have to handle them all, this could get messy quite quickly. Instead, we can pip install typeguard and do the following: . from type guard import typechecked @typechecked def calculate_risk(risk_factor: float) -&gt; str: &quot;&quot;&quot;Calculates how much risk you took&quot;&quot;&quot; return risk_factor * 3 . Now that’s a handy little decorator! It’ll handle all the raising of appropriate errors above based on whether you passed in the right type or not. It works for classes as well. You’re welcome, and thanks Vincent for making the introductory video! .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/01/18/robust-python-5.html",
            "relUrl": "/robustpython/python/books-i-read/2022/01/18/robust-python-5.html",
            "date": " • Jan 18, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "A Midway Report on my Computer Vision Project",
            "content": "(This post is adapted from a twitter thread, so is a bit more terse than usual.) . I recently switched what I spend the majority of my professional life doing (history -&gt; software engineering). I’m currently working as an ML Engineer at ZenML and really enjoying this new world of MLOps, filled as it is with challenges and opportunities. . I wanted to get some context for the wider work of a data scientist to help me appreciate the problem we are trying to address at ZenML, so looked around for a juicy machine learning problem to work on as a longer project. . I was also encouraged by Jeremy Howard’s advice to “build one project and make it great”. This approach seems like it has really paid off for those who’ve studied the fastai course and I wanted to really go deep on something myself. . Following some previous success working with other mentors from SharpestMinds on a previous project, I settled on Computer Vision and was lucky to find Farid AKA @ai_fast_track to mentor me through the work. . In the last 6 weeks, I’ve made what feels like good progress on the problem. This image offers an overview of the pieces I’ve been working on, to the point where the ‘solution’ to my original problem feels on the verge of being practically within reach. . . After just a few lessons of the FastAI course, I trained a classification model to ~95% accuracy to help me sort redacted images from unredacted images. . I used Explosion’s Prodigy to annotate an initial round of data to pass into the next step, enjoying how the labelling process brought me into greater contact with the dataset along the way. . I switched to using IceVision to help me with the more complicated object detection problem, using MMDetection and VFNet to get pretty good results early on. . I’m currently in the process of creating my own synthetic images to boost the annotations I’ve manually made. (I’ll be writing about this process soon as well, as I’m learning a lot about why this is so important for these kinds of computer vision problems.) . I’ve also been amazed at the effectiveness of self-training (i.e. using my initial model in my annotation loop to generate an initial set of annotations which I can easily amend as appropriate, then feeding those annotations in to create a better model and so on). More to follow on that step, too. . I started using Evidently to do some drift detection, inspired by some work I was doing for ZenML on adding Evidently as an integration to our own tool. This helped me think about how new data was affecting the model and the training cycle. I feel like there’s a lot of depth here to understand, and am looking forward to diving in. . I made a tiny little demo on HuggingFace Spaces to show off the current inference capabilities and to see the model in a setting that feels close to reality. This is a simple little Gradio app but I liked how easy this was to put together (a couple of hours, mainly involving some build issues and a dodgy requirements.txt file) . Along the way, I found it sometimes quite painful or fiddly to handle the PDF files that are the main data source for the project, so I built my own Python package to handle the hard work. I used fastai’s nbdev to very quickly get the starters of what I’m hoping might be a useful tool for others using PDF data for ML projects. . Throughout all this, Farid has been patiently helping guide me forward. He saved me from going down some dark rabbit holes, from spending too long studying skills and parts of the problem that needed relatively little mastery in order to get to where I am. . Farid has been a consistently enthusiastic and kind advocate for my work, moreover, and this has really helped me stay the course for this project that takes a decent chunk of my time (especially seeing as I do it completely aside / separately from my day job). . I feel like I’m consistently making progress and learning the skills of a data scientist working in computer vision, even though I have so much left to learn! My project still has a ways to go before it’s ‘done’, but I’m confident that I’ll get there with Farid’s support. (Thank you!) .",
            "url": "https://mlops.systems/python/fastai/tools/redactionmodel/2022/01/16/midway-report-redaction-project.html",
            "relUrl": "/python/fastai/tools/redactionmodel/2022/01/16/midway-report-redaction-project.html",
            "date": " • Jan 16, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Different ways to constrain types in Python",
            "content": "The fourth chapter of ‘Robust Python’ continues on from where we left off last time. We had previously learned about the benefits of type annotations in general terms, as well as started to understand how we might apply these annotations to simple code examples. But what if things are a bit more complicated? Then we have a few more options at our disposal. . Note that you can assign all of these type assignments to variables (‘type aliases’), which might just make your code that much more readable. . Optional to catch None references . Optional as a type annotation is where you want to allow a specific type or None to be passed in to a particular function: . from typing import Optional def some_function(value: Optional[int]) -&gt; int: # your code goes here . Note that you’ll probably want (and mypy will remind you if you forget) to handle what happens in both those cases inside your function. (You may need to specifically pass in the —strict-optional flag to catch this when using mypy.) . Union to group types together . This is used when multiple different types can be used for the same variable: . from typing import Union def returns_the_input(input: Union[str, int]) -&gt; Union[str, int]: return input . This function doesn’t really do anything, but you get the idea. Note, too, that Optional[int] is really a version of Union[int, None]. (The book gets into exactly why we might care about reducing the number of possible options by way of a little detour into set theory.) . Literal to include only specific values . A little like what I believe enumerations do, we also have the Literal type. It restricts you to whatever specific values are defined: . from typing import Literal def some_function(input: Literal[1, 2, 3]) -&gt; int: return input . Here the function is restricted to inputs that are either 1, 2 or 3. Note that these are a feature that applies to Python 3.8 and above. . Annotated for more complicated restrictions . These are available, but not really useful since they only function as a communication method. You can specify specific restrictions such as the following (example is taken from the book, p. 56: . from typing import Annotated x: Annotated[int, ValueRange(3,5)] y: Annotated[str, MatchesRegex(&#39;[abc]{2}&#39;) . Read more about it here. The book doesn’t spend much time on it and it seems like it’s probably best left alone for the moment. . NewType to cover different contexts applied to the same type . NewType, on the other hand, is quite useful. You can create new types which are identical to some other type, and those new values made with the new type will have access to all the methods and properties as the original type. . from typing import NewType class Book: # you implement the class here NewBook = NewType(&quot;NewBook&quot;, Book) def process_new_book(book: NewBook): # here you handle what happens to the new book . You can achieve something like the same thing with classes and inheritance, I believe, but this is a lightweight version which might be useful to achieve the same end goal. . Final to prevent reassignment / rebinding . You can specify that a particular variable should have a single value and that value only. (Note that mutations of an object etc are all still possible, but reassignment to a new memory address is not possible. . from typing import Final NAME: Final = &quot;Alex&quot; . If you tried to subsequently change this to a different name, mypy would catch that you’d tried to do this. This can be valuable across very large codebases, where the potential for someone to reassign a variable might be not insignificant. . So there you have it: a bunch of different ways to handle combinations of types and/or more complicated annotation scenarios. The next chapter will cover what happens when we throw collections into the mix, and what type annotation challenges are raised. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/01/08/robust-python-4.html",
            "relUrl": "/robustpython/python/books-i-read/2022/01/08/robust-python-4.html",
            "date": " • Jan 8, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Learning about 'nbdev' while building a Python package for PDF machine learning datasets",
            "content": "While working to develop a computer vision model that detects redactions in documents obtained as a result of FOIA requests, I have encountered some tasks that I end up repeating over and over again. Most of the raw data in the problem domain exists in the form of PDFs. These PDF files contain scanned images of various government documents. I use these images as the training data for my model. . The things I have to do as part of the data acquisition and transformation process include the following: . downloading all the PDF files linked to from a particular website, or series of web pages | converting and splitting all the downloaded PDF files into appropriately sized individual image files suitable for use in a computer vision model | generating statistics on the data being downloaded and processed, as well as (further down the line) things like detecting data drift for incoming training data | splitting up data as appropriate for train / validation / test data sets | extracting text data from the images via an OCR process | versioning, syncing and uploading those images to an S3 bucket or some other cloud equivalent for use in the overall workflow | . It’s not hard to see that many of these things likely apply to multiple machine learning data acquisition scenarios. While writing the code to handle these elements in my specific use case, I realised it might be worth gathering this functionality together in an agnostic tool that can handle some of these scenarios. . I had wanted to try out nbdev ever since it was announced back in 2019. The concept was different to what I was used, but there were lots of benefits to be had. I chose this small project to give it an initial trial run. I didn’t implement all of the above features. The two notable missing parts are text extraction and data versioning and/or synchronisation. . pdfsplitter is the package I created to scratch that itch. It’s still very much a work in progress, but I think I did enough with nbdev to have an initial opinion. . I think I had postponed trying it out because I was worried about a steep learning curve. It turned out that an hour or two was all it took before I was basically up and running, with an understanding of all the relevant pieces that you generally use during the development lifecycle. . Built in to nbdev in general is the ability to iterate quickly and driven by short, small experiments. This is powered by Jupyter notebooks, which are sort of the core of everything that nbdev is about. If you don’t like notebooks, you won’t like nbdev. It’s a few years since it first saw the light of day as a tool, and as such it felt like a polished way of working, and most of the pieces of a typical development workflow were well accounted for. In fact, a lot of the advantages come from convenience helpers of various kinds. Automatic parallelised testing, easy submission to Anaconda and PyPi package repositories, automatic building of documentation and standardising locations for making configuration changes. All these parts were great. . Perhaps the most sneakily pleasant part of using nbdev was how it encouraged best practices. There’s no concept of keeping test and documentation code in separate silos away from the source notebooks. Following the best traditions of literate programming, nbdev encourages you to do that as you develop. Write a bit of code here, write some narrative explanation and documentation there, and write some tests over there to confirm that it’s working in the way you expected. When Jeremy speaks of the significant boost in productivity, I believe that a lot of it comes from the fact that so much is happening in one place. . While working on pdfsplitter, I had the feeling that I could just focus on the problem at hand, building something to help speed up the process of importing and generating images from PDF data for machine learning projects. . Not everything was peaches and roses, however. I ran into a weird mismatch with the documentation pages generated and my GitHub fork of nbdev since I was using main as the default branch but nbdev still uses master. I will be submitting an issue to their repository, and it was an easy fix, but it was confusing to struggle with that early on in my process. I’m also not sure how well nbdev will gel with large teams of developers, especially when they’re working on the same notebooks / modules. I know reviewnb exists now and even is used within fastai for code reviews, but I would imagine an uphill battle trying to persuade a team to take a chance with that. . I’ve been using VSCode at work, supercharged with GitHub Copilot and various other goodies, so it honestly felt like a bit of a step back to be forced to develop inside the Jupyter notebook interface, absent all of my tools. I also found the pre-made CLI functions a little fiddly to use — fiddly in the sense that I wish I’d set up some aliases for them early on as you end up calling them all the time. In fact, any time I made a change I would find myself making all these calls to build the library and then the documentation, not forgetting to run the tests and so on. That part felt a bit like busy work and I wish some of those steps could be combined together. Maybe I’m using it wrong. . All in all, I enjoyed this first few hours of contact with nbdev and I will continue to use it while developing pdfsplitter. The experience was also useful to reflect back into my current development workflow and environment, especially when it comes to keeping that close relationship between the code, documentation and tests. . [Photo by Laura Ockel on Unsplash] .",
            "url": "https://mlops.systems/python/jupyter/fastai/tools/2022/01/06/nbdev-early-impressions.html",
            "relUrl": "/python/jupyter/fastai/tools/2022/01/06/nbdev-early-impressions.html",
            "date": " • Jan 6, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Getting practical with type annotations and `mypy`",
            "content": "The third chapter of ‘Robust Python’ offers a quick introduction to the practicalities of type annotations in Python. We also see tools like mypy being used to catch places where the reality of your code doesn’t necessarily match the type annotations that you’ve stated. . For the first, a quick example can suffice: . name: str = &quot;alex&quot; def some_function(some_number: int, some_text: str = &quot;some text&quot;) -&gt; str: # your code goes here return &quot;&quot; # returns a string . You can see the different places that type annotations might appear. You can annotate variables in your code. I’ve seen this one less often, but it’s possible. Then you can have type annotations for the parameters when defining functions (some even with default values assigned). You can also have type annotations for the return value of those functions. . Note that type hints are not used at runtime, so in that sense they are completely optional and don’t affect how your code runs when it’s passed through the Python interpreter. (Type hints were introduced in Python 3.5, though there is a way to achieve the same effect using comments and a standard way of listing type annotations that way if you are stuck with a 2.7 codebase, for example.) . With some type annotations added to our code, we can use a typechecker like mypy to see whether things are really as we imagine. In Viafore’s own words: . “type checkers are what allow the type annotations to transcend from communication method to a safety net. It is a form of static analysis.” . If your codebase uses type annotations to communicate intent, and you’re using mypy to catch any of those type errors, remember that typecheckers only catch this certain type of errors. You still need to be doing testing and all the other best practices to help catch the rest. . One forward-looking benefit covered by this chapter was how having code covered with type annotations and type checking could give you the confidence to change things in the codebase that otherwise you would have hesitated to even approach. There are, of course, also some tradeoffs and disadvantages to adding this in: particularly around speed of iteration and possibly flexibility, but the book makes a strong case for why most large Python codebases could probably use type checking as part of their arsenal. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/01/03/robust-python-3.html",
            "relUrl": "/robustpython/python/books-i-read/2022/01/03/robust-python-3.html",
            "date": " • Jan 3, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Counter: a shortcut to counting iterables in Python",
            "content": "I came across this special dictionary type while reading an earlier chapter of ‘Robust Python’ the other day. It’s perhaps best illustrated with an example: . from collections import Counter Counter([1,1,2,3]) # returns Counter({1: 2, 2: 1, 3: 1}) Counter(&#39;The Netherlands&#39;.lower()) # returns Counter({&#39;e&#39;: 3, &#39;t&#39;: 2, &#39;h&#39;: 2, &#39;n&#39;: 2, &#39; &#39;: 1, &#39;r&#39;: 1, &#39;l&#39;: 1, &#39;a&#39;: 1, &#39;d&#39;: 1, &#39;s&#39;: 1}) . I had no idea this existed, and of course usually default to some kind of a cookie-cutter loop when trying get counts of elements and put those counts into a dict. . To get the inividual elements, just call the elements method on the Counter object. To get the most common n elements, call the most_common(n) method. To get the total number of counts inside the dictionary, use the total method. To reset all the counts, use the clear method. . Just a nice little set of functionality, hiding in plain sight inside the Python standard library. . Photo by Ibrahim Rifath on Unsplash .",
            "url": "https://mlops.systems/python/2022/01/01/counter.html",
            "relUrl": "/python/2022/01/01/counter.html",
            "date": " • Jan 1, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "What's special about types in Python?",
            "content": "The first section of Robust Python dives into types. We begin by taking a step back to think about what exactly types are being used for, and what they might bring us. Python was not (until v3.5) a language with which you could easily use typing. I remember going to the Pylondinium conference in London in 2018 and going to a talk by Bernat Gabor about type hints in Python. Back then I didn’t have much of a sense of how new they were to many people, but even now I don’t get the feeling that they’ve been universally adopted. Hence Patrick’s book, I suppose… . A type is defined in the book as being “a communication method”, both to / for computers (“mechanical representation”) as well as for humans (“semantic representation”). For the computer, when a variable is of a certain type this determines what methods can be called on that particular object. As such, though I’m straying into territory I don’t fully understand, I believe it also helps with compilation efficiency. (Python is a dynamically-typed language so any errors or type mismatches will only become apparent at runtime, however). . For humans, types can help signal intent. This connects with my previous chapter summary from this book where I stated that code should communicate intent well to be considered ‘robust’. Take the following simple code snippet: . dates = [...] def process_date(input): date = extract_date(input) dates.append(date) return date . We have an extract_date function (defined elsewhere in the code), but we have no real sense of what this input parameter would be. Are we taking in strings as input? Are we taking in datetime.datetime objects? Does the extract_date function accept both, or do we need to ensure that we are only taking a specific type? All these questions could be cleared up with a simple type hint as part of the function definition, like so: . dates = [...] def process_date(input: datetime.datetime): date = extract_date(input) dates.append(date) return date . Now we know what the input should be, and we can also add a type hint to the extract_date function as well which will help communicate our intent. . We also learn how Python is more towards the ‘strongly-typed’ side of things on the language spectrum. If you try to concatenate a list with a dict in Python using the + operator, Python will throw a TypeError and fail. If you try to do the same in Javascript you get two different answers depending on the order of the two operands: . &gt;&gt;&gt; [] + {} &quot;[object Object]&quot; &gt;&gt;&gt; {} + [] 0 . For our purposes, using Python, we can use the strong typing to our advantage. . Python is dynamically typed, though, which takes a bit more caution to handle in a robust manner. Any type mismatches will only be found at runtime — at least using just the vanilla install of the language without any extra imports or modules. . The chapter ends with a brief discussion of duck typing, defined as “the ability to use objects and entities in a programming language as long as they adhere to some interface”. We gain a lot in terms of increased composability, but if you rely on this feature of the language too much then it can become a hindrance in terms of communicating intent. . This chapter didn’t add too many new concepts or skills to my current understanding of the benefits of types, but it was useful to have this concept of ‘communicating intent’ to be reiterated. When I think back to how I’ve heard types mentioned in the past, they often get cast in a technical sense, whereas thinking about communication between developers I think is a more motivating framing. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2021/12/30/robust-python-2.html",
            "relUrl": "/robustpython/python/books-i-read/2021/12/30/robust-python-2.html",
            "date": " • Dec 30, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "What makes code robust?",
            "content": "We use a lot of modern Python idioms, libraries and patterns at work, so I’ve been wanting to get up to speed on that and maybe even actively contribute to this general direction. A recently-published book, Robust Python: Write Clean and Maintainable Code by Patrick Viafore, seems like it answers many of the questions I have around this topic. It is quite dense in terms of the amount of new things per chapter, so I’ll be working my way through it in the coming months and reflecting on things as I encounter them. . The first chapter is mainly about setting the scene for all the technical pieces that follow. Patrick asks the core questions: what is robust code and why do we even care? What problems does it solve to think about code in this way. . What I took away was that a robust codebase emphasises good communication as well as avoiding accidental complexity. A lot has been written about ‘clean code’ and how to achieve this, but it seems that ‘Robust Python’ is arguing for looking a bit further into the future, when you have to come back to refactor your code three months after you wrote it, or when your colleague needs to do the same. . “Writing robust code means deliberately thinking about the future.” (p. 3) . You write robust code, in other words, because you know that the codebase is going to be changing and shifting and that whatever you write today may need to be modified at a later date: . “A robust codebase is resilient and error-free in spite of constant change.” (p. 4) . We’re trying to solve for the way that code is often hard to reason about or understand when you’re outside the original moment when it was written. Accordingly, it pays dividends to take a bit of extra time upfront to write code such that it does communicate intent well, and that you haven’t made things more complicated than they need to be. . Moreover, the communication of intent needs to be done in a way that is asynchronous. The book goes into a bit more detail about why communication practices that require minimal cost and minimal proximity are to be preferred. These include: the code itself, in-code comments, tests, version control history, wikis, and in-project documentation. . The first part of the book is all about type annotation, using mypy, and how working with types helps makes your code more robust. We use a lot of this at work so I’m excited to take a deep dive into this. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2021/12/29/robust-python-1.html",
            "relUrl": "/robustpython/python/books-i-read/2021/12/29/robust-python-1.html",
            "date": " • Dec 29, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Exploring J, an array programming language",
            "content": "I’ve long wanted to explore the J programming language. I think I probably first heard about it from Jeremy Howard amidst one of the early iterations of the fastai course. He’s since spoken about it in other places. . It is part of the family of programming languages that includes APL, K and Q. These can broadly be categorised as array-programming languages, where arrays are generally the core data structure and mental model to keep in mind. They used to be extremely popular in the 1970s and 1980s, particularly among institutions or businesses with a requirement for performant calculation / computation. One of these, Q, continues to live on (as a closed-source language) in the world of finance and trading. (Q is popular alongside the proprietary database kdb+). . You’re probably wondering why someone would want to use this fairly specialised and niche language. When you look at examples of J code — like the ones here, for example — it’s easy to simply dismiss it as an unreadable (‘write-only’) language. Indeed, many do dismiss it for this reason. Code is often compact, with single letters or symbols doing all the work. Defenders of J hold this up as a feature, not a problem. The compactness of the language means that you can fit the entirety of the solution (space) of a complex problem on a single screen, whereas in many (most?) other languages you would have to be scrolling up and down through dozens or even hundreds of lines of code. . The array languages seem to come at solving problems from a particular perspective. The symbols and letters that transform the arrays in J function as a pattern language. For a simple example, think of what you have to do when you want to find the count of a particular element from within an array/list. The array language paradigm argues that you don’t want to waste your time and screen space writing out boilerplate code to carry out this calculation, when it’s a common pattern that you can just use from the language itself. When problem-solving, therefore, spend your time thinking about the problem and not messing around with syntax or repeating yourself. . J and its cousins are extremely efficient. It is written in C, and I recently heard someone quote one of the early J pioneers as having said that “it is not theoretically possible to write J code that is more performant than C, but it often ends up being so”. For some math- or statistics-heavy domains (think the world of finance), it is extremely helpful to have this highly abstracted language that works performantly on large datasets. Moreover, it seems to be even more helpful when you have a hard problem to work on that isn’t fully understood. . Kenneth Iverson’s wrote a paper (“Notation as a Tool of Thought”) that is a classic in computer science and gets into some of the above arguments. (It is written using APL, but it also applies to J). I will probably return to that at a future date, because it often comes up and is recommended as a particularly rich document worth taking time to explore in depth. . Very much as a project to indulge my curiosity, I will be exploring J over the coming months. I have been listening to the back catalogue of The Array Cast podcast, and I will be slowly working my way through some of the resources listed on the official J site. Let me know if you have experience working with J! .",
            "url": "https://mlops.systems/j/2021/12/29/j-language.html",
            "relUrl": "/j/2021/12/29/j-language.html",
            "date": " • Dec 29, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "A Taxonomy of Redaction",
            "content": "One of the things that makes it hard to train a model to detect redactions in documents is the fact that there are lots of kinds of redactions. Not only were different tools or methods used at different times, but even organisations and agencies from the same country or government didn’t always share redaction practices. . I took a bit of time to try to understand the different kinds of redactions in my (pretty huge) data set. I didn’t have any special process for selecting these images; I randomly sorted the immediate ~70,000 images I have collected and looked through to try to identify some patterns. . Taking a close look at the actual parts of images that contain redactions gives me a better sense of the challenges involved in detecting those redactions. As I iterate through my collection of images, I can start to build up an intuitive sense of where class imbalances might exist. Among the images that contain redactions, for example, which ones are most represented and which contain fewer examples? In general, where do I need to focus my efforts when it comes to improving my model? . The first easy distinction to draw is that between digital and hand-applied redactions. . . It seems that the trend in this is towards digital redactions. Perhaps it is seen as less reliable, or perhaps it’s more time consuming to attach the reasons for redactions having happened. Perhaps, too, there are some legal reasons why each redaction needed to start having a specific reason applied to it. . At first glance, no pun intended, it would appear that digital redactions are much easier to recognise. They’re often uniform in how they are applied and are usually pretty blunt in their appearance. There are some non-redaction uses for totally black or grey boxes laid on top of text, but they aren’t common and it’s a pretty strong feature to have to predict. . Handwritten redactions are also easy to recognise, but potentially the borders are harder to make out. Sometimes having a thinner pen with which redactions are applied might make it slightly less accurate. . It is more practically important to distinguish between redactions that are easy to recognise vs ones that take some time to notice. I can use my own speed at noticing the redaction on a page as a gauge. It’s not a perfect analogy, but Jeremy Howard’s adage that if a human can reliably do some kind of classification or object detection, then probably a computer can as well. I guess the inverse is also true: if a human will find it hard to recognise a particular feature in an image, then a computer will probably also find it hard. . There isn’t much point spending too long with the ‘easy’ redactions. These are usually whatever is boxy and blunt. It’s the stereotype of a redacted document, one like what was used as the cover art on the (much-censored) Guantánamo Diary by Mohamedou Ould Slahi. . . Sometimes you see that the entire page has been redacted with some kind of a coloured box. Other times entire columns of information has been redacted from a table. These definitely feel like they are the more recent types of redactions. . One thing that makes detecting redactions hard, on the other hand, is if the number of redactions is small. It stands to reason that lots of small redactions can stand out at first glance, whereas a single small redaction on one corner of the page is maybe harder to notice. . The hardest of redactions seems like it is in examples like this: . . A white box on top of other white boxes! I often have to look quite closely at these to distinguish what is normal text and what is a redaction box. Some of them have a faint thin grey boundary box around them, which I guess ends up being pretty useful as a way to make that distinction. Surprisingly, the model that I’ve trained so far is not terrible at making these kinds of distinctions. . . I have a few hundred annotated images so far, but I now have an intuitive sense of the hard parts of the object detection test. I also have a sense of how represented I feel like those hard parts are — not very. . As I wrote in my previous update on my progress in this project, the next step is very much to find ways to increase the volume of good training data that I’m using to train my model. Part of that will involve creating synthetic data, part of that will be using self-training to speed up my annotation, and of course another part will just be doing more manual annotation. I’ve already started work on creating the synthetic data. More on that next time! .",
            "url": "https://mlops.systems/redactionmodel/2021/12/15/redaction-taxonomy.html",
            "relUrl": "/redactionmodel/2021/12/15/redaction-taxonomy.html",
            "date": " • Dec 15, 2021"
        }
        
    
  
    
        ,"post15": {
            "title": "73% accuracy for redaction object detection",
            "content": "Last time I wrote about my redaction model training project, I explained how I used Prodigy to annotate and label a bunch of images. I subsequently spent a long evening going through the process, getting to know my data. I managed to make 663 annotations, though quite a few of those were negative annotations: I was stating that a certain document contained no redactions at all. . Once I had my redactions, I needed to convert the files from a Prodigy format into a .coco annotation format. I am using IceVision, a really useful computer vision library, for which it is easier if I pass in the annotations in the .coco format. . From that point, it was fairly easy to follow the steps of the object detection tutorial outlined in the IceVision documentation. I ran into some problems with Paperspace Gradient not easily installing and importing IceVision. For some reason files don’t get unzipped on Paperspace, but it’s possible to just do this manually: . Do the basic install, including the import of icevision.all. Wait for the error to get raised, then open up a terminal and enter: | . cd /root/.icevision/mmdetection_configs/ rm v2.16.0.zip wget https://github.com/airctic/mmdetection_configs/archive/refs/tags/v2.16.0.zip unzip v2.16.0.zip . Then run it again as normal. Later on, another error will get raised. Fix it with this (again in the terminal): . jupyter nbextension enable --py widgetsnbextension . This enables ipywidgets in the notebook, I think. . Once through all of that, I was able to fine-tune a model based on the annotations which I currently have. I selected VFNet as the model I wanted to use as the pertained model. After training for 40 epochs, I reached an accuracy of 73%: . . If we look at some of the results (using model_type.show_results()) we can get a sense of the parts it found easy and the parts which it found hard. (All the boxes below are what it as predicted, not the ground truth annotations.) Some identification of boxes went as you might expect: . . I was surprised that something like this worked as well as it did: . . It wasn’t perfect, but I don’t remember having annotated too many of this specific redaction type, so I’m fairly happy with how it worked out. You can see it still makes a number of mistakes and isn’t always precise about where the boxes should go. I hope that’ll improve as I add more examples of this type of redaction. . My next steps for this project include the following: . create synthetic data. The redactions are probably easy enough to mimic where we’ll get a lot of value from the use of synthetic data (fake redactions on not-real document backgrounds). It’ll be an easy way to boost my training data set by a good amount, hopefully leading to big improvements in my model accuracy. | potentially add in either active learning (to help speed up my annotation process) or self-training (using the model to make annotation suggestions on unlabelled data and using only the suggestions with really high confidence estimates). | think through the augmentations that I use as part of my workflow. I basically want augmentations that are similar to however the production use case will be: i.e. the kinds of redacted images that it might see when being given real-world data at inference time post-training. | add in experiment tracking. I’ve never used something like Weights &amp; Biases, so I’m excited to try that out and have a real process for tracking my progress throughout this project. | cleaning up and refactoring (a bit) my repository where the code lives for processing the input data. It’s starting to get a bit unwieldy and I’m worried I’ll start to forget the order things were done and some of those small details. | .",
            "url": "https://mlops.systems/redactionmodel/computervision/progressreport/2021/12/11/redaction-progress-week-one.html",
            "relUrl": "/redactionmodel/computervision/progressreport/2021/12/11/redaction-progress-week-one.html",
            "date": " • Dec 11, 2021"
        }
        
    
  
    
        ,"post16": {
            "title": "What is VFNet?",
            "content": "VFNet is short for VariFocalNet. This method of object detection was first released in 2008 and it scored 55.1 on the COCO test-dev benchmark, state-of-the-art at the time. There have since been other improvements. . . The original paper is here. The implementation of this model is here. . The problem it solves is that when we’re training a model, we have a large number of possible options for objects detected in an image. What we need to do is rank these options in order of likelihood of being a correct bounding of a box. . It is based on and draws on the MMDetection model/toolbox. MMDetection is a Pytorch library for object detection. It is modular, allowing for greater customisability. . Other resources . Airctic Presentation on VFNet .",
            "url": "https://mlops.systems/redactionmodel/computervision/2021/11/30/vfnet-basics.html",
            "relUrl": "/redactionmodel/computervision/2021/11/30/vfnet-basics.html",
            "date": " • Nov 30, 2021"
        }
        
    
  
    
        ,"post17": {
            "title": "How to annotate image data for object detection with Prodigy",
            "content": "I’m back to working on the redaction model, though this time with a slightly more focused objective: object detection. . Object detection is when you put bounding boxes around the specific object that you are trying to locate within an image. The end goal for my project is to be able to identify — for an arbitrary image — which parts of the image are redacted, and then to be able to calculate what proportion of the image is redacted. . For this, I need annotations. Annotations are the data that I will use as the fuel for the model I hope to train. We need a lot of annotations of specific redactions in order for the computer to be able to learn to detect what is a redaction and what is just an empty box, for example. . I showed in an earlier post how I trained a model to detect whether there was any kind of redaction inside an image (to around 95% accuracy). For this next stage, it isn’t enough to offer a binary ‘yes’ or ‘no’ for whether it has been redacted. I need to specify the coordinates of a bounding box which encompasses each redaction. . In terms of the final output of the annotations, there are two main ways that this could go. I could either: . get x and y coordinates for the centre of the bounding box, and then a height and a width of the box around this centre point | get the four coordinates for each of the corners of the bounding box. | The COCO dataset format will eventually want datasets in the second format, but Prodigy has its own way of storing the data which I just left for now. Once I have a better handle on the annotation flow I will write a custom recipe which will save the data in exactly the format that I want. For now, it’s good enough. . Installing Prodigy into your development environment is a breeze now that you can do it with pip: . pip install prodigy -f https://XXXX-XXXX-XXXX-XXXX@download.prodi.gy # where the XXXs are your license code . Getting going with the image training was as easy as the following CLI command: . prodigy image.manual redaction-object-detection /path/to/image/data --label CONTENT,REDACTION --remove-base64 . Note that the --remove-base64 is to ensure that Prodigy doesn’t store the raw binary image data inside the database alongside the annotations. Prodigy (and their sister tool Spacy) is a little more focused on textual data, where storing the original data alongside the annotation doesn’t pose too much of an issue, but for image files this probably is a bit of an anti-pattern and could lead to a very large database. . You get a local URL to go visit and you see an interface where you can make the necessary annotations: . . You can see that I am distinguishing between two different classes: redactions and content. Redactions are what we’ve been talking about above. Content, however, is a bounding box for the content on a page. Remember that at the end of all of this we want a percentage of the page that has been redacted. Some images have reduced sized images, where the actual content which could have been redacted only takes up half of the A4 page. If that whole section was redacted, I’d want a final amount closer to 100% for that image rather than the 50% I’d get if I just went with the total percentage of redacted pixels on the whole image file. . Doing a few annotations, I ran into a couple of issues almost immediately. What do I do with a page like this: . . The whole text of the page is annotated, but the text only extended half-way down the page. There was only 50% of the page that could have been redacted, but should the content boundary box encompass more of the page, or just the only full-section redaction? . And for the following image, what is the right way to think about how to make the annotation? . . This redaction encompasses multiple lines, so to some extent it doesn’t make a difference whether we have overlapping annotations or two adjoining boundary boxes. But for the purposes of training our model, will this contribute to a less accurate model? Should I be using polygon boundaries (which Prodigy can also use for annotations)? . . As an aside, this is why annotating your own data is so valuable. You get to see the limits of the annotations, and you get to really own the decisions that are being made. It is a bit early for me to know which approach is the best solution to these two problems, but being aware of them is important. Once we’re done with our annotations, we can easily export our data to a jsonl file with the following CLI command: . prodigy db-out redaction-object-detection &gt; ./redaction-object-detection-annotations.jsonl . This gives us a file containing all our annotations. A sample for one image gives the idea: . { &quot;image&quot;: &quot;sample/path/redacted/04-F-0269_Global_Screening_Guidance-03.jpg&quot;, &quot;text&quot;: &quot;04-F-0269_Global_Screening_Guidance-03&quot;, &quot;meta&quot;: { &quot;file&quot;: &quot;04-F-0269_Global_Screening_Guidance-03.jpg&quot; }, &quot;path&quot;: &quot;sample/path/redacted/04-F-0269_Global_Screening_Guidance-03.jpg&quot;, &quot;_is_binary&quot;: false, &quot;_input_hash&quot;: 1413334570, &quot;_task_hash&quot;: 1588323116, &quot;_view_id&quot;: &quot;image_manual&quot;, &quot;width&quot;: 800, &quot;height&quot;: 1035, &quot;spans&quot;: [ { &quot;id&quot;: &quot;0ef6ccd0-4a79-471d-9aa1-9c903c83801e&quot;, &quot;label&quot;: &quot;CONTENT&quot;, &quot;color&quot;: &quot;yellow&quot;, &quot;x&quot;: 76.5, &quot;y&quot;: 112.5, &quot;height&quot;: 786.1, &quot;width&quot;: 587.6, &quot;center&quot;: [370.3, 505.55], &quot;type&quot;: &quot;rect&quot;, &quot;points&quot;: [ [76.5, 112.5], [76.5, 898.6], [664.1, 898.6], [664.1, 112.5] ] }, { &quot;id&quot;: &quot;cd05d521-8efb-416b-87df-4624f16ca7f3&quot;, &quot;label&quot;: &quot;REDACTION&quot;, &quot;color&quot;: &quot;cyan&quot;, &quot;x&quot;: 80.3, &quot;y&quot;: 786.2, &quot;height&quot;: 20.2, &quot;width&quot;: 428.4, &quot;center&quot;: [294.5, 796.3], &quot;type&quot;: &quot;rect&quot;, &quot;points&quot;: [ [80.3, 786.2], [80.3, 806.4], [508.7, 806.4], [508.7, 786.2] ] }, { &quot;id&quot;: &quot;3e268e33-4eba-457d-8d17-8271a79ee589&quot;, &quot;label&quot;: &quot;REDACTION&quot;, &quot;color&quot;: &quot;magenta&quot;, &quot;x&quot;: 108.1, &quot;y&quot;: 772.3, &quot;height&quot;: 15.1, &quot;width&quot;: 400.6, &quot;center&quot;: [308.4, 779.85], &quot;type&quot;: &quot;rect&quot;, &quot;points&quot;: [ [108.1, 772.3], [108.1, 787.4], [508.7, 787.4], [508.7, 772.3] ] } ], &quot;answer&quot;: &quot;accept&quot;, &quot;_timestamp&quot;: 1638214078 } . Everything we’re interested in is inside the spans attribute, and it actually contains both kinds of the annotation that I mentioned above. . As you can see, annotating images in this way is fairly painless, and it brings you in closer contact with your raw data which is an added bonus. .",
            "url": "https://mlops.systems/redactionmodel/computervision/datalabelling/2021/11/29/prodigy-object-detection-training.html",
            "relUrl": "/redactionmodel/computervision/datalabelling/2021/11/29/prodigy-object-detection-training.html",
            "date": " • Nov 29, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "Check your security vulnerabilities with `safety`",
            "content": "safety is a tiny tool that checks your package’s dependencies for security vulnerabilities. It is free to use for open-source projects, and using it is as a pip install safety followed by safety check. . It checks a database of known security vulnerabilities. This database is only updated once every month, but if you are not open-source or you need access to the more frequently-updated database, then you can subscribe via pyup. . With that caveat, it’s not perfect, but it’s better than nothing. An easy CI win for open-source projects. . [I first learned of this tool here. Many thanks to calmcode for continuing to make these really useful videos.] .",
            "url": "https://mlops.systems/security/tools/calmcode/2021/11/27/safety-vulnerability-checker.html",
            "relUrl": "/security/tools/calmcode/2021/11/27/safety-vulnerability-checker.html",
            "date": " • Nov 27, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "Launching a podcast about MLOps",
            "content": "I’ll be co-hosting a new podcast about MLOps, with new episodes out every fortnight. Pipeline Conversations: A Machine Learning Podcast by ZenML is the new podcast from the company where I work. (We build an open-source tool for data scientists to empower them to take control of how their models live in production.) . Our first episode gets into some of the background for why ZenML exists in the first place. Upcoming episodes will be discussions with guests from the data science and MLOps space. . I’m excited to get the opportunity to talk with so many interesting and smart people. .",
            "url": "https://mlops.systems/zenml/podcast/appearances/2021/11/27/pipeline-conversations.html",
            "relUrl": "/zenml/podcast/appearances/2021/11/27/pipeline-conversations.html",
            "date": " • Nov 27, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "How to set and get environment variables using Python",
            "content": "If you want to get and set environment variables using Python, simply use the relevant methods from os. To set an environment variable, do this: . import os os.environ[&#39;SOME_ENV_VARIABLE&#39;] = 13.5 . And to access an environment variable, there are actually a number of different ways. All these three are essentially the same: . os.getenv(&#39;SOME_ENV_VARIABLE&#39;) os.environ.get(&#39;SOME_ENV_VARIABLE&#39;) os.environ(&#39;SOME_ENV_VARIABLE&#39;) . For the final one (os.environ(&#39;SOME_ENV_VARIABLE&#39;)), if the variable doesn’t exist, it’ll return a KeyError, whereas the first two will just return None in that case. .",
            "url": "https://mlops.systems/python/2021/11/26/environment-variables.html",
            "relUrl": "/python/2021/11/26/environment-variables.html",
            "date": " • Nov 26, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "entr: a tool to run commands when files change",
            "content": "It’s a fairly common pattern that you have some code that you’re repeatedly running. Perhaps you’re fixing a failing test, and you just have to keep running it every time you make a fix. . Enter entr. This handy little tool reruns a particular command whenever changes are detected in a particular set of files. . Let’s take the example I mentioned above: you have a failing test that you’re debugging and you need to have it run every time you save a change to the file. Assuming your source code is stored in src and you’re using pytest, then you could use something like the following: . ls src/*.py | entr -c pytest test.py::test_some_feature . So now, any time you change any Python file inside the src folder, it’ll rerun your test. The -c flag will clear the terminal every time the test runs. . [Many thanks to calmcode for continuing to make these really useful videos.] .",
            "url": "https://mlops.systems/debugging/testing/tools/calmcode/2021/11/25/entr-reruns-tests.html",
            "relUrl": "/debugging/testing/tools/calmcode/2021/11/25/entr-reruns-tests.html",
            "date": " • Nov 25, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "On failure",
            "content": "I’ve been working as a machine learning engineer now for a few months now. If there’s one thing that I have found characterises my experience so far, it’s failure. Software fails; we even have a word for that: bugs. Learning new things might also be characterised as departing from a state of failing to understand. . There hasn’t been a week that’s gone by since I started where I didn’t encounter some kind of failure, usually my inability to understand why something was behaving in a particular way. My last post was about debugging, and finding ways to move forward in the face of failure is a key aspect of that process. . Failure isn’t fun. My initial reaction to hitting something I don’t understand is not one of glee and excitement at getting this opportunity to solve some kind of problem. But maybe it should be. It occurred to me this week that actually failure is sort of the name of the game. Solving hard problems is exactly what software engineers get paid to do. If it were just easy, it’d be a different kind of work. . Two posts by Julia Evans are pretty great on how a lot of being able to do this kind of work is about mindset. Ellen Ullman covers similar territory in ‘Life in Code’ and ‘Close to the Machine’. . The point is this: we are paid to confront this failure. This is the work. Thinking that it’s a distraction from the work — some kind of imaginary world where there are no blockers or failing tests — is the real illusion. .",
            "url": "https://mlops.systems/debugging/emotions/2021/11/21/on-failure.html",
            "relUrl": "/debugging/emotions/2021/11/21/on-failure.html",
            "date": " • Nov 21, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "Some things I learned about debugging",
            "content": "I’ve had to deal with a whole bunch of bugs in the past few days and weeks. I thought it’d be useful to put down some thoughts about things that I’ve learned along the way. . Logging &amp; Printing . These are maybe the first things that everyone says you should do when you have a bug you need to fix: log things somewhere where you can see them. . There are some scenarios where simple print calls aren’t enough. If you’re running code through a series of tests, then the test harness will often consume all output to stdout so you won’t see any of your print statements. Luckily, test environments can usually be configured to print debug statements of loggers. . Once you can see what’s happening at a particular moment, you can see if what you expected to happen at that moment is actually happening. . Breakpoint your way to infinity! . The breakpoint() function comes built-in with Python. It’s a convenience wrapper around some pdb magic, and practically speaking it means you can set a point where you can interrupt the Python execution. Your terminal will halt at that point, and you can inspect the variables or objects available at that particular moment. . I wish I had known about this earlier on. It’s extremely useful for understanding exactly how a function or piece of code is being executed. . Come with hypotheses . If you don’t have a sense of what you expect to happen, it’s going to be hard to determine if what you’re doing is having any effect or not. . I’ve been lucky to do some pairing sessions with people as they work through bugs and problems, and I’ve had this ‘come with a hypothesis’ behaviour modelled really well for me. . It’s not a panacea; there’s still a lot of work to be done around this, but it’s sort of the foundation, particularly for non-trivial bugs. . Leave your assumptions at the door . Don’t assume what’s written is what’s actually working. This applies to the code you’re working on, the documentation, docstrings, everything. This is especially true when your codebase is rapidly changing growing, such as at a startup or a smaller company where not everything has been cemented into place. . The rapid pace of change means that things can get out of date, or people can make mistakes. This applies to packages or modules you’re importing as well. Of course, it’s probably more likely that you’re misunderstanding something vs the Python standard library has got something wrong, but for many other open-source projects, you should at least be open to the possibility that weird things might show up. . Follow the thread wherever it leads . This is something about updating your assumptions as you move through the process of testing your assumptions. If you rule out certain pathways, then you should be prepared to go down the remaining ones as far as you need. . Be systematic . I’ve found a few times now, that there are certain moments where I notice I’m far far down the road. I’ll have kept making a bunch of decisions at the various crossroads that I passed. At a certain moment, though, I need to take stock and just note down all the decisions and assumptions I’ve made in order to reach this point. . I’ll write a short note to myself (mainly), but also for teammates, where I explain all the different assumptions and pathways that I’m travelling down. I’ll specifically write down all the conditions that need to be present for this bug to present (as far as I know them). . Quite often, just writing these assumptions down will help me solve the problem outright. Even when it doesn’t, it’s extremely useful in re-grounding myself and reminding me of why I’m going down rabbit hole x or y. . Know when to stop . In an ideal world you’d get to follow every windy road and to figure out everything that doesn’t make sense. But — and this is again especially true for fast-moving startups — you might not always have time to do that. . This is somehow connected to the Pareto Principle (also known as the 80/20 rule). At a certain point you should make sure to check in with how much time you’d planned on spending on a particular bug. If you’re finding that it’s taking far longer than expected, and you have other things you’re committed to completing, then you should maybe take an opportunity to connect to your team. Alternatively, you can rescope and find a way to disable or flag a particular bug for the next sprint, or see if someone can help you with it. . Remember: this is the work . Sometimes when I’m fixing bugs I have the feeling that I’m wasting my time somehow, or that I should be doing something more productive. It’s often the case, though, that this is the work. I’m low on experience, but proxy experience that I’ve gained through reading books tells me that finding, fixing and triaging bugs is a lot of what we do as software engineers. . Know when to ask for help . Sometimes there are bugs which turn out to be bigger than you’re able to handle. It’s certainly worth pushing back against that feeling the first few times you feel it. Early on it’s often going to feel like the bug is unsolvable. . But some times there are pieces of context you don’t have, which a quick overview of what you’ve done and tried might alert someone more season to the fact that you’re going down the wrong alley. Or it might remind them of something they knew implicitly but had forgotten. The important things is to judge when is the right time to seek outside advice. .",
            "url": "https://mlops.systems/debugging/2021/10/25/debugging.html",
            "relUrl": "/debugging/2021/10/25/debugging.html",
            "date": " • Oct 25, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "Writing Code",
            "content": "I read Daniel Roy Greenfeld’s post on how he found that coding a lot was key to improving his skills. It makes sense. Everything I’ve read so far and my previous experience at the metaskill of learning new things tells me that it is a good investment of time. . Just like you get good at writing by doing a lot of writing, on some level that is true for coding. (Of course, there are additional pieces to the puzzle: you have to develop some taste alongside the pure production side, you have to do some quality-control and refactor your code, and so on and so on.) . For me, this looks like the following: . coding at work during the week | smaller focused exercises from PythonMorsels, Exercism, LeetCode and AlgoExpert | code written while working my way through the fastai course; this will probably manifest as blog posts here as well, outlining some small project I completed along the way. | a bigger project, perhaps a package, that I’ll start building at some point. I have some ideas for things I want to implement. I’ll pick one soon. It’ll probably be related in some way to the fastai coding. I’m thinking right now of making a tool that allows you to download PDFs and use the pages of those PDFs as image files in computer vision problems; a data ingestion tool, in other words. | smaller scripts to solve daily problems in my digital life. I’ll store those on my GitHub somewhere and write up the design decisions around the more interesting ones here. | . One thing I took note of was how Daniel mentioned that it made sense to specialise and focus on one language at a time, particularly in the early days. Rather than indulging my curiosity and doing 1001 things using Go or lisp or whatever, I will try to stick to Python at least until I feel more confident with it. .",
            "url": "https://mlops.systems/python/skillbuilding/2021/09/18/writing-code.html",
            "relUrl": "/python/skillbuilding/2021/09/18/writing-code.html",
            "date": " • Sep 18, 2021"
        }
        
    
  
    
        ,"post25": {
            "title": "Reading Python Code",
            "content": "It’s a truism of sorts that in order to improve your skills, you have to practice them. For coding, the stereotypical image is of someone typing, actually creating new things. But as often as not, you’re going to be reading code instead. This code might be something you write yesterday or last year, or it might be something that someone else wrote. . One way or another, reading code is a great way to get increasing familiarity with stylistic, syntactic patterns and to get exposed to some best practices, especially if you get to pick the code you’re reading. . I’ll be doing the same as I ramp up my Python proficiency. I wanted to gather some lists of codebases and assorted resources in one place for myself, and I hope maybe it’ll be useful for someone else as well. . Good Quality Python Code . jinja — a templating engine written in Python (and see the recommendations for supplemental reading and watching for jinja here) | howdoi — a search tool for coding answers via the command line | flask — a micro-web framework for Python | FastAPI — another web framework that’s a bit larger than flask | diamond — a Python daemon that collects and publishes system metrics | werkzeug — a web server gateway library | requests — an HTTP library, now part of the Python standard library | tablib — library for Pythonic way to work with tabular datasets | click — a Python package for creating command line interfaces | pathlib — part of the Python standard library; a module to handle filesystem paths (also the corresponding PEP proposal #428) | dataclasses — a module in the Python standard library; reduces boilerplate of writing classes (also the corresponding PEP proposal #557) | joblib — a library to support lightweight pipelining in Python | . Other Resources . 500 Lines or Less — a book in which specific small open-source projects are profiled to understand how they approached their particular challenge. | The Architecture of Open Source Applications: Elegance, Evolution and a Few Fearless Hacks — examination of the structure of the software of some open-source software applications. | The Architecture of Open Source Applications: Volume II: Structure, Scale and a Few More Fearless Hacks — the second volume in the series. | .",
            "url": "https://mlops.systems/python/skillbuilding/2021/09/18/reading-python.html",
            "relUrl": "/python/skillbuilding/2021/09/18/reading-python.html",
            "date": " • Sep 18, 2021"
        }
        
    
  
    
        ,"post26": {
            "title": "Tensors all the way down",
            "content": "#!pip install -Uqq fastbook #!pip install fastai import fastbook fastbook.setup_book() from fastai.vision.all import * from fastbook import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . . In chapter 4 of the book, we start to really get into what&#39;s going on under the hood with deep learning. Turns out, tensors are a pretty important piece. We are still in the realm of computer vision, and we are going to work on distinguishing between handwritten digits. . First we use the untar_data function to grab a sample of data from the famous MNIST data set. This function returns the path where that data was stored locally. . path = untar_data(URLs.MNIST_SAMPLE) . Path.BASE_PATH = path . path . Path(&#39;.&#39;) . Now we want to briefly inspect the contents of one of our training data folders. This is for the number 7. You can see that it&#39;s just a series of .png image files. . threes_dir = (path/&#39;train/3&#39;).ls().sorted() sevens_dir = (path/&#39;train/7&#39;).ls().sorted() sevens_dir . (#6265) [Path(&#39;train/7/10002.png&#39;),Path(&#39;train/7/1001.png&#39;),Path(&#39;train/7/10014.png&#39;),Path(&#39;train/7/10019.png&#39;),Path(&#39;train/7/10039.png&#39;),Path(&#39;train/7/10046.png&#39;),Path(&#39;train/7/10050.png&#39;),Path(&#39;train/7/10063.png&#39;),Path(&#39;train/7/10077.png&#39;),Path(&#39;train/7/10086.png&#39;)...] . In order to look at a single image, we can just open it using Image.open which comes from the Python Image Library (PIL). . im3_path = threes_dir[1] im3 = Image.open(im3_path) im3 . Jupyter knows how to display various files, so we can see that image above. But what exactly is an image made up of? If we turn that image into an array, or to a tensor (the next two cells), slicing them so you aren&#39;t just seeing zeros on the edges, then you can see that these images are made up of a matrix of values from 0 to 255. . im3_arr = array(im3)[4:10, 4:10] im3_arr . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=uint8) . im3_tns = tensor(im3)[4:10, 4:10] im3_tns . tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) . We can use the show_image function to turn those 0-255 values back into an image, like so: . show_image(im3_arr) . &lt;AxesSubplot:&gt; . A really nice way of visualising exactly what is going on is to turn this image into a pandas dataframe and then for every individual pixel value, use that value as the background gradient for that cell. Here&#39;s an example of part of an image of a handwritten number 3. . im3_t = tensor(im3) df = pd.DataFrame(im3_t[4:15,4:22]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | . 3 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | . 4 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | . 5 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | . So now we have a toolkit of ways to view the pixel values that make up an image. We also have a mental model for how we can think about images and how computers represent those images stored on our machine. . But how might we then best go about knowing whether a particular image is a 3, let&#39;s say, or a 7? . One naive approach might be just to get the average value for each individual pixel for all of the threes in our training data, and then just compare the difference between our sample image and this average representation. . Let&#39;s try that now. . Getting the average values for our images . We&#39;ll set up two lists with images of the digits converted to tensors. You can see that we have 6131 images in our &#39;threes&#39; list. . threes_tensors = [tensor(Image.open(i)) for i in threes_dir] sevens_tensors = [tensor(Image.open(i)) for i in sevens_dir] len(threes_tensors) . 6131 . We can view an individual image, as before, with the show_image function: . show_image(threes_tensors[3]) . &lt;AxesSubplot:&gt; . Now in order to get the average values for each pixels, we can use the stack method to handle the first part of this. . Think of it as basically adding an extra dimension to your data structure, such that you have a &#39;stack&#39; (it&#39;s a useful mental image) of those images. . threes_stack = torch.stack(threes_tensors) . If we look at the shape of our Pytorch stack now, we can see we have our 28x28 image, but we have a stack of 6131 of them. . threes_stack.shape . torch.Size([6131, 28, 28]) . Each individual image is still a tensor: . a_three = threes_stack[3][4:16, 4:16] a_three . tensor([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 104, 253, 253, 253, 255, 253], [ 0, 0, 0, 0, 0, 178, 248, 252, 252, 252, 253, 252], [ 0, 0, 0, 0, 0, 186, 252, 252, 252, 252, 253, 252], [ 0, 0, 0, 0, 0, 186, 252, 243, 172, 172, 39, 39], [ 0, 0, 0, 0, 0, 39, 53, 47, 0, 0, 0, 29], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 54, 208], [ 0, 0, 0, 0, 0, 0, 0, 0, 3, 41, 253, 252], [ 0, 0, 0, 0, 0, 0, 5, 41, 165, 252, 253, 252], [ 0, 0, 0, 0, 0, 109, 163, 252, 252, 252, 253, 252], [ 0, 0, 0, 0, 0, 186, 252, 252, 252, 252, 253, 252], [ 0, 0, 0, 0, 0, 187, 253, 253, 253, 253, 134, 77]], dtype=torch.uint8) . Generally speaking, for some operations (like getting the mean average) we&#39;re going to want to convert the values to floats, and it also makes sense to normalise the values at the same time. Instead of having a range of 0-255, we want a range of 0-1. . threes_stack[3][4:16, 4:16].float()/255 . tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4078, 0.9922, 0.9922, 0.9922, 1.0000, 0.9922], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6980, 0.9725, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7294, 0.9882, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7294, 0.9882, 0.9529, 0.6745, 0.6745, 0.1529, 0.1529], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.2078, 0.1843, 0.0000, 0.0000, 0.0000, 0.1137], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2118, 0.8157], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.1608, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0196, 0.1608, 0.6471, 0.9882, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4275, 0.6392, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7294, 0.9882, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7333, 0.9922, 0.9922, 0.9922, 0.9922, 0.5255, 0.3020]]) . Now that we&#39;ve done it for a single image, we can perform the same operations on our whole Pytorch stack. . threes_stack = torch.stack(threes_tensors).float()/255 sevens_stack = torch.stack(sevens_tensors).float()/255 threes_stack.shape # it&#39;s good to keep in touch with the shape of our stack . torch.Size([6131, 28, 28]) . Now we&#39;re getting closer to our desired result. We can squash the stack down into just two dimensions with a simple call to .mean(0), where 0 is the index value of the dimension through which we want to calculate the mean. You&#39;ll see now that the shape property of our threes_means variable is simply a 28x28 image. . threes_means = threes_stack.mean(0) threes_means.shape . torch.Size([28, 28]) . When we show that image, you&#39;ll see that it&#39;s a sort of blurry &#39;ideal&#39; version of a three . show_image(threes_means) . &lt;AxesSubplot:&gt; . We can do the same for the sevens: . sevens_means = sevens_stack.mean(0) show_image(sevens_means) . &lt;AxesSubplot:&gt; . Validation: Comparing our average three with a specific three . Now we have our average values, we want to compare these with a single specific digit image. We&#39;ll get the difference between those values and whichever difference is the smallest will most likely be the best answer. . Our averaged three is still threes_means and we can get a single three from our validation set like this: . threes_dir_validation = (path/&#39;valid/3&#39;).ls().sorted() sevens_dir_validation = (path/&#39;valid/7&#39;).ls().sorted() im3_validation_path = threes_dir_validation[5] im3_validation = tensor(Image.open(im3_validation_path)).float()/255 im7_validation_path = sevens_dir_validation[3] im7_validation = tensor(Image.open(im7_validation_path)).float()/255 show_image(im3_validation) . &lt;AxesSubplot:&gt; . show_image(im7_validation) . &lt;AxesSubplot:&gt; . . Note: Calculating the difference between two objects . We can use two different measurements of the difference between our mean value and the individual image: . mean absolute difference (calculated by taking the mean of the absolute difference between the two tensor values). Also known as the L1 Norm. | root mean squared error (calculated by first squaring the difference between the two tensor values, taking the mean and then square rooting those values). Also known as the L2 Norm. | . The second option, the RMSE, gives a stronger signal, you might say, for the differences because you are taking the averages from the squared values. Squaring the difference also takes care of any negative values you might have. . mean_absolute_difference_3 = (im3_validation - threes_means).abs().mean() root_mean_squared_error_3 = ((im3_validation - threes_means)**2).mean().sqrt() mean_absolute_difference_3, root_mean_squared_error_3 . (tensor(0.1188), tensor(0.2160)) . mean_absolute_difference_7 = (im7_validation - threes_means).abs().mean() root_mean_squared_error_7 = ((im7_validation - threes_means)**2).mean().sqrt() mean_absolute_difference_7, root_mean_squared_error_7 . (tensor(0.1702), tensor(0.3053)) . We can now see that our individual three image is indeed closer to the threes_means composite image than to the sevens_means composite image. A smaller value at this point is what we&#39;re looking for, and the threes have it. . It turns out that there is another way to calculate the difference that&#39;s built in to Pytorch as loss functions: . F.l1_loss(im3_validation, threes_means), F.mse_loss(im3_validation, threes_means).sqrt() . (tensor(0.1188), tensor(0.2160)) . It&#39;s a bit more concise, though it does obscure what&#39;s going on under the hood in terms of calculations. . Results of the naive approach . So this tells us that our single three is closer to an ideal 3 than an ideal 7, which is great since it reflects the ground truth of our problem. But can we get a metric to know how well we perform on average against a large number of threes and sevens from our validation set? . Yes, since we have that dataset ready for use! . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]).float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]).float()/255 valid_3_tens.shape, valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . Now we can write a helper function that will allow us to calculate the distance between two images. We use the RMSE or L1 Norm for this difference calculation: . def mnist_distance(a, b): return (a - b).abs().mean((-1, -2)) . We can use this function on our previous example: . mnist_distance(im3_validation, threes_means) . tensor(0.1188) . We can continue onwards by comparing the rank 3 tensor with the rank 2 tensor. This brings a concept called &#39;broadcasting&#39; into play. . We are comparing a tensor with 2 dimensions with a tensor with 3 dimensions, so Pytorch behaves as if both tensors have three dimensions, and (without taking extra memory) pretends as if there are multiple copies of the image in 2 dimensions. This effectively makes it as if we&#39;re comparing two 3-dimensional tensors. . From this next calculation, we see returned back a collection of the distances between all of the validation images. . mnist_distance(valid_3_tens, threes_means) . tensor([0.1328, 0.1523, 0.1245, ..., 0.1383, 0.1280, 0.1138]) . In order to check whether an image is a 3, we basically need to know whether the difference for the number 3 is larger than the difference for the number 7. . We can write a helper function for that: . def is_3(img): return mnist_distance(img, threes_means) &lt; mnist_distance(img, sevens_means) . We can now check our ground truth examples: . is_3(im3_validation), is_3(im7_validation) . (tensor(True), tensor(False)) . That&#39;s what we expected to happen. Our 3 image is a 3, and our 7 image is not a 3. . If we want to check the distance in general for our validation set, we have to convert them into floats and then get the mean, but it&#39;s really easy. Again, this uses broadcasting: . validation_accuracy_3 = is_3(valid_3_tens).float().mean() validation_accuracy_7 = 1 - is_3(valid_7_tens).float().mean() validation_accuracy_3, validation_accuracy_7 . (tensor(0.9168), tensor(0.9854)) . Overall, then, we can calculate how good our toy or baseline model is for the entire problem: . (validation_accuracy_3 + validation_accuracy_7) / 2 . tensor(0.9511) . Pretty good! . This was of course just a naive way to solve the problem. There are more advanced techniques which we&#39;ll tackle next. .",
            "url": "https://mlops.systems/pytorch/fastai/computervision/2021/09/16/ch4-tensors.html",
            "relUrl": "/pytorch/fastai/computervision/2021/09/16/ch4-tensors.html",
            "date": " • Sep 16, 2021"
        }
        
    
  
    
        ,"post27": {
            "title": "A Baseline Python Development Setup",
            "content": "The world of Python versioning (and the downstream package versioning) is wild. This StackOverflow thread gives you a sense of some of the core issues at play. (As an indication of the importance of the issue, even BDFL Guido van Rossum himself has the current second most upvoted answer.) . For a really vanilla and close-to-core-python setup, a combination of venv and pip seem to be the way to go. venv is part of the standard library and as such is pretty close to a default option. . For something a bit more involved, that handles dependencies and package installation in a slightly more deft manner, the combination of pyenv, pyenv-virtualwrapper and poetry works really well. I’ll detail some of the setup gotchas and usage patterns below. . pyenv for versioning Python itself . pyenv lets you install multiple versions of Python on the same machine. The interface to switch between local versions and whatever you’ve decided will be your global option is pretty intuitive. . Visit the pyenv github page for more on installation. (If you’re on a Mac you can simply do a brew install pyenv.) . To see which versions of Python you have installed locally: . pyenv versions . To see versions of Python which are available for installation: . pyenv install —list . Note that, as I understand it, these versions are not dynamically updated. You get an updated list of new Python versions by updating pyenv, in other words. . To install a specific version of Python, and to make it available for use: . pyenv install 3.9.1 . To set that version of Python as the global version (i.e. running python will use this version by default): . pyenv global 3.9.1 . If you are in a project directory and wish to only use a particular version of Python in that directory (and its subdirectories): . pyenv local 3.8.2 . This creates a .python-version file in that directory with the desired local version. . pyenv-virtualenv for managing virtual environments . pyenv-virtualenv is a plugin that connects the work of selecting which version of Python to use (through pyenv, which we’ve previously installed) to the work of creating and running virtual environments to keep code contained in quasi-sandbox environments. When you install packages in virtual environments they don’t conflict with other locations where you might have conflicting versions of those same packages installed. . Read installation instructions and the docs here. (If you installed pyenv with homebrew, be sure to do the same with pyenv-virtualenv). . To create a virtual environment for the Python version used with pyenv, run pyenv virtualenv, specifying the Python version you want and the name of the virtual environment directory: . pyenv virtualenv 3.8.2 my-virtual-env-3.8.2 . This will create a virtual environment based on Python 3.8.2 under $(pyenv root)/versions in a folder called my-virtual-env-3.8.2. . To list what virtual environments have been created and are available to use: . pyenv virtualenvs . As a common workflow pattern, you’d create your directory and cd into it, and then you can set the virtual environment you just created as the one to use for that directory: . mkdir test-project &amp;&amp; cd test-project pyenv local my-virtual-env-3.8.2 . This should change the prompt in your terminal window and you’ll thus know that you’re now working out of that virtual environment. Any time you return to that folder you’ll automatically switch to that environment. . The manual way of turning on and off virtual environments is: . pyenv activate env-name pyenv deactivate env-name . To remove a virtual environment from your system: . pyenv uninstall my-virtual-env . (This is the functional equivalent of removing the directories in $(pyenv root)/versions and $(pyenv root)/versions/{version}/envs.) . poetry for handling package installation and dependencies . python-poetry is the latest standard tool for handling package installations and dependency management. . You can use poetry without the previous two tools, but really they work best all together. Follow the installation instructions documented on their page to get it going. . Then update poetry: . poetry self update . poetry is one of those tools that’s able to update itself. . For basic usage for a new project, you can follow the following workflow. There are two ways to start a new project using poetry: using new or init. For example: . poetry new some-project-name . This will kickstart your new project by creating a bunch of files and a directory structure suitable for most projects, like so: . some-project-name ├── pyproject.toml ├── README.rst ├── some-project-name │ └── __init__.py └── tests ├── __init__.py └── test_some-project-name.py . You might want to use a src folder (above the some-project-name in our example) which is fairly commonly used, in which case amend the command as follows: . poetry new --src some-project-name . poetry init doesn’t do all the extra work of creating a directory and file structure. It merely creates a pyproject.toml file interactively, using some smart defaults. For a minimal use of poetry, this is definitely the way to go. . The add command adds required packages to your pyproject.toml and installs them (along with all their dependencies). It does a lot under the hood to make sure that dependencies are correctly resolving before installing. For example: . poetry add zenml . To add packages only to be used in the development environment: . poetry add --dev zenml . To list all installed packages in your current environment / project: . poetry show . To uninstall a package and remove it (and its dependencies) from the project: . poetry remove zenml . To install all relevant packages and dependencies of a project that you’ve newly cloned into: . poetry install . Note that it is possibly worth creating some custom scripts to handle some of the overhead of using these tools, depending on your common development workflows. .",
            "url": "https://mlops.systems/python/tools/2021/09/14/python-versioning-package-managers.html",
            "relUrl": "/python/tools/2021/09/14/python-versioning-package-managers.html",
            "date": " • Sep 14, 2021"
        }
        
    
  
    
        ,"post28": {
            "title": "Six problems TFX was trying to solve in 2017",
            "content": "TensorFlow Extended or TFX is a platform for machine learning that claims to handle pretty much everything you’d need for end-to-end model training, deployment and retraining. It was developed for Google, the successor to Sibyl, and released in public in 2017. I read the original paper that accompanied its release to understand the problems it was trying to solve, as well as to get a handle on the specific context in which it was developed. (It’s worth being wary about tools developed at places like Google; after all, hardly any of us are operating at Google-scale). . . ‘A TensorFlow-based general-purpose machine learning platform’ . The engineers wanted a general-purpose tool, one that could serve many different use cases. I haven’t yet read the subsequent paper on the history of TFX, but from what I do know already there were other in-house solutions that existed before. Machine learning model training at scale, deployment and the general full-cycle behaviours are pretty involved and challenging, and it often seems like the needs of particular scenarios demand different approaches. This is as much true now as it was back int 2017, I imagine, though perhaps now we have some ideas of the broad pieces that make up the whole picture that needs to be addressed. . The problem here is that you might have certain parts that either are very compute intensive, or require special distributed computing setups, or where the models need to be trained off streaming data rather than from static stores. So with TFX they tried to make the tool sufficiently abstract that they could handle most cases someone would want to use it for. (They say at the end that there were some parts that they hadn’t anticipated, specifically sequence-to-sequence language models used in machine translation). . An end-to-end platform . The ambition for the platform and software tooling was not just to handle the smaller pieces of the training and deployment cycle, but rather to tackle the big overarching abstractions in a single approach. This of course contained some baked-in assumptions about how users would use TFX as well as what I’d say were quasi-philosophical positions on how best to approach these various parts. The paper characterises these as ‘best practices’, but certainly there hasn’t been uniform acceptance of these. . I imagine the end-to-end part was as much an attempt to encourage engineers to think of the problem in this exact way. If you are handling all the pieces of the training cycle, it’s easier to be fast and iterate and do all the things we expect of a more agile process. . Continuous training and serving . TFX was built to handle the kinds of models where the use cases demanded the ability to continuously retrain models using large quantities of streaming data. This is almost certainly not the norm, but for a company like Google I can understand that this would have been a key consideration if they wanted adoption of the tool across different teams. . In this way, certain scenarios (for example the Google Play Store case study outlined in the paper) saw a continuous retraining of models as more users used the service as well as new apps continued to be uploaded to the Play Store. If you have this kind of engineering need, and if you need to keep latency to certain boundaries (in the tens of milliseconds), it makes complete sense to have this whole structure that allows this to take place. Reading the specific example, it’s a pretty amazing feat, handling all that complexity underneath the surface. There must be many hundreds of other such services which similar levels of complexity concealed beneath the surface. . Reduce technical debt accrued via duplicated or ad hoc solutions . Prior to TFX and Sybil, it seems that there were many different approaches within Google, all addressing the same problem but in slightly different ways. . Having a series of best-practices built in to the service means that everyone can communicate about problems and about their issues using a shared language. It means that solutions discovered by one team can help other future teams. There’s a lot to be said for finding a solution that is sufficiently abstracted to work for many people. . Indeed, it seems this is the work of the MLOps community right now: find ways to abstract away problems that we all face, and to find the best abstractions that fit within the mental models we all have in our heads. The fact that there hasn’t been a grand convergence on a single solution indicates to me (at this current moment) that we haven’t found the right abstractions or flexibility within those abstractions. All the end-to-end tools handle much of the same stages of the model training and deployment process, but they each have opinions about the best practices to be employed along the way. (At least, that’s my current take on things). . Reliable serving models at scale . If you’re Google, you need to make sure that you aren’t serving garbage models to your users, or that inconsistencies in the input data aren’t polluting your retraining processes. At scale, even small mistakes compound really easily. . In the paper, two specific improvements are mentioned, tackling the challenges of low latency and high efficiency. The high efficiency example wasn’t entirely comprehensible for me, but what was clear was that they had very high expectations for how fast they wanted to make all parts of the pipelines and process. As above, the challenges of making it easy and fast to serve models — all of which had to happen in a reliable manner — was something that could be reused elsewhere in the company. TensorFlow Serving is what we get from their efforts in this regard. . Fast retraining with ‘warm-starting’ . For the specific challenge of retraining models with streaming data, engineers were finding that they couldn’t retrain the entire model from scratch, particularly with the scale of the training data that they had. Instead, they leveraged transfer learning (reframed here as ‘warm-starting’) to take all the hard work that had already been done, and adapting this pre-existing model with the new data. This makes a lot of sense, though the reframing with the new term is a bit less comprehensible to me. . Missing pieces . There are various pieces of what I think of as the machine learning workflow (as of 2021) which seem to be missing when I read this paper. Explainability or governance of models seems somewhat of an afterthought, if it is raised at all. I think the authors might argue that many of the checks and balances are made on the data ingestion phase, and that if all that checks out then this tackles a large piece of the problem surface area. . Similarly, there is relatively little said about model versioning and data versioning. Maybe coming at this from the present moment, where it seems obvious (with tools like DVC) that data versioning is a thing you’d want to care about. . As a general response, it seems clear that if you use TensorFlow to train your models, TFX might well be a pretty neat solution that handles many of your needs, particularly if you’re operating at serious scale. If you’re a researcher (perhaps using PyTorch) with less of those specific contextual needs, it seems less than certain that TFX would suit your purposes. . A couple of other interesting observations. The data observability and validation stage seemed to place a lot of emphasis on the automation of how pre-defined schemas might get updated. I’d be interested to see how that worked in practice. I understood the challenge that if there are too many error messages about dodgy data inputs, engineers are likely to grow inured to those alerts and maybe just ignore them. But at scale, I wonder about the risks of allowing automatic updates to those schema boundaries. . Again on the validation point, I found it interesting how the authors of the paper said that users of TFX internal to Google found the option to enable this was actually a hard sell unless or until the team had experienced some kind of failure connected to poor data validation. The TFX team ended up turning on the validation parts of the pipeline by default instead of assuming that users would choose to do so manually. . I wasn’t active in the field in 2017, so it’s hard for me to be able to reconstruct exactly how prescient or not this paper was in some of its diagnoses of the problem. It doesn’t seem that TFX was the total solution that perhaps it was pitched as being, but nonetheless it seems an important engineering achievement for Google. .",
            "url": "https://mlops.systems/tfx/tensorflow/google/mlops/papers-i-read/2021/09/11/tfx-paper.html",
            "relUrl": "/tfx/tensorflow/google/mlops/papers-i-read/2021/09/11/tfx-paper.html",
            "date": " • Sep 11, 2021"
        }
        
    
  
    
        ,"post29": {
            "title": "Managing Python Environments with pyenv and pipenv",
            "content": "It’s hardly news that that managing multiple versions of Python in a development environment is hard. Adding in dependency management on top of that makes everything harder. .",
            "url": "https://mlops.systems/python/2021/09/10/python-environments.html",
            "relUrl": "/python/2021/09/10/python-environments.html",
            "date": " • Sep 10, 2021"
        }
        
    
  
    
        ,"post30": {
            "title": "Retrieval Practice with fastai chapters 1 and 2",
            "content": "Retrieval practice is when you actively try to remember something as a way of making sure that you learn it well. (Read more about it here). Today I did that with the dogs vs cats example that the first two chapters cover. . We start with installing the fastai library and importing everything from the vision library. This was hard to remember since the pattern of .all and importing * is not something I&#39;ve seen much in Python imports. . from fastai.vision.all import * . Then we create the simple function that will be used to classify the images. The pets dataset relies on the first letter of the filename for knowing whether a picture is of a cat or a dog. So the function is pretty simple: it checks whether the first letter is a capital letter or not. . The simple assert testing was a little trick that I saw mentioned somewhere this past week. It&#39;s not a full-fledged test suite, but it&#39;s at least the start of something that can later be refactored out into whatever takes its place, be it using pytest or something else. . def is_cat(string): return string[0].isupper() assert is_cat(&quot;abs&quot;) == False assert is_cat(&quot;Abs&quot;) == True . Now we have to import the data for the files and apply whatever custom transforms we want applied to them. . I had certainly forgotten that untar_data was a method when I started out with this. I also am not familiar enough with the pathlib library as I need to be. . It&#39;s interesting that we actually don&#39;t even need to do any of the batch transformations on the images in order to get excellent results. I imagine that&#39;s because the task is so close to that of the original resnet architecture. . path = untar_data(URLs.PETS)/&#39;images&#39; dls = ImageDataLoaders.from_name_func(path, get_image_files(path), label_func=is_cat, item_tfms=Resize(224)) . Then it&#39;s all about passing the dataloaders object into the cnn_learner function, along with our desired architecture. We also set the error_rate (i.e. 1 minus the accuracy at making predictions) as the metric we&#39;ll see displayed in the output. . learner = cnn_learner(dls, resnet34, metrics=error_rate) # fine-tune the model learner.fine_tune(5) . epoch train_loss valid_loss error_rate time . 0 | 0.140326 | 0.019799 | 0.008119 | 00:19 | . epoch train_loss valid_loss error_rate time . 0 | 0.046906 | 0.021923 | 0.006089 | 00:24 | . 1 | 0.041144 | 0.009382 | 0.004060 | 00:25 | . 2 | 0.028892 | 0.004109 | 0.002030 | 00:25 | . 3 | 0.008950 | 0.002290 | 0.001353 | 00:25 | . 4 | 0.004486 | 0.002822 | 0.001353 | 00:25 | . And here you can see the results. In this training run, with 5 epochs, we were able to achieve a 99.9% accuracy. Not bad! . learner.show_results() .",
            "url": "https://mlops.systems/fastai/jupyter/computervision/2021/09/10/chapter1and2recall.html",
            "relUrl": "/fastai/jupyter/computervision/2021/09/10/chapter1and2recall.html",
            "date": " • Sep 10, 2021"
        }
        
    
  
    
        ,"post31": {
            "title": "How to set a Jupyter notebook to auto-reload external libraries",
            "content": "The code to insert somewhere into your Jupyter notebook is pretty simple: . %load_ext autoreload %autoreload 2 . When you’re working on an external library or piece of Python code outside the contents of your notebook, this snippet will make sure that the updated functions and constants will always be available in their most-recently edited state. .",
            "url": "https://mlops.systems/jupyter/2021/09/09/auto-reload-external-libraries.html",
            "relUrl": "/jupyter/2021/09/09/auto-reload-external-libraries.html",
            "date": " • Sep 9, 2021"
        }
        
    
  
    
        ,"post32": {
            "title": "A Baseline Understanding of MLOps",
            "content": "Next week I’m due to begin a job as a Machine Learning Engineer at a company that works in the MLOps field. It’s a new field to me. I’ve read a good deal on it in recent weeks, and listened to a few dozen episodes of the MLOps.community podcast, but I still very much consider myself a beginner in the space. To that end, I thought it worth clarifying my understanding of what MLOps is all about, the problem it is trying to solve, and where I see the opportunity there. . A top-down explanation is probably the best way to think of what we’re doing when we talk about ‘doing MLOps’: we’re doing all the things which make it possible to train, deploy and use machine learning models in the real world or ‘in production’. It isn’t just a series of tools, but also a series of best practices and a community that is constantly learning and iterating to improve. . The kinds of things that you can do with machine learning models are incredibly diverse, so it stands to reason that the people who operationalise all these models have quite varied opinions and approaches to how best to do this. Even the deployment scenarios are pretty different and involve different technology stacks. There is an idea of a ‘full stack machine learning engineer’, which apparently means someone who just knows everything across the board; I hope to be able to delve into some of these areas and the key technologies represented in each space in due course on this blog. .",
            "url": "https://mlops.systems/mlops/2021/09/08/baseline-mlops-understanding.html",
            "relUrl": "/mlops/2021/09/08/baseline-mlops-understanding.html",
            "date": " • Sep 8, 2021"
        }
        
    
  
    
        ,"post33": {
            "title": "Training a classifier to detect redacted documents with fastai",
            "content": "I am working my way through the fastai course as part of an online meetup group I host.1 . This week we finished the first and second chapters of the book, during which you train a model that can recognise if an image contains a cat or a dog. Later on, you train another model that distinguishes between different types of bears (‘grizzly’, ‘black’ and ‘teddy’). . Jeremy Howard, who is teaching the course, then prompts you to take what you learned and apply it to something that has meaning for you. (This is something that most of those who’ve found any success with the course emphasise repeatedly.) . I decided to work on something adjacent to my previous life / work, where I knew there was some real-world value to be gained from such a model. I chose to train an image classifier model which would classify whether a particular image was redacted or not. . The Problem Domain: Image Redaction . Under the Freedom of Information Act (FOIA), individuals can request records and information from the US government.2 This is one collection of some of the responses to this requests, sorted into various categories. You can read, for example, responses relating to UFOs and alien visits here. . Quite often, however, these images are censored or redacted. . . Knowing that this practice exists, I thought it might be interesting to train a model that could recognise whether a particular page contained some kind of redaction. This wasn’t completely in line with what we covered during the first two chapters; I wasn’t sure if the pre-trained model we used would work for this data set and use case. . It could be useful to have such a tool, because FOIA responses can sometimes contain lots of data. In order to prepare a request for more data, you might want to be able to show that even though you were sent thousands of pages, most of those pages contained redactions and so were effectively useless. . In the ideal vision of this tool and how it would work, you could run a programme out of a particular directory and it would tell you how many pages (and what proportion) of your PDF files were redacted. . Getting the Data . The first thing I did to gather my data was to download the PDF documents available on this site. I knew that they contained examples of redactions in FOIA documents. I used Automator to split the PDF files up into individual images.3 My Automator script did some downsampling of the images as part of the process, so the images were resized to something that wasn’t prohibitively large to use for training. . Note that this stage and the next was done on my local machine. A CPU was enough for my purposes at this point, though probably I’ll want to eventually port the entire process over to a single cloud machine to handle things end-to-end. . At the end of the splitting-and-resizing process, I had a little over 67,000 images (of individual pages) to train with. . Labelling the images with Prodigy . I had used Explosion.ai’s Prodigy data labelling tool in the past and so already had a license. The interface is clean and everything works pretty much as you’d hope. I had some teething issues getting it all working, but Prodigy co-creator Ines helped me work through those queries and I was up and running pretty quickly. . . It took about three hours to annotate some 4600+ images. Then I could export a .jsonl file that contained the individual annotations for whether a particular image contained a redaction or not: . . From that point it was pretty trivial to parse the file (using the json-lines package), and to resize the images down further in order to separate redacted from unredacted: . import json_lines from PIL import Image from pathlib import Path def save_resized_image_file(location_path): basewidth = 800 img = Image.open(record[&#39;image&#39;]) wpercent = (basewidth / float(img.size[0])) hsize = int((float(img.size[1]) * float(wpercent))) img = img.resize((basewidth, hsize), Image.ANTIALIAS) img.save(location_path) path = &#39;/my_projects_directory/redaction-model&#39; redacted_path = path + &quot;/redaction_training_data/&quot; + &quot;redacted&quot; unredacted_path = path + &quot;/redaction_training_data/&quot; + &quot;unredacted&quot; with open(path + &quot;/&quot; + &quot;annotations.jsonl&quot;, &quot;rb&quot;) as f: for record in json_lines.reader(f): if record[&quot;answer&quot;] == &quot;accept&quot;: save_resized_image_file(Path(redacted_path + &quot;/&quot; + record[&#39;meta&#39;][&#39;file&#39;])) else: save_resized_image_file(Path(unredacted_path + &quot;/&quot; + record[&#39;meta&#39;][&#39;file&#39;])) . Transferring the data to Paperspace with magic-wormhole . Once I had the two directories filled with the two sets of images, I zipped them up since I knew I’d want to use them on a GPU-enabled computer. . I used magic-wormhole to transfer the files over to my Paperspace Gradient machine. The files were only about 400MB in size so it took less than a minute to transfer the data. . Again, ideally I wouldn’t have this step of doing things locally first. I could certainly have done everything on the Paperspace machine from the very start, but it would have taken a bit of extra time to figure out how to process the data programatically. Moreover if I was using JupyterLab I could then use Prodigy from within my notebooks. . Using the labelled data in our training . The process of ingesting all our data (labels and raw images) is pretty easy thanks to the fastai library’s convenience classes and layered structure. We’re using the DataBlock class instead of ImageDataLoaders for extra flexibility. . path = Path(&#39;redaction_training_data&#39;) foia_documents = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(224)) dls = foia_documents.dataloaders(path) foia_documents = foia_documents.new( item_tfms=Resize(224, method=&#39;pad&#39;, pad_mode=&#39;reflection&#39;), batch_tfms=aug_transforms(max_zoom=1)) dls = foia_documents.dataloaders(path) learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(10) . The images get resized to 224x224 pixels, since this is the size that the resnet architecture expects. Since we have a good deal of labelled data, I’m comfortable using 80% of that data to train the model and the remaining 20% against which to validate. . I train it for 10 epochs as I don’t appear to reach a point where I’m overfitting. As you can see from this image, we reach an accuracy of around 96%. . . Experimenting with augmentations . Initially I had been using the RandomResizedCrop transformation on the data, but I was reminded by someone in our group (Jason) that cropping or zooming our images wouldn’t be useful since it is possible that both of those transformations would remove the small part of the image where a redaction was to be found. . In the end, I went with some settings that made sure we weren’t zooming into images or rotating them such that parts would be missing. I think there’s probably more I could squeeze out of the documentation here, particularly so that I’m not limiting myself too much in the arguments that I’m passing in. . I chose the pad method with the reflection mode since this seemed to give the best results. The zeros mode was too close to an actual redaction (i.e. a black box on the image) so I ruled that out pretty early on. . Experimenting with different architectures . The course mentions that architectures with more layers do exist. I saw that the next step up from resnet18 was resnet50. I’m certainly in the territory where I’m just turning knobs in the hope of seeing some kind of result, but I thought it was maybe worth a comparison. . The danger with having more layers (and thus more parameters) is that the model is more likely to overfit. The training process also takes much longer to execute: 44 seconds per epoch compared to 21 seconds with resnet18. It didn’t seem to measurably improve the accuracy. The best results I was able to get were still around 95%, give or take a percent or two. It seems that the real improvements are to be found in the pre-processing or augmentation stage, rather than from choosing an architecture with more layers. . Hosting the model with MyBinder . Chapter two of the course book goes into a decent amount of detail of some of the tradeoffs and issues around model deployment. Part of the exercise is to not only train a model on your own data, but go through the steps to get the model hosted online. . Using MyBinder and the voila library, alongside instructions from the book and the forums, I managed to get my model deployed. If you visit this address you’ll see an interface where you should first upload an image — i.e. a screenshot of a document. When you click ‘classify’, you’ll then see a prediction of whether the image is redacted or not, as well as the confidence/probability that that prediction is true. . . Next steps . I’m at the point in the course where I know enough to be dangerous (i.e. train models), but I don’t know how to improve them from here. Some ideas I had for ways to improve the model’s accuracy: . better augmentation choices — it’s possible that I’ve misconfigured some argument or made the wrong choices in which augmentations should be applied. | more labelled data — this one is pretty easy to fix, but I probably shouldn’t continue down this route unless I know it’s really going to help. I’m not in a position right now to be able to judge how much it’d help me. | different redaction types — currently I have a single ‘redacted’ vs ‘unredacted’ category choice, but in reality there are several different types of redaction in the data set: some have handwritten redactions, others are square computerised boxes, and there are a couple of other types as well. I wonder whether I should train the model to recognise the different types, and then to combine those together as a ‘redacted’ set of categories. (I may be thinking about this wrong). | . Otherwise and for now, I’m happy with where I managed to reach with this model. I have some other ideas for how to keep going with exploring this data set. For example, even better than a slightly dumb classification model would be to have a segmentation model that was able to determine what percentage of the pixels or total area of the page that were redacted. With a reasonably accurate segmentation model of that kind, we’d then be able to provide really interesting metrics on what percentage of the information provided was redacted. . I will probably also want to go back and add in the earlier processing steps into the notebook so that things are much closer to being an ‘end-to-end’ solution. . Footnotes . You can find our thread in the fastai forum here. &#8617; . | Other countries have variations of this law, like this from the United Kingdom. &#8617; . | I realise that there is a programatic way to do this. At this early stage in the project, I was more eager to get going with the labelling, so I took the easy path by using Automator. &#8617; . |",
            "url": "https://mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html",
            "relUrl": "/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html",
            "date": " • Sep 6, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Alex. . I am a software engineer based in London, UK. I recently built Ekko, an open-source framework allowing developers to easily add realtime infrastructure and in-transit message processing to web applications. . I have multiple years of experience in the Ruby and JavaScript ecosystems and am comfortable working with Go, PostgreSQL, AWS cloud infrastructure and Docker. . I have a PhD in History and authored several books based on my research work in Afghanistan. . I have a long-standing blog that I will combine with this one at some point, but for now I intend to post technical posts here. .",
          "url": "https://mlops.systems/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mlops.systems/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}