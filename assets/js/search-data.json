{
  
    
        "post0": {
            "title": "Using J for simple calculation",
            "content": "I‚Äôm curious to see if I can improve my command of some J idioms and syntax while working on my mathematics study at the Open University. People sometimes (pejoratively) say that J is just a calculator on steroids. I happen to need a calculator from time to time doing MU123, so I figure I‚Äôll just learn as I go. . I wrote last week about orders of precedence and notation in J (and for mathematics in general) so I won‚Äôt repeat myself here. The summary of that all is that J evaluates from right to left in the order that expressions are encountered. . Some ultra basics . Negative numbers are denoted by the underscore (_) symbol: . 3-4 _1 . Note that there can‚Äôt be any space in between the underscore and the number that‚Äôs negative. . Multiplication is handled by the asterisk (*) symbol, much like elsewhere in the world of computers: . 3*5 15 . Division is handled by the percentage (%) symbol: . 15 % 5 3 . While we‚Äôre here at simple operators, we can specify power operations with the caret: . 3^2 9 . The square root is calculated by using %: as in the following calculation: . %:9 3 . Fractions and Rational Numbers . In J, the letter r is used for the notation of rational numbers (i.e. the numbers which represent a ratio of the two integers). For example, to represent two-thirds, you would write 2r3. J is smart about interactions between rationals (i.e. fractions), so you can use them in calculations: . 1r2 * 6r4 3r4 . If you want to turn a decimal number into a fraction / rational number, use x: as in the following example: . x:0.3 3r10 x:0.97 97r100 . Assigning variables . Variables and algebra hasn‚Äôt come up too much so far in MU123, but as a sneak peek, in J you assign variables using =. as in: . a=.4 b=.0.6 c=._0.3 . Open Questions . I‚Äôm still looking for the J way to do rounding (i.e. decimal places and significant figures). I did see one example on the J wiki which went like this: . R=: &lt;.@(0.5&amp;+) R 9.5 10 . So that rounds numbers up. On the first line some kind of function is defined and then on the second line we‚Äôre applying it to the number 9.5 which rounds up to 10. I assume the logic of that function is ‚Äòround down to the nearest integer if the decimal value is less than 0.5, but round up if it‚Äôs greater than 0.5‚Äô. . I‚Äôm also trying to figure out how exactly to use J in Jupyter notebooks. I use fastpages for this blog and one feature is that you can publish your notebooks and they get converted into blog pages. If I had a way to write J-backed notebooks, that‚Äôd be great. (At the moment, it seems these files are the main options and guides.) . Acknowledgements . I learned a lot from the ‚ÄòNumbers‚Äô section in Learning J while figuring out how to do these things. .",
            "url": "https://mlops.systems/j/mathematics/q31/mu123/2022/10/21/maths-using-j.html",
            "relUrl": "/j/mathematics/q31/mu123/2022/10/21/maths-using-j.html",
            "date": " ‚Ä¢ Oct 21, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Deep learning tricks all the way down, with a bit of mathematics for good measure",
            "content": "(This is part of a series of blog posts relating to and responding to the live FastAI course (part 2) being taught October-December 2022. To read others, see the ones listed for the ‚Äòparttwo‚Äô tag.) . Much awaited and anticipated, the second part of the FastAI course is being taught live again. If part one is about getting solid foundations and learning how to get going in a practical/useful way, part two is about approaching things from the foundations but with research or ‚Äòimpractical‚Äô questions kept in mind. The backdrop of the current iteration is the breakthroughs happening in the world of generative computer vision models like Stable Diffusion, which we‚Äôll explore and deconstruct (and reconstruct?!) over the coming weeks. . Diving into the details of how things work means that along the way we‚Äôre much more likely to encounter (legitimate) specialist vocabulary and techniques as well as a decent dose of jargon. Whereas bringing up the intricacies of particular algorithms, architectures or mathematical methods was unnecessary during part one, it seems like part two is a little bit more of a venue for that kind of material. I will use these blogs as a way of reviewing materials and concepts introduced during the lectures as well as keeping track of the big questions I have. . In this blog, in particular, I‚Äôll keep a glossary at the bottom for some new terms which were introduced. I may repeat this for subsequent blog reviews, depending on what‚Äôs covered in those lessons. I‚Äôll also keep a section containing new mathematical symbols that are introduced. (This blog mainly relates to the core lecture given during week 1. I‚Äôll update it later with some small extras that came up from the 9A and 9B videos, or expand those into separate posts on their own.) . Stable Diffusion isn‚Äôt, in itself, a model that I‚Äôm especially interested in, except insofar as it teaches me fundamental principles about the craft of deep learning or about doing research in this field. As such, my plan and current intention is to stick to documenting core mental models or bigger-picture lessons that I‚Äôm taking away from the lessons rather than each individual step that Jeremy made along the way. (This seems to be the motivation behind including it in the course at all. Stable Diffusion touches so many topics (big and small) and getting to grips with this one thing will help understand many other things about machine learning and the world of research.) . üí¨üåÑ Stable Diffusion 101 . If you‚Äôve been on the internet at all during the past 6-12 months, you‚Äôll almost certainly have been exposed to examples of images that have been generated using techniques grounded in deep learning. Here is one that I generated just now: . . These images are generated by passing in a prompt into the model which then uses that to come up with something that represents the text you passed in. (Shoutout to the creators and maintainers of the tremendously useful diffusion-nbs notebooks.) The interface between the text and the image is still not as seamless as might be hoped, and a discipline of ‚Äòprompt engineering‚Äô has grown around finding the best ways to coax certain kinds of images out of the model. (See this book to learn more about what works with DALL-E 2, for example. Or visit Lexica to search images based on the prompts that were used to create them.) . There are some obvious interim questions that result from the existence of such models and their outputs, notably what this allows in terms of creativity and how it might transform the kinds of tools we use for image editing and creation. The advances are certainly impressive, but outside the field, (and without being moving on too quickly from the unquestioned achievement of these types of models) what does it mean for the rest of deep learning? . The first thing that is maybe interesting for the field is the way that these models are multi-modal, or in other words they aren‚Äôt stuck in the silo of being text-only, or image-only, and so on. We are able to translate (to a greater or lesser degree) between language and images with these models, which seems like it might open up a whole universe of interactions and behaviours that are interesting to explore. . At a very (very) high level, what‚Äôs going on with stable diffusion is that it starts with generating an image that is more or less purely random noise, and then (with subsequent iterations) slowly reveals an image and coherence that was contained within the random noise. Similar to how Michelangelo said of sculpture (‚ÄúIt is already there, I just have to chisel away the superfluous material‚Äù), what happens here is that we have to remove the superfluous noise. . üõ† Core Takeaways: How does it work? . Those of you not taking the course live will have to wait a few months for the lectures to be released and in any case I don‚Äôt want to parrot the order and progression of how Jeremy explained how Stable Diffusion works. With that said, I was pleasantly surprised by how much I was able to follow along given what is a fairly technically involved topic. (Note to self: the fundamentals continue to be important!) . Fundamentals still count . Even though there are a hundred and one small innovations and technologies which make something like Stable Diffusion possible, in the end we‚Äôre still dealing with Deep Learning and we‚Äôre still dealing with finding ways of converting things into numbers which can be used by machines to update weights by way of evaluating loss functions. So many of the individual pieces that make up how you build something like Stable Diffusion amount to: . figure out how to get this non-number-like thing into a numeric representation (ideally a vector of some kind) | do all the usual deep learning things that we‚Äôve done a thousand times and that we know work | at the end, maybe find a way to convert the numeric representation that our model learned into some kind of form that is useful to us | . Obviously the details are important and nobody is creating magical generative art with this very high-level hand-wavy explanation, but for someone at the earlier end of their journey into deep learning it is reassuring that the fundamentals continue to have relevance and that those mental models remain useful as a way of thinking about new developments. . The tricks are the way . The other pleasant surprise was the enduring relevance of ‚Äòtricks‚Äô. In chapter one of the FastAI book, Jeremy &amp; Sylvain showcase a number of examples where clever approaches are taken to solve problems with Deep Learning: . a malware classification program is made by converting malware code into an image which is used to train a model | a fraud detection algorithm is trained by converting images of computer mouse movements | ‚Ä¶and so on | . Even amongst the Delft FastAI study group, Kurian trained a classifier to detect genre in music samples using a similar method (i.e. using images as an intermediary form for the samples which were used in training). The book emphasises: . ‚ÄúIn general, you‚Äôll find that a small number of general approaches in deep learning can go a long way, if you‚Äôre a bit creative in how you represent your data! You shouldn‚Äôt think of approaches like the ones described here as ‚Äúhacky workarounds,‚Äù because actually they often (as here) beat previously state-of-the-art results. These really are the right ways to think about these problem domains.‚Äù . Most of these ‚Äòtricks‚Äô seem to relate to either performance improvements (i.e. how can we get this training to happen faster, or with fewer compute needs) or ways of getting your problem domain into a form that we can use deep learning techniques on them. In the case of Stable Diffusion, one of the problems we have to address is how to work in this multi-modal manner, where text is used to represent a particular idea (which in turn needs a vector/numeric representation) but where we also want to represent that same idea in image form. . At the same time, we have the whole autoencoder part of the story ‚Äî whereby we use an encoder to turn a large image into a (smaller-sized) latent representation which can be used in training, and then we use a decoder to turn a noisy latent into a full-sized image ‚Äî which seems to mainly be about making the training process more efficient. . Each of these techniques come with their own complexities and histories, but it‚Äôs just notable to me how the story of the development of machine learning techniques seems somehow to be a succession of these small incremental innovations that progressively accrue. That‚Äôs not to say that there aren‚Äôt big breakthroughs in either understanding why things work the way they do, or in the more tactical method space, but it just seemed very apparent in the unpacking of Stable Diffusion that a great deal of creative stitching together of ideas had taken place. . The historian in me is fascinated by the different pathways that the field has explored, or the reasons why certain techniques emerged when they did, or how hardware improvements gave tried-and-rejected techniques a new lease of life, but I‚Äôm guessing that probably doesn‚Äôt help much with the work of research. . üí™ What happens when we train the diffusion model . A diffusion model is a neural network that we train. The way it works is that it removes noise from an image (passed in as input along with a text prompt) such that the output more closely resembles the prompt. When we are training our network, we pass in the vectorised words along with the latent forms of the images (since those are much smaller file sizes and thus faster / more efficient to train). We use the encoder to get a latent representation of the image that we use for training. . For the text caption, we want a way to represent the association of images with text captions in vector space. In other words, if there are various phrases that all represent more or less the same image if you were to translate those phrases into an image, then those should be similar when represented as a vector. The technique or trick for this is to use ‚Äòcontrastive loss‚Äô, a particular kind of loss function which allows us to calculate the relative similarity of two vectors. This contrastive loss is what gives us the first two letters of ‚ÄòCLIP‚Äô, a neural network developed by OpenAI. . The CLIP model takes some text and outputs an embedding, i.e. some features in vector form that our unet can use for training along with the images in their latent representation form. . üé® What happens when we generate an image . When generating our image we can use the neural network we trained to progressively remove noise from our candidate image. We start off with a more or less completely noisy image, then apply the unet to it and it returns the noise that it calculates is sitting on top of a latent representation that approximates the vectorised version of our prompt. We take a fraction of that, remove it, and repeat a few times. (Currently that can take as many as 50 iterations before we reach a really impressive image, but new techniques are in review which would dramatically reduce the need for so many iterations.) . Note that it is during the inference stage where we need the decoder part of our (VAE) encoder to turn a latent tensor representation of an image into a fully-fledged large picture. . üé∫ How to play &amp; practice for part II . I also wanted to briefly take a second to reflect on what might be useful as ways to get practically involved during the coming weeks. In part one, the instruction was fairly simple: ‚Äútrain lots of models‚Äù. In part two, the practicality is initially still there, it seems, but there will be other areas of emphasis. The things that seem to make sense to me currently are: . continue to blog as a way of reflecting and developing my understanding | understand and digest the core concepts that are introduced | whenever the ‚Äòcode everything from scratch‚Äô part of the course starts, make sure to at least attempt this on my own alongside whatever is being showcased in lectures | discuss areas where concepts are unclear during the weekly Delft FastAI study group calls that I organise | . I suspect that the discipline of the coding will be most instructive, once we get to it, though by extension probably also one that comes with the most struggle. . Following a session of the Delft Study Group, I gathered some more suggestions for how to get the most out of this part 2: . get hands-on as much as possible | ‚Äòthe details matter‚Äô and try to go above and beyond with the course and you‚Äôll be rewarded | blog and explain what you‚Äôre learning | answer questions on the forums as a way of cementing your learning | don‚Äôt let yourself get blocked by ideas that you don‚Äôt understand along the way. Keep following along with the course and more likely than not these things will clear themselves up | . üìñ Glossary of Core Terms . (Listed alphabetically, not in the order of exposition. Also these reflect my current understanding which is not always complete, so I‚Äôll keep this updated as my understanding grows.) . Analytic derivatives ‚Äî This is a faster way of calculating the gradients for our image, such that we calculate the whole set at once. This is what PyTorch uses under the hood in conjunction with a GPU to speed up the training process. | Autoencoder (model) ‚Äî This is a combination of an encoder and a decoder. This model is a neural network with a series of layers that progressively ‚Äòcompress‚Äô an image (through convolutions) until the point where it is much smaller. At this point the representation is called a ‚Äòlatent‚Äô. Then (in the full autoencoder) the image is progressively scaled back up into its full version. | CLIP ‚Äî This is a model that turns text into images, powered by ‚Äòcontrastive loss‚Äô. It was developed by OpenAI. | Contrastive loss ‚Äî This is a loss function that allows us to compare the similarity of two vectors. We multiply them together and sum up all the values. (This process is also known as the dot product.) If the two vectors are similar, we would expect the number to be large. Contrastive loss is used in CLIP. | Convolutional layer ‚Äî This is a key part of computer vision and it is a way of representing images at different resolutions. Images are either scaled up or down through a convolutional layer (which seems to be some way of averaging the values of an image). Convolutional is the C in CNN. | Decoder ‚Äî This is the part of an autoencoder that takes a latent representation and scales it back up (i.e. ‚Äòdecompresses‚Äô it) to its full representation. | Differential equations ‚Äî This is a part of mathematics which is really important for Stable Diffusion and whose language forms the context and backdrop for discussions around this technique, but it is a fairly different set of vocabulary from what we use for deep learning. | Diffusion sampler ‚Äî This is the part of the process which relates to adding or subtracting noise from an image. | Dot product ‚Äî This is the process by which we multiply two vectors by each other and sum up the values. It is used when we are calculating contrastive loss, but it is a common linear algebra calculation. | Embedding ‚Äî This is a representation of something as a vector, particularly useful for deep learning. In particularly, it‚Äôs useful for areas like text where we might, for example have semantic fields that we want to represent as being similar to each other, but we need to do so in such a way as is comprehensible and processable by a machine. Embeddings allow us to do this in vector space. | Encoder ‚Äî This is one part of an autoencoder that takes a full sized image and passes it through a series of convolutions such that at the end we have a significantly reduced tensor that is known as a latent. | Finite differentiation ‚Äî This is one way of calculating the gradients for our image, but it is done pixel by pixel. It is quite slow. (Contrast with analytic derivative.) | Guidance + guidance scale ‚Äî This is the prompt that we pass into our Stable Diffusion model. The guidance scale is what we can pass in to our generation function call to specify how much we want the prompt to be strictly followed. | Latent representation(s) ‚Äî This is the intermediate product in the middle of an autoencoder. It is what is produced by an encoder, and it is what is consumed by a decoder. It is sort of a compressed version of all the important pieces of information relating to a particular image, for example. | Momentum ‚Äî This is a technique used by optimizers in which if we increase the same parameters (or weights) several times in a row, then it seems likely that we‚Äôll do that again so we can increase the learning rate for those parameters so that we don‚Äôt have to make so many iterations. | Negative prompts ‚Äî You can pass in a negative prompt along with your prompt and it is a way somehow of ensuring that the resulting image does not correspond to whatever was in the negative prompt. (Think of it as ‚Äòsubtracting‚Äô from the main prompt, which seems to be what is going on under the hood, in vector space). | Noise ‚Äî Noise is random data, with no meaning as such. It is important in the world of Stable Diffusion because the work of generating the image is the work of removing noise. | Perceptual loss ‚Äî This is another kind of loss function that may play a role in Stable Diffusion going forward int he course. | Pipeline ‚Äî This is the concept that is used by the HuggingFace Diffusers library, out of which our images are generated. | Score function ‚Äî This is another way of stating the gradients for our image. I.e. the representation of what needs adjusting (and by how much) in order to remove noise from our image. | Step ‚Äî This is one iteration of the inference process. | Textual Inversion ‚Äî This is the process of creating a new embedding for a single specific concept or item. (i.e. the Indian watercolour example in the course) | Time step ‚Äî This is a concept from the way the original Stable Diffusion creators thought about things. It represents a way to go from a value to an amount of noise that gets added to an image. | VAE ‚Äî This is the specific kind of autoencoder used in Stable Diffusion. | . ‚úñÔ∏è‚ûó New Mathematical Symbols . ‚àë ‚Äî means to sum up | ‚àá ‚Äî we use this symbol instead of something else (d something?) in representing the gradient because we are talking about the gradients of many pixel values and not just a single one | Œ≤ ‚Äî (beta) ‚Äî used in relation to the time steps to represent the amount of noise or variance. I think this is used instead of the letter œÉ (sigma), but I might be wrong on that. | . ‚ùì Enduring Questions . Some of the questions which I have in my mind following the class include: . These ‚Äògenerative‚Äô models intersect with the field of art and creativity, at least nominally, but to what extent can we even say that they are generating something versus simply repeating things that they‚Äôve already seen? (see also, the ‚Äòstochastic parrots‚Äô paper) | SKILL: are there tricks or best practices when deciphering jargon-rich papers down to their core and, in doing so, being able to see the parts of the paper that are new (versus the parts that are just standard practice)? | SKILL: what does it mean to be ‚Äòimpractical‚Äô i.e. do research in this field? What is involved and how is it generally or most usefully done? | What are the useful or fundamental innovations involved in Stable Diffusion? | What are the parts of ML/DL and/or Stable Diffusion that we do because of time or hardware or cost limitations as opposed to the things that we do because they are the right way to approach this particular problem? (provoked by the whole detour down into autoencoders that seems mainly to be there to improve iteration speed.) | What‚Äôs the bigger takeaway for the field as a whole? In other words, what things can we think about doing now with these new techniques? | Why does the whole ‚Äòtime steps‚Äô conversion stage happen at all? i.e. why can‚Äôt we just choose a random number to represent how much or little noise we apply to an image for our training data. | The autoencoder / compression step seems like an amazing technique all to its own. Is it really lossless, or is some information lost along the way? | .",
            "url": "https://mlops.systems/computervision/fastai/parttwo/2022/10/17/fastai-part-2-lesson-9-stable-diffusion.html",
            "relUrl": "/computervision/fastai/parttwo/2022/10/17/fastai-part-2-lesson-9-stable-diffusion.html",
            "date": " ‚Ä¢ Oct 17, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Avoiding BIDMAS, or how J does notation",
            "content": "One of the topics that comes up early on in Open University‚Äôs MU123 mathematics course is precedence. Those who grew up in English-speaking countries will probably know this as BODMAS or BIDMAS. The order of precedence for execution of a mathematical expression gives us an idea for how to resolve expressions that don‚Äôt make sense. For example, 3 + 1 x 4 can either amount to 7 or 16, depending on how when you do the multiplication step. . Brackets are one way to make things more precise, and that‚Äôs probably why they‚Äôre the B in BIDMAS and that they go first. We could write 3 + (1 x 4) to make it really clear that we wanted the 1 x 4 sub-expression to be evaluated first. . With the rules of precedence, we technically wouldn‚Äôt need to add in any brackets because we could (likely) assume that people would follow the standard rules and they would know that we have to evaluate multiplications before we evaluate the additions. So we have a way, but it maybe feels a bit unsatisfactory. . Some languages or domains, however, have notational rules which don‚Äôt rely on a meta-schema of precedence rules like BIDMAS to tell you which expressions should be evaluated first. Instead, the order is determined in other ways, with the option of brackets when needed. . Several of the languages in the APL family, like J, simply evaluate from right to left in the order that expressions are encountered. See this example in J: . 3 + 1 * 4 7 4 * 3 + 1 16 . The order in which the expressions are evaluated determines the answer. . Thinking and reading a bit about these orders of precedence brought me to learn a bit about other traditions of mathematical notation. The one most used and that you‚Äôll be most familiar with is called infix notation i.e. 3 + 4. . Prefix notation (AKA Polish notation) is when we write + 3 4 (to the same end) and postfix notation (AKA reverse Polish notation) is when we write 3 4 +. (The Polish part relates back to Jan ≈Åukasiewicz, who invented it in 1924.) These kinds of notation are used in Lisp and Clojure, for example. . Why would you want to use a notation style like this? Some possible reasons: . the operands in the expressions can handle arbitrary numbers of arguments, making them more efficient to write | they are consistent with the syntax used for functions in computer programming (which can be easier to get your mind round) | they‚Äôre clearer to read and (mostly) unambiguous, unlike infix notation which (see above) requires a whole order of precedence if you‚Äôre not using brackets | there‚Äôs no confusion or need for precedence rules | it‚Äôs faster for a machine to evaluate, since the way expressions are formulated is much easier to translate into computer code. | . So there you go. I‚Äôm unclear whether there are more fundamental benefits to living in the world of post-/prefix notation, and perhaps it‚Äôs a little like the people who argue that we‚Äôd all be better off if we lived in a base-12 world instead of base-10, but that‚Äôs beside the point for now. . I‚Äôll try to share some more diversions from my mathematics study along the way, hopefully powered by J which I‚Äôm trying to get back into. .",
            "url": "https://mlops.systems/j/mathematics/mu123/q31/notation/2022/10/16/notational-precedence.html",
            "relUrl": "/j/mathematics/mu123/q31/notation/2022/10/16/notational-precedence.html",
            "date": " ‚Ä¢ Oct 16, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Storing Bytes: what data serialisation is and why you need it for machine learning",
            "content": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.) . Serialisation and deserialisation. I ran headfirst into these two words on my first day in my new job. From the way my colleagues discussed them, it seemed like this was something I should have learned from a computer science degree; foundational concepts with practical applications throughout most places that computers touched. . A few months in, I‚Äôve come to appreciate a little more about what the underlying concept is about as well as some of the reasons why it remains both relevant and something that pops up regularly. I‚Äôll begin by setting out some of this context before showing an example of where I encountered it recently in my own project. By the end, you‚Äôll understand why this is such an important (and practical) concept and why you‚Äôll encounter it a lot while doing machine learning. . üî¢ The Basics . In the common definition, serialisation is the process by which you convert something into a sequence of bytes, and deserialisation is when you convert the other way (i.e. from bytes). In some domains it is also known as marshalling or pickling. . This commonly is encountered when you need to store some data on disk (i.e. not or no longer in memory). Perhaps you need some kind of permanent storage of that data, or you need to make the data available to another process. The process through which you transform the data (from something that is comprehensible to whatever environment or language you‚Äôre working on) is serialisation. . To give another example, in a language like Python we often think in and deal through a series of ‚Äòobjects‚Äô: think dictionaries or even classes in an OOP context. In order to save this to disk, we have to convert it to some other format that firstly is in some format that is stable when saved as a file. We might want to send that data across the network, or have it opened by a different process or a programme running in a different language. Serialisation is the process by which something context and perhaps language-specific gets transformed into this universal substrate (i.e. a sequence of bytes). . üçè Common ways to serialise data in Python . In the past, pickle was a commonly-used way of making this conversion. It has a lot of shortcomings, two of which sit at the top of the list: . there isn‚Äôt (as far as I‚Äôm aware) much interoperability for objects that are serialised with pickle. If you want to load an object that has been ‚Äòpickled‚Äô, the entity doing the ‚Äòunpickling‚Äô will have to be running the exact same version of Python as the one that did the pickling. (If I‚Äôm not mistaken, there might even be some cross platform interoperability issues as well.) | security concerns are serious when it comes to pickle: when you load(...) some pickled object, this will run whatever code is inside with the assumption that it is ‚Äòtrusted‚Äô. As such, it is unsuitable for use with untrusted data and generally people tend to turn their nose at pickle. (If you do have to interact with some pickled data, pickletools is a handy tool that allows you to inspect and interact with the file without running the arbitrary code packaged inside. While we‚Äôre at the library recommendations, it‚Äôs also worth checking out fickling which overlaps in functionality somewhat.) | . JSON has become a commonly-used format for serialising data (or its cousin JSONL, for too-much-to-load-into-memory-at-once data). This is a common format with many uses, but it does come with a serious shortcoming which is that it only supports certain data types. If you‚Äôre saving some custom object of your own creation, you‚Äôll first need to convert that into a format that can be transformed into a JSON object/file. If you don‚Äôt, then your object will not be able to be rehydrated from the on-disk representation. . Note that the Python pickle module serialises data into a binary format, whereas the json module converts it into a text format (i.e. readable and comprehensible to someone browsing files or displaying their contents with something like cat). Moreover, pickle does handle many (most?) objects and types that you can throw at it, though with all the caveats mentioned above. . I haven‚Äôt explored it at all, but while reading a bit about this area I was consistently pointed to Google‚Äôs Protobuf format / library which is another way to serialise structured data. I am unable to properly evaluate the extent to which this is an improvement on existing protocols. . üîê Serialisation and deserialisation in Machine Learning . I mentioned earlier that this concept and operation was something that I confronted more or less on my first day working in my new job. (We build an open-source framework that supports someone working to build and deploy machine learning models.) In order to understand why this is so important, a small detour showing a basic example of a ZenML pipeline is necessary. What follows is an extremely simple example showcasing how pipelines are composed of steps, and how those are in turn run: . from zenml.steps import step from zenml.pipelines import pipeline @step def read_integer() -&gt; int: return 3 @pipeline def basic_pipeline(read_integer) -&gt; None: read_integer() basic_pipeline(read_integer=read_integer()).run() . Pipelines are constructed out of a series of steps. The steps are defined with an @step decorator, and pipeline definitions are composed in a similar way. Finally, at the end we specify which steps correspond to which parts of the pipeline definition and then call the run() method to execute our pipeline. . You‚Äôll also note the presence of some type annotations as part of how we define our step and pipeline. These are required, and while they may seem simplistic and unnecessary at the moment, later on they will make things much clearer. . Our pipeline isn‚Äôt doing much at the moment, you might think. Behind the scenes, however, ZenML is doing a lot of legwork: . storing the outputs (and inputs, though there aren‚Äôt any in this basic example) of all steps | caching those output values or objects, such that if the code doesn‚Äôt change then we should just retrieve the cached value. | validating and checking the types of values that get returned so that we can be sure our code is returning what we hope / think it should be returning. | . Moreover, it does all this in a way that all this intermediary state is stored on disk and versioned. If you update your pipeline steps then rerun it, ZenML will save the new outputs such that you can go back and inspect where data came from and so on. . In order to save all these objects on disk, however, and to bring this story full-circle, ZenML serialises the data when saving the artifacts from pipeline runs, and deserialises that data when those artifacts are needed (by the cache, for example, or when you want to access a step output once your pipeline has completed its run). We call this part of the process ‚Äòmaterialisation‚Äô. (There‚Äôs more in our docs on materialisation here, and if you‚Äôre searching, be sure to search with a ‚Äòz‚Äô and not an ‚Äòs‚Äô, coz America.) . üõ† A basic custom materializer . For most kinds of ‚Äònormal‚Äô Python objects, this is no problem at all. But as we saw above, if we‚Äôre going to be able to reconstruct and rehydrate an object from a static sequence of bytes, we‚Äôre going to need to do a bit more to make this happen. Within ZenML this means that if you have some special kind of object or type, you‚Äôll need to define a ‚Äòcustom materialiser‚Äô; this is code that defines how ZenML should serialise and deserialise the objects that you want to be stored as state on disk. . To give you a sense of what this will look like, here‚Äôs our code from above but updated a little to fit this new scenario: . import os from typing import Type from zenml.artifacts import DataArtifact from zenml.io import fileio from zenml.materializers.base_materializer import BaseMaterializer from zenml.pipelines import pipeline from zenml.steps import step class MyCustomObject: def __init__(self, name): self.name = name class MyCustomMaterializer(BaseMaterializer): ASSOCIATED_TYPES = (MyCustomObject,) ASSOCIATED_ARTIFACT_TYPES = (DataArtifact,) def handle_input(self, data_type: Type[MyCustomObject]) -&gt; MyCustomObject: &quot;&quot;&quot;Read from artifact store&quot;&quot;&quot; super().handle_input(data_type) with fileio.open(os.path.join(self.artifact.uri, &quot;data.txt&quot;), &quot;r&quot;) as f: name = f.read() return MyCustomObject(name=name) def handle_return(self, my_obj: MyCustomObject) -&gt; None: &quot;&quot;&quot;Write to artifact store&quot;&quot;&quot; super().handle_return(my_obj) with fileio.open(os.path.join(self.artifact.uri, &quot;data.txt&quot;), &quot;w&quot;) as f: f.write(my_obj.name) @step def read_custom_object() -&gt; MyCustomObject: return MyCustomObject(&quot;aria&quot;) @pipeline def basic_pipeline(read_custom_object) -&gt; None: read_custom_object() basic_pipeline( read_custom_object=read_custom_object().with_return_materializers( MyCustomMaterializer ) ).run() . You‚Äôll notice a new piece of code which defines the MyCustomMaterializer class. This is subclassed off our BaseMaterializer class and we just have to define two methods, one that handles how to serialise or save the data to disk, and the other that handles how to deserialise or rehydrate the objects/data from disk. We add a special .with_return_materializers call when we run the pipeline; this lets ZenML that when we encounter a weird type of object, it can go ahead and use our custom defined materialiser to handle it. . I hope you‚Äôll agree that this stuff isn‚Äôt too hard to grok, and while the precise steps of how you implement all this might take a bit of getting used to, it‚Äôs conceptually not too hard once you understand the foundations of what you‚Äôre doing. It took me longer than I‚Äôm proud to admit to really understand the elegance of this way of doing things, but all these little pieces add up and you can then go off and use them in your real-life projects. . üïµÔ∏è Materialisation in practice: IceVision and Custom Objects . Case in point: my object detection pipeline. I took a bit of a break over the summer, but now I‚Äôm back and working to get my pipeline production-ready. Defining the basic steps of my pipeline were fairly easy; I‚Äôve already described that in my last blog post. . The moment I started defining my pipeline in code, I immediately hit a whole array of non-standard objects. My data loading steps returned IceVision-specific parsers custom to COCO BBoxes and my training step returned a collection of various custom objects combining code with the trained model parameters. (Note: for some common use cases like training with raw PyTorch or Tensorflow etc, ZenML has defined many standard materialisers already to get you going quickly.) I realised that I‚Äôd have to define custom materialisers to handle these different inputs and outputs. . Some of this wasn‚Äôt trivial to implement. Sometimes you might get lucky and the library you work with has implemented some handy features to help with serialisation and deserialisation. From what I can tell, this seems to be the case when saving models with PyTorch, for example. But for the rest it‚Äôs often less clear what need to happen and why code works in the way it does. To save the IceVision RecordCollection object, for example, I had to jump through some hoops, converting several sub levels of custom objects along the way, to make sure that my objects were serialisable. . Here‚Äôs the custom materialiser code responsible for handling those conversions and serialisation for the RecordCollection. (Think of RecordCollection just as a type of stored data, parsed and ready to use for model training.) . import os import pathlib from typing import Any, Dict, List, Type from icevision.all import * import srsly from zenml.artifacts import DataArtifact from zenml.io import fileio from zenml.materializers.base_materializer import BaseMaterializer class COCOMaterializerParser(Parser): def __init__(self, template_record, records: List[Dict[str, Any]]): super().__init__(template_record=self.template_record()) self.records = records self.class_map = ClassMap(records[0][&quot;common&quot;][&quot;classes&quot;]) print(self.class_map) def __iter__(self) -&gt; Any: yield from self.records def __len__(self) -&gt; int: return len(self.records) def record_id(self, o: Any) -&gt; Hashable: return o[&quot;common&quot;][&quot;filepath&quot;] def template_record(self) -&gt; BaseRecord: return BaseRecord( ( FilepathRecordComponent(), InstancesLabelsRecordComponent(), AreasRecordComponent(), IsCrowdsRecordComponent(), BBoxesRecordComponent(), ) ) def filepath(self, o) -&gt; Path: return pathlib.Path(o[&quot;common&quot;][&quot;filepath&quot;]) def img_size(self, o) -&gt; ImgSize: return ImgSize(width=o[&quot;common&quot;][&quot;width&quot;], height=o[&quot;common&quot;][&quot;height&quot;]) def labels_ids(self, o) -&gt; List[Hashable]: return o[&quot;detection&quot;][&quot;label_ids&quot;] def areas(self, o) -&gt; List[float]: return o[&quot;detection&quot;][&quot;areas&quot;] def iscrowds(self, o) -&gt; List[bool]: return o[&quot;detection&quot;][&quot;iscrowds&quot;] def bboxes(self, o) -&gt; List[BBox]: boxes = [] for bbox in o[&quot;detection&quot;][&quot;bboxes&quot;]: a, b, c, d = bbox new_bbox = BBox.from_xyxy(a, b, c, d) boxes.append(new_bbox) return boxes def parse_fields(self, o: Any, record: BaseRecord, is_new: bool): if is_new: record.set_filepath(self.filepath(o)) record.set_img_size(self.img_size(o)) record.detection.set_class_map(self.class_map) record.detection.add_areas(self.areas(o)) record.detection.add_iscrowds(self.iscrowds(o)) record.detection.add_bboxes(self.bboxes(o)) record.detection.add_labels(o[&quot;detection&quot;][&quot;labels&quot;]) def detection_record_collection_to_json(rcoll: RecordCollection) -&gt; str: indexes = list(rcoll._records) records = [rcoll._records[index] for index in indexes] classes = rcoll[0].detection.class_map.get_classes() dict_records = [record.as_dict() for record in records] for record in dict_records: record[&quot;common&quot;][&quot;filepath&quot;] = str(record[&quot;common&quot;][&quot;filepath&quot;]) bboxes = record[&quot;detection&quot;][&quot;bboxes&quot;] new_bboxes = [] for bbox in bboxes: a, b, c, d = bbox.xyxy new_bbox = [a, b, c, d] new_bboxes.append(new_bbox) record[&quot;detection&quot;][&quot;bboxes&quot;] = new_bboxes record[&quot;common&quot;][&quot;classes&quot;] = classes return srsly.json_dumps(dict_records) def detection_json_str_to_record_collection(records: str) -&gt; RecordCollection: r = srsly.json_loads(records) template_record = ObjectDetectionRecord() parser = COCOMaterializerParser(template_record, r) parsed_records, *_ = parser.parse(data_splitter=SingleSplitSplitter()) return parsed_records class COCOBBoxRecordCollectionMaterializer(BaseMaterializer): ASSOCIATED_TYPES = (RecordCollection,) ASSOCIATED_ARTIFACT_TYPES = (DataArtifact,) def handle_input(self, data_type: Type[RecordCollection]) -&gt; RecordCollection: &quot;&quot;&quot;Read from artifact store&quot;&quot;&quot; super().handle_input(data_type) with fileio.open( os.path.join(self.artifact.uri, DEFAULT_RECORD_COLLECTION), &quot;r&quot; ) as f: return detection_json_str_to_record_collection(f.read()) def handle_return(self, record_collection_obj: RecordCollection) -&gt; None: &quot;&quot;&quot;Write to artifact store&quot;&quot;&quot; super().handle_return(record_collection_obj) json_string = detection_record_collection_to_json(record_collection_obj) with fileio.open( os.path.join(self.artifact.uri, DEFAULT_RECORD_COLLECTION), &quot;w&quot; ) as f: f.write(json_string) . As you can see, there‚Äôs a decent amount going on here. In my custom materialiser, I have a detection_record_collection_to_json method that constructs the JSON representation of my custom RecordCollection object. I use Explosion‚Äôs handy srsly package for their forks + bundling together of various Python serialisation libraries. For the rest, that requires a bit more knowledge of how IceVision handles things like BBox objects and COCO Records under the hood, but you can get the idea that it‚Äôs not completely trivial. . ü•≥ Serialisation is for Everyone! . It‚Äôs also not completely impossible to implement either, though, lest you feel like I‚Äôm leaving you without hope. My aim with this article was to guide you to the point where you feel you can understand why serialisation is important and to know why you might well encounter it during your data science journey. The moment you need to do something just slightly longer-lasting than an ephemeral training run that is tracked nowhere and just lives in a Colab notebook, that‚Äôs when you‚Äôll hit serialisation. . Moreover, I showed how you can incrementally build up your pipelines with a tool like ZenML to handle lots of parts of the complexity that come with your modelling work. . [Image credit: Photo by fabio on Unsplash] .",
            "url": "https://mlops.systems/redactionmodel/computervision/mlops/python/tools/zenml/2022/09/07/serialisation.html",
            "relUrl": "/redactionmodel/computervision/mlops/python/tools/zenml/2022/09/07/serialisation.html",
            "date": " ‚Ä¢ Sep 7, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "It takes a tribe: how I'm thinking about putting my object detection model into production",
            "content": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.) . So we‚Äôve trained our model‚Ä¶ now what? Are we done? . If we‚Äôre just exercising our curiosity or have a tight focus for our work, then we might well be done with our work. Our responsibility within a larger team might only be for this specific step of training the model, for example. For many cases, however, we‚Äôre going to want to do something with our model, perhaps making it available to others via some web interface or an online API. . The next blog posts in this series will focus on the challenges and practicalities of getting a model ‚Äòin production‚Äô. I‚Äôll sidestep the nuances of exactly what we mean by ‚Äòin production‚Äô for the moment, but suffice it to say that the end goal is to have a way to not only make our model available to other consumers but also to deal with re-training and/or re-deploying new models to take the place of older or stale models. There is a whole spectrum of variety in this context that goes by the name ‚ÄúMLOps‚Äù. This blog post will try to provide a high level overview of some of the basic elements relevant to getting my redaction model into production. . . üö¶ TL;DR: What will you learn? . ü§ñ Deploying a model and automating everything on the way requires a decent number of steps and tools to do it properly. | üëÄ I take you through the core steps you need to think about when working with a continuously deployed object detection model. | üí™ I end by outlining the specific pieces I will need to build as I get my own model out into the world and ‚Äòin production‚Äô. | . üöÄ What is ‚Äòin production‚Äô for the redaction model? . ‚ÄúProduction‚Äù relates heavily to the use case. An image classification model used across the United States to identify dents on rental cars is going to have a very different profile to a privately hosted language model being used as part of an internal company chatbot interface. In the case of our redaction model, there are two main scenarios that I‚Äôm interested in supporting: . an online API which can power other services building on top of the functionality the model enables, albeit document-by-document. A user could upload a specific document and they‚Äôd receive specific predictions and results for just that document. | an offline-first model which handles large volumes of input data and that does not require internet connectivity. For example, legal teams trying to get a sense of what kinds of documents are redacted as part of a case (and to what extent) could run an extended inference process over an entire set of documents. | . These two scenarios have different requirements. Of the two, the online version is perhaps slightly more complex given the more complex serving needs and potential guarantees around speed of execution and so on. The second use offline / on-prem scenario has its own challenges around making sure that inference is fast enough to be able to work over massive document collections, but I‚Äôm not sure I‚Äôd consider that an MLOps challenge so I will mostly be focusing on the online deployment in this series. . ‚öñÔ∏è Handling failure and success . One way to think about what‚Äôs required to get a model in production is to think of the answers to the following two questions: . what could go wrong with my deployed online model? | what are the possible consequences if everything went really well? | . In terms of the redaction model, there are lots of potential complications: . our model could be slow and therefore users wouldn‚Äôt want to hang around for inference to take place | the data on which our model was originally trained could start to show its age and so maybe our model wouldn‚Äôt perform as well on newer documents being released (perhaps with a different style or method of redaction) | some software bug or managed infrastructure outage could bring our hosting down and we‚Äôd have to redeploy everything | there could be a legal or ethical challenge to our deployed model, and we‚Äôd perhaps be required to show the exact process used that resulted in a particular model | maybe the way we choose to make our model available is expensive and/or unreliable | and so on‚Ä¶ | . If things went really well, we have a different set of problems: perhaps a million people would be interested in using the service at the same time. This scalability problem could bring down the model inference service completely. Or perhaps the model would be adopted for use in a legal setting and people would start to trust its predictions blindly without taking into account the fact that its performance was starting to decline the further away we got from when it was originally trained. Maybe some legal institution would start using it to make decisions making assumptions about the accuracy of the model‚Äôs predictions, or maybe even the various government departments responsible for creating and applying the redactions in the first place would use it as a way of more efficiently or voluminously adding redactions, an unintended consequence that could potentially be harmful. . All the above scenarios and more are some of the reasons why MLOps exists. We care about having repeatable and robust processes for getting our models out in the world because the use cases are themselves often complex. We also care that our models are actually a net positive when released into the world rather than just some process that happens after model training is completed from which we‚Äôre completely disconnected. . üß± Basic MLOps building blocks . . These are some of the tools available to those trying to put their models into production. This ‚Äòlandscape‚Äô showcases both the explosion of options for various parts of the full lifecycle as well as the way that this space hasn‚Äôt yet settled on a set of best-in-class tools. For my redaction project, there are a few basics that will seek to have in place in order to meet the needs of the use case(s): . ‚éÇ Code standardisation . Perhaps not even worth mentioning, but having some standards around how the code looks and having processes to enforce this is important. Using pre-commit alongside tools like black, isort, darglint, pydocstyle and so on will take you a long way in this direction. . This would be especially important if I were working as part of a team. These tools would ensure some kinds of standards and baseline uniformity within the codebase. In my case, I‚Äôm doing everything on my own so it matters less, but these tools all help me collaborate with my future self, several months from now, perhaps when I need to fix some bug or add a new feature. Having code that reads clearly and cleanly goes a long way to getting started on that work. . üì∏ Code Versioning . Versioning your code with a tool like git is also another basic requirement of any process involving software. I‚Äôd almost say this barely requires mentioning, but I know that the use of regular atomic git commits is by no means standard practice in the world of data science so it bears restating here. . We version our code because we want to be able to go back to earlier points in our code‚Äôs history. If we wanted to see the difference between the code used to train our model today and the code used last week, we‚Äôd require a tool like git to help us with that. (Code versioning tools also help tremendously when collaborating with a larger team or to an open-source community project.) . There are various options for what tool to use but the vast majority of people use git for this as of 2022. . üß≥ Data, Model and Artifact Versioning . Last week I wrote about DVC and the ways it can be used as a lightweight way to add data versioning to a project. Since data is as important to a model as the code used to train it, we want ways to step backwards and forwards with our data. This will not only enable us to debug and retrain older models but it will help in general with managing our assets such that we don‚Äôt just have a sprawling series of folders with names like training_data_FINAL_july_2021 or validation_data_minus_synthetic_FINAL_FINAL_march_2020. . Not only does this make sense from the perspective of productivity, but for sensitive or popular ML applications there are increasing legal requirements around this kind of flexibility to introspect how you trained your models. . DVC is commonly used for this kind of use case, but there are other alternatives such as Pachyderm or LakeFS that might be worth considering if you have larger amounts of data. . üß™ Testing . This testing is primarily around ensuring that the code does what you think it‚Äôs doing, but we also care about preventing regressions in one part of the codebase when you change something somewhere else. . There isn‚Äôt much in the way of rocket science to testing, but it is a whole world unto its own. For my project, there is a decent amount of code that handles somewhat complicated conversions between different kinds of image formats, multiple competing ideas for how bounding boxes or BBoxes should be handled as data structures and so on. Having a way to be sure that my code is actually doing what I intended is a surefire way to letting me sleep better at night if I intend my model to be used by others. . üëÅ Data Validation . Just like our code needs to have some kinds of checks and balances, so does the lifeblood of our project: our data. I recently finished a three-part series on data validation in the context of this project, and both Great Expectations and Evidently are excellent options worth considering, depending on your exact requirements. . üìù Experiment Tracking . Something that old boring process literature talk about (Toyota, The Goal etc) that applies to software engineering 10000000% is that minimizing the size of the feedback loop is much more important for productivity than minimizing idleness . &mdash; Erik Bernhardsson (@bernhardsson) May 17, 2022 When starting a project from scratch, you want to iterate quickly, trying out lots of training options or combinations of features and/or data. Not only do you want to be able to replicate the precise combination of data and code used to train a particular model, but you also want to be able to compare these various efforts with one another. . Experiment trackers like Weights &amp; Biases, MLflow, Tensorboard and Neptune enable you to compare the results of your models as well as practically any combination of the hyperparameters used to train them. I‚Äôve used charts from my own (Weights &amp; Biases-powered) experiment tracker to showcase the different results obtained as part of my process. Not only is it useful for outward-facing engagement with users, stakeholders or other third-parties, but it can be useful to step back from your experiments and evaluate where your model is performing well and how you might improve it. . üì∫ Monitoring . There‚Äôs a lot of potential complexity packed into the simple term ‚Äòmonitoring‚Äô. (I‚Äôd recommend you check out Shreya Shankar‚Äôs four-part series on monitoring if you‚Äôre curious to learn more.) For our purposes, this will mainly involve making sure our model doesn‚Äôt drift or become stale. We‚Äôll need to make sure that the data used to periodically (re)train or fine-tune our model is somewhat within the original parameters of the original training data. We‚Äôll also want to be monitoring the kinds of predictions that our deployed model is making to make sure that they‚Äôre more or less within the reasonable ranges of values that we‚Äôd expect. If we start to regularly diverge from these kinds of boundary values it probably should prompt an examination of what‚Äôs going on and why we‚Äôre overstepping. This is an essential part of what it means to robustly deploy a model in production; you need to know when things are going wrong. . üèÅ Automated Model Deployment . Automation is a big part of what people generally consider to be ‚Äòmature‚Äô MLOps practices. It is useful to have an automated way to take your model from when training is complete to having it deployed and user-facing. Perhaps you know you need to retrain or fine-tune your model once a week because the data context is continually changing. Perhaps (or likely!) you have rigorous monitoring processes and in the event of a failure or series of non-sensical / out-of-bounds predictions you want a way to revert the model used in production to something more stable. . This is where the process of putting a model in production resembles the processes and values of DevOps most closely. For my redaction model, I‚Äôll want to have a way to handle the two cases mentioned above as a starting point along with more complex versions of those cases. I‚Äôll also want to automate the process of converting my IceVision VFNet model into something that can be used in the offline ‚Äòon-prem‚Äô use case I described at the beginning of this post. . üÉè DAG Cards, Model Cards, Data Cards, All the Cards . The basic idea is that you write some notes on the context surrounding your data, or your model or the pipelines you‚Äôre using as part of your overall workflow. Your processes and artifacts will likely change with the project, and I know from bitter experience that it‚Äôs easy to forget the reasoning behind why you chose to do one thing or another. So you write notes to describe what you were thinking when you created or modified this or that asset. You describe the decisions you made and what tradeoffs and downstream effects this might have. Not only is this a good practice that benefits FutureYou‚Ñ¢Ô∏è and your project, but anything developed in the open will maybe have users or contributors and they‚Äôll also benefit from these notes. . This is the only part of my ‚Äòrequirements‚Äô that is (at least currently) a bit more of a ‚Äòfad‚Äô and I wouldn‚Äôt say was commonly found. Even five years from now, I imagine that we‚Äôll have more sophisticated or standardised ways of achieving what cards bring, but for now they resonate strongly with some processes I used when working as a historian, archivist and researcher in my previous life. Some tools like the Huggingface Model Hub offer this as a built-in standard. . üë¥ MLOps Maturity Models . The pieces I described above relate to my particular use case. Different project will require different levels of automation, or even potentially other additional stages or toolkits. There is a vast spectrum of options and decisions to be made and now is probably a good time to mention that various players have tried to define what it means to do the whole ‚Äòputting a model into production and keeping it healthy‚Äô thing in a good way. These ‚ÄúMLOps Maturity Models‚Äù are not completely without value, but remember that what works for Google may not be (or is probably not) applicable to you as an individual working on a small side-project. I wrote an overview of the two most commonly cited maturity models (from Microsoft and Google) over on the ZenML blog and I encourage you to give that a read if you want to learn more. . But what does this all mean for my project? What specifically will it all look like and how am I implementing it? I‚Äôll get into some of the details in the coming weeks, but for now let me just outline my two main workflows. . ‚úçÔ∏è Redaction Project Workflow #1: Annotation . . There are two main pieces to this pipeline that happens before we train our model. My model is still thirsty for annotations and data, so from the very beginning I want to integrate the annotation process in as part of how I set up the workflows. This way, I make it as easy as possible to annotate data and use that data for subsequent training or fine-tuning. . Ingestion / Import | . Here I will check a series of pre-defined URLs to see if there are any new PDF files available for download. If new files are available (and we‚Äôve confirmed that we haven‚Äôt already downloaded them, I can download those files. Those PDFs then get converted into image files and the metadata for each image gets saved centrally so we have that to hand when annotating the files. This point is a good one to save a checkpoint version of our data using DVC. . Annotation | . We only want to annotate images that haven‚Äôt been annotated, so that check is the first to be performed before spinning up Prodigy to randomly select pages from the PDFs (now in the format of image files) to be annotated. 10% of the images that are annotated get saved in a separate ‚Äòtest data‚Äô location. This test data is never used in training and is simply held out for a more realistic final validation of the project. We version the annotations file whenever we are done annotating for the day. . üêô Redaction Project Workflow #2: Continuous Training, Continuous Deployment . . This longer pipeline contains the core value and most compute-intensive processes like training. We take the data from the raw state as annotations and go all the way to deployment. . Annotation Checker | . We first check to see if there are any new annotations available since we last ran the pipeline. We will probably need some kind of threshold number of annotations which will make it worth our while to trigger the retraining process. . Data Validation | . We‚Äôll use Great Expectations to validate the incoming new annotation data. . Synthetic Data Generation | . If / as we hit certain thresholds, we might want to generate more synthetic data and add it to the dataset. . Training | . We train or fine-tune the model for a certain number of epochs. We log our experiment metadata with Weights &amp; Biases. . Deployment Trigger / Decision | . At this point we need to decide whether to deploy the model or not, based on some evaluation criteria. Our decision will determine the path of the rest of the workflow. . Deployment | . We take the trained model and make it available for online inference. We save a version of the model using DVC, and we also package it up for use in on-prem / offline settings. . Inference &amp; Monitoring | . This step is crucial. We monitor the performance of our deployed model along with the predictions it is making. We want to be able to catch any cases where we notice the predictions to start to drift, or be aware of sluggish response times from our server and so on. . Final Thoughts . Nice work on making it all the way to the end! We took a long tour through the various considerations you need to bear in mind when deploying a model, and finished off with a preview of the kinds of things I‚Äôll be building over the coming weeks to actually put my own object detection model in production. . If you have parts of this that you‚Äôd like me to cover in more detail, or questions based on what you read here today, please leave a comment below! .",
            "url": "https://mlops.systems/tools/redactionmodel/computervision/mlops/2022/05/31/redaction-production-introduction.html",
            "relUrl": "/tools/redactionmodel/computervision/mlops/2022/05/31/redaction-production-introduction.html",
            "date": " ‚Ä¢ May 31, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "More Data, More Problems: Using DVC to handle data versioning for a computer vision problem",
            "content": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.) . If you‚Äôve been following along as I train an object detection model to detect redactions, you‚Äôll know that there have been a few iterations in how I go about doing this. For the most part, though, the dataset has remained relatively static. I downloaded a huge tranche of publicly-released government documents right at the beginning and aside from my experiments in synthetic data creation I haven‚Äôt really been adding to this data. . When it comes to turning this model into something that can work in production, it won‚Äôt be enough to have a big bucket of image files that I train on. I‚Äôll need to have a bit more control and fine-grained segmentation of the ways this data is being used. In short, if I want to be able to reproduce my workflows then I need some kind of data versioning. . üö¶ TL;DR: Data Versioning for Computer Vision . ‚õΩÔ∏è We version our data because it is the fuel for our model development and experimentation process. | üíª Data versioning tools like DVC allow you to apply the same mental model you have for git to your data. | ‚ãî The ability to ‚Äòbranch‚Äô off your data gives you the flexibility to experiment just as the same is true for branching off your code to try out some new behaviour. | DVC is probably the leading tool that allows you to version your data and flexibly access all the previous ‚Äòcommits‚Äô and checkpoints you make along the way. | . ü§î Why do we need data versioning? Isn‚Äôt git enough? . If the lifeblood of traditional software engineering is code then the equivalent for machine learning is data. We solve the problem of checkpointing what our code looked like at a particular moment with git and online hubs like Github. Until recently there weren‚Äôt many equivalent options for data. We‚Äôre trying to solve the problem that often occurs if you‚Äôre asked to reproduce the data that was used to train a particular iteration of a model from some point in the past. Without some kind of data version control this is more or less impossible, particularly if your data is constantly changing. . Even in my case for this redaction project, I wasn‚Äôt ingesting new data all the time but I was removing bad annotations or updating those annotations as I conducted error analysis or used tools like FiftyOne to understand why my model wasn‚Äôt performing as well as I‚Äôd have liked. . Luckily there‚Äôs a pretty great tool in this space that seems to be more or less unchallenged for what it does in the data versioning domain: Data Version Control or DVC. . üë©‚Äçüíª DVC Use Cases . DVC does many things, but for our purposes at this moment its core value is that it helps us version our data. It also handles the case where we have large files or a dataset that changes a lot and where we might end up having problems with storing all the versions of this data. . The core behaviour we want to use with a data versioning tool is to access our data at one particular moment. Just like you incrementally annotate your code updates using git, with sometimes atomic progressions as you do your work, so it is with DVC that you can checkpoint your data as you make changes. . At the beginning this was a slight mental adjustment for me. When working on a project it is now second nature to regularly make git commits along the way, but I wasn‚Äôt in the habit of making regular data commits as a second step. In the long-run, this requires a bit of a mental shift but this is exactly what will enable the benefits that using a tool like DVC brings. . In particular, being able to experiment with data in a way that you can always roll-back from feels pretty liberating once you‚Äôve covered your back with DVC. Just as you can use create git branches for your code, so you can create branches for your versioned data. Checking out the precise data used for some zany experiment you did is pretty painless. If you realise that the experiment is a dead-end and it‚Äôs not helping you move forward, just rewind and reset your data back to a useable state from before you had that crazy idea to create a million synthetic images :) . One other thing: DVC is built on top of git and it follows many of the mental models you might have about how versioning works. In this way, DVC luckily is smart about how it allows you to make incremental changes to your data. When it calculates the diff of your dataset before and after, it really is able to do some atomic updates and logging of what changed rather than just storing all the files multiple times over. This helps prevent you building up a really huge data cache and it helps the whole process be efficient. . . I&#39;ve mentioned this for other tools like Evidently before so I should also note that the DVC online community (https://dvc.org/community) is a pretty friendly and helpful place to hang out and learn about data versioning or to troubleshoot your problems. Nobody will tell you to RTFM here and their community events are generally beginner-friendly in my experience. This makes a big difference so they should be commended for the efforts they take to foster this kind community atmosphere. ‚ù§Ô∏è üöÄ How to get started with DVC . The basics are mostly similar to how you‚Äôd use a tool like git: . You init your repository. This add some DVC superpowers on top of what you already have with git. | You specify which files you want to have DVC manage and track. It would make a lot of sense, for example, to have DVC handle tracking your models, your image files and your data annotations (if those exist as separate files). | You can optionally also specify a remote location where you want these files to be stored. (DVC supports several types of remote storage: local file system, SSH, Amazon S3, Google Cloud Storage, HTTP, HDFS, among others.) | . (To get a taste of the full workflow when using DVC for data tracking I‚Äôd recommend something like the basic tutorial they have here. They also recently added a three-part tutorial specific to computer vision that you might want to check out.) . If you want to use DVC programmatically using their Python API, you can get some information on this in their docs here. Unfortunately, these docs are incomplete and you‚Äôll have to experiment a bit if you want to do anything beyond the simple functionality they themselves list. I‚Äôm told it behaves very similarly to how a tool like GitPython works, where you can just use the equivalent add() or checkout() function call that corresponds to a DVC CLI command, but given the lack of documentation it‚Äôs a bit harder to get a full sense of what is possible. . . DVC includes a lot of extra functionality around experiment tracking and pipelining of your code. You can safely ignore all that and just use DVC for data versioning. No shame in that :) üõ† When to use DVC . It probably is a good practice to use something like DVC from the start of most projects. If you know you‚Äôre never going to need to update the data you use, or if you will only ever generate one model, then maybe you have no need for data versioning. But realistically, when are you going to do that? Generally speaking you‚Äôll be iterating a lot and you‚Äôll be trying things out, so perhaps just start using DVC at the start of any new project: a git init can just as easily be followed by a dvc init‚Ä¶ . DVC will thrive in long-lived projects where you go down certain rabbit-holes, trying out different approaches and techniques. If you have a decent amount of data ‚Äî and you probably do if you‚Äôre bringing deep learning to the table ‚Äî then you can leverage how DVC makes it easy to store your data in the remote infrastructure of your choice with dvc remote. . üìÑ How I‚Äôm using DVC in my redaction project . For my purposes, the things I‚Äôm tracking with DVC include: . the models I train | the PDF documents I downloaded from public sources that form the basis of the data in this project | the images that I extracted from the PDF documents | the annotations I make on the images using the standard COCO Dataset format. | . This covers the core data that I expect to be working with for this project. I keep all this data synced to a remote Amazon S3 bucket which allows me to easily get set up on a new remote machine if needed. . I‚Äôll next be writing about how to move towards a ‚Äòproduction-ready‚Äô system in the coming weeks, but one thing I‚Äôll hope to be adding to the current way I do things is to add some kind of ‚Äòdata cards‚Äô. I think a combination of manual comments and annotations alongside some auto-generated data profiles would be a useful thing to get a sense of for every checkpoint we make, particularly as the data grows and is augmented. . Let me know if you‚Äôre using DVC to version your data for computer vision projects! I‚Äôm curious if there are any tricks I‚Äôm missing out‚Ä¶ . üèÉ Appendix: How to switch from git-lfs to DVC . When I first started this project, git-lfs or Git Large File Storage seemed the best option that didn‚Äôt constrain my choices. It allowed me to store any large files I had inside my repository and allowed for some sort of versioning. Over time this ended up being less robust, especially in the context of an ML workflow, so I decided to switch to using DVC backed by an Amazon S3 bucket. . I didn‚Äôt find any useful information on the DVC website or forums on how to make this switch so I‚Äôm including my notes on how I switched over myself. . . Lots of caution is advised when doing this for your own project or work. I hit some major roadblocks along the way while doing this owing to some quirks of how I&#39;d set &#39;git-lfs&#39; up in the beginning. Please take all necessary backups and snapshots of your data in case something goes wrong along the way! Some resources I consulted to understand how to do this: . This Stackoverflow thread | A Github issue on the same topic | A super useful Gist I reached via the previous Github issue that ended up guiding me most of the way | . This is what I did, step by step: . Commit and push everything to Git / Github / git-lfs | Create a branch, something like fix/remove-lfs | Remove the hooks using git las uninstall | Go into the .gitattributes file and delete whatever tracking you don‚Äôt want git-lfs to handle from now on. For me, this involved removing lines referring to .pth (model) files and .jpg (image) files. | Get a list of all the files that git-lfs currently is storing using the following command: git lfs ls-files &gt; files.txt | Modify the file to remove the beginnings of each line that we don‚Äôt need. At the end we want our files.txt to contain just a series of paths to our files. I did it with a simple Python script: | . with open(&quot;filenames.txt&quot;, &quot;w&quot;) as f: f.write(&quot;&quot;) with open(&quot;files.txt&quot;, &quot;r&quot;) as f2: for line in f2: with open(&quot;filenames.txt&quot;, &quot;a&quot;) as f: f.write(line.split(&quot; &quot;)[-1]) . Run run git rm --cached for each file that git-lfs is storing. I did this with a simple bash command that uses the file I‚Äôd created in the previous step: | . while read line; do git rm --cached &quot;$line&quot;; done &lt; files.txt . Initialise the DVC repository with dvc init | Add whatever data sources you want tracked (dvc add FOLDERNAME) | Allow for autostaging with DVC with the dvc config core.autostage true command | Commit everything | Check that no git-lfs files are left with the git lfs ls-files command. Whatever you uncached in previous steps should not show up any more. | Remove any lfs with rm -rf .git/lfs | Merge your branch into main (if you‚Äôre using git-lfs as a team, now is probably the time when other collaborators can uninstall git-lfs as specified above) | . | If needed, add your DVC remote storage with dvc remote add ‚Ä¶ (consult the docs for exactly how to set this up for your specific needs) | dvc push to get your files synced with your remote storage | . At this point you should be fully transitioned over. As I mentioned above, there are a ton of weird edge cases and quirks to this process and you probably shouldn‚Äôt follow this list blindly. I‚Äôm mainly writing this up for my own records as much as anything else, so perhaps it‚Äôs helpful for someone else seeking to transition but maybe it should be taken less as a direct list of instructions than an inspiration or general template. (I wish DVC would provide some official-ish guidance on this process through their documentation. I imagine that it‚Äôs a fairly common path for someone to outgrow git-lfs and want to get going with DVC but currently there are no instructions for how to think this through.) . UPDATE: I originally made reference to ‚Äòcontinuous training‚Äô in the title of this blogpost but I didn‚Äôt actually get into this specific use case in what I covered, so I took that out of the title and we‚Äôll save the specifics for a subsequent post! .",
            "url": "https://mlops.systems/tools/redactionmodel/computervision/mlops/2022/05/24/data-versioning-dvc.html",
            "relUrl": "/tools/redactionmodel/computervision/mlops/2022/05/24/data-versioning-dvc.html",
            "date": " ‚Ä¢ May 24, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Redaction Image Classifier: NLP Edition",
            "content": "I&#39;ve previously written about my use of fastai&#39;s vision_learner to create a classification model that was pretty good (&gt; 95% accuracy) at detecting whether an image contained redactions or not. . This week in the course we switched domains and got to know HuggingFace&#39;s transformers library as a pathway into NLP (natural language processing) which is all about text inputs. I struggled quite a bit trying to think of interesting yet self-contained / small uses of NLP that I could try out this week. A lot of the common uses for simple NLP modelling seem to be in the area of things like &#39;sentiment analysis&#39; where I couldn&#39;t really see something I could build. Also there are a lot of NLP uses cases which feel unethical or creepy (perhaps more so than in the computer vision, it felt to me). . I emerged at the end of this thought process with the idea to try to pit image classification and text classification against one another: could I train an NLP model that would outperform my image classifier in detecting whether a specific document or page contains a redaction or not? . Of course, the first thing I had to do was to OCR all the pages in my image dataset and convert this all into a text dataset. When it comes to OCR tools, there are a number of different options available and I&#39;d luckily experimented around with them. (A pretty useful overview of three leading options can be found in this blogpost by Francesco Pochetti.) I went with Tesseract as I knew had pretty good performance and accuracy for English-language documents. . My process for converting the documents wasn&#39;t particularly inspired. Essentially I just loop over the image files one by one, run the OCR engine over them to extract the text and then create a new .txt file with the extracted text. At the end, I had two folders with my data, one containing texts whose corresponding images I knew had contained redactions, and one where there were no redactions. . I had two hunches that I hoped would help my NLP model. . I hoped that the redactions would maybe create some kind of noise in the extracted text that the training process could leverage to learn to distinguish redacted from unredacted. | I knew that certain kinds of subjects were more likely to warrant redaction than others, so perhaps even the noise of the OCR trying to deal with a missing chunk of the image wouldn&#39;t be as important as just grasping the contents of the document. | What follows is my attempt to follow steps initially outlined in Jeremy Howard&#39;s Kaggle notebook that the course reviewed this week in the live lesson. My code doesn&#39;t depart from the original notebook much. . !pip install datasets transformers tokenizers -Uqq from pathlib import Path import numpy as np import pandas as pd . . I save my .txt files on the machine and I get a list of all the paths of those files. . path = Path(&quot;redaction_texts&quot;) p = path.glob(&quot;**/*.txt&quot;) files = [x for x in p if x.is_file()] . I iterate through all the paths, making of list of all the redacted texts as strings. . texts = [] for file_path in files: with open(file_path) as file: texts.append(file.read()) . !ls {path} . redacted unredacted . Converting text files into a Pandas DataFrame . I needed a way of obtaining the labels for my dataset. These labels were the parent label for each path name. The training process below needed the labels to be floats. . def is_redacted(path): &quot;Extracts the label for a specific filepath&quot; if str(path.parent).split(&quot;/&quot;)[-1] == &quot;redacted&quot;: return float(1) else: return float(0) is_redacted(files[1]) . 0.0 . Converting a Python dict into a Pandas DataFrame is pretty simple as long as you provide the data in the right formats. I had to play around with this a little when I was getting this to work. . data = { &quot;input&quot;: texts, &quot;labels&quot;: [is_redacted(path) for path in files], } . df = pd.DataFrame(columns=[&quot;input&quot;, &quot;labels&quot;], data=data) # df . df.describe(include=&#39;object&#39;) . input . count 3886 | . unique 3830 | . top | . freq 35 | . We now have a DataFrame containing 3886 rows of data. You can see here that 35 rows have no visible text. Potentially something went wrong with the OCR extraction, or the redaction covered the entire image. I didn&#39;t really know or want to fiddle around with that too much, so I left those rows in. . Moving into HF Transformers Land . We create a Dataset object from our DataFrame. It requires that our targets have the column name labels. . from datasets import Dataset, DatasetDict ds = Dataset.from_pandas(df) . ds . Dataset({ features: [&#39;input&#39;, &#39;labels&#39;], num_rows: 3886 }) . We&#39;re finetuning a pre-trained model here, so I start with the small version of Deberta which will allow me (I hope!) to iterate quickly and come up with an initial baseline and sense of whether this is even a viable approach to solving the problem. . model_nm = &#39;microsoft/deberta-v3-small&#39; . Before we finetune our model, we have to do two things to our text data in order that it works within our gradient descent powered training process: . we have to tokenise our text data | we have to turn those tokens into numbers so they can be crunched within our GPU as numbers. | . Tokenisation is the process of splitting our words into shorter stubs of text -- there are varying schools of thought and use cases on the extent to which you break the words down. We have to use the same tokenisation process that was used by our pretrained model, so we let transformers grab the original tokenisers that was used with deberta-v3-small. . from transformers import AutoModelForSequenceClassification, AutoTokenizer tokz = AutoTokenizer.from_pretrained(model_nm) . Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained. /opt/conda/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text. warnings.warn( Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained. . def tok_func(x): return tokz(x[&quot;input&quot;]) . tok_ds = ds.map(tok_func, batched=True) . We split our data into training and validation subsets as per usual so that we know how our model is doing while training. . dds = tok_ds.train_test_split(0.25, seed=42) dds . DatasetDict({ train: Dataset({ features: [&#39;input&#39;, &#39;labels&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 2914 }) test: Dataset({ features: [&#39;input&#39;, &#39;labels&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 972 }) }) . We define our metric as Pearson&#39;s r AKA the Pearson correlation coefficient, a metric I don&#39;t feel an immense instinctual understanding for, but suffice it for this blogpost to know that a higher value (up to a maximum of 1) is better. . def corr(x, y): return np.corrcoef(x, y)[0][1] def corr_d(eval_pred): return {&quot;pearson&quot;: corr(*eval_pred)} . from transformers import TrainingArguments,Trainer . Here we define our batch size, the number of epochs we want to train for as well as the learning rate. The defaults in Jeremy&#39;s NLP notebook were far higher than what you see here. His batch size was 128. When I ran the cells that follow, I hit the infamous &quot;CUDA out of memory&quot; error more or less immediately. I was running on a machine with a 16GB RAM GPU, but this apparently wasn&#39;t enough and the batch size was far too large. I had to reduce it down to 4, as you can see, in order to even be able to train the model. There are tradeoffs to this in terms of how well the model learns, but without spending lots of money on fancy machines this was the compromise I had to make. . bs = 4 epochs = 5 lr = 1e-4 . args = TrainingArguments( &quot;outputs&quot;, learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type=&quot;cosine&quot;, fp16=True, evaluation_strategy=&quot;epoch&quot;, per_device_train_batch_size=bs, per_device_eval_batch_size=bs * 2, num_train_epochs=epochs, weight_decay=0.01, report_to=&quot;none&quot;, ) . model = AutoModelForSequenceClassification.from_pretrained( model_nm, num_labels=1 ) trainer = Trainer( model, args, train_dataset=dds[&quot;train&quot;], eval_dataset=dds[&quot;test&quot;], tokenizer=tokz, compute_metrics=corr_d, ) . Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: [&#39;lm_predictions.lm_head.dense.weight&#39;, &#39;lm_predictions.lm_head.LayerNorm.weight&#39;, &#39;mask_predictions.dense.bias&#39;, &#39;lm_predictions.lm_head.LayerNorm.bias&#39;, &#39;mask_predictions.classifier.weight&#39;, &#39;lm_predictions.lm_head.bias&#39;, &#39;mask_predictions.LayerNorm.bias&#39;, &#39;lm_predictions.lm_head.dense.bias&#39;, &#39;mask_predictions.dense.weight&#39;, &#39;mask_predictions.classifier.bias&#39;, &#39;mask_predictions.LayerNorm.weight&#39;] - This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;, &#39;pooler.dense.bias&#39;, &#39;pooler.dense.weight&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. Using amp half precision backend . trainer.train(); . The following columns in the training set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. /opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning warnings.warn( ***** Running training ***** Num examples = 2914 Num Epochs = 5 Instantaneous batch size per device = 4 Total train batch size (w. parallel, distributed &amp; accumulation) = 4 Gradient Accumulation steps = 1 Total optimization steps = 3645 . . [3645/3645 09:15, Epoch 5/5] Epoch Training Loss Validation Loss Pearson . 1 | 0.250100 | 0.168366 | 0.705429 | . 2 | 0.171600 | 0.134761 | 0.748499 | . 3 | 0.118200 | 0.114869 | 0.784274 | . 4 | 0.089600 | 0.093946 | 0.818484 | . 5 | 0.063100 | 0.091717 | 0.822977 | . &lt;/div&gt; &lt;/div&gt; Saving model checkpoint to outputs/checkpoint-500 Configuration saved in outputs/checkpoint-500/config.json Model weights saved in outputs/checkpoint-500/pytorch_model.bin tokenizer config file saved in outputs/checkpoint-500/tokenizer_config.json Special tokens file saved in outputs/checkpoint-500/special_tokens_map.json The following columns in the evaluation set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 972 Batch size = 8 Saving model checkpoint to outputs/checkpoint-1000 Configuration saved in outputs/checkpoint-1000/config.json Model weights saved in outputs/checkpoint-1000/pytorch_model.bin tokenizer config file saved in outputs/checkpoint-1000/tokenizer_config.json Special tokens file saved in outputs/checkpoint-1000/special_tokens_map.json The following columns in the evaluation set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 972 Batch size = 8 Saving model checkpoint to outputs/checkpoint-1500 Configuration saved in outputs/checkpoint-1500/config.json Model weights saved in outputs/checkpoint-1500/pytorch_model.bin tokenizer config file saved in outputs/checkpoint-1500/tokenizer_config.json Special tokens file saved in outputs/checkpoint-1500/special_tokens_map.json Saving model checkpoint to outputs/checkpoint-2000 Configuration saved in outputs/checkpoint-2000/config.json Model weights saved in outputs/checkpoint-2000/pytorch_model.bin tokenizer config file saved in outputs/checkpoint-2000/tokenizer_config.json Special tokens file saved in outputs/checkpoint-2000/special_tokens_map.json The following columns in the evaluation set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 972 Batch size = 8 Saving model checkpoint to outputs/checkpoint-2500 Configuration saved in outputs/checkpoint-2500/config.json Model weights saved in outputs/checkpoint-2500/pytorch_model.bin tokenizer config file saved in outputs/checkpoint-2500/tokenizer_config.json Special tokens file saved in outputs/checkpoint-2500/special_tokens_map.json The following columns in the evaluation set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 972 Batch size = 8 Saving model checkpoint to outputs/checkpoint-3000 Configuration saved in outputs/checkpoint-3000/config.json Model weights saved in outputs/checkpoint-3000/pytorch_model.bin tokenizer config file saved in outputs/checkpoint-3000/tokenizer_config.json Special tokens file saved in outputs/checkpoint-3000/special_tokens_map.json Saving model checkpoint to outputs/checkpoint-3500 Configuration saved in outputs/checkpoint-3500/config.json Model weights saved in outputs/checkpoint-3500/pytorch_model.bin tokenizer config file saved in outputs/checkpoint-3500/tokenizer_config.json Special tokens file saved in outputs/checkpoint-3500/special_tokens_map.json The following columns in the evaluation set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 972 Batch size = 8 Training completed. Do not forget to share your model on huggingface.co/models =) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; At the end of all this, we have a Pearson&#39;s score of 0.82 on our validation set which doesn&#39;t seem to be as good as our image classifier. I&#39;m not sure how I would go about comparing these two different metrics. I imagine I&#39;d want to ensure that both my metrics were identical to make a like-for-like comparison. . My model is available on the Huggingface Hub here. . What did I learn? . Training NLP models feels like a bit of a different world from that of computer vision. There are different constraints in the process that I wasn&#39;t previously aware of and working with the transformers library exposed me to a bunch of new errors and hoops I had to jump through. | It seems that the RAM needed on the GPU is directly correlated with the length of the text documents. Mine were on the long-ish end of the scale (particularly when compared with tweets which was what Jeremy was training on in his notebook). I wonder how people solve this problem, since mine by were by no means incredibly long. | NLP models take longer to train than computer vision models; at least, the transformer-based models that I was working with. | It&#39;s hard to compare two models together that don&#39;t share the same metric or loss function. | There are MANY fiddly knobs to twist with NLP, particularly around the pre-processing of text samples, tokenisation strategies and so on. I wonder how much of those will be abstracted away from the high-level fastai abstraction when the library integrates with transformers in the coming months. | The end-to-end process is broadly the same, however, and it was good to have the foundation that we&#39;ve been building up over the previous weeks in the course. | . The next model I train hopefully will not be relating to redactions, I promise! . UPDATE: I read a bit in the new O&#39;Reilly book by the transformers team, Natural Language Processing with Transformers, which seems to address the issue of the same text size: . &quot;Transformer models have a maximum input sequence length that is referred to as the maximum context size. For applications using DistilBERT, the maximum context size is 512 tokens, which amounts to a few paragraphs of text. [...] Texts that are longer than a model&#39;s context size need to be truncated, which can lead to a loss in performance if the truncated text contains crucial information.&quot; (pages 28-29 of the paperback edition) . The book suggests plotting out the number of tokens to get a sense of the distribution of the data by size: . import matplotlib.pyplot as plt df[&quot;Tokens per document&quot;] = df[&quot;input&quot;].apply(lambda x: len(x.split())) df.boxplot( &quot;Tokens per document&quot;, by=&quot;labels&quot;, grid=False, showfliers=False, ) plt.suptitle(&quot;&quot;) plt.xlabel(&quot;&quot;) plt.show() . Here we can see that we have a fairly wide distribution, with quite a few texts going all the way up to 800 tokens in length, so that is probably responsible for the large amounts of RAM needed, but perhaps the truncation of texts is also harming our performance. . When I visit the deberta-v3-small model card on Huggingface, I also see reference to a maximum sequence length of 256 which would indeed harm my model and its ability to learn, I reckon. . &lt;/div&gt; .",
            "url": "https://mlops.systems/fastai/nlp/redactionmodel/computervision/huggingface/2022/05/21/nlp-redaction-classifier.html",
            "relUrl": "/fastai/nlp/redactionmodel/computervision/huggingface/2022/05/21/nlp-redaction-classifier.html",
            "date": " ‚Ä¢ May 21, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "A neural network for Fashion MNIST data",
            "content": "!pip install -Uqq fastbook nbdev torch import fastbook fastbook.setup_book() from fastai.vision.all import * from fastbook import * import torch from torch.utils.data import Dataset from torchvision import datasets from torchvision.transforms import ToTensor import matplotlib.pyplot as plt training_data = datasets.FashionMNIST( root=&quot;data&quot;, train=True, download=True, transform=ToTensor() ) test_data = datasets.FashionMNIST( root=&quot;data&quot;, train=False, download=True, transform=ToTensor() ) training_dresses = [item[0][0] for item in training_data if item[1] == 3] training_pullovers = [item[0][0] for item in training_data if item[1] == 2] test_dresses = [item[0][0] for item in test_data if item[1] == 3] test_pullovers = [item[0][0] for item in test_data if item[1] == 2] training_dresses_tensor = torch.stack(training_dresses) training_pullovers_tensor = torch.stack(training_pullovers) test_dresses_tensor = torch.stack(test_dresses) test_pullovers_tensor = torch.stack(test_pullovers) train_x = torch.cat([training_dresses_tensor, training_pullovers_tensor]).view(-1, 28*28) train_y = torch.cat([torch.ones(len(training_dresses)), torch.zeros(len(training_pullovers))]).unsqueeze(1) valid_x = torch.cat([test_dresses_tensor, test_pullovers_tensor]).view(-1, 28*28) valid_y = torch.cat([torch.ones(len(test_dresses)), torch.zeros(len(test_pullovers))]).unsqueeze(1) train_dset = list(zip(train_x, train_y)) valid_dset = list(zip(valid_x, valid_y)) train_dl = DataLoader(train_dset, batch_size=256, shuffle=True) valid_dl = DataLoader(valid_dset, batch_size=256, shuffle=True) dls = DataLoaders(train_dl, valid_dl) def initialise_params(size, std=1.0): return (torch.randn(size) * std).requires_grad_() def fashion_mnist_loss(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1 - predictions, predictions).mean() def batch_accuracy(x_batch, y_batch): preds = x_batch.sigmoid() correct = (preds &gt; 0.5) == y_batch return correct.float().mean() . . In the previous post we used stochastic gradient descent to train a model to fit a linear function to our Fashion MNIST data, specifically the difference between a pullover and a dress. . In this final stage, we will take the next step to creating a neural network in code that will be used to detect that same difference between a pullover and a dress. The key difference here is that we will need to &#39;add non-linearity&#39; to our function. I have no mathematics background so I have very little intuitive (or learned!) understanding of specifically what that means, but my current mental model as learned during the course is that linear functions just aren&#39;t flexible enough to learn more complex patterns. In the end, what we want is a function that will fit to the patterns in our training data (as mapped to a multidimensional space). Simple linear functions aren&#39;t going to cut it. . What this looks like in code is this: . weights1 = initialise_params((28*28, 30)) bias1 = initialise_params(30) weights2 = initialise_params((30, 1)) bias2 = initialise_params(1) def simple_network(x_batch): result = x_batch@weights1 + bias1 result = result.max(tensor(0.0)) result = result@weights2 + bias2 return result . You can see the three layers of our simple network pretty clearly in the code above. The middle layer is what otherwise is known as a ReLU or rectified linear unit. It basically means that negative values passing through that function become zero and all positive values are unchanged. When you plot the function it looks like this: . plot_function(F.relu) . When we put a non-linear function in between two linear functions, then this network is able to encode and express more complicated patterns. This is basically all we&#39;re doing with deep learning: we stack these layers on to make the functions more and more capable of modelling and representing complex things. . We can express the above simple network in PyTorch-specific code (functionally it&#39;s the same): . simple_net = nn.Sequential( nn.Linear(28*28, 30), nn.ReLU(), nn.Linear(30, 1) ) . At this point, training a model is similar to what we did last time round: . learn = Learner(dls, simple_net, opt_func=SGD, loss_func=fashion_mnist_loss, metrics=batch_accuracy) learn.fit(30, 0.1) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.166980 | 0.079354 | 0.959000 | 00:00 | . 1 | 0.087369 | 0.057471 | 0.961000 | 00:00 | . 2 | 0.059999 | 0.050108 | 0.963500 | 00:00 | . 3 | 0.048050 | 0.046509 | 0.963500 | 00:00 | . 4 | 0.041847 | 0.044165 | 0.964000 | 00:00 | . 5 | 0.038310 | 0.042841 | 0.963500 | 00:00 | . 6 | 0.036514 | 0.041464 | 0.964500 | 00:00 | . 7 | 0.034653 | 0.040640 | 0.964500 | 00:00 | . 8 | 0.033204 | 0.039827 | 0.965000 | 00:00 | . 9 | 0.032370 | 0.039344 | 0.965000 | 00:00 | . 10 | 0.032060 | 0.038778 | 0.965000 | 00:00 | . 11 | 0.031814 | 0.038854 | 0.965500 | 00:00 | . 12 | 0.031212 | 0.037872 | 0.965000 | 00:00 | . 13 | 0.030837 | 0.037836 | 0.964500 | 00:00 | . 14 | 0.030193 | 0.037372 | 0.965000 | 00:00 | . 15 | 0.030213 | 0.037250 | 0.965000 | 00:00 | . 16 | 0.030058 | 0.036885 | 0.965000 | 00:00 | . 17 | 0.029849 | 0.036862 | 0.965500 | 00:00 | . 18 | 0.029551 | 0.036518 | 0.965500 | 00:00 | . 19 | 0.029381 | 0.036236 | 0.966000 | 00:00 | . 20 | 0.029358 | 0.035996 | 0.966500 | 00:00 | . 21 | 0.028987 | 0.036024 | 0.966000 | 00:00 | . 22 | 0.028436 | 0.035731 | 0.966500 | 00:00 | . 23 | 0.028494 | 0.035813 | 0.967000 | 00:00 | . 24 | 0.028174 | 0.035535 | 0.966000 | 00:00 | . 25 | 0.028089 | 0.035361 | 0.966500 | 00:00 | . 26 | 0.027961 | 0.035035 | 0.966500 | 00:00 | . 27 | 0.027761 | 0.035248 | 0.966000 | 00:00 | . 28 | 0.028016 | 0.035006 | 0.966500 | 00:00 | . 29 | 0.027430 | 0.034841 | 0.967500 | 00:00 | . plt.plot(L(learn.recorder.values).itemgot(2)) . [&lt;matplotlib.lines.Line2D at 0x150bb3490&gt;] . learn.recorder.values[-1][2] . 0.9674999713897705 . We don&#39;t actually emerge at the end of this with a vastly superior score to what we had at the end of the last notebook, but this basis (the simple neural network) has far more open vistas within which we can work and build upon. . Finally, to round out my understanding, I put together a little diagram showing the various pieces that go into the Learner class when we instantiate it, adding some of the other concepts etc below it as I felt was appropriate. This isn&#39;t a complete picture by any means, but I find it helpful to visualise how things are layered and pieced together: . .",
            "url": "https://mlops.systems/fastai/computervision/pytorch/2022/05/15/fashion-mnist-neural-network.html",
            "relUrl": "/fastai/computervision/pytorch/2022/05/15/fashion-mnist-neural-network.html",
            "date": " ‚Ä¢ May 15, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Using the seven-step SGD process for Fashion MNIST",
            "content": "!pip install -Uqq fastbook nbdev torch import fastbook fastbook.setup_book() from fastai.vision.all import * from fastbook import * import torch from torch.utils.data import Dataset from torchvision import datasets from torchvision.transforms import ToTensor import matplotlib.pyplot as plt . . In the previous post I used the seven-step process to fit to an unknown function. The process as a whole is fairly simple to get your head around, but there are a good few details to keep track of along the way. This will continue to be the case as we get into this walkthrough of how to do the same for the Fashion MNIST pullover vs dress data. . Getting our data into the right format . The first thing we need to handle is making sure our data is in the right format, shape and so on. We begin by downloading our data and splitting the data into training and test sets. . training_data = datasets.FashionMNIST( root=&quot;data&quot;, train=True, download=True, transform=ToTensor() ) test_data = datasets.FashionMNIST( root=&quot;data&quot;, train=False, download=True, transform=ToTensor() ) training_dresses = [item[0][0] for item in training_data if item[1] == 3] training_pullovers = [item[0][0] for item in training_data if item[1] == 2] test_dresses = [item[0][0] for item in test_data if item[1] == 3] test_pullovers = [item[0][0] for item in test_data if item[1] == 2] training_dresses_tensor = torch.stack(training_dresses) training_pullovers_tensor = torch.stack(training_pullovers) test_dresses_tensor = torch.stack(test_dresses) test_pullovers_tensor = torch.stack(test_pullovers) training_dresses_tensor.shape, test_dresses_tensor.shape . (torch.Size([6000, 28, 28]), torch.Size([1000, 28, 28])) . train_x = torch.cat([training_dresses_tensor, training_pullovers_tensor]).view(-1, 28*28) train_y = torch.cat([torch.ones(len(training_dresses)), torch.zeros(len(training_pullovers))]).unsqueeze(1) valid_x = torch.cat([test_dresses_tensor, test_pullovers_tensor]).view(-1, 28*28) valid_y = torch.cat([torch.ones(len(test_dresses)), torch.zeros(len(test_pullovers))]).unsqueeze(1) train_x.shape, train_y.shape . (torch.Size([12000, 784]), torch.Size([12000, 1])) . We transform our images tensors from matrices into vectors with all the values one after another. We create a train_y vector with our labels which we can use to check how well we did with our predictions. . We create datasets out of our tensors. This means that we can feed our data into our training functions in the way that is most convenient (i.e. an image is paired with the correct label). . train_dset = list(zip(train_x, train_y)) valid_dset = list(zip(valid_x, valid_y)) . Initialising our weights and bias . As in the previous times where we&#39;ve done this, we initialise our parameters or weights with random values. This means that for every pixel represented in the images, we&#39;ll start off with purely random values. We initialise our bias to a random number as well. . def initialise_params(size, std=1.0): return (torch.randn(size) * std).requires_grad_() weights = initialise_params((28*28, 1)) bias = initialise_params(1) . (train_x[0]*weights.T).sum() + bias . tensor([2.8681], grad_fn=&lt;AddBackward0&gt;) . Matrix multiplication to calculate our predictions . We&#39;ll need to make many calculations like the one we just made, and luckily the technique of matrix multiplication helps us with exactly the scenario we have: we want to multiply the values of our image (laid out in a single vector) with the weights and to add the bias. . In Python, matrix multiplication is carried out with a simple @ operator, so we can bring all of this together as a function: . def linear1(x_batch): return x_batch@weights + bias preds = linear1(train_x) preds . tensor([[ 2.8681], [ -7.6810], [-17.5719], ..., [ -3.8665], [ 2.0646], [ -2.5148]], grad_fn=&lt;AddBackward0&gt;) . We can check our accuracy for these predictions: . corrects = (preds &gt; 0.0).float() == train_y corrects.float().mean().item() . 0.35324999690055847 . Our accuracy is pretty poor! A lot worse than even 50/50 luck which is what you&#39;d expect to get on average from a random set of initial weights. Apparently we had a bad draw of luck this time round! . A loss function to evaluate model performance . We now need a loss function which will tell us how well we are doing in our predictions, and that can be used as part of the gradient calculations to let us know (as we iterate) how to update our weights. . The problem, especially in the data set we&#39;re working with, is that we have a binary probability: either it&#39;s a dress or a pullover. Zero or one. Unlike in a regression problem, or something similar, we don&#39;t have any smooth selection of contiguous values that get predicted. We have zero or one. . At this point we learn about the sigmoid function which is a way to reframe this problem in a way that we can use to our advantage. The sigmoid function when plotted looks like this: . def sigmoid(x): return 1/(1+torch.exp(-x)) plot_function(torch.sigmoid, title=&quot;Sigmoid&quot;, min=-5, max=5) . This function, as you can see, takes any input value and squashes it down such that the output value is between 0 and 1. It also has a smooth curve, all headed in the same direction, between those values. This is ideal for our situation. The first thing we must do as part of our loss function, therefore, is to apply the sigmoid function to the inputs. . def fashion_mnist_loss(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1 - predictions, predictions).mean() . This torch.where(...) function is a handy way of iterating through all our data, checking whether our target is 1 or not, then outputting the distance from the correct prediction and calculating the mean of these predictions across the entire dataset. . DataLoaders and Datasets . We&#39;ve already created datasets for our training and validation data. The process of iterating through our data, however, requires some thought as to how we&#39;ll do it. Our options: . we could iterate through the entire dataset, making the relevant loss and gradient calculations and adjusting the weights but this might make the process quite long, even though we&#39;d benefit from the increased accuracy this would bring since we&#39;d be seeing the entire dataset each iteration. | we could do our calculations after just seeing a single image, but then our model would be over-influenced and perturbed by the fluctuations from image to image. This also wouldn&#39;t be what we want. | . In practice, we&#39;ll need to choose something in between. This is where mini-batches or just &#39;batches&#39; come in. These will be need to be large enough (and randomly populated!) that our model can meaningfully learn from them, but not so large that our process takes too long. . Luckily, we have the abstraction of the DataLoader which will create all our randomly assigned batches for us. . train_dl = DataLoader(train_dset, batch_size=256, shuffle=True) valid_dl = DataLoader(valid_dset, batch_size=256, shuffle=True) . Training our model . Now we can bring the whole process together and train our model: . weights = initialise_params((28*28, 1)) bias = initialise_params(1) def calculate_gradient(x_batch, y_batch, model): preds = model(x_batch) loss = fashion_mnist_loss(preds, y_batch) loss.backward() def train_epoch(model, learning_rate, params): # iterate over the training data, batch by batch for x_batch, y_batch in train_dl: # calculate the gradients calculate_gradient(x_batch, y_batch, model) for param in params: param.data -= param.grad * learning_rate # set the gradients to zero param.grad.zero_() def batch_accuracy(x_batch, y_batch): preds = x_batch.sigmoid() correct = (preds &gt; 0.5) == y_batch return correct.float().mean() def validate_epoch(model): accs = [batch_accuracy(model(x_batch), y_batch) for x_batch, y_batch in valid_dl] return round(torch.stack(accs).mean().item(), 4) validate_epoch(linear1) . 0.2173 . We start there, but now we can train and watch our accuracy improving: . learning_rate = 1. params = weights, bias for _ in range(30): train_epoch(linear1, learning_rate, params) print(validate_epoch(linear1), end=&quot; &quot;) . 0.5001 0.5016 0.7994 0.9357 0.9481 0.9523 0.9537 0.9555 0.9572 0.9582 0.9578 0.9604 0.9608 0.9609 0.962 0.9611 0.9622 0.9626 0.9631 0.9625 0.963 0.963 0.9633 0.9638 0.964 0.9631 0.9638 0.9638 0.9643 0.9645 . We had 91% accuracy on our validation dataset last time we tried this with pixel similarity. . After 30 epochs of training with our new process we&#39;ve achieved 96%, but we could still do better! We&#39;ll tackle that in the next post. . Optimising with an Optimiser . Everything that we&#39;ve been doing so far is so common that there is pre-built functionality to handle all of the pieces. . our linear1 function (which calculated predictions based on our weights and biases) can be replaced with PyTorch&#39;s nn.Linear module. Actually, nn.Linear does the same thing as our initialise_params and our linear1 function combined. | . linear_model = nn.Linear(28*28, 1) . Our PyTorch module carries an internal representation of our weights and our biases: . weights, bias = linear_model.parameters() weights.shape, bias.shape . (torch.Size([1, 784]), torch.Size([1])) . an optimiser bundles the step functionality and the zero_grad_ functionality. In the book we see how to create our own very basic optimiser, but fastai provides the basic SGD class which we can use that handles these same behaviours. | . We&#39;ll need to amend our training function a little to take this into account: . linear_model = nn.Linear(28*28, 1) opt = SGD(linear_model.parameters(), learning_rate) def train_epoch(model): for x_batch, y_batch in train_dl: calculate_gradient(x_batch, y_batch, model) opt.step() opt.zero_grad() def train_model(model, epochs): for _ in range(epochs): train_epoch(model) print(validate_epoch(model), end=&quot; &quot;) train_model(linear_model, 30) . 0.96 0.9642 0.966 0.9659 0.9663 0.9672 0.9677 0.9668 0.9678 0.9684 0.9681 0.9674 0.9681 0.9671 0.9678 0.9677 0.9684 0.968 0.9687 0.9677 0.9681 0.968 0.9693 0.9684 0.968 0.9686 0.9688 0.9693 0.9698 0.9697 . Some extra fastai abstractions . fastai handles so much of this for us all, such that the Learner is actually the thing we can use to get all of the above logic built in. . The Learner takes all of the pieces that we&#39;ve spent the last few blogs creating: . the DataLoaders (iterators providing the data in batches, in the right format with paired x and y values) | the model itself (our function that we&#39;re trying to optimise) | the optimisation function (which receives our weights and bias parameters as well as the learning rate) | the loss function | any optional metrics we want printed | . dls = DataLoaders(train_dl, valid_dl) learn = Learner(dls, nn.Linear(28*28, 1), opt_func=SGD, loss_func=fashion_mnist_loss, metrics=batch_accuracy) learn.fit(15, lr = learning_rate) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.063289 | 0.045487 | 0.963500 | 00:00 | . 1 | 0.044268 | 0.042236 | 0.964500 | 00:00 | . 2 | 0.037112 | 0.040228 | 0.965500 | 00:00 | . 3 | 0.034010 | 0.038743 | 0.967000 | 00:00 | . 4 | 0.032013 | 0.038781 | 0.966000 | 00:00 | . 5 | 0.030633 | 0.037635 | 0.966500 | 00:00 | . 6 | 0.030458 | 0.037530 | 0.967500 | 00:00 | . 7 | 0.029747 | 0.036593 | 0.967000 | 00:00 | . 8 | 0.029511 | 0.036479 | 0.967500 | 00:00 | . 9 | 0.029305 | 0.035645 | 0.967500 | 00:00 | . 10 | 0.028643 | 0.035400 | 0.966500 | 00:00 | . 11 | 0.028562 | 0.035477 | 0.966500 | 00:00 | . 12 | 0.028788 | 0.035191 | 0.968000 | 00:00 | . 13 | 0.028261 | 0.034843 | 0.968000 | 00:00 | . 14 | 0.028172 | 0.034883 | 0.968500 | 00:00 | . So there we have it. We learned how to create a linear learner. Obviously 96.8% accuracy is pretty good, but it could be better. Next time we&#39;re going to add the final touches to this process by creating a neural network, adding layers of nonlinearity to ensure our function can fit the complex patterns in our data. .",
            "url": "https://mlops.systems/fastai/computervision/pytorch/2022/05/14/SGD-fashion-mnist.html",
            "relUrl": "/fastai/computervision/pytorch/2022/05/14/SGD-fashion-mnist.html",
            "date": " ‚Ä¢ May 14, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Stochastic Gradient Descent: a mini-example of the whole game",
            "content": "!pip install -Uqq fastbook nbdev from fastai.vision.all import * . . An End-To-End Example of SGD . In the previous post we learned about the high-level seven-step process that we can use to update the weights of our function or model. We can now replicate that process and try it out on a very simple function to show how it works from start to finish. We will iterate a few times so that the gradual improvement is visible and clear. . Note this example is pretty closely taken from chapter 4 of the fastai fastbook, available for free here. . Let&#39;s assume that we&#39;re plotting out the speed a rollercoaster is running at as it climbs up to one of its peaks, slowing as it reaches the top but then accelerating as it passes over the peak. . # setup twenty values to represent time passing time = torch.arange(0, 20).float() time . tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]) . speed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1 plt.scatter(time, speed); . The curve is overall distinguishable, but the noise makes it non-trivial to just make a prediction. . If we think of this function as a quadratic equation of the form a * (time ** 2) + (b * time) + c, we can boil our problem down to the following: . for any value time, we want to be able to find the speed value | given that we&#39;ve stated that the equation is a quadratic equation of the form stated above, we basically just have to find the three values a, b and c. | . If we were doing this manually, we could manipulate each value and find the perfect combination such that we had the best possible prediction. (If we throw in loss calculation as part of this process, then we could say that we&#39;d know that we have the right values for a, b and c when we have the lowest loss.) . def f(t, params): a, b, c = params return a*(t**2) + (b*t) + c . # for continuous values, the mean squared error is a good choice def mse(predictions, targets): return ((predictions - targets)**2).mean().sqrt() . Now we can go through the seven-step process applying what we already know we need to do. . 1. Initialise the parameters . We begin with random values. We also make sure to set up our Tensor so that we&#39;re able to calculate the gradients. . params = torch.randn(3).requires_grad_() . 2. Calculate the predictions . We make the calculations by passing our parameter values into our function f. We can visualise what our predictions would look like with those parameters. . preds = f(time, params) . def show_preds(preds, ax=None): if ax is None: ax = plt.subplots()[1] ax.scatter(time, speed) ax.scatter(time, to_np(preds), color = &#39;red&#39;) show_preds(preds) . You can see that there&#39;s a decent amount of difference between the curve denoting our predictions for the params (in red) and the actual function (in blue). . 3. Calculate the loss . We use the mean squared error as a way of calculating our loss. . loss = mse(preds, speed) loss . tensor(200.6502, grad_fn=&lt;SqrtBackward0&gt;) . This number is a measure of how far off our predictions are from the actual values. We want to improve this loss and drive it down even further, and for that we&#39;ll need the gradients. . 4. Calculate the gradient . As described in the previous post, we use PyTorch&#39;s inbuilt ability to calculate gradients. . loss.backward() params.grad . tensor([166.3746, 10.6914, 0.6876]) . We can update our parameters based on the learning rate. For our purposes here we can choose a really small learning rate: 0.00001 or 1e-5. This is what the values of our parameters would look like after that operation: . params * 0.00001 . tensor([ 1.2895e-05, 5.8427e-06, -2.3174e-06], grad_fn=&lt;MulBackward0&gt;) . params . tensor([ 1.2895, 0.5843, -0.2317], requires_grad=True) . 5. Step our weights . We can step our parameters using the formula previously described: multiply the learning rate by the gradients. . learning_rate = 0.00001 params.data -= learning_rate * params.grad.data params.grad . tensor([166.3746, 10.6914, 0.6876]) . We can visualise whether this has improved our function&#39;s curve or not: . preds = f(time, params) mse(preds, speed) . tensor(200.3723, grad_fn=&lt;SqrtBackward0&gt;) . show_preds(preds) . Our loss has gone from 268.4112 to 268.1312. An improvement, but it feels like a small improvement! . 6. Repeat and iterate . To save ourselves some time, we can create a function that will help us in iterating through and repeating the above steps: . def repeat_sgd(params, prn=True): preds = f(time, params) loss = mse(preds, speed) loss.backward() params.data -= learning_rate * params.grad.data params.grad = None if prn: print(loss.item()) return preds . for i in range(10): repeat_sgd(params) . 200.3722686767578 199.81639099121094 199.53848266601562 199.2605743408203 198.98269653320312 198.70480346679688 198.4269561767578 198.14910888671875 197.87124633789062 197.59344482421875 . We see that our loss is going down, so everything is improving as we&#39;d hope. The progress seems slow, but it&#39;s progress nonetheless. I imagine we could increasing the learning rate to make the loss go down faster. . 7. Stop when we&#39;ve iterated enough . We&#39;ve only iterated a few times here, but really what we&#39;d want to do is keep going until we reached our stopping point (either we&#39;ve taken too long or our model is &#39;good enough&#39; for our needs). . Takeaways . This remains a fairly simple example. We are optimising our three parameter values a, b and c and the calculations are pretty easy to visualise. We&#39;ve seen how our loss function increases as we repeat the steps of gradient and loss calculation, &#39;stepping&#39; and so on. .",
            "url": "https://mlops.systems/fastai/computervision/pytorch/2022/05/13/sgd-whole-game.html",
            "relUrl": "/fastai/computervision/pytorch/2022/05/13/sgd-whole-game.html",
            "date": " ‚Ä¢ May 13, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Some foundations for machine learning with PyTorch",
            "content": "!pip install -Uqq fastbook nbdev torch import fastbook fastbook.setup_book() from fastai.vision.all import * from fastbook import * import torch from torch.utils.data import Dataset from torchvision import datasets from torchvision.transforms import ToTensor import matplotlib.pyplot as plt . . In the previous post I showed a naive approach to calculating the similarity or difference between images, and how that could be used to create a function that did pretty well at estimating whether any particular image was a pullover or a dress. . Chapter 4 of the fastbook then takes us on a journey showing a smarter approach where the computer can make even better estimations and predictions. The broad strokes of this approach are simple to grasp, but of course the individual details are where the nuances of machine learning are to be found. . Seven steps for a machine to learn . 1. Initialise a set of weights . We use random values start with. There could potentially be more elaborate / fancy ways to calculate some starting values that are closer to our end goal, but in practice it is unnecessary because we have a process that we are going to use to update our values. . 2. Use the weights to make a prediction . We check whether the sets of weights we have currently set as part of our function are the right ones. We check the prediction (i.e. what came out the other end of our model / function) and get a sense of how well our model did. . 3. Loss: see how well we did with our predictions . We calculate the loss for our data. How well did we do at predicting what we were trying to predict? We use a number that will be small if our function is doing well. (Note: this is just a convention and otherwise there&#39;s no special reason for this.) . 4. Calculate the gradients across all the weights . We are calculating the gradient for lots of numbers, not just a single value. For every stage, we get back the gradient for all of the numbers. This is done sequentially, where we calculate the gradient for one weight / number, keeping all the other numbers constant, and then we repeat this for all the other weights. . Gradient calculation relates to calculating derivatives and with this we are stepping firmly into the space of calculus. I don&#39;t fully understand how all this works, but intuitively, the important thing to know is this: we are calculating the change of the value, not the value itself. I.e. we want to know how things will change (by how much, and in what direction) if we shift this value slightly. . Note that the process of calculating the gradients is a performance optimisation. We could just as well have done this with a (slower) manual process where we adjust a little bit each time. With the gradient calculations we can take bigger steps in the direction we want, with more precision guiding our guesses about the direction and distance we want to go. . 5. &#39;Step&#39;: Update the weights . This is where we increase or decrease our own weights by a small amount and see whether the loss goes up or down. As hinted in step 4, we use calculus to figure out: . which direction to go in | how much we should increase or decrease our own weights | . 6. Repeat starting at step 2 . This is an iterative process. . 7. Iterate until we decide to stop . There are various criteria determining when we should stop. Some possible stopping points might include: . when the model is &#39;good enough&#39; for our use case | when we have run out of time (or money!) | when the accuracy starts getting worse (i.e. the model isn&#39;t performing as well) | . Calculating gradients with PyTorch . For those of us who don&#39;t bring a strong mathematics foundation into this space, the mention of calculus, derivatives and gradients isn&#39;t especially reassuring, but rest assured that PyTorch can help us out during this process. . There are three main parts to using PyTorch to calculate gradients (i.e. step four of the seven steps listed above.) . 1. Setup: add .requires_grad_() to a tensor . For any Tensor where we know we&#39;re going to want to calculate the gradients of values, we call .require_grad() on that Tensor. . def f(x): return x**2 x_tensor = torch.tensor(3.).requires_grad_() y_tensor = f(x_tensor) y_tensor . tensor(9., grad_fn=&lt;PowBackward0&gt;) . Here we can see that 3 squared is indeed 9, and we can see the grad_fn as part of the Tensor. . 2. Use .backward() to calculate the gradient . This actually refers to backpropagation, something which is explained much later in the book. This step is also known as the &#39;backward pass&#39;. Note, that this is again another piece of jargon that we just have to learn. In reality this method might as well have been called .calculate_gradients(). . y_tensor.backward() . 3. Access the gradient via the .grad attribute . We view the gradient by checking this .grad attribute. . x_tensor.grad . tensor(6.) . I can&#39;t explain why this is the case, since I&#39;ve never learned how to calculate gradients or derivatives (or anything about calculus, for that matter!) but in any case it&#39;s not really important. . Note that we can do this whole process over Tensors that are more complex than illustrated in the above simple example: . complex_x = tensor([3., 5., 12.]).requires_grad_() def f(x): return (x**2).sum() complex_y = f(complex_x) complex_y.backward() complex_x.grad . tensor([ 6., 10., 24.]) . Something else I discovered while doing this was that gradients can only be calculated on floating point values, so this is why when we create x_tensor and complex_x we create them with floating point values (3. etc) instead of just integers. In reality, I think there will be some kind of normalisation of our values as part of the process, so they would probably already be floats, but it&#39;s worth noting. . &#39;Stepping&#39;: using learning rates to figure out how much to step . Now that we know how to calculate gradients, the key question for the fifth step in our seven-step process is the following: . &quot;How much should we shift our values based on what we get back from our gradient calculations? . We call this amount the &#39;learning rate&#39;, and it is usually a value between 0.001 and 0.1. Very small, in other words :) We adjust our weights / parameters by this basic equation: . weights -= weights.grad * learning_rate . This process is called &quot;stepping the parameters&quot; and it uses an optimisation step. . The learning rate shouldn&#39;t be too high, else our loss can get higher / worse or otherwise we can just bounce around within the boundaries of our function without ever reaching the optimum (== lowest) loss. . At the same time, we shouldn&#39;t use a very tiny learning rate (i.e. even tinier than the 0.001-0.1 mentioned above) since then the process will take a really long time and while we might reach the optimum loss at some point, it might not be the fastest way to get there. . Takeaways from the seven-step process . Most of this makes a lot of intuitive sense to me. The parts that don&#39;t are what is going on with the gradient calculations and finding of derivatives and so on. For now, it appears that I can get away without understanding precisely how that works. It is enough to appreciate that we have a way to make these calculations, and those calculations are optimised for us .",
            "url": "https://mlops.systems/fastai/computervision/pytorch/2022/05/12/seven-steps-gradient-calculations.html",
            "relUrl": "/fastai/computervision/pytorch/2022/05/12/seven-steps-gradient-calculations.html",
            "date": " ‚Ä¢ May 12, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "A dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset",
            "content": "!pip install -Uqq fastbook nbdev torch import fastbook fastbook.setup_book() from fastai.vision.all import * from fastbook import * import torch from torch.utils.data import Dataset from torchvision import datasets from torchvision.transforms import ToTensor import matplotlib.pyplot as plt matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . . Get the data . First thing to do: we&#39;ll download the MNIST dataset for us to use as our example. . path = untar_data(URLs.MNIST_SAMPLE) . Path.BASE_PATH = path . path.ls() . (#3) [Path(&#39;valid&#39;),Path(&#39;labels.csv&#39;),Path(&#39;train&#39;)] . (path / &#39;train&#39;).ls() . (#2) [Path(&#39;train/7&#39;),Path(&#39;train/3&#39;)] . 3 and 7 are the labels or targets of our dataset. We are trying to predict whether, given a particular image, we are looking at a 3 or a 7. . threes = (path / &#39;train&#39;/&#39;3&#39;).ls().sorted() sevens = (path / &#39;train&#39;/&#39;7&#39;).ls().sorted() threes . (#6131) [Path(&#39;train/3/10.png&#39;),Path(&#39;train/3/10000.png&#39;),Path(&#39;train/3/10011.png&#39;),Path(&#39;train/3/10031.png&#39;),Path(&#39;train/3/10034.png&#39;),Path(&#39;train/3/10042.png&#39;),Path(&#39;train/3/10052.png&#39;),Path(&#39;train/3/1007.png&#39;),Path(&#39;train/3/10074.png&#39;),Path(&#39;train/3/10091.png&#39;)...] . im3_path = threes[1] im3 = Image.open(im3_path) im3 . Now we&#39;re getting a slice of the image to see how it is represented underneath as numbers. . The first index value is the rows we want, and then we get the columns. . array(im3)[4:10,4:10] . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=uint8) . array(im3)[4:10,4:15] . array([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29, 150, 195, 254, 255, 254], [ 0, 0, 0, 48, 166, 224, 253, 253, 234, 196, 253], [ 0, 93, 244, 249, 253, 187, 46, 10, 8, 4, 10], [ 0, 107, 253, 253, 230, 48, 0, 0, 0, 0, 0], [ 0, 3, 20, 20, 15, 0, 0, 0, 0, 0, 43]], dtype=uint8) . We can also have the same as a tensor. This is a PyTorch object which will allow us to get the benefits of everything PyTorch has to offer (plus speed optimisation if we use a GPU). . tensor(im3)[4:10,4:15] . tensor([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29, 150, 195, 254, 255, 254], [ 0, 0, 0, 48, 166, 224, 253, 253, 234, 196, 253], [ 0, 93, 244, 249, 253, 187, 46, 10, 8, 4, 10], [ 0, 107, 253, 253, 230, 48, 0, 0, 0, 0, 0], [ 0, 3, 20, 20, 15, 0, 0, 0, 0, 0, 43]], dtype=torch.uint8) . It basically looks the same except for the name of the object at this moment. . If we plot out the values of a different slice we can visualise how the numbers are used to output the actual image. . im3_tensor = tensor(im3) df = pd.DataFrame(im3_tensor[4:15,4:22]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . &nbsp; 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | . 3 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | . 4 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | . 5 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | . Note that the rest of this is 28x28 pixels for the whole image, and each pixel can contain values from 0 to 255 to represent all the shades from white to black. . APPROACH 1: PIXEL SIMILARITY . Find the average pixel value for each pixel of the 3s and 7s | Use this &#39;ideal&#39; 3 and 7 to compare a single image that we want to know whether it&#39;s a 3 or a 7 | three_tensors = [tensor(Image.open(img)) for img in threes] seven_tensors = [tensor(Image.open(img)) for img in sevens] len(three_tensors), len(seven_tensors) . (6131, 6265) . We can view these images with the fastai show_image function. This takes a tensor and makes it viewable in a Jupyter Notebook: . show_image(three_tensors[8]); . torch.stack(three_tensors)[0][4:15, 4:15] . tensor([[ 0, 0, 0, 0, 0, 0, 0, 42, 118, 219, 166], [ 0, 0, 0, 0, 0, 0, 103, 242, 254, 254, 254], [ 0, 0, 0, 0, 0, 0, 18, 232, 254, 254, 254], [ 0, 0, 0, 0, 0, 0, 0, 104, 244, 254, 224], [ 0, 0, 0, 0, 0, 0, 0, 0, 207, 254, 210], [ 0, 0, 0, 0, 0, 0, 0, 0, 84, 206, 254], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 209], [ 0, 0, 0, 0, 0, 0, 0, 0, 91, 137, 253], [ 0, 0, 0, 0, 0, 0, 40, 214, 250, 254, 254], [ 0, 0, 0, 0, 0, 0, 81, 247, 254, 254, 254], [ 0, 0, 0, 0, 0, 0, 0, 110, 246, 254, 254]], dtype=torch.uint8) . It&#39;s fairly common to have to convert a collection (i.e. in our case, a list) of tensors into a single tensor object with an extra dimension, so that&#39;s why we have the torch.stack method which takes a collection and returns a rank-3 tensor. . . Tip: You&#8217;ll see here that we also convert our values into floats and normalise them (i.e. divide by 255) since this will help us at a later stage. . stacked_threes = torch.stack(three_tensors).float()/255 stacked_sevens = torch.stack(seven_tensors).float()/255 . We can see that the first axis of this tensor is our 6131 images, and each image has 28x28 pixels. The shape tells you the length of each axis. . stacked_threes.shape . torch.Size([6131, 28, 28]) . Note: the length of a tensor&#39;s shape is its rank. We have a rank-3 tensor: . len(stacked_threes.shape) . 3 . stacked_threes.ndim . 3 . Rank = the number of axes in a tensor. | Shape = the size of each axis of a tensor. | . Getting the ideal 3 . We take our stack of threes (i.e. a rank-3 tensor) and we calculate the mean across the first dimension (i.e. the 6131 individual threes). . By the end, you can see that mean3 is actually a 28x28 tensor (i.e. a single image) that represents the ideal version of our threes data. . mean3 = stacked_threes.mean(0) . mean3.shape . torch.Size([28, 28]) . show_image(mean3); . mean7 = stacked_sevens.mean(0) show_image(mean7); . Comparing our ideal numbers with an individual sample . sample_3 = stacked_threes[35] sample_7 = stacked_sevens[35] show_image(sample_3); . There are two main ways we can measure the distance (i.e. similarity) in this context: . the Mean Absolute Difference aka L1 Norm -- the mean of the absolute value of differences (our absolute value replaces negative values with positive values) | the Root Mean Squared Error (RMSE) aka L2 Norm -- we square all the differences (making everything positive), get the mean of those values, and then square root everything (which undoes all the squaring). | . def get_l1_norm(tensor1, tensor2): return (tensor1 - tensor2).abs().mean() def get_l2_norm(tensor1, tensor2): return ((tensor1 - tensor2)**2).mean().sqrt() . l1_norm_distance_3 = get_l1_norm(sample_3, mean3) l2_norm_distance_3 = get_l2_norm(sample_3, mean3) l1_norm_distance_3, l2_norm_distance_3 . (tensor(0.1401), tensor(0.2545)) . l1_norm_distance_7 = get_l1_norm(sample_3, mean7) l2_norm_distance_7 = get_l2_norm(sample_3, mean7) l1_norm_distance_7, l2_norm_distance_7 . (tensor(0.1670), tensor(0.3121)) . The differences from our sample_3 to the mean3 is less than the differences between our sample_3 and the mean7. This totally makes sense and is what we were expecting! . assert l1_norm_distance_3 &lt; l1_norm_distance_7 assert l2_norm_distance_3 &lt; l2_norm_distance_7 . The PyTorch built-in ways of calculating loss . PyTorch exposes a variety of loss functions at torch.nn.functional which it recommends importing as F. These are available by default under that name in fastai. . # !pip install -Uqq rich # from rich import inspect as rinspect # rinspect(F, methods=True) . pytorch_l1_norm_distance_3 = F.l1_loss(sample_3.float(), mean3) pytorch_l1_norm_distance_7 = F.l1_loss(sample_3.float(), mean7) assert pytorch_l1_norm_distance_3 &lt; pytorch_l1_norm_distance_7 pytorch_l1_norm_distance_3, pytorch_l1_norm_distance_7 . (tensor(0.1401), tensor(0.1670)) . For the L2 norm, the PyTorch function only calculates the mean squared error loss, so we have to add in the square root at the end ourselves: . pytorch_l2_norm_distance_3 = F.l1_loss(sample_3.float(), mean3).sqrt() pytorch_l2_norm_distance_7 = F.l1_loss(sample_3.float(), mean7).sqrt() assert pytorch_l2_norm_distance_3 &lt; pytorch_l2_norm_distance_7 pytorch_l2_norm_distance_3, pytorch_l2_norm_distance_7 . (tensor(0.3743), tensor(0.4087)) . When to choose one vs the other? . MSE penalises bigger mistakes more than the L1 norm | MSE is more lenient with small mistakes than the L1 norm | . Fashion MNIST . Load the Fashion MNIST dataset with PyTorch . The Fashion MNIST dataset was created as a more advanced / difficult (and perhaps interesting) alternative to MNIST for computer vision tasks. It was first released in 2017 and I thought it might be interesting to try the same technique as we tried there on this new data set. . training_data = datasets.FashionMNIST( root=&quot;data&quot;, train=True, download=True, transform=ToTensor() ) test_data = datasets.FashionMNIST( root=&quot;data&quot;, train=False, download=True, transform=ToTensor() ) . labels_map = { 0: &quot;T-Shirt&quot;, 1: &quot;Trouser&quot;, 2: &quot;Pullover&quot;, 3: &quot;Dress&quot;, 4: &quot;Coat&quot;, 5: &quot;Sandal&quot;, 6: &quot;Shirt&quot;, 7: &quot;Sneaker&quot;, 8: &quot;Bag&quot;, 9: &quot;Ankle Boot&quot;, } figure = plt.figure(figsize=(8, 8)) cols, rows = 3, 3 for i in range(1, cols * rows + 1): sample_idx = torch.randint(len(training_data), size=(1,)).item() img, label = training_data[sample_idx] figure.add_subplot(rows, cols, i) plt.title(labels_map[label]) plt.axis(&quot;off&quot;) plt.imshow(img.squeeze(), cmap=&quot;gray&quot;) plt.show() . I found it a bit hard to extract the data for the images, stored as it was together with the labels. Here you can see me getting the image and the label. . show_image(training_data[1][0][0]); . training_data[1][1] . 0 . There are 60,000 items in this training dataset, and 10 categories, so it makes sense that the set of images for the two categories I extracted would be 6000 each. . training_dresses = [item[0][0] for item in training_data if item[1] == 3] training_pullovers = [item[0][0] for item in training_data if item[1] == 2] len(training_dresses), len(training_pullovers) . (6000, 6000) . training_dresses_tensor = torch.stack(training_dresses) training_pullovers_tensor = torch.stack(training_pullovers) . training_dresses_tensor.shape . torch.Size([6000, 28, 28]) . Calculate the mean image for a dress and a pullover . mean_training_dress = training_dresses_tensor.mean(0) mean_training_pullover = training_pullovers_tensor.mean(0) . mean_training_dress.shape . torch.Size([28, 28]) . show_image(mean_training_dress); . show_image(mean_training_pullover); . I chose the two because I wondered whether there might be a decent amount of crossover between the two items. You can see even in the &#39;mean&#39; / average versions of the items that they look fairly similar. . Assemble the validation data . test_dresses = [item[0][0] for item in test_data if item[1] == 3] test_pullovers = [item[0][0] for item in test_data if item[1] == 2] len(test_dresses), len(test_pullovers) . (1000, 1000) . test_dresses_tensor = torch.stack(test_dresses) test_pullovers_tensor = torch.stack(test_pullovers) . test_dresses_tensor.shape . torch.Size([1000, 28, 28]) . I also extract a single dress and a single pullover to check the loss in the next section. . sample_test_dress = test_dresses_tensor[0] sample_test_pullover = test_pullovers_tensor[10] . show_image(sample_test_dress); . show_image(sample_test_pullover); . Calculate the loss comparing a random pullover with validation data . def get_l1_norm(tensor1, tensor2): return (tensor1 - tensor2).abs().mean() def get_l2_norm(tensor1, tensor2): return ((tensor1 - tensor2)**2).mean().sqrt() . l1_norm_distance_dress = get_l1_norm(sample_test_dress, mean_training_dress) l2_norm_distance_dress = get_l2_norm(sample_test_dress, mean_training_dress) l1_norm_distance_dress, l2_norm_distance_dress . (tensor(0.1134), tensor(0.1766)) . l1_norm_distance_pullover = get_l1_norm(sample_test_pullover, mean_training_pullover) l2_norm_distance_pullover = get_l2_norm(sample_test_pullover, mean_training_pullover) l1_norm_distance_pullover, l2_norm_distance_pullover . (tensor(0.1713), tensor(0.2220)) . The differences from our sample_test_dress to the mean_training_dress is less than the differences between our sample_test_dress and the mean_training_pullover. This totally makes sense and is what we were expecting! . assert l1_norm_distance_dress &lt; l1_norm_distance_pullover assert l2_norm_distance_dress &lt; l2_norm_distance_pullover . Using broadcasting to check our predictions on our validation data . This function returns the L1 norm loss when calculated between two tensors. Because of the final tuple passed into .mean() (i.e. (-1, -2)), we can actually use this function to calculate the distance between a single image as compared to a full rank-3 tensor. . def fashion_mnist_distance(tensor1, tensor2): return (tensor1 - tensor2).abs().mean((-1, -2)) . fashion_mnist_distance(sample_test_dress, mean_training_dress) . tensor(0.1134) . fashion_mnist_distance(sample_test_dress, mean_training_pullover) . tensor(0.2864) . Again, our dress is &#39;closer&#39; to the mean_training_dress than it is to the mean_training_pullover. . We can now create a function that predicts (by way of calculating and comparing these two losses) whether an item is a dress or not. . def is_dress(x): return fashion_mnist_distance(x, mean_training_dress) &lt; fashion_mnist_distance(x, mean_training_pullover) . is_dress(sample_test_dress) . tensor(True) . is_dress(sample_test_pullover) . tensor(False) . ...as expected... . Finally, we can get an overall sense of how well our prediction function is on average. We get it to calculate predictions for the entire test dataset and average them out. . dress_accuracy = is_dress(test_dresses_tensor).float().mean() . pullover_accuracy = 1 - is_dress(test_pullovers_tensor).float().mean() . combined_accuracy = (dress_accuracy + pullover_accuracy) / 2 combined_accuracy, dress_accuracy, pullover_accuracy . (tensor(0.9175), tensor(0.9730), tensor(0.8620)) . Overall, I think 91% accuracy using this fairly simple mechanism isn&#39;t too bad at all! That said, in the fastai course we&#39;re here to learn about deep learning, so in the next post I will dive more into the beginnings of a more advanced approach to making the same calculation. .",
            "url": "https://mlops.systems/fastai/computervision/pytorch/2022/05/11/fashion-mnist-pixel-similarity.html",
            "relUrl": "/fastai/computervision/pytorch/2022/05/11/fashion-mnist-pixel-similarity.html",
            "date": " ‚Ä¢ May 11, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "A painless way to create an MVP demo using computer vision models",
            "content": "üö¶ Motivation . After the second class of the fastai course, we‚Äôre encouraged to create mini-projects that result in models we can deploy online. Deployment is a huge field with its own complexities, of course, but having an option to get something out in the world that‚Äôs visible and usable is extremely useful. . In this post, I will walk you through how I built a super quick MVP of my redacted document detector project. I used: . fastai to classify and extract redacted pages extracted from PDFs | icevision (@ai_fast_track) to detect the redacted areas | HuggingFace Spaces (with Gradio and Streamlit) to deploy my MVP | . The post shows how I went about thinking through the task, showcasing some examples of small prototypes I built along the way, including the final stage where I built: . an app including everything that would be needed by a final ‚Äòdeployed‚Äô use case of my model | two models working in tandem in the same app (one classification, one object detection) | optional PDF generation of items detected by the model (!) | . I also explore why you might want to have a minimal deployed version of your application in the first place! . üêæ Step by step, iteration by iteration . This week I chose to use my previous work on redacted images to leverage a dataset I‚Äôd previously collected. I wanted to showcase something useful and interesting and I ended up slightly blocked as to what I was going to build. After discussing it with the study group briefly, I was reminded not to try to bite off too much: start small with the smallest possible next version of what you want, and then continue from there. . Since I already had a large dataset of redacted and unredacted images (extracted from PDF documents available online), I used this to train a classification model that could tell whether a page contained redactions or not. . With that model exported, it was then easy to get a simple Gradio app demo up and running, particularly with the suggestions in Tanishq Abraham‚Äôs really useful tutorial blogpost. . It‚Äôs an easy step to go from having a Gradio app deployed to then hosting that same demo as a Huggingface Space, so I then did that. You can access the demo here at strickvl/fastai_redaction_classifier. . . At this first stage I had the exported model itself uploaded inside the Spaces repository, but this useful blog by Omar Espejel showed how I could just upload my model directly to the Huggingface model hub. Instead of calling learn.export(&#39;model.pkl&#39;) and uploading the model file itself, I could just run the following code after authentication: . from huggingface_hub import push_to_hub_fastai repo_id = &quot;MY_USERNAME/MY_LEARNER_NAME&quot; push_to_hub_fastai(learner=learn, repo_id=repo_id) . My model lives here on the Huggingface model hub and can be directly saved or just used via the hosted Inference API. . ‚ö°Ô∏è Using the inference API for more flexibility . Buoyed on by Tanishq‚Äôs blog and the workflow we‚Äôd seen in the lecture that week, I thought it might be worth running my inference requests through the HTTP API instead of letting Huggingface handle all that. . Thanks to a really simple and comprehensible example made by @Nuvic I was quickly able to get something up and running. The forked source code is available here and the main website where you can try out the tool is here: https://strickvl.github.io/predict_redaction_classification/. . If you search for ‚Äòredacted document‚Äô images and save one of them do your local computer you can use those to try it out. It uses simple Javascript code to pass the image you upload into the inference API using a simple HTTP request. It parses the results and displays them as shown here: . . While the demo gives a sense of the model‚Äôs capabilities, in reality you would probably not find it very helpful to use a web app that required you to feed a document‚Äôs pages to it one by one. I started to think about a more complex application where you could upload a PDF and it would split the PDF for you and do all the inference behind the scenes. . üöÄ Building an MVP of a redaction detection application . I spent a brief half-hour considering deploying a simple Flask web app hosted somewhere for free before realising I didn‚Äôt even need to go that far to create a proof of concept that would have the required functionality. I returned back to Huggingface Spaces hoping that I‚Äôd be able to build everything out. . You can access the demo / MVP app that I created here: https://huggingface.co/spaces/strickvl/redaction-detector . . This MVP app runs two models to mimic the experience of what a final deployed version of the project might look like. . The first model (a classification model trained with fastai, available on the Huggingface Hub here and testable as a standalone demo here), classifies and determines which pages of the PDF are redacted. I‚Äôve written about how I trained this model here. | The second model (an object detection model trained using IceVision (itself built partly on top of fastai)) detects which parts of the image are redacted. This is a model I‚Äôve been working on for a while and I described my process in a series of blog posts. | . This MVP app does several things: . it extracts any pages it considers to contain redactions and displays that subset as an image carousel. It also displays some text alerting you to which specific pages were redacted. | if you click the ‚ÄúAnalyse and extract redacted images‚Äù checkbox, it will: | pass the pages it considered redacted through the object detection model | calculate what proportion of the total area of the image was redacted as well as what proportion of the actual content (i.e. excluding margins etc where there is no content) | create a PDF that you can download that contains only the redacted images, with an overlay of the redactions that it was able to identify along with the confidence score for each item. | . Converting a Gradio app over to Streamlit . I was curious about the differences between the main two options enabled by Huggingface Spaces, so I then worked a little on converting my Gradio app to a Streamlit app. The process of conversion was fairly easy for the most part; the only difference is the style of programming expected by Streamlit: . Streamlit is a less declarative style of creating your app. It runs your code from top to bottom, rendering whatever elements you specify. | This seems to result in more verbose code (e.g. compare this with this). | . There are two easy ways to deploy a Streamlit app: either host it natively on Streamlit itself, or host it on Huggingface Spaces. The advantage of hosting natively on Streamlit is that you essentially have what looks and feels like a custom website that is 100% your application. In the end, I didn‚Äôt go down this route for two reasons: . Hosting via Huggingface Spaces keeps the connection between your demo app and your username. You can click through to view all of my demos and applications here, for example. On Streamlit there is currently no concept of a user‚Äôs portfolio. If you‚Äôre trying to showcase your work, Huggingface Spaces is the clear winner in this regard. | Hosting on Streamlit seems to have restrictive memory constraints. I frequently ran into restrictions on the machine that was running my application and would quite often be encouraged to reboot my app, clearing its cache, and instructed to refer to docs on how to make my application more efficient. The docs were useful, but I ran into issues using the Streamlit cache (the main solution offered) because of the models I was using. Luckily, Huggingface Spaces‚Äô backend instances seem far more generous in terms of resources. For small / trivial apps not doing much you‚Äôll be fine with Streamlit, but for anything more involved there‚Äôs more of a decision to be made. | I didn‚Äôt convert all the various parts of my Gradio app over to work on Streamlit ‚Äî in particular extraction of images and display as a carousel was non-trivial ‚Äî but you can get a sense of the flexibility with this image: . . (Alternatively, you can try it out over on Huggingface Spaces here.) . You can see that I was able to insert a chart to display the proportion calculations. This is much more pleasant than the pure text version. Streamlit‚Äôs documentation is pretty great and their basic ‚ÄòGet started‚Äô tutorial should indeed be your first port of call. . ü§î Lessons learned . . In this post you learned: . 1Ô∏è‚É£ to start with simple prototypes . 2Ô∏è‚É£ how to easily deploy fastai models on Huggingface Spaces and the Hub and . 3Ô∏è‚É£ that you can create functional MVP demos of real products and applications . I was ‚Äî and continue to be ‚Äî surprised that the free Huggingface Spaces environment has no problem running all this fairly compute-intensive inference on their backend. (That said, if you try to upload a document containing dozens or hundreds of pages and you‚Äôll quickly hit up against the edge of what they allow.) . I became very familiar with the Gradio interface docs while creating this app and was impressed by how customisable the final application could be. You don‚Äôt have as much freedom as a web application written from scratch, but you still do have a lot of freedom. . üìê When to use Gradio . if you have a simple use case that you want to highlight | if your inputs and outputs are clearly defined | if you have a single model to showcase | if you want to get something quickly deployed | . üåä When to use Streamlit . if your use case is more interactive or less simple than just basic input-then-output | if you want more control on how your demo application is displayed | if you enjoy a more imperative style of programming | . Given how much inference is going on behind the scenes, I‚Äôm surprised that these applications run as fast as it does. For a document with 4 or 5 redacted pages, it takes around 10 seconds to do all the steps described above. 10 seconds is still far too long for a scenario where you wanted to run inference over millions of pages, but in that scenario you wouldn‚Äôt be manually uploading them on a web app either. . It‚Äôs extremely gratifying to have these kinds of tools available to use for free, and really exciting that you get to build out prototypes of this kind after just two weeks of study on the fastai course. .",
            "url": "https://mlops.systems/fastai/computervision/redactionmodel/tools/2022/05/07/redaction-mvp-huggingface.html",
            "relUrl": "/fastai/computervision/redactionmodel/tools/2022/05/07/redaction-mvp-huggingface.html",
            "date": " ‚Ä¢ May 7, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "How my pet cat taught me a lesson about validation data for image classification",
            "content": "I‚Äôm participating in the latest iteration of the fastai course as taught by Jeremy Howard. This past week we got a very high-level overview of some of the ways deep learning is proving very powerful in solving problems as well as how we can use its techniques to fairly quickly get great results on image classification problems. . I‚Äôve done the earlier parts of the course before, so some of these demonstrations were less mind-blowing than the first time I saw them. For this iteration of the course, Jeremy showcased a Kaggle notebook which trains a model to distinguish whether an image is of a bird or not. . Last time I did the course, I trained an image classifier model to distinguish whether an image was redacted or not to around 95% accuracy. (This actually was the genesis of my larger redaction object detection project that I‚Äôve been blogging about for the past few months.) . The key ingredients: what goes into a model? . The course teaches things top-down, so we start off with both the practical experience of training state-of-the-art models as well as the overall context to what goes into these high-level functions. These pieces include: . your input data ‚Äî this style of programming differs from traditional software engineering where your functions take data in order to ‚Äòlearn‚Äô how to make their predictions | the ‚Äòweights‚Äô ‚Äî when we‚Äôre using pre-trained models, you can think of these as an initial set of variables that are already pretty useful in that configuration and can do a lot of things. | your model ‚Äî this is what you‚Äôre training and, once trained, you can think of it as a function in and of itself that takes in inputs and outputs predictions. | the predictions ‚Äî these are the guesses that your model makes, based on whatever you pass in as inputs. So if you pass in an image of a cat to a model (see below), the prediction could be whether that cat is one particular kind or another. | your ‚Äòloss‚Äô ‚Äî this is a measure of checking how well your model is doing as it trains. | a means of updating your weights ‚Äî depending on how well (or badly) the training goes, you‚Äôll want a way of updating the weights so that each time it gets a bit better at optimising for whatever you‚Äôve set up your model to do. In lesson one we learn about stochastic gradient descent, a way of optimising and updating these weights automatically. | your labels ‚Äî these are the ground truth assertions that get used to determine how well the model is doing as it trains. | transformations &amp; augmentations ‚Äî more on this will come in lesson two, but these allow you to squeeze more value out of your data. This is especially valuable when you‚Äôre fine-tuning a model and don‚Äôt have massive amounts of data to use for training. | . Represented in code, the classic fastai example where you train a model to distinguish between cats and dogs is as follows: . from fastai.vision.all import * path = untar_data(URLs.PETS)/&#39;images&#39; def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func( path, get_image_files(path), valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) learn = vision_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(1) . This small code snippet contains all the various parts just mentioned. The high-level API and abstractions that fastai provides allows you to work with these concepts in a way that is fast and flexible, though if you need to dive into the details you can do so as well. . Image classification isn‚Äôt just about images . One of the parts of the first chapter I enjoy the most is the examples of projects where image classification was applied to problems or scenarios where it doesn‚Äôt first appear that the problem has anything to do with computer vision. . We see malware converted into images and distinguished using classification. We see sounds in an urban environment converted into images and classified with fastai. In the study group I host for some student on the course, one of our members presented an initial proof of concept of using images of music to distinguish genre: . Saturday evening side-project: Trained a baseline ML model to classify audio files to identify their music genre using @fastdotai based on a kaggle dataset.Acheived only 50% accuracy, probably because problem is hard. Next job is to check what @DienhoaT has done to win a GPU. pic.twitter.com/EahvgtYBDL . &mdash; Kurian Benoy (@kurianbenoy2) April 30, 2022 I like the creativity needed to think of how to turn problems and data into a form such that they can become computer vision problems. . My own efforts: classifying my cat . True story: a few years ago my cat escaped from the vet and a reward was mentioned for anyone who found our cute ginger cat. Throughout the course of the day, the vets were perplexed to see people coming in with random ginger cats that they‚Äôd found in the neighborhood, but none of them were ours! With this iteration of the course, therefore, I was curious to try out this simple but slightly silly example and see how well a deep learning model could do at recognising distinguishing Mr Blupus ‚Äî don‚Äôt ask! ‚Äî from other random photos of ginger cats. . Training the model was pretty easy. Like any cat owner, I have thousands of photos of our cat so an initial dataset to use was quick to assemble. I downloaded a few hundred random ginger cat photos via DuckDuckGo using some code Jeremy had used in his bird vs forest Kaggle notebook. A few minutes and ten epochs later, I had achieved 96.5% accuracy on my validation data after fine-tuning resnet50! . Little Saturday afternoon side-project: training an image classification model using @fastdotai to distinguish my cat (&#39;Mr Blupus&#39; -- don&#39;t ask!) from other random ginger cat photos.Achieved 96.5% accuracy on the validation set within a few minutes! pic.twitter.com/by5ZlM0Kkp . &mdash; Alex Strick van Linschoten (@strickvl) April 30, 2022 After the initial excitement died down, I realised that the result was probably an illusion. Our cat is an indoor cat and we have a relatively small house. Couple that with the fact that the backdrops to the photos of Mr Blupus are relatively distinctive (particular kinds of sheets or carpets) and it seems pretty clear that the model wasn‚Äôt learning how to identify our cat, but rather it was learning how to distinguish photos of our house or our carpets. . ‚òπÔ∏è . Luckily, chapter one gets into exactly this problem, showing an example of how exactly this validation issue can give you a false sense of confidence in your model. When I evaluated my model on the validation data it wasn‚Äôt a fair test, since in all likeliness may model had already seen a similar backdrop to whatever was found inside the validation set. . I discussed this when I presented this to those at the study group / meetup yesterday and we agreed that it‚Äôd be best if I held out some settings or locations from the training entirely. I took 30 minutes to do that in the evening and had a third ‚Äòtest‚Äô dataset which consisted of 118 images of our cat in certain locations that the model wasn‚Äôt trained on and thus couldn‚Äôt use to cheat. I added a few more photos to the training data so that there were enough examples from which to learn. . . I was supposedly getting 98% accuracy now, but I knew that number to be false. I then needed to figure out how to get the accuracy for my held-out test set. With a lot of help from Francesco and a really useful blogpost on doing batch inference with fastai, I first got the predictions for my test data: . test_files = [fn for fn in sorted((Path(&quot;/path/to/test_set_blupus_photos&quot;)).glob(&#39;**/*&#39;)) if fn.is_file()] test_dl = learn.dls.test_dl(test_files) preds, _ = learn.get_preds(dl=test_dl) . I then created a tensor with the ground truth predictions for my test set and compared them with what my model had predicted: . gts = torch.tensor([0 for _ in range(118)]) accuracy = (gts == preds.argmax(dim=1)) . At this point, getting the final accuracy was as simple as getting the proportion of correct guesses: . sum([1 for item in accuracy if item]) / len(preds) . This gave me an accuracy on my held-out test set of 93.2% which was surprisingly good. . I half wonder whether there is still some cheating going on somehow, some quality of the photos or the iPhone camera I used to take them that is being used to distinguish the photos of my cat vs other ginger cats. . Nevertheless, this was a useful lesson for me to learn. I realised while working with the tensors in the final step above that I‚Äôm not at all comfortable manipulating data with PyTorch so luckily that‚Äôll get covered in future lessons. . UPDATE: . Following some discussion in the fastai forums, it was suggested that I take a look at Grad-CAM in chapter 18. This is a technique to visualise the activations which allows you to see which parts of the image it is paying the most attention to (sort of). I ran the code using a sample Blupus image and this was the result. I don‚Äôt understand how most (any?) of this works, but it was really cool to have a working result of sorts nonetheless! . .",
            "url": "https://mlops.systems/fastai/computervision/2022/05/02/pet-cat-image-classifier-fastai.html",
            "relUrl": "/fastai/computervision/2022/05/02/pet-cat-image-classifier-fastai.html",
            "date": " ‚Ä¢ May 2, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "How to trust the data you feed your model: alternative data validation solutions in a computer vision context (part 3)",
            "content": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.) . The previous two posts in this series have made the case for why you might want to consider adding a Great Expectations step or stage to your computer vision project, particularly once it becomes something you‚Äôre going to want to iterate on a few times. . This post begins by showcasing how you can use Evidently‚Äôs open-source library to calculate and visualise comparisons between your data. I list some of the lighter alternatives to Great Expectations and Evidently, concluding with some thoughts on when you might use it as part of your computer vision pipeline. . TL;DR: Alternatives for data validation using Python . üõ† Data validation tools come in many flavours, from full-featured libraries like Great Expectations down to the humble assert statement in Python. . | ‚ö†Ô∏è The tool you choose should be appropriate to your particular use case and situation. You might not need or want to add a large dependency or take on extra code / project complexity, in which case there are alternative options available to you. . | ‚è∞ You‚Äôll also want to think about when you‚Äôre doing your validation. Two key moments stand out for machine learning projects: when you‚Äôre ingesting data prior to training or fine-tuning a model, and at the moment where you‚Äôre doing inference on a trained model. . | üìÉ For my project, I‚Äôm using a variety of tools as part of my process because I‚Äôve found it gives me confidence in the predictions my model is making and it gives me freedom to experiment and iterate, without needing to also worry that I‚Äôm silently breaking something with downstream effects on my model performance. . | . Alternatives: Using Evidently for drift detection . I‚Äôve previously written about why Evidently is a great tool to use for drift detection and data monitoring over on the ZenML blog. At its core, Evidently takes two chunks of data and compares them. The statistical comparisons going on under the hood are quite sophisticated, but as an interface to be used it is extremely trivial to get going. . In the case of my redaction project data, I did the work of transforming my annotation and image metadata into Pandas DataFrames for Great Expectations already, so using it with Evidently at this point is trivial: . from evidently.dashboard import Dashboard from evidently.dashboard.tabs import DataDriftTab from evidently.pipeline.column_mapping import ColumnMapping real_annotations = main_annotations_df[[&#39;area&#39;, &#39;category_name&#39;, &#39;top_left_x&#39;, &#39;top_left_y&#39;, &#39;width&#39;, &#39;height&#39;, &#39;orientation&#39;]] easy_synth_annotations = easy_synth_annotations_df[[&#39;area&#39;, &#39;category_name&#39;, &#39;top_left_x&#39;, &#39;top_left_y&#39;, &#39;width&#39;, &#39;height&#39;, &#39;orientation&#39;]] hard_synth_annotations = hard_synth_annotations_df[[&#39;area&#39;, &#39;category_name&#39;, &#39;top_left_x&#39;, &#39;top_left_y&#39;, &#39;width&#39;, &#39;height&#39;, &#39;orientation&#39;]] column_mapping = ColumnMapping( numerical_features=[&quot;area&quot;, &quot;width&quot;, &quot;height&quot;, &#39;top_left_x&#39;, &#39;top_left_y&#39;], categorical_features=[&quot;category_name&quot;, &#39;orientation&#39;], ) drift_report = Dashboard(tabs=[DataDriftTab()]) drift_report.calculate(real_annotations, hard_synth_annotations, column_mapping=column_mapping) drift_report.save(&quot;reports/my_report.html&quot;) . In this code, I‚Äôm comparing between the real (i.e. manually annotated) annotations and the ‚Äòhard‚Äô synthetic annotations that I created (and blogged about recently). I choose the columns I care about, tell Evidently which columns are numerical vs categorical features and save the report. (I can also display the report directly within a Jupyter notebook.) When I open the report, I see this: . . You can unfold the graphs to dive into the details for specific features, as in the following example where I take a look at the orientation of my annotations and see the difference between my manual annotations and the synthetically generated ‚Äòhard‚Äô batch: . . It doesn‚Äôt surprise me too much that we have this disparity, since the only annotations that are portrait in the synthetically-generated set are those for the content box around the whole page. All the rest are landscape, and that‚Äôs by design. (Note: you can make the comparisons using different statistical tests depending on your use case. I‚Äôm told that the next Evidently release will increase the number of available options for this.) . I can repeat the same test for the image DataFrame. I‚Äôve included some metadata for each image such as how many annotations are associated with the image, or how many redaction vs content annotations are associated and so on. The code is basically the same, except now taking into account the different columns and their types: . # comparing between real images and hard_synth images column_mapping = ColumnMapping( numerical_features=[&quot;area&quot;, &quot;width&quot;, &quot;height&quot;, &#39;annotation_count&#39;, &#39;content_annotation_count&#39;, &#39;redaction_annotation_count&#39;, &#39;area&#39;, &#39;file_size_bytes&#39;], categorical_features=[&#39;orientation&#39;, &#39;format&#39;, &#39;mode&#39;], ) drift_report = Dashboard(tabs=[DataDriftTab()]) drift_report.calculate(main_images, hard_synth_images, column_mapping=column_mapping) drift_report.save(&quot;reports/my_report-real-vs-hard-images.html&quot;) . And we get this report: . . You can immediately see how certain things like the number of annotations and the number of redactions in an image was a bit different when comparing the two. We also seem to have a far more even distribution of file sizes in the synthetically generated images and that makes sense since that was essentially randomly determined. . Note that all the data that goes into making these reports can be accessed programatically as a Python object or JSON through Evidently‚Äôs Profile feature, which is probably what you‚Äôre going to want when assessing for drift as part of a continuous training / continuous deployment cycle. . If you change just a few things once more, you get a really useful data quality report showing distributions, correlations, and various other features of your data at a single glance: . # profiling data quality from evidently.dashboard.tabs import DataQualityTab quality_dashboard = Dashboard(tabs=[DataQualityTab()]) quality_dashboard.calculate(main_images, hard_synth_images, column_mapping=images_column_mapping) quality_dashboard.save(&quot;reports/quality-report.html&quot;) . You can get an idea of the report that it produces in the following screen recording from my browser: . . As a place to get started with understanding a dataset, this is a pretty nice visualisation and report to have in your toolkit, but even after immersion in your data it can be useful to take a step back with something like this data quality overview. For instance, it reveals quite clearly how the average number of annotations in my manually annotated dataset is quite a bit lower than that of my synthetically generated examples. Of course, that was by intention, but it is nice to see that confirmed in the data. . Once you have your model ready, there are other reports that Evidently offers which perhaps I‚Äôll return to in a subsequent blogpost but for now I hope this has given you a flavour of the tool and how easy it is to get going with it. . (As a side-note, Evidently‚Äôs community is friendly, welcoming and filled with interesting people thinking about these issues. I find it a welcome breath of fresh air when compared with some other tools‚Äô forums or chat platforms, so it also has that going for it!) . Alternatives: some other options . With Evidently, we drifted a little into the ‚Äòvisualise your data‚Äô territory which wasn‚Äôt really the point of this post, but you can see how they combined clear visualisation with the statistical validation working underneath to calculate whether data was drifting. The following are some other tools I‚Äôve come across that might help you in validating data in a computer vision context. I haven‚Äôt found a use for them in my project, but it‚Äôs possible that they might gel with what you‚Äôre doing: . TensorFlow Data Validation (TFDV) ‚Äî This is a part of TensorFlow and tfx which uses schemas to validate your data. If you‚Äôre using TensorFlow, you might have heard of this and might even be using it already, but I don‚Äôt get the sense that this is often much recommended. I include it as it is a prominent option available to you. | Deepchecks ‚Äî Deepchecks is adjacent to what Great Expectations offers, albeit with an emphasis on the kinds of tests you might want to do for ML model training code. It has some features and documented use cases for computer vision (object detection and classification) but I haven‚Äôt used it myself. Feels like a tool worth keeping your eye on, however. (Works on Pandas dataframes and numpy arrays.) | pandera ‚Äî This is a statistical tool for validating data inside dataframes, and it overlaps quite a bit in its functionality with Great Expectations, particularly with the hypothesis testing functionality. Worth checking out. | Cerberus ‚Äî Offers a lightweight schema-based validation functionality for Python objects. | jsonschema ‚Äî similar in approach to Cerberus, above, this is a lightweight way to test your JSON files based on how they conform to a defined schema. Useful in the case of annotations files, perhaps, if you really want something minimal. | schema ‚Äî More of the same: a Pythonic way to validate JSON or YAML files based on schema. | assert ‚Äî We shouldn‚Äôt forget the humble assert statement, which I have sprinkled in various places within my code where it makes sense to make sure that data flowing through conforms to whatever implicit or explicit contracts exist. | . I mention these various options not to suggest that you should use them all, but rather to state that you have options ranging the whole spectrum of complexity and dependency. . When to do data validation in your project . Regularly! I‚Äôve written previously about how you can think about data validation as testing for your data. Just like many (most?) engineering teams run their tests every time you add a new commit to the codebase, it‚Äôs worth thinking of these kinds of tests as something that get run at any point where the underlying data gets updated. . There are three points where it might make sense to do some data validation: . at the point of data ingestion | at the point just prior to training a model, i.e. after your data has been split into training and validation sets | at the point of inference (i.e. using the data being passed into the trained model) | . . The first (at data ingestion) is essential, especially if you have any kind of continuous training or continuous deployment loop going on. You don‚Äôt want to be training on data that clearly is unsuitable for training, or where the distribution has shifted so much that it‚Äôs going to cause hidden problems down the line. . The second (at training-validation split time) may or may not be important depending on your use case. For my redaction project I don‚Äôt think there is a great deal of benefit from this and so haven‚Äôt incorporated it as such. . The third (at inference time) is quite important to have, even though the behaviour when an anomaly is detected might be different from if you were to detect issues earlier on in the process. You might choose to just log the result of your validation check internally, or you could potentially also feed the result back to a user in the terms of some sort of warning (i.e. if the image that they were uploading was a very different kind of image from the data that had been used to train the model). . What I‚Äôm using for my redaction project . I don‚Äôt have any general advice as to which tool you should use as part of your computer vision model training pipeline. It‚Äôs likely to be heavily context-dependent and will differ based on the particular use case or problem you‚Äôre trying to solve. For my own project, however, I can be more specific. . I‚Äôm using plain assert statements liberally scattered through my code, in part leftover from when writing the code but also as a failsafe should strange or anomalous data make its way into my functions. I‚Äôm not sure if this is a best practice or not ‚Äî I could imagine someone telling me that it‚Äôs not advised ‚Äî but for now it‚Äôs helpful, especially as I continue to change things in the innards of various parts of the process. . I‚Äôm using Great Expectations as a general-purpose validation tool to lay out my ‚Äòexpectations‚Äô of my data in a assertive and declarative way, and even though it took a little longer to wrap my head round how it worked, I‚Äôm glad I made the effort as it seems really helpful. . I‚Äôm using Evidently to do similar things as Great Expectations, but I find they have different strengths and benefits even as they serve the same purpose. Evidently is a bit more of a lighter piece in the process, I feel, and as such it‚Äôs a bit more flexible and you can iterate faster with it. I am not quite at the point where I‚Äôm serving my model to accept inference requests from outside, but Evidently will be part of that process when I do, for sure. . Finally, FiftyOne is also somehow part of the validation process. (I‚Äôve written about that previously.) Having visual tools that allow you to quickly test out a hypothesis or debug something unexpected in your training results is an essential part of the work of developing computer vision models. . This brings my short series on data validation for computer vision to a close. I‚Äôm fully conscious that I might have missed some obvious opportunities, tricks or workflows that may be widely used in this field, so I welcome any comments and feedback that you might have. .",
            "url": "https://mlops.systems/tools/redactionmodel/computervision/datavalidation/2022/04/28/data-validation-great-expectations-part-3.html",
            "relUrl": "/tools/redactionmodel/computervision/datavalidation/2022/04/28/data-validation-great-expectations-part-3.html",
            "date": " ‚Ä¢ Apr 28, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "How to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 2)",
            "content": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.) . In the first part of this series, I made the case for why you might want to include some kind of data validation if you‚Äôre working on training a model in general, and if your working on object detection in specific. There are many things that can go wrong with your data inputs and you ought to have some kind of safeguards in place to prevent some tricky failures and bugs. . TL;DR for data validation with Great Expectations . üëÄ Data validation helps give you confidence in the raw ingredients that feed into your models, especially in scenarios where you retrain or fine-tune regularly. . | ‚úÖ For object detection problems, there are many ways your data can fail in some silent way. You should want to be aware of when your training data isn‚Äôt meeting your assumptions of what it should look like. . | üõ† Great Expectations is a general purpose data validation tool that goes a long way to restoring trust in your data, and their automatic profiling feature is really useful when getting started. . | üí™ In this second post on data validation for the computer vision context, I show how you can use the automatic profiling feature of Great Expectations to get you started with increasing your confidence in your object detection annotations. I will show you a concrete example where I created some validation rules for my manually-annotated dataset. I then applied those rules to my synthetic dataset in order to validate it. . | . Initial notebook-based setup . In the last post I showed how you can easily use the Great Expectations library directly on a Pandas DataFrame, manually specifying values you expect to be the case for your data. For example, perhaps your data should always have certain columns, or the values of a certain column should always be a certain type or mostly range between certain values. You can define all these fairly easily, leveraging your domain knowledge of the data. . If you know you‚Äôre going to want to use Great Expectations as a more fully-fledged part of your pipeline or workflow, you‚Äôll probably want to go through the more extensive setup stages and create a dedicated ‚Äòcontext‚Äô which can be longer-lasting than just length of your script runtime. Think of the ‚Äòcontext‚Äô as somewhere all your expectations and configuration of how to access your data is stored. . Full instructions on how to set all this up can be found in the docs, but for the most part it‚Äôs a matter of pip installing Great Expectations, running great_expectations init , and then great_expectations datasource new. . That final command will take you through an interactive setup that has you fill in and amend Jupyter notebooks. (I‚Äôm not fully sold on the prominence of this workflow that has you spinning up a Jupyter runtime, dynamically editing notebooks and so on, but I found doing it for my project wasn‚Äôt as inconvenient as I‚Äôd expected. Plus, there are non-interactive and pure Pythonic ways to get everything configured if you need or prefer that.) . Once you have your context created and your data sources connected, you can move on to the main course: using the Profiler. . Using the Great Expectations Profiler . Setting up your validations (i.e. your ‚Äòexpectations‚Äô) for your data can be done in a number of different ways. We saw last time how you can define these manually, but in this post I want to show how you can follow another recommended workflow by allowing the profiler to review your data and to make an initial set of assumptions about the boundaries and patterns embedded in those values. . Note, as the docs mention, the expectations that are automatically generated from your dataset are ‚Äúdeliberately over-fitted on your data‚Äù. This means that if your DataFrame has 10,321 rows, one of the expectations generated will be that datasets due for validation with this suite of expectations will also have exactly 10,321 rows: . ‚ÄúThe intention is for this Expectation Suite to be edited and updated to better suit your specific use case - it is not specifically intended to be used as is.‚Äù (source) . You‚Äôll want and have to do a decent amount of manual checking through, amending and updating any expectations that get created during this process. That said, I am finding that it makes a lot of sense to start with some kind of initial baseline of assumptions that can be corrected versus starting from complete zero and building things up purely based on your domain knowledge of the data. . Needless to say, this whole process assumes you have a decent grasp on the domain context and have explored your data already. You probably wouldn‚Äôt go to the trouble of setting up Great Expectations if you were doing something that required only a quick solution, but it bears repeating that the expectations you define are only as good as your understanding of the limits and underlying realities of your data. This is probably why something like Great Expectations lends itself quite well to a data-centric approach. . Getting the profiler to work requires a few interlocking abstractions to be created or instantiated: . expectation_suite_name = &quot;redaction_annotations_suite&quot; main_batch_request = RuntimeBatchRequest( datasource_name=&quot;redaction_data&quot;, data_connector_name=&quot;default_runtime_data_connector_name&quot;, data_asset_name=&quot;main_annotations_df&quot;, # This can be anything that identifies this data_asset for you runtime_parameters={&quot;batch_data&quot;: main_annotations_df}, # df is your dataframe batch_identifiers={&quot;default_identifier_name&quot;: &quot;default_identifier&quot;}, ) context.create_expectation_suite( expectation_suite_name=expectation_suite_name, overwrite_existing=True # toggle this as appropriate ) validator = context.get_validator( batch_request=main_batch_request, expectation_suite_name=expectation_suite_name ) profiler = UserConfigurableProfiler(profile_dataset=validator) suite = profiler.build_suite() context.save_expectation_suite(suite) # use this to save your suite in the context for reuse . The above code perhaps seems like a lot, but really all you‚Äôre doing is getting your data, making the relevant connections between Great Expectations and your context, and then running the profiler so it can work its magic. . . You can‚Äôt yet see the specific values that were imputed from your data, but even this high-level output shows you some of the expectations that it‚Äôs thinking would be useful to create. . At this stage, you‚Äôll want to take some time to review the specific expectations. You‚Äôll want to: . ensure that they make sense for your dataset | remove any of the really rigid expectations (e.g. that any dataset must have exactly the same number of rows) | use the inputed expectations as a springboard for any other ideas that might come to mind | . Note that this is an essential step to complete before moving forward. You could use the unedited auto-generated expectations suite as your data validation, but it would almost certainly have little use or value for you. The auto-generated suite is a starting place that you need to amend and tailor to your specific situation. . In my case, I was able to amend some of the min / max values to more suitable defaults. (You amend these expectations in the .json file that was created inside the expectations subfolder within your context.) I also included some other domain-driven expectations that the profiler couldn‚Äôt have known to include. For example, I know from having immersed myself in this data for several months now that most annotations should have a ‚Äòhorizontal‚Äô or ‚Äòsquare‚Äô orientation. Great Expectations doesn‚Äôt create this expectation automatically, so I add it to the list of basic assumptions already generated. . Viewing Data Docs reports on validated data . Once you have a suite of expectations set up to your liking, you can run a checkpoint against your original data just to make sure you haven‚Äôt introduced or amended something that doesn‚Äôt match up with the original data. You should get no errors at this point. . checkpoint_config = { &quot;name&quot;: &quot;my_checkpoint&quot;, &quot;config_version&quot;: 1, &quot;class_name&quot;: &quot;SimpleCheckpoint&quot;, &quot;validations&quot;: [ { &quot;batch_request&quot;: { &quot;datasource_name&quot;: &quot;redaction_data&quot;, &quot;data_connector_name&quot;: &quot;default_runtime_data_connector_name&quot;, &quot;data_asset_name&quot;: &quot;main_annotations_df&quot;, }, &quot;expectation_suite_name&quot;: expectation_suite_name, } ], } context.add_checkpoint(**checkpoint_config) results = context.run_checkpoint( checkpoint_name=&quot;my_checkpoint&quot;, batch_request={ &quot;runtime_parameters&quot;: {&quot;batch_data&quot;: main_annotations_df}, &quot;batch_identifiers&quot;: { &quot;default_identifier_name&quot;: &quot;default_identifier&quot; }, }, ) context.build_data_docs() # builds data docs to inspect the results . What you really want, however, is to run your expectations suite against new data. That‚Äôs the real value of what Great Expectations brings, i.e. to check that incoming data due to be added to your larger base dataset conforms to the broad realities of that base dataset. . In my case, the first thing I was interested to check was whether the synthetic images I created would match the expectations suite I‚Äôd created based off my core hand-annotated data. (Quick context if you haven‚Äôt been following the project so far: I have a core dataset which is manually annotated for the objects inside images. I also created two sets of synthetic data to supplement the manual annotations, which boosted my model performance considerably.) . . The web UI is where you can go to get a visual overview of where your data is passing and failing to meet your (great) expectations. You will want (and I will need) to configure your expectations suite to meet the core assumptions you make about your data derived from your particular domain. . For my case, some expectations I will add that are specific to my use case: . redaction annotations should mostly be of horizontal orientation | content annotations should mostly be of portrait orientation | most images should have only one content annotation | annotations shouldn‚Äôt be larger than the associated image, or positioned outside the boundaries of that image. (Because of how you define them, in reality this is several expectations, but conceptually it‚Äôs just one or two). | the area taken up by most annotations should be less than half that taken up by the total image | . ‚Ä¶and so on. I hope it‚Äôs clear now how Great Expectations can be a tremendous asset that can give you confidence in your data. . In the next and final post of the series, I will explore some other tools that you can consider when performing these kinds of validation. I will also offer my take on when each tool would be appropriate, as well as where they would be appropriate to use within the machine learning workflow and lifecycle. .",
            "url": "https://mlops.systems/tools/redactionmodel/computervision/datavalidation/2022/04/26/data-validation-great-expectations-part-2.html",
            "relUrl": "/tools/redactionmodel/computervision/datavalidation/2022/04/26/data-validation-great-expectations-part-2.html",
            "date": " ‚Ä¢ Apr 26, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "How to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 1)",
            "content": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.) . Data validation is a process of verifying that data is accurate and consistent. It plays a crucial role in end-to-end machine learning pipelines. . There is a lack of validation tools in Computer Vision (CV) due the complexity of the data used by the domain. . In this series of articles, I will show you how to leverage the Great Expectations open-source library to validate object detection data. This will help you to feed your model with data less prone to break your model performance. . When something goes wrong with a newly trained or newly deployed version of your model, where do you look first? Where does your gut tell you the bug or issue is likely to be found? For me, knee deep into my redaction model project, I immediately think of my data. For sure, I could probably have more in the way of testing to be sure my code is working how I expect it to work, but issues with the data are far more likely to be silent killers. Issues with data are unlikely to raise a loud exception and suddenly bring my training to a stop. Instead, my training will continue, but I‚Äôll either get really unsatisfactory results or I‚Äôll get results that are underperforming the real potential of the data I‚Äôm feeding into my model. That‚Äôs the scary part: I most likely won‚Äôt even know that my data is broken or faulty. . For software engineers, testing your code is a tried-and-tested way to find some confidence in what you‚Äôre trying to accomplish. (I am exploring some of these best practices in my review series about Patrick Viafore‚Äôs excellent Robust Python book, which covers testing along with typing and various other patterns.) For critical systems, testing is one of the things that allows you to sleep soundly. For those living in the world of machine learning or data science, data validation is like writing tests for your data. You can be confident that your data looks and has the shape of what you feel it should when you address the data quality issue head-on. . If you think of your model training workflow as a pipeline, there are certain places where it makes sense to do some kind of data validation: . at the very beginning, when you‚Äôre seeing your data for the first time: a lot of exploration and basic analysis really helps at this point. It will help you build up intuition for the general patterns and boundaries of the data you‚Äôre going to use to train your model. | any time you do some kind of conversion: perhaps you have to ‚Äî as I do with my project ‚Äî convert from one image annotation format into another and you‚Äôre juggling x and y coordinates constantly, or maybe you‚Äôre using different image formats at different points? | prior to training your model: ‚Äògarbage in, garbage out‚Äô as the saying goes‚Ä¶ You probably want to make sure that you only have high quality data passing through into your model training pipeline step. | as part of a continuous training loop: perhaps you‚Äôve trained and deployed a model, but now a few months have passed, you have more data and you want to retrain your model. Are you confident that the new data retains the same characteristics and qualities of your original data? | . As you can see, there are many different approaches that you might take. To discuss where you might want to validate your data is to discuss where your processes might be flawed in some way. For most projects of any size or significance, you probably will find that taking the care with your data inputs will pay dividends. . Data validation and computer vision . It often seems like computer vision exists in a world unto its own, particularly when it comes to the data used to train models. These idiosyncrasies amount to a strong case for some kind of data validation: . image data isn‚Äôt always easily introspectable, especially on the aggregate level (i.e. what is the ‚Äòaverage‚Äô of a series of images, or how to think of the standard deviation of your images?) | for something like object detection, the annotations are stored in a separate location from the images to which they correspond, leaving the door open for a creeping data drift between the original image locations and what is listed in the annotations. | For massive data sets, the original data will likely not be stored in the environment where you‚Äôre doing some fine-tuning with new data. | Different model architectures require different kinds of pre-processing for your data and sometimes annotations need converting into slightly different formats (perhaps for your evaluation metric) | The pure images (or image-adjacent objects like medical scans) contain a lot of sub-surface metadata that isn‚Äôt easily accessed and isn‚Äôt automatically used as criteria for comparison or error detection. | . In short, there are lots of ways that training a computer vision model can go wrong, and implementing even basic safeguards against this can give you confidence in the data you‚Äôre using. Unfortunately, the landscape of tooling for data validation in the computer vision space feels like it‚Äôs lagging behind what exists for tabular data, for example, but that‚Äôs almost certainly because it‚Äôs just a harder problem. The big data validation libraries don‚Äôt really cater towards computer vision as a core domain, and (as you‚Äôll see below) you‚Äôll probably have to crowbar your data into the formats they expect. . Big picture: what might this look like for my project? . As I outlined above, there are lots of different places where you might want to use some kind of data validation strategy. At the level of code, you might want to make your input and output validation a bit more solid by using type annotations and a type checker like mypy. You can add tests to ensure that edge cases are being handled, and that your assumptions about your code behaviour are proven. You also have your tests to ensure that changes in one function or area of your codebase don‚Äôt break something somewhere else. . At the level of your data, you can of course use simple assert statements within the functional meat of your codebase. For example, at the point where you‚Äôre ingesting data pre-training you could assert that each image is of a certain format and size, and perhaps even that annotations associated with that image ‚Äòmake sense‚Äô as per the context of whatever problem you‚Äôre solving. You can handle some of these assertions and checks with simple conditionals, perhaps, earlier on in the process when you are ingesting or pre-processing your data. . A significant benefit of having these simple assertions inside your core functions is that you are handling the ways things can go wrong at the same time as you‚Äôre writing the functionality itself. A disadvantage is that your code can easily become cluttered with all this non-core behaviour. It feels a little like the validation can become an afterthought in this scenario. For this reason, it seems to make sense to me that you‚Äôd want to have one or more dedicated checkpoints where your data undergoes some kind of validation process. In the context of a pipeline, this means you probably will want one or more steps where this happens. . Tradeoffs . For tiny throwaway projects, or for proof-of-concept experimentation, it might not make sense to start off by working up a massive data validation suite. A really rigorous validation process early on might slow you down more than is useful. Instead, simple assert statements coupled with type annotations on your functions might be the way to go for safeguards and will-this-be-readable-in-the-future sanity checks. . Ideally, you‚Äôll want to create some kind of end-to-end pipeline or workflow at the beginning of your process, since this will allow you to iterate faster in a manner that‚Äôs meaningful for whatever you‚Äôre trying to solve. With a basic pipeline in place, data validation can be added as a stage of its own without too much disruption once you have an initial working prototype. As with most things in life, investing for the longer term is going to take a bit more upfront effort but that shouldn‚Äôt be too much an issue as long as your project has that kind of a horizon to it. . What kind of validation does Great Expectations offer? . Great Expectations is an open-source data validation tool. It is somewhat agnostic as to what specific use case you have, but I don‚Äôt think it‚Äôd be wrong to say that it isn‚Äôt primarily developed for those working on computer vision problems; tabular data seems to be a much cosier fit. . I stated above that Great Expectations could be used as if you were adding tests for your data. At a very high level, you can think of it as a fancier way of adding assertions about your data. The ‚Äòexpectations‚Äô in the title are like those assertion statements, only in this case there are dozens of different pre-made ‚Äòexpectations‚Äô you can choose from. For example, you could assert that you expect that the values of a particular column of a Pandas DataFrame be between 0 and 100, and that if they exceeded those boundaries then it would be only a very small proportion that did so. . Your expectations make up a ‚Äòsuite‚Äô, and you run your suite of expectations against a batch or data asset. There are another 10 or 20 concepts or terms that I‚Äôd need to define and connect together in a mental map before we covered everything about how Great Expectations works. Unfortunately, this is one of the things I found most confusing about getting to know the library through its documentation. From the outside, it appears that they had one set of terminology, but now it‚Äôs partially changed to a different set of terms or abstractions. Presumably for reasons of backwards compatibility, some of the old abstractions remain in the documentation and explanations, which makes it not always clear to understand how the various pieces fit together. . . You can read the glossary over at their documentation site if you want to learn more, but for now everything I explained above should suffice. . There seem to be two main ways of setting up and using Great Expectations. One is heavily interactive and driven by executing cells in a series of notebooks. The other is as you‚Äôd expect ‚Äî code-based using a Python library, backed by some external configuration files and templates. I didn‚Äôt find the notebook-based configuration and setup very compelling, but it is the one emphasised in the documentation and in online materials, so I will give it due attention in the next part of this blog series. For now, it might suffice to show a very simple version of how the code-based use works: . A simple example of using Great Expectations for data validation . The first thing I did was to convert my annotations data into a Pandas DataFrame. You can use Pandas, SQL and Apache Spark as sources for your data to be validated through Great Expectations, and luckily my COCO annotations file was just a JSON file so it was easily converted. While doing the conversion, I made sure to add some extra metadata along the way: a column noting whether an image or a redaction was horizontal or vertical in its orientation, for example, or splitting the bbox array into its four constituent parts. . import great_expectations as ge annotations_df = ge.from_pandas(pd.DataFrame(annotations)) feature_columns = [&#39;area&#39;, &#39;iscrowd&#39;, &#39;image_id&#39;, &#39;category_id&#39;, &#39;id&#39;, &#39;synthetically_generated&#39;, &#39;category_name&#39;] for col in feature_columns: annotations_df.expect_column_to_exist(col) annotations_df.expect_column_values_to_be_in_set( &quot;category_name&quot;, [&quot;content&quot;, &quot;redaction&quot;] ) . Great Expectations wraps the Pandas library, so importing the data was easy. Then adding the expectations (methods beginning with expect‚Ä¶) was trivial. Below you can see the result from the second of the expectations. All of the column values were in that set, so the test passed. . { &quot;success&quot;: true, &quot;result&quot;: { &quot;element_count&quot;: 6984, &quot;missing_count&quot;: 0, &quot;missing_percent&quot;: 0.0, &quot;unexpected_count&quot;: 0, &quot;unexpected_percent&quot;: 0.0, &quot;unexpected_percent_total&quot;: 0.0, &quot;unexpected_percent_nonmissing&quot;: 0.0, &quot;partial_unexpected_list&quot;: [] }, &quot;meta&quot;: {}, &quot;exception_info&quot;: { &quot;raised_exception&quot;: false, &quot;exception_traceback&quot;: null, &quot;exception_message&quot;: null } } . In the second part of this series, I‚Äôll explore how the interactive way of using Great Expectations works, and I‚Äôll also show the web results interface for your expectations suite. It‚Äôs much fancier than the dictionary / object that was output above, and what‚Äôs even better is that you can have Great Expectations make some of its own guesses about what the right expectations for your particular dataset might be. . I hope for now that I‚Äôve made the case for why data validation is probably worth doing, and started you thinking about how that might apply to a computer vision use case. .",
            "url": "https://mlops.systems/tools/redactionmodel/computervision/datavalidation/2022/04/19/data-validation-great-expectations-part-1.html",
            "relUrl": "/tools/redactionmodel/computervision/datavalidation/2022/04/19/data-validation-great-expectations-part-1.html",
            "date": " ‚Ä¢ Apr 19, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "'I guess this is what data-centric AI is!': Performance boosts after training with synthetic data",
            "content": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.) . A clean and focused dataset is probably at the top of the list of things that would be nice to have when starting to tackle a machine learning problem. For object detection, there are some useful starting points, but for many use cases you‚Äôre probably going to have to start from scratch. This is what I‚Äôve been doing for the past few months: working to bootstrap my way into a dataset that allows me to get decent performance training a model that can recognise redactions made on documents. . As part of that journey so far, some of the big things that I‚Äôve taken time to do include: . manually annotating 1000+ images | using a model-in-the-loop to help bootstrap that annotation process by pre-filling annotation suggestions on an image that I could then correct | creating synthetic images to increase the size of my dataset used in training | spending time looking at what the model found difficult, or what it got wrong | . At the end of my synthetic data creation blogpost, I mentioned that the next step would be to test the effect of adding in the new synthetic examples. Well‚Ä¶ the results are in! . A failed attempt to train with synthetic data . I wasn‚Äôt sure exactly how much synthetic data would be appropriate or performant to use, so created a loose experiment where I started with 20% of the total images and increasing up until I reached 50%. (I figured that more than 50% synthetic data probably wasn‚Äôt a great idea and would probably not help my model perform out in the real world.) . . As you can see above: my initial experiment did not show great results. In fact, in several places, if I added synthetic data my model actually performed worse. This was a strong repudiation of my intuition of what would happen. After all, the whole point of adding the synthetic data was to get the model more of a chance to learn / train and thus improve its ability to recognise redaction object in documents. . I dug into the data that I‚Äôd generated and the data I‚Äôd been using to train, and discovered a nasty bug which was tanking the performance. A week of debugging mislabelled bounding boxes in evenings after work and I was back with results that finally made sense. . Performance boosts after adding synthetic data . . In this chart, at the bottom you can see how training the model without the synthetic data (no-synthetic-batch16) performed. Ok, not great. Then the next best performing (combined-75real-25synthetic-randomsplit)was when 25% of the total number of images was synthetic, and the rest were real manually annotated images. At the top, with around an 81% COCO score, was the model where I used 50% synthetic and 50% real images. This seemed to fit what my intuition said would happen. . More synthetic data helped. I guessed that if I had millions of labelled images then the synthetic data would perhaps have been less useful, but starting from scratch it was really supporting the process. . I was curious what would happen when I returned to FiftyOne to carry out some error analysis on the new model‚Äôs performance. Even before I had reached those results, I had a hunch that the synthetic images I‚Äôd created were perhaps too generic. I think they probably were helping boost some baseline performance of my model, but I knew they weren‚Äôt helping with the hard parts of detecting redactions. . ‚ÄòHard examples‚Äô: creating targeted synthetic data . As a reminder, this is the kind of image that is ‚Äòhard‚Äô for my model (or even a human) to be able to identify all the redactions: . . The FiftyOne visualisations of what was and wasn‚Äôt working validated my hunch: yes, synthetic data helped somewhat, but the model‚Äôs low performance seemed much more vulnerable to misrecognition of the hard examples. Even with a 50/50 split between synthetic data and real manually annotated data, the hard examples were still hard! (And the converse was also true: the model was already pretty good at identifying ‚Äòeasy‚Äô redactions (e.g. of the black box type). . If we look back at the example of a ‚Äòhard‚Äô redaction above, two things stood out: . They‚Äôre hard, even for a human! This was borne out in the way I needed to take special care not to forget or mislabel when I was adding manual annotations. | There are lots of redactions on a single page/image. | The second point was probably important, not only in the sense that there were more chances of getting something wrong on a single page, but also in the sense that the redactions were (relatively) small. The detection of small objects is almost its own field in the world of computer vision and I don‚Äôt know too much about it, but I do know it‚Äôs somewhat an unsolved problem. That said, finding a way to boost the performance of the models on these ‚Äòhard‚Äô examples (there were a few other types of hard image) seemed like it might tackle a significant shortcoming of my model. . I decided to try creating a separate batch of synthetic image data, this time fully tailored to tackling some of the hardness mentioned above: it would have many small redactions on a single page, they would all be white boxes and there might also be things like tables with white box-like shapes coexisting next to redactions. . Luckily, the work I‚Äôd done previously on creating synthetic data helped me get started quickly. I returned to borb, an open-source tool for quickly creating PDF documents that allows for a pretty flexible prototyping of layouts with all sorts of bells and whistles added. These were some of the documents I generated: . . The hard images were hard, and I had created some synthetic chimeras that (I believed) approximated some of the features of the original hard images. I did not want to overbalance my training data, however, and took care not to create too many of this type of image. . My script ‚Äî as with the previous synthetic data ‚Äî also required me to create the annotation files at the same time as creating the document. With borb it was relatively trivial to get the bounding box data for objects created, and there was even in-built functionality to create and apply redactions onto a document. (I‚Äôm moving fairly quickly over the mechanics of how this all worked, but it‚Äôs not too far distant from how I described it in my previous post so I‚Äôd refer you there for more details). . Once the images were created and added to my datasets, it was time to retrain the model and see what benefit it brought. . . As you can see, the model jumped up from around 80.5 to 84% when I aded the hard synthetic examples in. That‚Äôs a pretty nice jump as far as I‚Äôm concerned, especially given that I only added in 300 images to the training data. I still had a little over a thousand of the original basic synthetic images that I was using, but this result showed me that tackling the badly performing parts of the model head-on seemed to have a positive outcome. . At this point, I did some more experiments around the edges, applying other things I knew would probably boost the performance even more, notably first checking what would happen if I increased the image size from 512 to 640. I got up to an 86% COCO score with that improvement alone. . In a final twist, I second-guessed myself and wondered whether the original synthetic data was even helping at all‚Ä¶ I removed the thousand or so ‚Äòbasic‚Äô synthetic images from the data and retrained the model. To my surprise, I achieved more or less the same COCO score as I had with the basic synthetic images. I‚Äôm taking this as a strong suggestion that my basic synthetic images aren‚Äôt actually helping as much as I‚Äôd thought, and that probably a smaller number of them as a % of the total would be beneficial. . Reflections on experimenting with synthetic data . So, what can I conclude from this whole excursion into the world of synthetic image creation as a way of boosting model performance? . adding synthetic data really can help! | the world of synthetic data creation is a huge rabbit hole and potentially you can get lost trying to create the perfect synthetic versions of your original data. (I mean this both in the sense of ‚Äòthere‚Äôs lots to learn‚Äô as well as ‚Äòyou can spend or lose a ton of time here‚Äô.) | Targeted synthetic data designed to clear up issues where the model has been identified as underperforming is probably best. (Conversely, and I‚Äôll be careful how much I generalise here, middle-of-the-road synthetic data that doesn‚Äôt resemble the original dataset may not be worth your time.) | Knowing your original data and domain really well helps. A lot. My intuition about what things the model would stumble on was fuelled by this knowledge of the documents and the domain, as well as by the experience of having done manual annotations for many hours. | . There are probably many (many) more things I can do to continually tinker away at this model to improve it: . continue down the path of more error analysis, which would fuel more targeted addition of annotations, and so on. | create better versions of synthetic data with more variation to encompass the various kinds of documents out in the real world. | more self-training with the model in the loop to fuel my manual annotation process. | further increases to the image size (perhaps in conjunction with progressive resizing). | increasing the backbone from resnet50 to resnet101. | . In general, improving the quality of the data used to train my model seems to have been (by far) the best way to improve my model performance. Hyper-parameter tuning of the sort that is often referenced in courses or in blog posts does not seem to have had much of a benefit. . It is probably (mostly) good enough for my use case and for where I want to be heading with this project. There are other things that need addressing around the edges, notably parts of the project that could be made more robust and ‚Äòproduction-ready‚Äô. More about that in due course, but for now please do comment below if you have suggestions for things that I haven‚Äôt thought of that might improve my model performance! .",
            "url": "https://mlops.systems/tools/redactionmodel/computervision/2022/04/06/synthetic-data-results.html",
            "relUrl": "/tools/redactionmodel/computervision/2022/04/06/synthetic-data-results.html",
            "date": " ‚Ä¢ Apr 6, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "Some characteristics of best-in-class ML portfolio projects",
            "content": "Ekko was the last time I worked on a big project that would be presented publicly. An open-source framework that provided realtime infrastructure and in-transit message processing for web applications was a group project that I worked on together with three other colleagues, and we took the time to really make the how and the why really explicit. We made animations, diagrams, charts, and I learned a lot about what‚Äôs hard when explaining technical projects, even when the audience is expected to be (mostly) technically literate. . . I‚Äôve been working on my redaction project since December and slowly but surely I‚Äôm tying the ends together and getting ready for it to come to a close. As part of the final touches, I want to offer something equivalent to how we presented Ekko. From reading around and exposure to various projects over the years, it seems to me that machine learning projects sometimes have different emphases and conventions. This blog post is my attempt to list some of the characteristics of really great ML portfolio projects, with an emphasis on how the project is presented. . Some top projects . Healthsea by Edward Schmuhl (@aestheticedwar1) is my current favourite project writeup, blending amazing visuals, full explanation and a clear overview | This project (by @ahmed_besbes_) was recommended to me and although it‚Äôs more of a step-through of how the project works and was created, it also is clearly presented and very visual. | For computer vision projects, Hugging Face Spaces is a great place to find interesting Gradio demos, though after a while they blend into each other a little. HF Spaces also doesn‚Äôt seem like it gets used for full project explanation that often. | . Characteristics of top projects . Some things I think make a great portfolio project stand out: . visual design ‚Äî looks count for a lot, for better or for worse. | interactivity ‚Äî if there is some kind of a demo or application that I can play around with in order to relate to concepts being written about, that‚Äôd be great. | visual explanations alongside pure text ‚Äî a diagram or animation can really help bring explanations to life. | a clear overview ‚Äî the structure of the writeup should be clear and readers should be able to get a high-level overview first without necessarily needing to read through every last detail. | explain what problem you‚Äôre solving ‚Äî spend (probably) more time than you think is necessary to explain what problem you‚Äôre solving and set up the context for the work you did. | code snippets are ok, but don‚Äôt just dump your source code. | present your dead ends ‚Äî don‚Äôt just present the happy path; feel free to present things that didn‚Äôt work out as well. Readers will want to know that you encountered difficulties and there are benefits from seeing how you made decisions along the way. | present further work and next steps ‚Äî offer hints at what other work could be done on the project, even if you‚Äôre done with it for now. | don‚Äôt lose track of the use case ‚Äî show that you thought about the specific problem you were solving and not just as a technical problem in a void. (Real-world use cases have constraints, and your solution should live within a universe where those constraints directed you). | Feel free to link out ‚Äî you can easily link to other places where you‚Äôve gone into the details about a particular problem you encountered. No need to cram every single last detail into the project portfolio. | Don‚Äôt forget the purpose of the portfolio ‚Äî it doesn‚Äôt need to be an exhaustive catalogue of every last detail; it just needs to offer a compelling overview that is understandable as an independent entity. | . There are other aspects which are more table stakes for anything you write online ‚Äî no typos, clear writing and so on. . I took the time to step back from the project to write this down as I move into a phase where I‚Äôll increasingly focus on the full writeup and I wanted to have a list to remind me of the things I valued in these kinds of projects. . If you have good examples of ML portfolio projects (or really great blog write-ups with interactivity and so on), please let me know in the comments! .",
            "url": "https://mlops.systems/computervision/skillbuilding/2022/04/04/ml-portfolio-best-practices.html",
            "relUrl": "/computervision/skillbuilding/2022/04/04/ml-portfolio-best-practices.html",
            "date": " ‚Ä¢ Apr 4, 2022"
        }
        
    
  
    
        ,"post19": {
            "title": "Building my own image to use IceVision with Paperspace",
            "content": "I‚Äôve been using Paperspace right to fuel my ML/Deep Learning experimentation since more or less the beginning. It was one of the recommended platforms that offered GPUs for the fastai course and when I started working on my redaction project I chose to keep going since I had little reason to change. . Fast-forward a few months, and I‚Äôve had a few issues along the way. Paperspace works by provisioning a Docker image, connecting it to a fixed filesystem / storage backend and then serving this up to you in a web interface as a Jupyter notebook. I found that sometimes there were issues with dependencies breaking, or special pip install magic I had to include in my notebook so that things would work again. . Included in this is the reality that a full install of IceVision ‚Äî an amazing library for computer vision that handles a lot of the pain around integrating various libraries and use cases ‚Äî simply takes a while as it has to download and setup some pretty hefty dependencies. I had found that going from zero to working on the day‚Äôs specific issue took around 20 minutes when you factored in all the setup, updates from the Github repo, syncing data and so on. . Inspired by my reading and study of Docker ‚Äî and with a tip from a Paperspace engineer about how I could get started ‚Äî I set out to build a custom image that handled most of the setup upfront and automatically updated with the latest changes and data. . Amazingly, it worked more or less immediately! I created a new Dockerfile based of the original suggestion and the core additions were the following: . RUN wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh &amp;&amp; bash icevision_install.sh cuda11 &amp;&amp; rm icevision_install.sh RUN pip install torchtext==0.11.0 --upgrade RUN pip install opencv-python ipywidgets icevision-dashboards RUN apt update &amp;&amp; apt install -y libsm6 libxext6 RUN apt-get install -y libxrender-dev CMD make lfs &amp;&amp; git lfs pull . In order to set this up with Paperspace, you first have to go to your notebooks inside a project and click to create a new Paperspace notebook. . . Once there, you can ignore the suggestion to ‚Äúselect a runtime‚Äù, but rather select your machine from the available GPUs. I usually choose the RTX5000 and set it up for an auto-shutdown after 6 hours. . . Then you want to click the ‚ÄòAdvanced Options‚Äô toggle so you can add in all the details of the image being used. . . This is what worked for me. In order to use JupyterLab, the container command should be: . jupyter lab --allow-root --ip=0.0.0.0 --no-browser --ServerApp.trust_xheaders=True --ServerApp.disable_check_xsrf=False --ServerApp.allow_remote_access=True --ServerApp.allow_origin=&#39;*&#39; --ServerApp.allow_credentials=True . I enter my private GitHub repo (along with my username and a custom token generated to allow Paperspace to download the repo) in the ‚ÄòWorkspace‚Äô section. . Then when I click ‚ÄòStart Notebook‚Äô, it works! No more hanging around for IceVision to install. My Docker image already has this! . I realise that I‚Äôm probably a little late to the party in terms of using Docker and seeing how it can bring some real improvements in terms of reproducibility of environments as well as these little quality-of-life perks like not hanging around to install everything each time you want to use it. This was a really useful experience for me to learn from, and I‚Äôll certainly be using this going forward in other projects I work on. .",
            "url": "https://mlops.systems/tools/docker/computervision/2022/03/25/paperspace-docker-icevision.html",
            "relUrl": "/tools/docker/computervision/2022/03/25/paperspace-docker-icevision.html",
            "date": " ‚Ä¢ Mar 25, 2022"
        }
        
    
  
    
        ,"post20": {
            "title": "Starting Docker In A Month Of Lunches",
            "content": "As far as software engineers go, I‚Äôm still barely a spring chicken, six months into my job with ZenML. Working at a startup is fairly fast-paced and the ability to get going quickly with any number of libraries and tools is a requirement of the work. The number of things I could study, learn or practice is vastly larger than the amount of time I have. Accordingly, it helps to try to pick things that will be long-lasting, teach a cross-cutting skill or that are somehow fundamental. . Two closely-connected technologies that I‚Äôve realised I can no longer avoid are Docker and Kubernetes. I have some high-level knowledge of both, having worked with Docker images on Ekko and having encountered Kubernetes in recent months, but it‚Äôs become clear in the last few weeks that they aren‚Äôt going away. More than that, it seems that not having a better (practical) grasp of some of the ins and outs of both is holding me back from grasping more complex decisions that are being made at work. . [Side-note: I‚Äôm very curious about Podman as a Docker-adjacent alternative, but I need to understand Docker better first before I can make comparisons. I‚Äôd also note that I‚Äôm pretty sure that there are lots of cases where Kubernetes is overkill, and where it doesn‚Äôt make much sense to add all that complexity, particularly for smaller teams and projects. It‚Äôs nevertheless a feature of life in the MLOps space, it seems, so I must understand it.] . I‚Äôve had my eye on two Manning books by Elton Stoneman for a while, and now seems the perfect time to dive in. Learn Docker in a Month of Lunches and Learn Kubernetes in a Month of Lunches are very practical introductions to their subjects, come with good reviews and feedback and were published relatively recently. I‚Äôm especially happy that both books are extremely hands-on and even though I won‚Äôt in any way be an expert in either technology by the end, I‚Äôll at least have some experience of having encountered the core use cases of both and maybe have a strong idea of what I do and don‚Äôt know. . I‚Äôm not sure whether I‚Äôll complete each one in exactly a month, but I‚Äôll try to fast-track my reading. The chapters are written in such a way as to be digestible (including exercises) in around an hour. Stoneman says in the introduction to the Kubernetes book that it‚Äôs best to start with the Docker one, which I suppose makes sense given that one builds on the other. . Just like my posts as I read through Robust Python (which I haven‚Äôt stopped doing), I‚Äôll write up various things that I learn along the way, mainly as notes for myself but perhaps it will have value beyond this limited goal. So far I‚Äôve read through the first three chapters of the Docker book, so what follows are some notes on the key points from that. . Core Docker Commands . The book has you running a lot of examples. Two commands mentioned (specific to this book) to help clean up the images and containers that you were using: . # clean up containers and application packages docker container rm -f $(docker container ls -aq) # to reclaim disk space docker image rm -f $(docker image ls -f reference=&#39;dial/*&#39; -q) . Some core commands for interacting with a container: . # run a container docker container run CONTAINERNAME # run an interactive container with a connected terminal session docker container run --interactive --tty CONTAINERNAME # list running containers docker container ls # list all containers with any status docker container ls --all # list processes running in a container # CONTAINERNAME could be part of the container ID as well docker container top CONTAINERNAME # show logs for a container docker container logs CONTAINERNAME # view all details about a container docker container inspect CONTAINERNAME # get stats on a particular container docker container stats CONTAINERNAME # special flags # --detach starts the container in the background # --publish publishes a port from the container to the computer # delete containers # the force flag shuts it down if still running docker container rm --force CONTAINERNAME # the nuclear option docker container rm --force $(docker container ls --all --quiet) . Building your own images with Dockerfiles . Some commands which are useful for making your own images: . # gets the image from DockerHub registry docker image pull IMAGENAME . Key mental models for Docker images: . images are made up of ‚Äòlayers‚Äô | Docker images are stored as lots of small files, brought together as one image when you build with a Dockerfile. | each layer of an image corresponds to a line in your Dockerfile | layers are successively built on top of each other | the order of the layers determines the cache invalidation. If something changes in a lower layer, then all subsequent layers are regenerated. It‚Äôs for this reason that it pays to be careful about the order in which you write the commands that make up your Dockerfile. | . It seems to be considered a good practice (at least where I am right now in the book) to pass in environment variables from the outside into your Docker image. This way you can keep configuration separate from how you actually run it. So you‚Äôd have a command something like this: . docker container run --env EPOCHS=30 SOMEUSER/CONTAINERNAME . which would pass the EPOCHS environment variable into the runtime of the Docker image if it had been set up with something like this as a line inside the Dockerfile: . ENV EPOCHS=&quot;1&quot; . Note that only the environment variables that you specifically select to be passed into the container get passed in. . Dockerfile layout . Dockerfiles seem to have some commonalities in terms of the structure: . you start with a base image on which you‚Äôre building. These seem usually or often to be a base image containing a runtime for whatever language your application uses | Then there‚Äôs often environment variables afterwards | Then you can specify a WORKDIR which is the working directory for the application | Then you can COPY files from your local filesystem into that working directory | Then at the end you specify which CMD needs to be run in order to execute the application. | . Once you‚Äôre done with writing your Dockerfile, use the following command to build your image: . docker image build --tag SOMENAME . . Note that final . trailing that command. The . states that the current directory is the ‚Äòcontext‚Äô and thus is used for when you‚Äôre copying in files using the COPY command. . You can view the layers of your Docker image with the docker image history IMAGENAME command (which will output them to the terminal). . To see how much disk space your containers and images are taking up, type docker system df. . When you rebuild an image, you can specify this with a :v2 after the name, as in this command: . docker image build -t web-app:v2 . . When it comes to optimising your Dockerfile, bear the following in mind: . ‚ÄúAny Dockerfile you write should be optimised so that the instructions are ordered by how frequently they change ‚Äî with instructions that are unlikely to change at the start of the Dockerfile, and instructions most likely to change at the end. The goal is for most builds to only need to execute the last instruction, using the cache for everything else. That saves time, disk space, and network bandwidth when you start sharing your images.‚Äù (pp. 42-43) . Some command tips and tricks: . combine multiple commands onto the same line | put the CMD instruction early on as it‚Äôs unlikely to change | .",
            "url": "https://mlops.systems/tools/dockerinamonthoflunches/books-i-read/2022/03/21/docker-in-a-month.html",
            "relUrl": "/tools/dockerinamonthoflunches/books-i-read/2022/03/21/docker-in-a-month.html",
            "date": " ‚Ä¢ Mar 21, 2022"
        }
        
    
  
    
        ,"post21": {
            "title": "Figuring out why my object detection model is underperforming with FiftyOne, a great tool you probably haven't heard of",
            "content": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.) . So you‚Äôve trained a computer vision model, but you think it could do better. What do you do next? This is a common scenario, especially for computer vision problems where fine-tuning someone else‚Äôs pre-trained model is a pretty normal initial step that gets taken. You emerge with a decent score on whatever metric you care about, but it also isn‚Äôt great. . One part of the solution is certainly ‚Äòmore data‚Äô. This approach was recently highlighted by Boris Dayma on Twitter: . Easy recipe to get quickly a cool classification model on your own dataset ü§ì‚úÖ spend 1-2h to sort part of your data‚úÖ split in train/val as 90/10‚úÖ fine-tune a model (HuggingFace makes it easy)‚úÖ use that model to sort faster more data‚úÖ train again &amp; repeat until happy! . &mdash; Boris Dayma üñçÔ∏è (@borisdayma) March 11, 2022 In my case, I currently have a little over 1200 images that have been annotated, but of those some 600 of them don‚Äôt contain any redactions at all (i.e. they just have content boxes). I did mention that I was using a similar approach early on, where I‚Äôd use the model to help pre-annotate images, but I haven‚Äôt been using that recently. . I‚Äôm realising that more important than pure volume of data is to annotate types of images that are the hardest for the model to learn. So what I really want to know at this point is where I should place my focus when it comes to supplementing the training data. My images aren‚Äôt currently divided into separate classes, but I have a proxy (the filename) which will be really helpful once I‚Äôve identified which types I need to supplement. . When seeking to improve computer vision models with error analysis, some kind of visual inspection is essential. fastai had a number of utility methods that helped in the interpretation of where a model was underperforming, but for object detection I think you do need something that was built to purpose, where you can really dive into the specific ways each object was or wasn‚Äôt detected. . Enter FiftyOne. . FiftyOne is an open-source tool built specifically to support the curation and creation of datasets for computer vision models. It is almost two years old in its open-source incarnation, and (or but?) it feels very solid and robust in its implementation. Voxel51, the company behind it, has taken great pains to write excellent documentation and guides, and they have a supportive community behind the scenes, too. . FiftyOne Basics . FiftyOne is a Python library that offers a visual interface to your data. For my redaction model, the base interface looks something like this: . . You need to convert your dataset such that FiftyOne can interpret the structure of where images are stored as well as the annotations themselves, but many commonly-used formats are supported. In my case, COCO annotations are supported out of the box, so it was trivial to import the data to generate the above visualisation. . You can use the FiftyOne application inside a Jupyter Notebook, or you can have it open in a separate tab. A separate tab is my preference as it allows for a larger interface. (There is also a completely separate Desktop app interface you can use, but I think not all functionality works there so you might want to stick to a separate tab). . Luckily for me, my computer vision framework of choice is IceVision, and they recently integrated with FiftyOne which makes creating datasets a breeze. . So how did FiftyOne help me understand how my model was performing? (Note: the sections that follow were significantly helped by following this, this and this part of the FiftyOne docs.) . Comparing ground truth with predictions . The first thing I did was visualise the ground truth annotations alongside the predictions of my model. (This is the model mentioned in my last blogpost, which had a COCO score of almost 80%.) . This requires performing inference on a slice of our images. Unfortunately, I had to do that inference on my local (CPU) machine because FiftyOne doesn‚Äôt work on Paperspace cloud machines on account of port forwarding choices that Paperspace make. This makes for a slightly slower iteration cycle, but once the inference is done you don‚Äôt have to do it again. . . You can see here that it‚Äôs possible to selectively turn off and on the various overlaid annotations. If you want to compare how redactions are detected (and not see the content box), then this is an easy way to toggle between. . Viewing only high-confidence predictions . Not all predictions are created equal, too, so it would be useful to view only those predictions where the confidence was higher than 75%. FiftyOne makes this kind of conditional view easy. You can do it in code, as in the following snippet, or you can do it via the GUI inside the app. . from fiftyone import ViewField as F # Only contains detections with confidence &gt;= 0.75 # `dataset` is the FiftyOne core object that was created before high_conf_view = dataset.filter_labels(&quot;prediction&quot;, F(&quot;confidence&quot;) &gt; 0.75) . . ‚ÄòPatches‚Äô: detailed views for detected objects . For a more fine-grained understanding on the ways our model is predicting redactions, we can create what are called ‚Äòpatches‚Äô to view and scroll through prediction-by-prediction. . . This is an excellent way to view things through the eyes of your model. These are all the objects it considers to be redactions. We‚Äôll get to finding the ones where it doesn‚Äôt do as well in a bit, but this view allows us to immerse ourselves in the reality of how our model is predicting redaction boxes. We can see that certain types of boxes are well-represented in our dataset: coloured or shaded rectangles in particular. . Understanding how our model performs for separate classes . We only have two classes in our training data: redaction and content, so doing a class analysis doesn‚Äôt help us too much for this problem, but using the mean average precision (MAP) calculation we can see the difference between how well our model does on redactions vs content: . . We can also easily plot an interactive chart that quite clearly displays these differences: . . Viewing the false positives and false negatives . The previous calculations also added some metadata to each image, denoting whether it was considered a true positive, false positive or false negative. It‚Äôs really useful to be able to easily switch between these views, and identifying the images with the largest numbers of false positives and false negatives will help appreciate what our model struggles with. . This view is sorted by total number of false positives in an image. False positives are where the model confidently has predicted something to be a redaction box, for example, that is not actually a redaction box. . . In this image you can see that the model predicts a redaction box with 82% confidence that is clearly not a redaction. Note, too, how the smaller redactions to the right and the large partial redaction to the left were not detected. . False negatives are where there were some redactions to be predicted, but our model never made those predictions (or was very unconfident in doing so). . . In this image excerpt, you can see that some predictions were made, but many were also missed. This image shows the ground truth reality of what should have been predicted: . . Scrolling through the examples with high numbers of false positives and false negatives gives me a really useful indication of which kinds of redactions with which I need to annotate and supplement my training data. I already had a sense of this from my own intuition, but it‚Äôs excellent to see this confirmed in the data. . Finding detection mistakes with FiftyOne Brain‚Äôs mistakenness calculation . FiftyOne is not only the visual interface, but it also has something called the FiftyOne brain. . . It&#39;s worth being aware of the distinction between the two: FiftyOne itself is open-source and free to use. The brain is closed-source and free for non-commercial uses. The brain allows you to perform various calculations on your dataset to determine (among other things): . visual similarity | uniqueness | mistakenness | hardness | . (You can also visualise embeddings to cluster image or annotation types, but I haven‚Äôt used that feature yet so can‚Äôt comment as its effectiveness.) . For my dataset, visualising similarity and uniqueness revealed what I already knew: that lots of the images were similar. Knowing the context of the documents well means I‚Äôm familiar with how a lot of the documents look the same. Not much of a revelation there. . The mistakenness calculation is useful, however. It compares between the ground truth and the predictions to get a sense of which images it believes contains annotations that might be wrong. I can filter these such that we only show images where it is more than 80% confident mistakes have been made. Instantly it reveals a few examples where there have been annotation mistakes. To take one example, here you can see the ground truth annotations: . . And here you can see what was predicted: . . In this example, it was even clear from the beginning that redactions had been missed, and that the single annotation that had been made (a content box) was incorrect. . Finding missing annotations . We can also view images that the FiftyOne brain tagged as containing missing annotations: . session.view = dataset.match(F(&quot;possible_missing&quot;) &gt; 0) . . Unfortunately the compute_hardness method only works for classification models currently, but regardless I think we have a lot to work with already. . Conclusions and Next Steps . I hope this practical introduction to FiftyOne has given you a high-level overview of the ways the tool can be useful in evaluating your computer vision models. . For my redaction project, I‚Äôm taking some clear action steps I need to work on as a result of some of this analysis. . I need do annotate more of the kinds of images it struggles with. Specifically, this means images containing redactions that are just white boxes, with a bonus for those white redaction boxes being superimposed on top of a page filed with white boxes (i.e. some sort of table or form). | I need to remove some of the bad/false ground truth annotations that the FiftyOne brain helpfully identified. | I will probably want to repeat this process together in a model that was trained together with the synthetic data to see what differences can be observed. | As a general point, I probably want to incorporate visual inspection of the data at various points in the training pipeline, not just after the model has been trained. | . If you know any other tools that help with this kind of visual analysis of model performance and how to improve in a data-driven approach, please do let me know! .",
            "url": "https://mlops.systems/redactionmodel/computervision/tools/debugging/jupyter/2022/03/12/fiftyone-computervision.html",
            "relUrl": "/redactionmodel/computervision/tools/debugging/jupyter/2022/03/12/fiftyone-computervision.html",
            "date": " ‚Ä¢ Mar 12, 2022"
        }
        
    
  
    
        ,"post22": {
            "title": "Incremental Improvements to my Redaction Detection Model",
            "content": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.) . Last time I wrote about my work on this project, I‚Äôd just finished creating synthetic image data to supplement my manual annotations. Before integrating those into the model training, I wanted to make changes to a few hyper parameters to ensure that I‚Äôm getting as much out of the current configuration as possible. . I focused on three ways of improving the model‚Äôs performance, each of which ended up having an effect albeit in different ways. . Increasing the image size . When I started the project, I set the size of the images that would be passed into the training as the training dataset to 384x384 pixels. (It is a convention relating to how some of the older pre-trained models (like EfficientDet) were such that the image size must be divisible by 128.) This turned out to be too small. . The next steps up were 512 and 640. The GPU / hardware on which I was training my model seemed to have no problem with either of these image sizes and the performance increased as I worked with either 512 or 640 as the base image sizes. . Increasing the batch size . Another important lever at my disposal was either to increase the batch size (the number of images that are used in each epoch) or to decrease the learning rate. (A useful Twitter thread by Boris Dayma explains some of the tradeoffs for one versus the other, along with some references to things to read). . I had started off with a batch size of 8, but increasing to 16 and then 32 had a big effect on my model‚Äôs performance: . . Batch sizes of both 16 and 32 eventually converged on more or less the same COCOMetric score of around 74%. The validation loss rate showed pretty clearly that the highest (32) batch size overfit far faster than for 16. It seems that 16 is the best choice for now. . Backbone model size . The way I‚Äôve set things up to train this object detection model requires two main choices in terms of architecture: a particular pre-trained model and a backbone. VFNet (as mentioned previously) outperformed basically everything else I‚Äôve tried and I think it seems to be a clear best choice for the model. In terms of the backbone, I‚Äôd been using resnet50 until now, but following some of the above experiments, it made sense to try increasing the backbone size as well. (An obvious disadvantage to this approach was slower training times and a larger final model size.) . . In this image you can see the stages of improvements we made throughout this whole process. vfnet-pre-synthetic-base was the lowest performer at the beginning, then doubling the batch size gave another boost of almost 8% to our model performance. Then the final increase to the backbone size added another 4% increase bringing us to a score of around 78% for the COCOMetric. . It remains to be seen how much of these changes will make sense when I introduce the synthetic data, or if there are more effective boosters to the model performance in the form of adding annotations to areas where the model struggles the most. .",
            "url": "https://mlops.systems/redactionmodel/computervision/tools/2022/03/03/model-improvements.html",
            "relUrl": "/redactionmodel/computervision/tools/2022/03/03/model-improvements.html",
            "date": " ‚Ä¢ Mar 3, 2022"
        }
        
    
  
    
        ,"post23": {
            "title": "Three Python Helpers for Parsing Inputs",
            "content": "I continue to slowly work my way through the calmcode back catalogue. This week I learned about three tiny utility packages that make certain parsing tasks less painful. . parse (introduced here) is a way of turning simple text patterns into restructured data. Take the following example as an illustration: . from parse import parse url = &quot;https://github.com/strickvl/some-repo/&quot; parse(&quot;https://github.com/{owner}/{repo}/&quot;, url).named # returns {&#39;owner&#39;: &#39;strickvl&#39;, &#39;repo&#39;: &#39;some-repo&#39;} . As Vincent explains, it‚Äôs sort of the inverse or opposite operation to what happens with an f-string. . For URLs of various kinds that you want to decompose easily, yarl (introduced here) is a great way to approach that in Python. . For dates stored in some kind of a string format, you might want to try datefinder (introduced here), an elegant if not always perfect way for converting date strings into datetime.datetime objects. .",
            "url": "https://mlops.systems/python/tools/2022/02/27/python-parsers.html",
            "relUrl": "/python/tools/2022/02/27/python-parsers.html",
            "date": " ‚Ä¢ Feb 27, 2022"
        }
        
    
  
    
        ,"post24": {
            "title": "It's raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
            "content": ". This blog outlines my process (and a few false starts) for generating a series of synthetic images (and corresponding annotations) to supplement training data used in a machine learning project. This problem is one for which there aren‚Äôt many (any?) pre-existing data sets that I can repurpose so I‚Äôve been trying to find ways to bootstrap and improve the performance of the model I‚Äôm training. . Before I dive into the details, I wanted to include a little context on the wider project and what I‚Äôm seeking to accomplish. It is a relatively common practice for documents released as part of FOIA requests to contain redactions. With so many documents being released ‚Äî and perhaps in specific cases where legal teams are dealing with huge numbers of those redacted documents ‚Äî it can be useful to identify which documents are redacted and/or to get a sense of just how much has been redacted. If you have 10 or 20 documents you can fairly easily get that overview, but if you have 10,000 or a million documents? That‚Äôs where my project comes in: I want to train a model to make it easy to detect redactions in a document and to generate statistics on what proportion of a document or documents have been redacted. . You can read more about the problem domain, about my initial forays into annotating a dataset for this problem, as well as view some examples of these redactions (and perhaps why they‚Äôre not as easy to identify as you might think). You can even try out a demo showing some of what the model can identify here. Note that this isn‚Äôt the latest version of the model so it‚Äôs not the absolute best performance. . What‚Äôs the deal with synthetic images? . It‚Äôs a truism that in computer vision projects you probably need or want a lot of data to get good results. For the Facebooks and Bytedances of the world this perhaps isn‚Äôt an issue: they have access to a ton of data, for better or for worse. But for me, I don‚Äôt have teams of data annotators or millions of users generating all this data. This is probably the norm for small- to medium-sized computer vision problems being solved out in the world, especially with more non-traditional entrants into the field who are just trying to do things with the skills instead of generating research and so on. . Instead of using huge amounts of data, we need to be smarter about how we work, levelling ourselves up with whatever tricks of the trade we can muster. The fastai course contains a great number of these best practices, perhaps unsurprisingly since it is in some way targeted at individuals seeking to solve their domain-specific problems. One of the key insights I took away from earlier parts of the fastai book was the benefits of using pre-trained models. With a wealth of these models available and accessible, you don‚Äôt need to start your work from scratch. Instead, fine-tune your model and benefit from the expertise and hard work of others. . You do need some data to get started with fine-tuning a pre-trained model, however. That‚Äôs why I took a bit of time to make some initial annotations. I currently have annotated 2097 images, labelling where I have found redactions on the images as well as a box to show which parts of the image contain text or content. That approach has done pretty well so far, with in the low to mid seventies in terms of a % COCO score. (This is a commonly-used metric to assess the performance for object detection problems.) I want to go further, though, which is where synthetic images come in. . The big bottleneck in the annotation process is, of course, me. Depending on how many redactions any particular image contains, it could take me 5-10 minutes for a single image‚Äôs worth of annotations. This does not scale. Part of the speedup for this process is to use self-training, but I‚Äôll write about that separately. Another option that has is often used is to generate images which approximate (to a greater or lesser degree) the actual real images. The useful thing about generating the images yourself is that you know where you placed the redactions, so you have the annotations at the same time. . My overall goal here was to boost my model‚Äôs performance. I didn‚Äôt know how how well these synthetic images would contribute, or even if they‚Äôd contribute to any boost at all. I was also quite conscious of the fact that you could probably spend a year generating pixel-perfect synthetic redacted documents. I didn‚Äôt want to waste too much time doing that, so at various points I had to make decisions as to whether a particular stage was good enough. . Phase 1: Get a Baseline / Naive Trial . When I started this, I didn‚Äôt know how hard or easy it was going to be, so I set myself a low bar. I knew it was theoretically possible to create images with Python, but I‚Äôd never done it before so didn‚Äôt have a sense of the range of possibilities. . In situations like this, I find Jupyter notebooks really reveal their strengths. Experimentation is easy and the pace of iteration can be really high. A few minutes of searching around and it seemed like Pillow (aka ‚ÄòPIL‚Äô) was probably the best option to go with. I noted that you could edit, resize, copy and paste images. For my basic version of a synthetic image generator, that‚Äôs most of what I needed to do: . Take an image that we know contains no redactions. | Get a separate image file that is of a redaction box / squiggle or shape. | Randomly resize the redaction shape. | Paste the redaction shape at a random location on top of the base unredacted image. | . And voila! Finding unredacted images was easy since I had previously used fastai to build a model that could detect to ~95% accuracy whether an image contained a redaction or not. For the redactions, it took me about an hour with Pixelmator Pro and its ‚Äòquick selection‚Äô tool to extract 100 examples of various kinds of redaction that I knew were commonly found in the data set. You can see some of this variety in the illustration that follows, though note that each individual redaction snippet was its own separate image for the purposes of my synthetic generation. . . I found that it was pretty trivial to generate images of the kind I proposed above. The placement of the redactions didn‚Äôt always make sense, and sometimes the random resize that the redaction underwent meant that it was either far too small or far too large. I also hadn‚Äôt included any steps to capture the annotation in this prototype, but I knew it was possible so continued onwards. . Detour: Get Stuck Pretty Quickly, Experience bbox Sprawl . Buoyed by my success in the prototype stage, I immediately added a bunch of improvements and features to what I wanted to achieve. I knew I wanted to make sure that the redaction stayed within the boundaries of the original base image. I also wanted to ensure that it stayed within the boundaries of the content of the base image ‚Äî i.e. redactions generally tend to be made on top of content which tends not to be right on the outer margins. . I rushed into things too fast without thinking the problem through and quite quickly got into deep waters as all the various pieces started to overlap. I was somehow still in notebook mode, passing various objects through various other custom libraries, not sure what I was passing where. In short: it was a mess. . One thing that tripped me up really fast was bboxes. (A bbox, in case this means nothing to you, is a data structure or type that allows you to represent where a box is positioned if you were to paste it on top of a base image (for example). It seems that there are different conventions about how to represent this concept of the location of a box on top of some other larger space. Some people represented it with pairs of coordinates, such that for each of the four corners of the box you‚Äôd have an [x, y] pair to represent each point. Others took this bbox type to contain references to the xmin, ymin, xmax, and ymax values of the box. In this way you could reconstruct the various corners since you had two opposite corners specified. Another option was that used by COCO, which was [xmin, ymin, width, height]. And yet another option was to represent a bounding box by [x_center, y_center, width, height]. (This is a useful article that details some of these representation differences.) . I‚Äôm sure there are people who are really good at keeping multiple types of x and y coordinates, each with slightly different nuances, in their heads. I am not such a person and after an hour or two of struggling in these deep waters I realised I needed to regroup. . My notebook experiments had been good for uncovering the range of possibility, but now that I had a better sense of the edges of the problem ‚Äî and the twists and turns of dealing with bounding boxes ‚Äî I had to take a more systematic approach. I spent some time with pen and paper thinking through the flow that this synthetic generation process would have to include. I thought through what the various independent parts of this could be, and how data would flow through this set of steps. . Phase 2: Generate My Own Base Images . The first part of this process was to generate my own base images. In general, the types of base unredacted images in the core data set were relatively unremarkable. These were mostly letters, reports or some kind of form / table. I figured I could approximate this pretty quickly. By chance, that very weekend I happened to listen to an episode of the Real Python podcast which interviewed the creator of borb, a Python package for creating and manipulating PDFs. I knew I wanted images in the end, but I had already created a tool to extract images from PDFs and I figured borb would probably save me time, even if it meant I had to do some converting back and forth between images and PDF files. . The great thing about borb is that it offers an easy abstraction with which to reason about creating PDF documents. Have some text and want it to be displayed on a page? Done. Want that text to be displayed in three columns? Done. Want do insert some images and have the text flow round it? Done. Have styling requirements? Done. And on and on. I figured that this was just the level of abstraction I needed ‚Äî rather than staying in the world of pixel primitives like lines and boxes. . Once I got going it was easy to generate base images with multi-column text and some random coloured shapes thrown in here and there. (I used lorem-text to generate random Latin texts.) After I created the PDF I then had to convert it into an image format for use elsewhere in the generator pipeline but I think that speed hit was a price worth paying. . Phase 3: Generate My Own Redactions . The redactions weren‚Äôt quite as easy as the base images. The easiest version of a redaction box was literally that: a black box that sits on top of the base image. That much was easy to create. Pillow had some useful interfaces that I could use to quickly create randomly sized boxes. I could even add text to them in the upper left corner as I‚Äôd noticed that many of the real redactions did that. . It was less clear to me how I‚Äôd go about generating the other kinds of redactions, particularly ones that resembled a handwritten mark in thick black marker over the top of a document. In the end, I decided not to go any further with anything that wasn‚Äôt a box, but I did make the redaction boxes more varied. I set it such that the box would be filled with a random colour. If the colour was dark enough, I made sure that the text was in a light (contrasting) colour. And ensure that there wasn‚Äôt always a text on the box. . Not perfect, but still it gave me a way to move forward. . The Big Picture: Bringing It All Together . With these pieces complete, I had the basics of the next version of my synthetic image generation. You can see the flow and progression of my script in the following diagram: . . You‚Äôll note that there were a number of other steps that supported the image creation. I did again descend into bbox hell when calculating exactly where to paste the redaction image, but with a much more modularised approach to my code I didn‚Äôt get lost. Type hints also kept me honest about what variables I was passing in and out of the functions I‚Äôd created. . I ended up using the initial model I‚Äôd trained so far in the step that figured out where the content of the image was. You‚Äôll recall that this was one of the annotations I‚Äôd already been generating when I annotated my data, and since it‚Äôs a fairly simple computer vision task I was already seeing excellent performance from that specific class in object detection. IceVision, a library that I‚Äôm using for the computer vision and deep learning parts of this project, allowed me to fairly easily make this inference on the images and extract the bbox coordinates for the content box. . I made sure to include a lot of random variation in the first two steps where the base and redaction images were created. I didn‚Äôt remove the original naive approach completely. Instead, I made it 50% likely that we‚Äôd generate an image versus just picking one of the unredacted images from our store. Then I gave the same chance for the redaction as to whether we‚Äôd use an actual redaction snippet or one of the computer-generated boxes. There was lots of resizing and colouring and various other randomisation that was also included. . Phase 5: Make The Images Look Old and Worn . Only one step remained. I realised that when I generated the images completely from scratch, not using any of the real base images or redaction snippets, that they looked very new and unrealistic. A significant proportion of the documents in the collection looked like they‚Äôd been photocopied a thousand times and in general had seen better days. Sometimes the quality was such to make them unreadable. I realised if I was going to get good results with the overall goal (i.e. improve my model‚Äôs performance) I‚Äôd have to make the synthetic creations look old somehow. . After some exploration I settled on augraphy as how I‚Äôd process the newly generated images to look old and worn. Luckily for me, this package seems to have been created explicitly to support machine learning workflows for synthetic data creation, and it seemed to be (somewhat) actively maintained. There was a default set of so-called ‚Äòaugmentations‚Äô that Augraphy suggested I apply to my image. Unfortunately it was simply too aggressive. I guess for some workflows it would have been great, but the page ended up looking somewhat unintelligible by the end. Compare these two examples: . . Not only did the default Augraphy transforms often make the redaction indistinguishable, it shifted parts of the image around on the page for these crinkle and scrunch effects, which would have rendered my annotations inaccurate. . That said, as you can see from the left image, it was pretty easy to switch out the default for a set of random transforms to be applied that wasn‚Äôt quite so aggressive. I‚Äôm thankful that tools like this exist out in the open-source space and that allow me to get on with the work of solving the actual problem I‚Äôm interested in working on. . Final Results: 2097 Synthetic Images . . This gif gives you a brief sense of some of the images I generated as a result of the process I‚Äôve detailed above. They‚Äôre not perfect, and as I write I currently don‚Äôt know how well they will perform when training my model. . I have 2097 real annotated images, so I‚Äôm going to combine them with a maximum of an equal number of synthetic images. I‚Äôll try out different proportions of real to synthetic, but that‚Äôs also a topic for another blogpost to follow. Stay tuned! . It took about three and a half hours to create these 2000+ images on my laptop. There are LOTS of places where I could have made speed improvements, notably all the conversion between PDF and image objects, the inference for the content box and also the fact that the pipeline wasn‚Äôt performed in parallel on all my CPU cores. I spent about 30 minutes exploring Ray as a means to getting this process to be executed in parallel but it ended up being not as simple as I‚Äôd initially thought so I‚Äôve left that to one side for now. In any case, I won‚Äôt be creating so many synthetic images at once so often, so it wasn‚Äôt a real blocking point for my work. . Note, too, that the annotations get created as part of the same script. I append them to a synthetic annotations file at the same time as the synthetic images is generated, and the file is subject to being combined with the real annotations at a later stage. . There are obviously lots of ways this synthetic data creation process could be optimised, but I was recently reminded that it‚Äôs also important not to lose momentum and not to let the perfect be the enemy of the good. . The next step is to carry out an experiment to see the effect of adding in the synthetic annotations on model performance. There are a bunch of really tricky aspects to this (most notably finding ways to make sure not to allow my training data to leak into the validation data) but I‚Äôll save all that for my next blogpost. . (If you got all the way to the end, well done!) .",
            "url": "https://mlops.systems/redactionmodel/computervision/python/tools/2022/02/10/synthetic-image-data.html",
            "relUrl": "/redactionmodel/computervision/python/tools/2022/02/10/synthetic-image-data.html",
            "date": " ‚Ä¢ Feb 10, 2022"
        }
        
    
  
    
        ,"post25": {
            "title": "What are invariants and how can they help make your Python classes more robust?",
            "content": "We‚Äôve read about enumerations and we‚Äôve read about data classes. Now it‚Äôs the turn of classes. Chapter 10 of Patrick Viafore‚Äôs excellent book, ‚ÄòRobust Python‚Äô, is the last of the user-defined types to be covered. Early on he makes a good point that classes are often taught really early to those new to Python and/or programming, and that maybe the story is a bit more complicated. As I‚Äôve mentioned before, things like enums and data classes are more or less unmentioned in such educational materials and as such I found this book really helped me fill in the conceptual gaps. . First off, for someone who has just learned about data classes, how would you explain what is new or distinct when it comes to classes? They‚Äôre slightly different syntactically, with classes requiring you to write a bit more boilerplate. Compare the following: . from dataclasses import dataclass import datetime from typing import Literal # data class definition @dataclass class Cat: name: str breed: CatBreed birth_date: datetime.date gender: Literal[&#39;male&#39;, &#39;female&#39;] # class definition class Dog: def __init__(self, name: str, breed: CatBreed, birth_date: datetime.date, gender: Literal[&#39;male&#39;, &#39;female&#39;]): self.name = name self.breed = breed self.birth_date = birth_date self.gender = gender . You can note how it seems like the data class version is much more readable and involves less boilerplate to achieve the same effect, and for a simple example like this you‚Äôre probably right. The difference, and where classes make sense and shine, is when you have a conceptual grouping or type that includes some notion of invariants. . What is an invariant? . Most of this chapter is about invariants and how they relate to classes, and I‚Äôll admit I had never heard of the concept before reading in this book. An invariant is defined as ‚Äúa property of an entity that remains unchanged throughout the lifetime of that entity.‚Äù You can think of it as some kind of context or a property about that particular type that you need to encode and that won‚Äôt change. . The book gives a pizza example (where a Pizza object could encode that in its list of toppings, the cheese could only be the final topping (i.e. on top) of the pizza). An alternative might be some kind of rule relating to an ID number, where either it must be unique to some kind of specification, or where the ID must conform to some kind of specification. . Even with this rudimentary definition, you can see how there might be some advantages to being able to account for these rules and properties of the object type. (With data classes, you don‚Äôt have as much flexibility to specify all these nuances.) So what happens when you‚Äôre instantiating a class and you hit one of those scenarios where your contextual rules dictate that something can‚Äôt happen? (i.e. someone tries to create a Pizza object that has cheese as the bottom-layer topping) The book offers up two options: . Throw an exception ‚Äî this will break you out of the code flow and prevent the object from being constructed | Do something to make the data fit ‚Äî you can perform some kind of transformation which sees the cheese ingredient as being forced onto the top layer of the pizza toppings (or whatever is the equivalent for your specific scenario) | Note that the kinds of restrictions posed by these invariants are things that can‚Äôt fully be captured by the typing system. We‚Äôve covered type hints and how they can help make your code more robust, but types don‚Äôt help much when it comes to the order of a list, for example. . Why code around invariants? . So why go to all of this trouble in the first place? How does it benefit to code with the invariants in mind? To start with, it‚Äôll probably help you think through edge cases and exceptions that you could do well to be wary of. The invariants alert you to the fact that arguments passed into functions and methods will not always be in the form that you would ideally like. (As a side note, this might also encourage you to add unit tests.) . It will help you keep the code that handles the invariants together instead of mixing it in with the code that instantiates the objects. In general, it will enhance your ability to reason about the code and the concepts that your code reflects. This is important not only for the implementation in code, but for how you think about any particular part and how it relates to the rest of your code base. . The goal for all of this: fewer bugs and a more robust system. Yes, it takes a bit more effort to think whether there are implicit or explicit invariants, but doing so makes your code and your system more reliable. In Viafore‚Äôs words: . ‚ÄúYou‚Äôre making an easier API for people to think about, and you reduce the risk of people using your objects incorrectly. [‚Ä¶] You never want someone to be surprised when using your code.‚Äù (p. 141) . Invariants and class consumers . The rest of the chapter is about the implementation consequences of thinking about classes in this invariants-first way. For consumers of the class, how should you ensure that the invariants handled are clear? Aside from the implementation itself (in the constructor), docstrings and code comments are suggested as a means to this end. Of course, README files and documentation in general can serve the same purpose, but it‚Äôs best if the context and information about invariants is as close to the code as possible. . Invariants and class maintainers . For (future) maintainers of the class, unit tests are the way to go. Make sure that the relevant scenarios and invariants are covered by testing code and you will have extra confidence that your object instantiation really does do what you intend. Your code should already be doing the checking for invariants on the instantiation side, but unit tests are a way of ensuring that this is actually the case (and also that these invariants remain covered as the code base continues to evolve. . (The book offers one way of doing such tests for invariants with contextlib.contextmanager on page 145.) . Encapsulation and classes . As the final chunk of the chapter, we learn about private, protected and public access to the properties and methods of a class, and how they relate to the maintenance of invariants. . This is an important part of the story. As users interface with your class and API, encapsulation is a way to ensure that they update and interact with the these properties in a way that is under your control. For example, even if at instantiation you enforce the Pizza object having cheese as the top-layer topping, what do we have in place to ensure that the user doesn‚Äôt just amend the toppings property such that the cheese is the bottom-layer topping (i.e. AFTER instantiation)? Encapsulation ‚Äî having an entity hide or restrict access to certain properties and actions ‚Äî is how you handle that. . The book goes into a fair amount of detail on the uses of these different levels of access, and introduces the idea of ‚Äòaccessors‚Äô and ‚Äòmutators‚Äô as an alternative to the more commonly-used ‚Äògetters‚Äô and ‚Äòsetters‚Äô. . Remember, ‚Äúyou use invariants to allow users to reason about your objects and reduce cognitive load.‚Äù (p. 151) . So what am I supposed to use? . . The end of the chapter offers this really helpful flowchart diagram which summarises the choices that we‚Äôve covered during the previous three chapters. I really want to highlight that this chapter helped me think about classes in a way I hadn‚Äôt, despite having been through courses, having read numerous articles and of course coded in this class-oriented fashion for several years. . The next few chapters continue onwards by thinking about how to design your interfaces such that they make sense for your users and allow your code base to grow with as few headaches as possible. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/02/08/robust-python-10.html",
            "relUrl": "/robustpython/python/books-i-read/2022/02/08/robust-python-10.html",
            "date": " ‚Ä¢ Feb 8, 2022"
        }
        
    
  
    
        ,"post26": {
            "title": "Upgrade your Python dicts with data classes",
            "content": "I‚Äôve been curious about data classes since more or less my first day at work when someone mentioned to me that Pydantic was built on the shoulders of data classes. I hadn‚Äôt taken the opportunity to dive into all the details of what data classes do until now, prompted by their being part of Patrick Viafore‚Äôs book, ‚ÄòRobust Python‚Äô, specifically chapter nine. . An example upfront might help ground the conversation. Here is a data class in action: . import datetime from dataclasses import dataclass from typing import Literal @dataclass class CatPassport: name: str breed: CatBreed issue_date: datetime.date expiry_date: datetime.date gender: Literal[&#39;male&#39;, &#39;female&#39;] aria = CatPassport(&quot;Aria&quot;, CatBreed(&#39;bengal&#39;), datetime.date(2022, 01, 05), datetime.date(2025, 01, 04), &#39;female&#39;) print(aria.name) # prints &#39;Aria&#39; . From this you can see that it‚Äôs an easy way to represent structured data made up of different types. Where it excels over simply using a dict or a class you write yourself is the fact that it auto-generates a number of __ dunder helper methods. You get __str__ and __repr__ to handle what this object looks like when you try to print() it. It also creates an __eq__ method which allows you to check for equality between two objects of the same type with the == comparison operator. . (If you want to add a way to compare between your data class objects, you can add arguments to the @dataclass decorator like @dataclass(eq=True, order=True) which will handle the creation of the relevant dunder methods. . The fact that data classes are just classes at heart mean that you can also add behaviours to these collections of values, something that isn‚Äôt possible with a plain dict. . You can specify that your data class should be frozen (@dataclass(frozen=True)) which effectively makes it an immutable data store, though taking note that objects stored as values on the data class‚Äô properties might themselves not be immutable (think lists and dicts). . After reading the chapter in ‚ÄòRobust Python‚Äô, I read around a little to get a sense of this concept. I read the official docs which were fairly helpful, but in fact it was the PEP document (557) that I found most interesting. I haven‚Äôt previously taken the time to dive into the specifics of PEP specifications before, but I discovered that they are pretty readable and you get a real sense of what problem a particular feature or addition to the language was trying to solve. . PEP 557 explains some of the alternatives and why it might be useful to include this new feature. I also learned about the attrs package and how data classes are actually just a subset of what attrs offers. (As a side note, I was surprised that attrs seems to have been mentioned nowhere in ‚ÄòRobust Python‚Äô, even in the context of the upcoming Pydantic chapter. Perhaps it was just too confusing to have all these things alongside one another.) . Other options to consider alongside data classes when dealing with heterogenous data inside a single object or structure include TypedDict and namedtuple, but it seems like the default for this kind of scenario should probably just be a data class, though I should add that it is only part of the standard library for Python 3.7 and above. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/02/05/robust-python-9.html",
            "relUrl": "/robustpython/python/books-i-read/2022/02/05/robust-python-9.html",
            "date": " ‚Ä¢ Feb 5, 2022"
        }
        
    
  
    
        ,"post27": {
            "title": "How and where to use enums in Python",
            "content": "The second part of Viafore‚Äôs ‚ÄòRobust Python‚Äô is all about user-created types. We start simple in chapter eight and consider the Enum type as a way of defining a particular restricted set of values. An example might help get us started: . from enum import Enum class TrafficLightsState(Enum): RED = &quot;red&quot; YELLOW = &quot;yellow&quot; GREEN = &quot;green&quot; OFF = &quot;off&quot; current_state = TrafficLightsState.GREEN print(current_state.value) # prints &#39;green&#39; . We subclass off Enum and define the pairings of values that belong together. I hope you can see already that this is a readable way to define these values and show that they are part of the same semantic grouping. . If we‚Äôre using these definitions not because we care about the values themselves but because we want to be able to evaluate whether the state of one particular traffic light is the same as a different traffic light, we can use auto to automatically assign values (ascending integers, by default) in the following way: . from enum import Enum, auto class TrafficLightsState(Enum): RED = auto() YELLOW = auto() GREEN = auto() OFF = auto() current_state = TrafficLightsState.GREEN print(current_state.value) # prints 3 . You can iterate through your enums or get their length just as if it was a list, too. . While writing the above text, I realised that I was getting confused about the difference between types and classes in Python. It turns out that whatever differences once existed, they aren‚Äôt much of a thing any more and to all intents and purposes they‚Äôre practically the same thing. . A lot of the enum-related definitions at work are defined in this file. You can see that we tend not to use auto, though I‚Äôm not really sure why. (We don‚Äôt ever seem to compare against actual values.) . If you want to make sure that the actual values assigned to these grouped constants are unique, you can add the @unique decorator which will enforce that you aren‚Äôt duplicating values. . Better still for the readability of your code, you can use this collective type in your type annotations. For sure the difference between these two options should be clear: . def get_status(some_input: str) -&gt; str: # code goes here def get_status(some_input: str) -&gt; TrafficLightsState: # code goes here . In the first case, it is far less clear what‚Äôs going on. . Note that if you‚Äôre purely looking for a way to restrict the assignation to a particular variable, you can also use the Literal type, introduced in Python 3.8, though remember that it doesn‚Äôt help with iteration, runtime checking or map values from name to value. For all that, you‚Äôll want to be using Enum.‚Äù . If you want a way to combine Enums together, you can subclass from enum.Flag. Consider the case of when you have a list of enums for days of the week, but you want to represent the weekend as a pairing of Saturday and Sunday (if you were in Europe, e.g.). You could do the following: . from enum import Flag, auto class Weekday(Flag): MONDAY = auto() TUESDAY = auto() WEDNESDAY = auto() THURSDAY = auto() FRIDAY = auto() SATURDAY = auto() SUNDAY = auto() weekend = Weekday.SATURDAY | Weekday.SUNDAY . You can perform bitwise operations on these combined groupings, but note that the values must support bitwise operations. (Strings don‚Äôt support them, while integers do.) . Finally, the chapter covers the special case of IntEnum and IntFlag which allows for the conversion of integer values. This can be confusing and lead to non-robust behaviours, so the book discourages this particular usage. . Next up is Data Classes, something I‚Äôm extremely interested in getting to grips with as it comes up in our codebase at work a decent amount. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/01/30/robust-python-8.html",
            "relUrl": "/robustpython/python/books-i-read/2022/01/30/robust-python-8.html",
            "date": " ‚Ä¢ Jan 30, 2022"
        }
        
    
  
    
        ,"post28": {
            "title": "Using mypy for Python type checking",
            "content": "The final two chapters of part one of Patrick Viafore‚Äôs ‚ÄòRobust Python‚Äô cover more practical advice on how to actually use and implement type checking in either a new project or a legacy codebase. . mypy is the most commonly used option for type checking in Python and it does most of what you probably need it for. You can run it via the command line, inline as part of your IDE, or as part of a CI/CD pipeline. At work we do all three. . You can configure mypy to your heart‚Äôs desire either with inline comments in your code, or via a configuration file. A configuration file is probably the way to go, particularly if you‚Äôre versioning your code and sharing these kinds of settings across a team. . Chapter 6 goes into detail about some of the specific options or settings you can tweak to make mypy more or less sensitive to certain kinds of errors. For example, in a previous post we mentioned how you can implicitly accept None as a type with the Optional type annotation wrapper. But maybe you don‚Äôt want to allow this behaviour because it‚Äôs generally not a great idea: if so, you can use the ‚Äîstrict-optional flag to get notified whenever you‚Äôre using that particular construction. . mypy also allows for the export of its results to html and xml, and you can run it in the background as a daemon which (particularly for large code bases) might speed it up. . We also learn about some alternatives to mypy, namely Pyre and Pyright. Pyre runs as a daemon in the background and allows you to run queries relating to type usage in your codebase. It also includes a static code analyser called Pysa that runs a kind of security analysis on your code called ‚Äòtaint analysis‚Äô. A quick summary of this would be to say that you can specify specific kinds of security flaws that you want to address and/or prevent being part of your codebase. . Pyright is interesting since it has a useful VS Code integration (via the Pylance extension). You get all sorts of autocompletion and tooltip goodness by using Pyright/Pylance. . Finally, chapter 7 thinks through how you might want to approach actually using type checking and type hints in a larger codebase, perhaps one that already exists. It‚Äôs useful this was included as I imagine these sorts of practicalities are much more of a blocker to adoption than any technical issues. After a brief discussion of tradeoffs, we learn about some different options for where you might want to start with introducing types to a legacy codebase. . Focusing on the pain points ‚Äî i.e. where the lack of type hints has already seen bugs emerge in the past | or perhaps adding them to new code only | or perhaps type annotating the pieces of the codebase that actually drive the product or business‚Äô profits | or maybe whatever is complex to understand | . All of these are options and it will definitely depend on your particular situation. . We also learn about two tools that might help get you started with type annotation: MonkeyType and Pytype. Both auto-generate type hints for your codebase. MonkeyType does so dynamically, so it only generates type hints for parts of your code that it accesses while running the code. Pytype does so by static analysis. Both deliver some kind of output that you can then use (perhaps) as the basis of some annotations of your codebase. My instinct is that these two tools feel like they might lead to some faulty assumptions or errors if you rely on them too much and that in fact it would be better to just methodically go through your code and incrementally add type hints as suggested above. . This concludes the type hints part of the book. I feel like I really got a solid overview of why type hints are used in large or complex Python codebases as well as how to implement this practically. I will be writing separately about how we use mypy and type hinting at ZenML as I think it offers an interesting case study on some of the benefits and tradeoffs that we‚Äôve observed on a day-to-day basis. . Next up in Robust Python: defining your own types with Enums, data classes, classes and how this fits into libraries like Pydantic. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/01/22/robust-python-6.html",
            "relUrl": "/robustpython/python/books-i-read/2022/01/22/robust-python-6.html",
            "date": " ‚Ä¢ Jan 22, 2022"
        }
        
    
  
    
        ,"post29": {
            "title": "Using type annotation with collections in Python",
            "content": "The fifth chapter of ‚ÄòRobust Python‚Äô continues on from where we left off last time. We saw how to apply type annotations when simple things like strings, integers and floats were involved. This chapter deals with the different ways you annotate your types when collections get involved. . We start with the context for why this is even something that requires a separate chapter to deal with. This involves the difference between homogenous and heterogeneous types. For a Python list, we could say it had homogenous types if all the items were of the same type (strings, e.g.). If this list contains multiple different types (a mix of strings and integers, e.g.) then we‚Äôd have to say it contained heterogenous types. This is of importance given that the presence of multiple types in a single list is going to require you to handle the types differently. Even in the most trivial of examples (as with strings and integers being together), the interfaces for both are different. Try adding a string to an integer in Python and see what happens. . So it‚Äôs actually not quite true to say that a collection of homogenous types have to all be exactly the same type, but they must share common interfaces and ideally be handled using the same logic. If you think about it, in the real world heterogenous types are pretty common occurrences. There are often situations where, for example, you have to handle the output of API calls or data that doesn‚Äôt derive from code that‚Äôs in yous control and then you‚Äôll perhaps be dealing with a dictionary that contains all sorts of types. . In Python we do have the typing.Any annotation, but it‚Äôs pretty clear ‚Äî and the book emphasises this ‚Äî that isn‚Äôt really useful in the vast majority of cases. You might as well not bother with type annotations if you‚Äôre going to liberally be using Any. . The first of our collection type helpers: TypedDict . TypedDict was introduced in Python 3.8 and allows you to communicate intent when it comes to the types that are being passed through your code. Note that, as with a lot of what we‚Äôre talking about here, this is all information that‚Äôs useful for a type checker and isn‚Äôt something that is dynamically checked. . You can use TypedDict to define structures that specify the types of fields of your dictionary in a way that is easier to parse as a human reader than just using dict. See this example, adapted from one in the book: . from typing import TypedDict class Range(TypedDict): min: float max: float class Stats(TypedDict): value: int unit: str confidenceRange: Range our_stats = Stats(value=3, unit=&quot;some_name&quot;, confidenceRange=Range(min=1.3, max=5.5)) print(our_stats) # returns {&#39;value&#39;: 3, &#39;unit&#39;: &#39;some_name&#39;, &#39;confidenceRange&#39;: {&#39;min&#39;: 1.3, &#39;max&#39;: 5.5}} . If TypedDict doesn‚Äôt do everything you need it to, we have some other options. . Custom Collections with TypeVar . TypeVar in Python is how you can implement generics. Generics, as I learned while reading, are ways of representing things that are the same, like when you don‚Äôt care what specific type is being used. Take this example from the book, where you want to reverse items in a list, but only if the items are all of the same type. You could write the following: . from typing import TypeVar T = TypeVar(&#39;T&#39;) def reverse(coll: list[T]) -&gt; list[T]: return coll[::-1] . You can use generics in other ways to create new kinds of collections or groupings. For example, again this one is adapted from the book, if you were writing a series of methods that returned either something useful or a particular error message: . def get_weather_data(location: str) -&gt; Union[WeatherData, APIError]: # ‚Ä¶ def get_financial_data(transaction: str) -&gt; Union[FinancialData, APIError]: # ‚Ä¶ . ‚Ä¶and so on, you could use generics as a way of simplifying how this gets presented: . T = TypeVar(&#39;T&#39;) APIResponse = Union[T, APIError] def get_weather_data(location: str) -&gt; APIResponse[WeatherData]: # ‚Ä¶ def get_financial_data(transaction: str) -&gt; APIResponse[FinancialData]: # ‚Ä¶ . That looks and feels so much cleaner! . Tweaking existing functionality with collections . If you‚Äôre just making slight changes to the behaviour of collections, instead of subclassing dictionaries or lists or whatever, it‚Äôs better to override the methods of collections.UserDict, collections.UserString and/or collections.UserList. . You‚Äôll run into fewer problems when you actually implement this. Of course, there is a slight performance cost to importing these collections, so it‚Äôs worth making sure this cost isn‚Äôt too high. . You‚Äôll maybe have noticed that there isn‚Äôt a collections.UserSet in the list above. For sets we‚Äôll have to use abstract base classes which are found in collections.abc. The big difference between the User* pattern of classes, there is no built-in storage for the abc classes. You have to provide your own storage if you need it. So for sets, we‚Äôd use collections.abc.Set and then implement whatever group of methods are required for that particular class. . In the set example, we have to implement __contains__, __iter__ and __len__, and then the other set operations will automatically work. There are currently (as of Python 3.10.2) 25 different ABCs available to use. I definitely will be exploring those as they seem really useful. . Even though this chapter got into the weeds of collections a little, I learned a lot and I‚Äôm already finding places in the ZenML codebase where all of this is being used. . Typeguard . Before I leave, since we‚Äôre still thinking about types, I wanted to share this little package I discovered the other day: typeguard. You can use it in a bunch of different ways, but a useful short video from calmcode.io showed how a simple decorator can simplify code and catch type errors. . Consider the following example code: . def calculate_risk(risk_factor: float) -&gt; str: &quot;&quot;&quot;Calculates how much risk you took&quot;&quot;&quot; return risk_factor * 3 # arbitrary return value :) . What if someone passes in a wrong type into this function? It‚Äôll fail. So maybe we want to handle that particular situation: . def calculate_risk(risk_factor: float) -&gt; str: &quot;&quot;&quot;Calculates how much risk you took&quot;&quot;&quot; if not isinstance(risk_factor, float): raise ValueError(&quot;Wrong type for risk_factor&quot;) return risk_factor * 3 . If you have lots of parameters in your function and you have to handle them all, this could get messy quite quickly. Instead, we can pip install typeguard and do the following: . from type guard import typechecked @typechecked def calculate_risk(risk_factor: float) -&gt; str: &quot;&quot;&quot;Calculates how much risk you took&quot;&quot;&quot; return risk_factor * 3 . Now that‚Äôs a handy little decorator! It‚Äôll handle all the raising of appropriate errors above based on whether you passed in the right type or not. It works for classes as well. You‚Äôre welcome, and thanks Vincent for making the introductory video! .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/01/18/robust-python-5.html",
            "relUrl": "/robustpython/python/books-i-read/2022/01/18/robust-python-5.html",
            "date": " ‚Ä¢ Jan 18, 2022"
        }
        
    
  
    
        ,"post30": {
            "title": "A Midway Report on my Computer Vision Project",
            "content": "(This post is adapted from a twitter thread, so is a bit more terse than usual.) . I recently switched what I spend the majority of my professional life doing (history -&gt; software engineering). I‚Äôm currently working as an ML Engineer at ZenML and really enjoying this new world of MLOps, filled as it is with challenges and opportunities. . I wanted to get some context for the wider work of a data scientist to help me appreciate the problem we are trying to address at ZenML, so looked around for a juicy machine learning problem to work on as a longer project. . I was also encouraged by Jeremy Howard‚Äôs advice to ‚Äúbuild one project and make it great‚Äù. This approach seems like it has really paid off for those who‚Äôve studied the fastai course and I wanted to really go deep on something myself. . Following some previous success working with other mentors from SharpestMinds on a previous project, I settled on Computer Vision and was lucky to find Farid AKA @ai_fast_track to mentor me through the work. . In the last 6 weeks, I‚Äôve made what feels like good progress on the problem. This image offers an overview of the pieces I‚Äôve been working on, to the point where the ‚Äòsolution‚Äô to my original problem feels on the verge of being practically within reach. . . After just a few lessons of the FastAI course, I trained a classification model to ~95% accuracy to help me sort redacted images from unredacted images. . I used Explosion‚Äôs Prodigy to annotate an initial round of data to pass into the next step, enjoying how the labelling process brought me into greater contact with the dataset along the way. . I switched to using IceVision to help me with the more complicated object detection problem, using MMDetection and VFNet to get pretty good results early on. . I‚Äôm currently in the process of creating my own synthetic images to boost the annotations I‚Äôve manually made. (I‚Äôll be writing about this process soon as well, as I‚Äôm learning a lot about why this is so important for these kinds of computer vision problems.) . I‚Äôve also been amazed at the effectiveness of self-training (i.e. using my initial model in my annotation loop to generate an initial set of annotations which I can easily amend as appropriate, then feeding those annotations in to create a better model and so on). More to follow on that step, too. . I started using Evidently to do some drift detection, inspired by some work I was doing for ZenML on adding Evidently as an integration to our own tool. This helped me think about how new data was affecting the model and the training cycle. I feel like there‚Äôs a lot of depth here to understand, and am looking forward to diving in. . I made a tiny little demo on HuggingFace Spaces to show off the current inference capabilities and to see the model in a setting that feels close to reality. This is a simple little Gradio app but I liked how easy this was to put together (a couple of hours, mainly involving some build issues and a dodgy requirements.txt file) . Along the way, I found it sometimes quite painful or fiddly to handle the PDF files that are the main data source for the project, so I built my own Python package to handle the hard work. I used fastai‚Äôs nbdev to very quickly get the starters of what I‚Äôm hoping might be a useful tool for others using PDF data for ML projects. . Throughout all this, Farid has been patiently helping guide me forward. He saved me from going down some dark rabbit holes, from spending too long studying skills and parts of the problem that needed relatively little mastery in order to get to where I am. . Farid has been a consistently enthusiastic and kind advocate for my work, moreover, and this has really helped me stay the course for this project that takes a decent chunk of my time (especially seeing as I do it completely aside / separately from my day job). . I feel like I‚Äôm consistently making progress and learning the skills of a data scientist working in computer vision, even though I have so much left to learn! My project still has a ways to go before it‚Äôs ‚Äòdone‚Äô, but I‚Äôm confident that I‚Äôll get there with Farid‚Äôs support. (Thank you!) .",
            "url": "https://mlops.systems/python/fastai/tools/redactionmodel/2022/01/16/midway-report-redaction-project.html",
            "relUrl": "/python/fastai/tools/redactionmodel/2022/01/16/midway-report-redaction-project.html",
            "date": " ‚Ä¢ Jan 16, 2022"
        }
        
    
  
    
        ,"post31": {
            "title": "Different ways to constrain types in Python",
            "content": "The fourth chapter of ‚ÄòRobust Python‚Äô continues on from where we left off last time. We had previously learned about the benefits of type annotations in general terms, as well as started to understand how we might apply these annotations to simple code examples. But what if things are a bit more complicated? Then we have a few more options at our disposal. . Note that you can assign all of these type assignments to variables (‚Äòtype aliases‚Äô), which might just make your code that much more readable. . Optional to catch None references . Optional as a type annotation is where you want to allow a specific type or None to be passed in to a particular function: . from typing import Optional def some_function(value: Optional[int]) -&gt; int: # your code goes here . Note that you‚Äôll probably want (and mypy will remind you if you forget) to handle what happens in both those cases inside your function. (You may need to specifically pass in the ‚Äîstrict-optional flag to catch this when using mypy.) . Union to group types together . This is used when multiple different types can be used for the same variable: . from typing import Union def returns_the_input(input: Union[str, int]) -&gt; Union[str, int]: return input . This function doesn‚Äôt really do anything, but you get the idea. Note, too, that Optional[int] is really a version of Union[int, None]. (The book gets into exactly why we might care about reducing the number of possible options by way of a little detour into set theory.) . Literal to include only specific values . A little like what I believe enumerations do, we also have the Literal type. It restricts you to whatever specific values are defined: . from typing import Literal def some_function(input: Literal[1, 2, 3]) -&gt; int: return input . Here the function is restricted to inputs that are either 1, 2 or 3. Note that these are a feature that applies to Python 3.8 and above. . Annotated for more complicated restrictions . These are available, but not really useful since they only function as a communication method. You can specify specific restrictions such as the following (example is taken from the book, p. 56: . from typing import Annotated x: Annotated[int, ValueRange(3,5)] y: Annotated[str, MatchesRegex(&#39;[abc]{2}&#39;) . Read more about it here. The book doesn‚Äôt spend much time on it and it seems like it‚Äôs probably best left alone for the moment. . NewType to cover different contexts applied to the same type . NewType, on the other hand, is quite useful. You can create new types which are identical to some other type, and those new values made with the new type will have access to all the methods and properties as the original type. . from typing import NewType class Book: # you implement the class here NewBook = NewType(&quot;NewBook&quot;, Book) def process_new_book(book: NewBook): # here you handle what happens to the new book . You can achieve something like the same thing with classes and inheritance, I believe, but this is a lightweight version which might be useful to achieve the same end goal. . Final to prevent reassignment / rebinding . You can specify that a particular variable should have a single value and that value only. (Note that mutations of an object etc are all still possible, but reassignment to a new memory address is not possible. . from typing import Final NAME: Final = &quot;Alex&quot; . If you tried to subsequently change this to a different name, mypy would catch that you‚Äôd tried to do this. This can be valuable across very large codebases, where the potential for someone to reassign a variable might be not insignificant. . So there you have it: a bunch of different ways to handle combinations of types and/or more complicated annotation scenarios. The next chapter will cover what happens when we throw collections into the mix, and what type annotation challenges are raised. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/01/08/robust-python-4.html",
            "relUrl": "/robustpython/python/books-i-read/2022/01/08/robust-python-4.html",
            "date": " ‚Ä¢ Jan 8, 2022"
        }
        
    
  
    
        ,"post32": {
            "title": "Learning about 'nbdev' while building a Python package for PDF machine learning datasets",
            "content": "While working to develop a computer vision model that detects redactions in documents obtained as a result of FOIA requests, I have encountered some tasks that I end up repeating over and over again. Most of the raw data in the problem domain exists in the form of PDFs. These PDF files contain scanned images of various government documents. I use these images as the training data for my model. . The things I have to do as part of the data acquisition and transformation process include the following: . downloading all the PDF files linked to from a particular website, or series of web pages | converting and splitting all the downloaded PDF files into appropriately sized individual image files suitable for use in a computer vision model | generating statistics on the data being downloaded and processed, as well as (further down the line) things like detecting data drift for incoming training data | splitting up data as appropriate for train / validation / test data sets | extracting text data from the images via an OCR process | versioning, syncing and uploading those images to an S3 bucket or some other cloud equivalent for use in the overall workflow | . It‚Äôs not hard to see that many of these things likely apply to multiple machine learning data acquisition scenarios. While writing the code to handle these elements in my specific use case, I realised it might be worth gathering this functionality together in an agnostic tool that can handle some of these scenarios. . I had wanted to try out nbdev ever since it was announced back in 2019. The concept was different to what I was used, but there were lots of benefits to be had. I chose this small project to give it an initial trial run. I didn‚Äôt implement all of the above features. The two notable missing parts are text extraction and data versioning and/or synchronisation. . pdfsplitter is the package I created to scratch that itch. It‚Äôs still very much a work in progress, but I think I did enough with nbdev to have an initial opinion. . I think I had postponed trying it out because I was worried about a steep learning curve. It turned out that an hour or two was all it took before I was basically up and running, with an understanding of all the relevant pieces that you generally use during the development lifecycle. . Built in to nbdev in general is the ability to iterate quickly and driven by short, small experiments. This is powered by Jupyter notebooks, which are sort of the core of everything that nbdev is about. If you don‚Äôt like notebooks, you won‚Äôt like nbdev. It‚Äôs a few years since it first saw the light of day as a tool, and as such it felt like a polished way of working, and most of the pieces of a typical development workflow were well accounted for. In fact, a lot of the advantages come from convenience helpers of various kinds. Automatic parallelised testing, easy submission to Anaconda and PyPi package repositories, automatic building of documentation and standardising locations for making configuration changes. All these parts were great. . Perhaps the most sneakily pleasant part of using nbdev was how it encouraged best practices. There‚Äôs no concept of keeping test and documentation code in separate silos away from the source notebooks. Following the best traditions of literate programming, nbdev encourages you to do that as you develop. Write a bit of code here, write some narrative explanation and documentation there, and write some tests over there to confirm that it‚Äôs working in the way you expected. When Jeremy speaks of the significant boost in productivity, I believe that a lot of it comes from the fact that so much is happening in one place. . While working on pdfsplitter, I had the feeling that I could just focus on the problem at hand, building something to help speed up the process of importing and generating images from PDF data for machine learning projects. . Not everything was peaches and roses, however. I ran into a weird mismatch with the documentation pages generated and my GitHub fork of nbdev since I was using main as the default branch but nbdev still uses master. I will be submitting an issue to their repository, and it was an easy fix, but it was confusing to struggle with that early on in my process. I‚Äôm also not sure how well nbdev will gel with large teams of developers, especially when they‚Äôre working on the same notebooks / modules. I know reviewnb exists now and even is used within fastai for code reviews, but I would imagine an uphill battle trying to persuade a team to take a chance with that. . I‚Äôve been using VSCode at work, supercharged with GitHub Copilot and various other goodies, so it honestly felt like a bit of a step back to be forced to develop inside the Jupyter notebook interface, absent all of my tools. I also found the pre-made CLI functions a little fiddly to use ‚Äî fiddly in the sense that I wish I‚Äôd set up some aliases for them early on as you end up calling them all the time. In fact, any time I made a change I would find myself making all these calls to build the library and then the documentation, not forgetting to run the tests and so on. That part felt a bit like busy work and I wish some of those steps could be combined together. Maybe I‚Äôm using it wrong. . All in all, I enjoyed this first few hours of contact with nbdev and I will continue to use it while developing pdfsplitter. The experience was also useful to reflect back into my current development workflow and environment, especially when it comes to keeping that close relationship between the code, documentation and tests. . [Photo by Laura Ockel on Unsplash] .",
            "url": "https://mlops.systems/python/jupyter/fastai/tools/2022/01/06/nbdev-early-impressions.html",
            "relUrl": "/python/jupyter/fastai/tools/2022/01/06/nbdev-early-impressions.html",
            "date": " ‚Ä¢ Jan 6, 2022"
        }
        
    
  
    
        ,"post33": {
            "title": "Getting practical with type annotations and `mypy`",
            "content": "The third chapter of ‚ÄòRobust Python‚Äô offers a quick introduction to the practicalities of type annotations in Python. We also see tools like mypy being used to catch places where the reality of your code doesn‚Äôt necessarily match the type annotations that you‚Äôve stated. . For the first, a quick example can suffice: . name: str = &quot;alex&quot; def some_function(some_number: int, some_text: str = &quot;some text&quot;) -&gt; str: # your code goes here return &quot;&quot; # returns a string . You can see the different places that type annotations might appear. You can annotate variables in your code. I‚Äôve seen this one less often, but it‚Äôs possible. Then you can have type annotations for the parameters when defining functions (some even with default values assigned). You can also have type annotations for the return value of those functions. . Note that type hints are not used at runtime, so in that sense they are completely optional and don‚Äôt affect how your code runs when it‚Äôs passed through the Python interpreter. (Type hints were introduced in Python 3.5, though there is a way to achieve the same effect using comments and a standard way of listing type annotations that way if you are stuck with a 2.7 codebase, for example.) . With some type annotations added to our code, we can use a typechecker like mypy to see whether things are really as we imagine. In Viafore‚Äôs own words: . ‚Äútype checkers are what allow the type annotations to transcend from communication method to a safety net. It is a form of static analysis.‚Äù . If your codebase uses type annotations to communicate intent, and you‚Äôre using mypy to catch any of those type errors, remember that typecheckers only catch this certain type of errors. You still need to be doing testing and all the other best practices to help catch the rest. . One forward-looking benefit covered by this chapter was how having code covered with type annotations and type checking could give you the confidence to change things in the codebase that otherwise you would have hesitated to even approach. There are, of course, also some tradeoffs and disadvantages to adding this in: particularly around speed of iteration and possibly flexibility, but the book makes a strong case for why most large Python codebases could probably use type checking as part of their arsenal. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/01/03/robust-python-3.html",
            "relUrl": "/robustpython/python/books-i-read/2022/01/03/robust-python-3.html",
            "date": " ‚Ä¢ Jan 3, 2022"
        }
        
    
  
    
        ,"post34": {
            "title": "Counter: a shortcut to counting iterables in Python",
            "content": "I came across this special dictionary type while reading an earlier chapter of ‚ÄòRobust Python‚Äô the other day. It‚Äôs perhaps best illustrated with an example: . from collections import Counter Counter([1,1,2,3]) # returns Counter({1: 2, 2: 1, 3: 1}) Counter(&#39;The Netherlands&#39;.lower()) # returns Counter({&#39;e&#39;: 3, &#39;t&#39;: 2, &#39;h&#39;: 2, &#39;n&#39;: 2, &#39; &#39;: 1, &#39;r&#39;: 1, &#39;l&#39;: 1, &#39;a&#39;: 1, &#39;d&#39;: 1, &#39;s&#39;: 1}) . I had no idea this existed, and of course usually default to some kind of a cookie-cutter loop when trying get counts of elements and put those counts into a dict. . To get the inividual elements, just call the elements method on the Counter object. To get the most common n elements, call the most_common(n) method. To get the total number of counts inside the dictionary, use the total method. To reset all the counts, use the clear method. . Just a nice little set of functionality, hiding in plain sight inside the Python standard library. . Photo by Ibrahim Rifath on Unsplash .",
            "url": "https://mlops.systems/python/2022/01/01/counter.html",
            "relUrl": "/python/2022/01/01/counter.html",
            "date": " ‚Ä¢ Jan 1, 2022"
        }
        
    
  
    
        ,"post35": {
            "title": "What's special about types in Python?",
            "content": "The first section of Robust Python dives into types. We begin by taking a step back to think about what exactly types are being used for, and what they might bring us. Python was not (until v3.5) a language with which you could easily use typing. I remember going to the Pylondinium conference in London in 2018 and going to a talk by Bernat Gabor about type hints in Python. Back then I didn‚Äôt have much of a sense of how new they were to many people, but even now I don‚Äôt get the feeling that they‚Äôve been universally adopted. Hence Patrick‚Äôs book, I suppose‚Ä¶ . A type is defined in the book as being ‚Äúa communication method‚Äù, both to / for computers (‚Äúmechanical representation‚Äù) as well as for humans (‚Äúsemantic representation‚Äù). For the computer, when a variable is of a certain type this determines what methods can be called on that particular object. As such, though I‚Äôm straying into territory I don‚Äôt fully understand, I believe it also helps with compilation efficiency. (Python is a dynamically-typed language so any errors or type mismatches will only become apparent at runtime, however). . For humans, types can help signal intent. This connects with my previous chapter summary from this book where I stated that code should communicate intent well to be considered ‚Äòrobust‚Äô. Take the following simple code snippet: . dates = [...] def process_date(input): date = extract_date(input) dates.append(date) return date . We have an extract_date function (defined elsewhere in the code), but we have no real sense of what this input parameter would be. Are we taking in strings as input? Are we taking in datetime.datetime objects? Does the extract_date function accept both, or do we need to ensure that we are only taking a specific type? All these questions could be cleared up with a simple type hint as part of the function definition, like so: . dates = [...] def process_date(input: datetime.datetime): date = extract_date(input) dates.append(date) return date . Now we know what the input should be, and we can also add a type hint to the extract_date function as well which will help communicate our intent. . We also learn how Python is more towards the ‚Äòstrongly-typed‚Äô side of things on the language spectrum. If you try to concatenate a list with a dict in Python using the + operator, Python will throw a TypeError and fail. If you try to do the same in Javascript you get two different answers depending on the order of the two operands: . &gt;&gt;&gt; [] + {} &quot;[object Object]&quot; &gt;&gt;&gt; {} + [] 0 . For our purposes, using Python, we can use the strong typing to our advantage. . Python is dynamically typed, though, which takes a bit more caution to handle in a robust manner. Any type mismatches will only be found at runtime ‚Äî at least using just the vanilla install of the language without any extra imports or modules. . The chapter ends with a brief discussion of duck typing, defined as ‚Äúthe ability to use objects and entities in a programming language as long as they adhere to some interface‚Äù. We gain a lot in terms of increased composability, but if you rely on this feature of the language too much then it can become a hindrance in terms of communicating intent. . This chapter didn‚Äôt add too many new concepts or skills to my current understanding of the benefits of types, but it was useful to have this concept of ‚Äòcommunicating intent‚Äô to be reiterated. When I think back to how I‚Äôve heard types mentioned in the past, they often get cast in a technical sense, whereas thinking about communication between developers I think is a more motivating framing. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2021/12/30/robust-python-2.html",
            "relUrl": "/robustpython/python/books-i-read/2021/12/30/robust-python-2.html",
            "date": " ‚Ä¢ Dec 30, 2021"
        }
        
    
  
    
        ,"post36": {
            "title": "What makes code robust?",
            "content": "We use a lot of modern Python idioms, libraries and patterns at work, so I‚Äôve been wanting to get up to speed on that and maybe even actively contribute to this general direction. A recently-published book, Robust Python: Write Clean and Maintainable Code by Patrick Viafore, seems like it answers many of the questions I have around this topic. It is quite dense in terms of the amount of new things per chapter, so I‚Äôll be working my way through it in the coming months and reflecting on things as I encounter them. . The first chapter is mainly about setting the scene for all the technical pieces that follow. Patrick asks the core questions: what is robust code and why do we even care? What problems does it solve to think about code in this way. . What I took away was that a robust codebase emphasises good communication as well as avoiding accidental complexity. A lot has been written about ‚Äòclean code‚Äô and how to achieve this, but it seems that ‚ÄòRobust Python‚Äô is arguing for looking a bit further into the future, when you have to come back to refactor your code three months after you wrote it, or when your colleague needs to do the same. . ‚ÄúWriting robust code means deliberately thinking about the future.‚Äù (p. 3) . You write robust code, in other words, because you know that the codebase is going to be changing and shifting and that whatever you write today may need to be modified at a later date: . ‚ÄúA robust codebase is resilient and error-free in spite of constant change.‚Äù (p. 4) . We‚Äôre trying to solve for the way that code is often hard to reason about or understand when you‚Äôre outside the original moment when it was written. Accordingly, it pays dividends to take a bit of extra time upfront to write code such that it does communicate intent well, and that you haven‚Äôt made things more complicated than they need to be. . Moreover, the communication of intent needs to be done in a way that is asynchronous. The book goes into a bit more detail about why communication practices that require minimal cost and minimal proximity are to be preferred. These include: the code itself, in-code comments, tests, version control history, wikis, and in-project documentation. . The first part of the book is all about type annotation, using mypy, and how working with types helps makes your code more robust. We use a lot of this at work so I‚Äôm excited to take a deep dive into this. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2021/12/29/robust-python-1.html",
            "relUrl": "/robustpython/python/books-i-read/2021/12/29/robust-python-1.html",
            "date": " ‚Ä¢ Dec 29, 2021"
        }
        
    
  
    
        ,"post37": {
            "title": "Exploring J, an array programming language",
            "content": "I‚Äôve long wanted to explore the J programming language. I think I probably first heard about it from Jeremy Howard amidst one of the early iterations of the fastai course. He‚Äôs since spoken about it in other places. . It is part of the family of programming languages that includes APL, K and Q. These can broadly be categorised as array-programming languages, where arrays are generally the core data structure and mental model to keep in mind. They used to be extremely popular in the 1970s and 1980s, particularly among institutions or businesses with a requirement for performant calculation / computation. One of these, Q, continues to live on (as a closed-source language) in the world of finance and trading. (Q is popular alongside the proprietary database kdb+). . You‚Äôre probably wondering why someone would want to use this fairly specialised and niche language. When you look at examples of J code ‚Äî like the ones here, for example ‚Äî it‚Äôs easy to simply dismiss it as an unreadable (‚Äòwrite-only‚Äô) language. Indeed, many do dismiss it for this reason. Code is often compact, with single letters or symbols doing all the work. Defenders of J hold this up as a feature, not a problem. The compactness of the language means that you can fit the entirety of the solution (space) of a complex problem on a single screen, whereas in many (most?) other languages you would have to be scrolling up and down through dozens or even hundreds of lines of code. . The array languages seem to come at solving problems from a particular perspective. The symbols and letters that transform the arrays in J function as a pattern language. For a simple example, think of what you have to do when you want to find the count of a particular element from within an array/list. The array language paradigm argues that you don‚Äôt want to waste your time and screen space writing out boilerplate code to carry out this calculation, when it‚Äôs a common pattern that you can just use from the language itself. When problem-solving, therefore, spend your time thinking about the problem and not messing around with syntax or repeating yourself. . J and its cousins are extremely efficient. It is written in C, and I recently heard someone quote one of the early J pioneers as having said that ‚Äúit is not theoretically possible to write J code that is more performant than C, but it often ends up being so‚Äù. For some math- or statistics-heavy domains (think the world of finance), it is extremely helpful to have this highly abstracted language that works performantly on large datasets. Moreover, it seems to be even more helpful when you have a hard problem to work on that isn‚Äôt fully understood. . Kenneth Iverson‚Äôs wrote a paper (‚ÄúNotation as a Tool of Thought‚Äù) that is a classic in computer science and gets into some of the above arguments. (It is written using APL, but it also applies to J). I will probably return to that at a future date, because it often comes up and is recommended as a particularly rich document worth taking time to explore in depth. . Very much as a project to indulge my curiosity, I will be exploring J over the coming months. I have been listening to the back catalogue of The Array Cast podcast, and I will be slowly working my way through some of the resources listed on the official J site. Let me know if you have experience working with J! .",
            "url": "https://mlops.systems/j/2021/12/29/j-language.html",
            "relUrl": "/j/2021/12/29/j-language.html",
            "date": " ‚Ä¢ Dec 29, 2021"
        }
        
    
  
    
        ,"post38": {
            "title": "A Taxonomy of Redaction",
            "content": "One of the things that makes it hard to train a model to detect redactions in documents is the fact that there are lots of kinds of redactions. Not only were different tools or methods used at different times, but even organisations and agencies from the same country or government didn‚Äôt always share redaction practices. . I took a bit of time to try to understand the different kinds of redactions in my (pretty huge) data set. I didn‚Äôt have any special process for selecting these images; I randomly sorted the immediate ~70,000 images I have collected and looked through to try to identify some patterns. . Taking a close look at the actual parts of images that contain redactions gives me a better sense of the challenges involved in detecting those redactions. As I iterate through my collection of images, I can start to build up an intuitive sense of where class imbalances might exist. Among the images that contain redactions, for example, which ones are most represented and which contain fewer examples? In general, where do I need to focus my efforts when it comes to improving my model? . The first easy distinction to draw is that between digital and hand-applied redactions. . . It seems that the trend in this is towards digital redactions. Perhaps it is seen as less reliable, or perhaps it‚Äôs more time consuming to attach the reasons for redactions having happened. Perhaps, too, there are some legal reasons why each redaction needed to start having a specific reason applied to it. . At first glance, no pun intended, it would appear that digital redactions are much easier to recognise. They‚Äôre often uniform in how they are applied and are usually pretty blunt in their appearance. There are some non-redaction uses for totally black or grey boxes laid on top of text, but they aren‚Äôt common and it‚Äôs a pretty strong feature to have to predict. . Handwritten redactions are also easy to recognise, but potentially the borders are harder to make out. Sometimes having a thinner pen with which redactions are applied might make it slightly less accurate. . It is more practically important to distinguish between redactions that are easy to recognise vs ones that take some time to notice. I can use my own speed at noticing the redaction on a page as a gauge. It‚Äôs not a perfect analogy, but Jeremy Howard‚Äôs adage that if a human can reliably do some kind of classification or object detection, then probably a computer can as well. I guess the inverse is also true: if a human will find it hard to recognise a particular feature in an image, then a computer will probably also find it hard. . There isn‚Äôt much point spending too long with the ‚Äòeasy‚Äô redactions. These are usually whatever is boxy and blunt. It‚Äôs the stereotype of a redacted document, one like what was used as the cover art on the (much-censored) Guant√°namo Diary by Mohamedou Ould Slahi. . . Sometimes you see that the entire page has been redacted with some kind of a coloured box. Other times entire columns of information has been redacted from a table. These definitely feel like they are the more recent types of redactions. . One thing that makes detecting redactions hard, on the other hand, is if the number of redactions is small. It stands to reason that lots of small redactions can stand out at first glance, whereas a single small redaction on one corner of the page is maybe harder to notice. . The hardest of redactions seems like it is in examples like this: . . A white box on top of other white boxes! I often have to look quite closely at these to distinguish what is normal text and what is a redaction box. Some of them have a faint thin grey boundary box around them, which I guess ends up being pretty useful as a way to make that distinction. Surprisingly, the model that I‚Äôve trained so far is not terrible at making these kinds of distinctions. . . I have a few hundred annotated images so far, but I now have an intuitive sense of the hard parts of the object detection test. I also have a sense of how represented I feel like those hard parts are ‚Äî not very. . As I wrote in my previous update on my progress in this project, the next step is very much to find ways to increase the volume of good training data that I‚Äôm using to train my model. Part of that will involve creating synthetic data, part of that will be using self-training to speed up my annotation, and of course another part will just be doing more manual annotation. I‚Äôve already started work on creating the synthetic data. More on that next time! .",
            "url": "https://mlops.systems/redactionmodel/2021/12/15/redaction-taxonomy.html",
            "relUrl": "/redactionmodel/2021/12/15/redaction-taxonomy.html",
            "date": " ‚Ä¢ Dec 15, 2021"
        }
        
    
  
    
        ,"post39": {
            "title": "73% accuracy for redaction object detection",
            "content": "Last time I wrote about my redaction model training project, I explained how I used Prodigy to annotate and label a bunch of images. I subsequently spent a long evening going through the process, getting to know my data. I managed to make 663 annotations, though quite a few of those were negative annotations: I was stating that a certain document contained no redactions at all. . Once I had my redactions, I needed to convert the files from a Prodigy format into a .coco annotation format. I am using IceVision, a really useful computer vision library, for which it is easier if I pass in the annotations in the .coco format. . From that point, it was fairly easy to follow the steps of the object detection tutorial outlined in the IceVision documentation. I ran into some problems with Paperspace Gradient not easily installing and importing IceVision. For some reason files don‚Äôt get unzipped on Paperspace, but it‚Äôs possible to just do this manually: . Do the basic install, including the import of icevision.all. Wait for the error to get raised, then open up a terminal and enter: | . cd /root/.icevision/mmdetection_configs/ rm v2.16.0.zip wget https://github.com/airctic/mmdetection_configs/archive/refs/tags/v2.16.0.zip unzip v2.16.0.zip . Then run it again as normal. Later on, another error will get raised. Fix it with this (again in the terminal): . jupyter nbextension enable --py widgetsnbextension . This enables ipywidgets in the notebook, I think. . Once through all of that, I was able to fine-tune a model based on the annotations which I currently have. I selected VFNet as the model I wanted to use as the pertained model. After training for 40 epochs, I reached an accuracy of 73%: . . If we look at some of the results (using model_type.show_results()) we can get a sense of the parts it found easy and the parts which it found hard. (All the boxes below are what it as predicted, not the ground truth annotations.) Some identification of boxes went as you might expect: . . I was surprised that something like this worked as well as it did: . . It wasn‚Äôt perfect, but I don‚Äôt remember having annotated too many of this specific redaction type, so I‚Äôm fairly happy with how it worked out. You can see it still makes a number of mistakes and isn‚Äôt always precise about where the boxes should go. I hope that‚Äôll improve as I add more examples of this type of redaction. . My next steps for this project include the following: . create synthetic data. The redactions are probably easy enough to mimic where we‚Äôll get a lot of value from the use of synthetic data (fake redactions on not-real document backgrounds). It‚Äôll be an easy way to boost my training data set by a good amount, hopefully leading to big improvements in my model accuracy. | potentially add in either active learning (to help speed up my annotation process) or self-training (using the model to make annotation suggestions on unlabelled data and using only the suggestions with really high confidence estimates). | think through the augmentations that I use as part of my workflow. I basically want augmentations that are similar to however the production use case will be: i.e. the kinds of redacted images that it might see when being given real-world data at inference time post-training. | add in experiment tracking. I‚Äôve never used something like Weights &amp; Biases, so I‚Äôm excited to try that out and have a real process for tracking my progress throughout this project. | cleaning up and refactoring (a bit) my repository where the code lives for processing the input data. It‚Äôs starting to get a bit unwieldy and I‚Äôm worried I‚Äôll start to forget the order things were done and some of those small details. | .",
            "url": "https://mlops.systems/redactionmodel/computervision/progressreport/2021/12/11/redaction-progress-week-one.html",
            "relUrl": "/redactionmodel/computervision/progressreport/2021/12/11/redaction-progress-week-one.html",
            "date": " ‚Ä¢ Dec 11, 2021"
        }
        
    
  
    
        ,"post40": {
            "title": "What is VFNet?",
            "content": "VFNet is short for VariFocalNet. This method of object detection was first released in 2008 and it scored 55.1 on the COCO test-dev benchmark, state-of-the-art at the time. There have since been other improvements. . . The original paper is here. The implementation of this model is here. . The problem it solves is that when we‚Äôre training a model, we have a large number of possible options for objects detected in an image. What we need to do is rank these options in order of likelihood of being a correct bounding of a box. . It is based on and draws on the MMDetection model/toolbox. MMDetection is a Pytorch library for object detection. It is modular, allowing for greater customisability. . Other resources . Airctic Presentation on VFNet .",
            "url": "https://mlops.systems/redactionmodel/computervision/2021/11/30/vfnet-basics.html",
            "relUrl": "/redactionmodel/computervision/2021/11/30/vfnet-basics.html",
            "date": " ‚Ä¢ Nov 30, 2021"
        }
        
    
  
    
        ,"post41": {
            "title": "How to annotate image data for object detection with Prodigy",
            "content": "I‚Äôm back to working on the redaction model, though this time with a slightly more focused objective: object detection. . Object detection is when you put bounding boxes around the specific object that you are trying to locate within an image. The end goal for my project is to be able to identify ‚Äî for an arbitrary image ‚Äî which parts of the image are redacted, and then to be able to calculate what proportion of the image is redacted. . For this, I need annotations. Annotations are the data that I will use as the fuel for the model I hope to train. We need a lot of annotations of specific redactions in order for the computer to be able to learn to detect what is a redaction and what is just an empty box, for example. . I showed in an earlier post how I trained a model to detect whether there was any kind of redaction inside an image (to around 95% accuracy). For this next stage, it isn‚Äôt enough to offer a binary ‚Äòyes‚Äô or ‚Äòno‚Äô for whether it has been redacted. I need to specify the coordinates of a bounding box which encompasses each redaction. . In terms of the final output of the annotations, there are two main ways that this could go. I could either: . get x and y coordinates for the centre of the bounding box, and then a height and a width of the box around this centre point | get the four coordinates for each of the corners of the bounding box. | The COCO dataset format will eventually want datasets in the second format, but Prodigy has its own way of storing the data which I just left for now. Once I have a better handle on the annotation flow I will write a custom recipe which will save the data in exactly the format that I want. For now, it‚Äôs good enough. . Installing Prodigy into your development environment is a breeze now that you can do it with pip: . pip install prodigy -f https://XXXX-XXXX-XXXX-XXXX@download.prodi.gy # where the XXXs are your license code . Getting going with the image training was as easy as the following CLI command: . prodigy image.manual redaction-object-detection /path/to/image/data --label CONTENT,REDACTION --remove-base64 . Note that the --remove-base64 is to ensure that Prodigy doesn‚Äôt store the raw binary image data inside the database alongside the annotations. Prodigy (and their sister tool Spacy) is a little more focused on textual data, where storing the original data alongside the annotation doesn‚Äôt pose too much of an issue, but for image files this probably is a bit of an anti-pattern and could lead to a very large database. . You get a local URL to go visit and you see an interface where you can make the necessary annotations: . . You can see that I am distinguishing between two different classes: redactions and content. Redactions are what we‚Äôve been talking about above. Content, however, is a bounding box for the content on a page. Remember that at the end of all of this we want a percentage of the page that has been redacted. Some images have reduced sized images, where the actual content which could have been redacted only takes up half of the A4 page. If that whole section was redacted, I‚Äôd want a final amount closer to 100% for that image rather than the 50% I‚Äôd get if I just went with the total percentage of redacted pixels on the whole image file. . Doing a few annotations, I ran into a couple of issues almost immediately. What do I do with a page like this: . . The whole text of the page is annotated, but the text only extended half-way down the page. There was only 50% of the page that could have been redacted, but should the content boundary box encompass more of the page, or just the only full-section redaction? . And for the following image, what is the right way to think about how to make the annotation? . . This redaction encompasses multiple lines, so to some extent it doesn‚Äôt make a difference whether we have overlapping annotations or two adjoining boundary boxes. But for the purposes of training our model, will this contribute to a less accurate model? Should I be using polygon boundaries (which Prodigy can also use for annotations)? . . As an aside, this is why annotating your own data is so valuable. You get to see the limits of the annotations, and you get to really own the decisions that are being made. It is a bit early for me to know which approach is the best solution to these two problems, but being aware of them is important. Once we‚Äôre done with our annotations, we can easily export our data to a jsonl file with the following CLI command: . prodigy db-out redaction-object-detection &gt; ./redaction-object-detection-annotations.jsonl . This gives us a file containing all our annotations. A sample for one image gives the idea: . { &quot;image&quot;: &quot;sample/path/redacted/04-F-0269_Global_Screening_Guidance-03.jpg&quot;, &quot;text&quot;: &quot;04-F-0269_Global_Screening_Guidance-03&quot;, &quot;meta&quot;: { &quot;file&quot;: &quot;04-F-0269_Global_Screening_Guidance-03.jpg&quot; }, &quot;path&quot;: &quot;sample/path/redacted/04-F-0269_Global_Screening_Guidance-03.jpg&quot;, &quot;_is_binary&quot;: false, &quot;_input_hash&quot;: 1413334570, &quot;_task_hash&quot;: 1588323116, &quot;_view_id&quot;: &quot;image_manual&quot;, &quot;width&quot;: 800, &quot;height&quot;: 1035, &quot;spans&quot;: [ { &quot;id&quot;: &quot;0ef6ccd0-4a79-471d-9aa1-9c903c83801e&quot;, &quot;label&quot;: &quot;CONTENT&quot;, &quot;color&quot;: &quot;yellow&quot;, &quot;x&quot;: 76.5, &quot;y&quot;: 112.5, &quot;height&quot;: 786.1, &quot;width&quot;: 587.6, &quot;center&quot;: [370.3, 505.55], &quot;type&quot;: &quot;rect&quot;, &quot;points&quot;: [ [76.5, 112.5], [76.5, 898.6], [664.1, 898.6], [664.1, 112.5] ] }, { &quot;id&quot;: &quot;cd05d521-8efb-416b-87df-4624f16ca7f3&quot;, &quot;label&quot;: &quot;REDACTION&quot;, &quot;color&quot;: &quot;cyan&quot;, &quot;x&quot;: 80.3, &quot;y&quot;: 786.2, &quot;height&quot;: 20.2, &quot;width&quot;: 428.4, &quot;center&quot;: [294.5, 796.3], &quot;type&quot;: &quot;rect&quot;, &quot;points&quot;: [ [80.3, 786.2], [80.3, 806.4], [508.7, 806.4], [508.7, 786.2] ] }, { &quot;id&quot;: &quot;3e268e33-4eba-457d-8d17-8271a79ee589&quot;, &quot;label&quot;: &quot;REDACTION&quot;, &quot;color&quot;: &quot;magenta&quot;, &quot;x&quot;: 108.1, &quot;y&quot;: 772.3, &quot;height&quot;: 15.1, &quot;width&quot;: 400.6, &quot;center&quot;: [308.4, 779.85], &quot;type&quot;: &quot;rect&quot;, &quot;points&quot;: [ [108.1, 772.3], [108.1, 787.4], [508.7, 787.4], [508.7, 772.3] ] } ], &quot;answer&quot;: &quot;accept&quot;, &quot;_timestamp&quot;: 1638214078 } . Everything we‚Äôre interested in is inside the spans attribute, and it actually contains both kinds of the annotation that I mentioned above. . As you can see, annotating images in this way is fairly painless, and it brings you in closer contact with your raw data which is an added bonus. .",
            "url": "https://mlops.systems/redactionmodel/computervision/datalabelling/2021/11/29/prodigy-object-detection-training.html",
            "relUrl": "/redactionmodel/computervision/datalabelling/2021/11/29/prodigy-object-detection-training.html",
            "date": " ‚Ä¢ Nov 29, 2021"
        }
        
    
  
    
        ,"post42": {
            "title": "Check your security vulnerabilities with `safety`",
            "content": "safety is a tiny tool that checks your package‚Äôs dependencies for security vulnerabilities. It is free to use for open-source projects, and using it is as a pip install safety followed by safety check. . It checks a database of known security vulnerabilities. This database is only updated once every month, but if you are not open-source or you need access to the more frequently-updated database, then you can subscribe via pyup. . With that caveat, it‚Äôs not perfect, but it‚Äôs better than nothing. An easy CI win for open-source projects. . [I first learned of this tool here. Many thanks to calmcode for continuing to make these really useful videos.] .",
            "url": "https://mlops.systems/security/tools/calmcode/2021/11/27/safety-vulnerability-checker.html",
            "relUrl": "/security/tools/calmcode/2021/11/27/safety-vulnerability-checker.html",
            "date": " ‚Ä¢ Nov 27, 2021"
        }
        
    
  
    
        ,"post43": {
            "title": "Launching a podcast about MLOps",
            "content": "I‚Äôll be co-hosting a new podcast about MLOps, with new episodes out every fortnight. Pipeline Conversations: A Machine Learning Podcast by ZenML is the new podcast from the company where I work. (We build an open-source tool for data scientists to empower them to take control of how their models live in production.) . Our first episode gets into some of the background for why ZenML exists in the first place. Upcoming episodes will be discussions with guests from the data science and MLOps space. . I‚Äôm excited to get the opportunity to talk with so many interesting and smart people. .",
            "url": "https://mlops.systems/zenml/podcast/appearances/2021/11/27/pipeline-conversations.html",
            "relUrl": "/zenml/podcast/appearances/2021/11/27/pipeline-conversations.html",
            "date": " ‚Ä¢ Nov 27, 2021"
        }
        
    
  
    
        ,"post44": {
            "title": "How to set and get environment variables using Python",
            "content": "If you want to get and set environment variables using Python, simply use the relevant methods from os. To set an environment variable, do this: . import os os.environ[&#39;SOME_ENV_VARIABLE&#39;] = 13.5 . And to access an environment variable, there are actually a number of different ways. All these three are essentially the same: . os.getenv(&#39;SOME_ENV_VARIABLE&#39;) os.environ.get(&#39;SOME_ENV_VARIABLE&#39;) os.environ(&#39;SOME_ENV_VARIABLE&#39;) . For the final one (os.environ(&#39;SOME_ENV_VARIABLE&#39;)), if the variable doesn‚Äôt exist, it‚Äôll return a KeyError, whereas the first two will just return None in that case. .",
            "url": "https://mlops.systems/python/2021/11/26/environment-variables.html",
            "relUrl": "/python/2021/11/26/environment-variables.html",
            "date": " ‚Ä¢ Nov 26, 2021"
        }
        
    
  
    
        ,"post45": {
            "title": "entr: a tool to run commands when files change",
            "content": "It‚Äôs a fairly common pattern that you have some code that you‚Äôre repeatedly running. Perhaps you‚Äôre fixing a failing test, and you just have to keep running it every time you make a fix. . Enter entr. This handy little tool reruns a particular command whenever changes are detected in a particular set of files. . Let‚Äôs take the example I mentioned above: you have a failing test that you‚Äôre debugging and you need to have it run every time you save a change to the file. Assuming your source code is stored in src and you‚Äôre using pytest, then you could use something like the following: . ls src/*.py | entr -c pytest test.py::test_some_feature . So now, any time you change any Python file inside the src folder, it‚Äôll rerun your test. The -c flag will clear the terminal every time the test runs. . [Many thanks to calmcode for continuing to make these really useful videos.] .",
            "url": "https://mlops.systems/debugging/testing/tools/calmcode/2021/11/25/entr-reruns-tests.html",
            "relUrl": "/debugging/testing/tools/calmcode/2021/11/25/entr-reruns-tests.html",
            "date": " ‚Ä¢ Nov 25, 2021"
        }
        
    
  
    
        ,"post46": {
            "title": "On failure",
            "content": "I‚Äôve been working as a machine learning engineer now for a few months now. If there‚Äôs one thing that I have found characterises my experience so far, it‚Äôs failure. Software fails; we even have a word for that: bugs. Learning new things might also be characterised as departing from a state of failing to understand. . There hasn‚Äôt been a week that‚Äôs gone by since I started where I didn‚Äôt encounter some kind of failure, usually my inability to understand why something was behaving in a particular way. My last post was about debugging, and finding ways to move forward in the face of failure is a key aspect of that process. . Failure isn‚Äôt fun. My initial reaction to hitting something I don‚Äôt understand is not one of glee and excitement at getting this opportunity to solve some kind of problem. But maybe it should be. It occurred to me this week that actually failure is sort of the name of the game. Solving hard problems is exactly what software engineers get paid to do. If it were just easy, it‚Äôd be a different kind of work. . Two posts by Julia Evans are pretty great on how a lot of being able to do this kind of work is about mindset. Ellen Ullman covers similar territory in ‚ÄòLife in Code‚Äô and ‚ÄòClose to the Machine‚Äô. . The point is this: we are paid to confront this failure. This is the work. Thinking that it‚Äôs a distraction from the work ‚Äî some kind of imaginary world where there are no blockers or failing tests ‚Äî is the real illusion. .",
            "url": "https://mlops.systems/debugging/emotions/2021/11/21/on-failure.html",
            "relUrl": "/debugging/emotions/2021/11/21/on-failure.html",
            "date": " ‚Ä¢ Nov 21, 2021"
        }
        
    
  
    
        ,"post47": {
            "title": "Some things I learned about debugging",
            "content": "I‚Äôve had to deal with a whole bunch of bugs in the past few days and weeks. I thought it‚Äôd be useful to put down some thoughts about things that I‚Äôve learned along the way. . Logging &amp; Printing . These are maybe the first things that everyone says you should do when you have a bug you need to fix: log things somewhere where you can see them. . There are some scenarios where simple print calls aren‚Äôt enough. If you‚Äôre running code through a series of tests, then the test harness will often consume all output to stdout so you won‚Äôt see any of your print statements. Luckily, test environments can usually be configured to print debug statements of loggers. . Once you can see what‚Äôs happening at a particular moment, you can see if what you expected to happen at that moment is actually happening. . Breakpoint your way to infinity! . The breakpoint() function comes built-in with Python. It‚Äôs a convenience wrapper around some pdb magic, and practically speaking it means you can set a point where you can interrupt the Python execution. Your terminal will halt at that point, and you can inspect the variables or objects available at that particular moment. . I wish I had known about this earlier on. It‚Äôs extremely useful for understanding exactly how a function or piece of code is being executed. . Come with hypotheses . If you don‚Äôt have a sense of what you expect to happen, it‚Äôs going to be hard to determine if what you‚Äôre doing is having any effect or not. . I‚Äôve been lucky to do some pairing sessions with people as they work through bugs and problems, and I‚Äôve had this ‚Äòcome with a hypothesis‚Äô behaviour modelled really well for me. . It‚Äôs not a panacea; there‚Äôs still a lot of work to be done around this, but it‚Äôs sort of the foundation, particularly for non-trivial bugs. . Leave your assumptions at the door . Don‚Äôt assume what‚Äôs written is what‚Äôs actually working. This applies to the code you‚Äôre working on, the documentation, docstrings, everything. This is especially true when your codebase is rapidly changing growing, such as at a startup or a smaller company where not everything has been cemented into place. . The rapid pace of change means that things can get out of date, or people can make mistakes. This applies to packages or modules you‚Äôre importing as well. Of course, it‚Äôs probably more likely that you‚Äôre misunderstanding something vs the Python standard library has got something wrong, but for many other open-source projects, you should at least be open to the possibility that weird things might show up. . Follow the thread wherever it leads . This is something about updating your assumptions as you move through the process of testing your assumptions. If you rule out certain pathways, then you should be prepared to go down the remaining ones as far as you need. . Be systematic . I‚Äôve found a few times now, that there are certain moments where I notice I‚Äôm far far down the road. I‚Äôll have kept making a bunch of decisions at the various crossroads that I passed. At a certain moment, though, I need to take stock and just note down all the decisions and assumptions I‚Äôve made in order to reach this point. . I‚Äôll write a short note to myself (mainly), but also for teammates, where I explain all the different assumptions and pathways that I‚Äôm travelling down. I‚Äôll specifically write down all the conditions that need to be present for this bug to present (as far as I know them). . Quite often, just writing these assumptions down will help me solve the problem outright. Even when it doesn‚Äôt, it‚Äôs extremely useful in re-grounding myself and reminding me of why I‚Äôm going down rabbit hole x or y. . Know when to stop . In an ideal world you‚Äôd get to follow every windy road and to figure out everything that doesn‚Äôt make sense. But ‚Äî and this is again especially true for fast-moving startups ‚Äî you might not always have time to do that. . This is somehow connected to the Pareto Principle (also known as the 80/20 rule). At a certain point you should make sure to check in with how much time you‚Äôd planned on spending on a particular bug. If you‚Äôre finding that it‚Äôs taking far longer than expected, and you have other things you‚Äôre committed to completing, then you should maybe take an opportunity to connect to your team. Alternatively, you can rescope and find a way to disable or flag a particular bug for the next sprint, or see if someone can help you with it. . Remember: this is the work . Sometimes when I‚Äôm fixing bugs I have the feeling that I‚Äôm wasting my time somehow, or that I should be doing something more productive. It‚Äôs often the case, though, that this is the work. I‚Äôm low on experience, but proxy experience that I‚Äôve gained through reading books tells me that finding, fixing and triaging bugs is a lot of what we do as software engineers. . Know when to ask for help . Sometimes there are bugs which turn out to be bigger than you‚Äôre able to handle. It‚Äôs certainly worth pushing back against that feeling the first few times you feel it. Early on it‚Äôs often going to feel like the bug is unsolvable. . But some times there are pieces of context you don‚Äôt have, which a quick overview of what you‚Äôve done and tried might alert someone more season to the fact that you‚Äôre going down the wrong alley. Or it might remind them of something they knew implicitly but had forgotten. The important things is to judge when is the right time to seek outside advice. .",
            "url": "https://mlops.systems/debugging/2021/10/25/debugging.html",
            "relUrl": "/debugging/2021/10/25/debugging.html",
            "date": " ‚Ä¢ Oct 25, 2021"
        }
        
    
  
    
        ,"post48": {
            "title": "Writing Code",
            "content": "I read Daniel Roy Greenfeld‚Äôs post on how he found that coding a lot was key to improving his skills. It makes sense. Everything I‚Äôve read so far and my previous experience at the metaskill of learning new things tells me that it is a good investment of time. . Just like you get good at writing by doing a lot of writing, on some level that is true for coding. (Of course, there are additional pieces to the puzzle: you have to develop some taste alongside the pure production side, you have to do some quality-control and refactor your code, and so on and so on.) . For me, this looks like the following: . coding at work during the week | smaller focused exercises from PythonMorsels, Exercism, LeetCode and AlgoExpert | code written while working my way through the fastai course; this will probably manifest as blog posts here as well, outlining some small project I completed along the way. | a bigger project, perhaps a package, that I‚Äôll start building at some point. I have some ideas for things I want to implement. I‚Äôll pick one soon. It‚Äôll probably be related in some way to the fastai coding. I‚Äôm thinking right now of making a tool that allows you to download PDFs and use the pages of those PDFs as image files in computer vision problems; a data ingestion tool, in other words. | smaller scripts to solve daily problems in my digital life. I‚Äôll store those on my GitHub somewhere and write up the design decisions around the more interesting ones here. | . One thing I took note of was how Daniel mentioned that it made sense to specialise and focus on one language at a time, particularly in the early days. Rather than indulging my curiosity and doing 1001 things using Go or lisp or whatever, I will try to stick to Python at least until I feel more confident with it. .",
            "url": "https://mlops.systems/python/skillbuilding/2021/09/18/writing-code.html",
            "relUrl": "/python/skillbuilding/2021/09/18/writing-code.html",
            "date": " ‚Ä¢ Sep 18, 2021"
        }
        
    
  
    
        ,"post49": {
            "title": "Reading Python Code",
            "content": "It‚Äôs a truism of sorts that in order to improve your skills, you have to practice them. For coding, the stereotypical image is of someone typing, actually creating new things. But as often as not, you‚Äôre going to be reading code instead. This code might be something you write yesterday or last year, or it might be something that someone else wrote. . One way or another, reading code is a great way to get increasing familiarity with stylistic, syntactic patterns and to get exposed to some best practices, especially if you get to pick the code you‚Äôre reading. . I‚Äôll be doing the same as I ramp up my Python proficiency. I wanted to gather some lists of codebases and assorted resources in one place for myself, and I hope maybe it‚Äôll be useful for someone else as well. . Good Quality Python Code . jinja ‚Äî a templating engine written in Python (and see the recommendations for supplemental reading and watching for jinja here) | howdoi ‚Äî a search tool for coding answers via the command line | flask ‚Äî a micro-web framework for Python | FastAPI ‚Äî another web framework that‚Äôs a bit larger than flask | diamond ‚Äî a Python daemon that collects and publishes system metrics | werkzeug ‚Äî a web server gateway library | requests ‚Äî an HTTP library, now part of the Python standard library | tablib ‚Äî library for Pythonic way to work with tabular datasets | click ‚Äî a Python package for creating command line interfaces | pathlib ‚Äî part of the Python standard library; a module to handle filesystem paths (also the corresponding PEP proposal #428) | dataclasses ‚Äî a module in the Python standard library; reduces boilerplate of writing classes (also the corresponding PEP proposal #557) | joblib ‚Äî a library to support lightweight pipelining in Python | . Other Resources . 500 Lines or Less ‚Äî a book in which specific small open-source projects are profiled to understand how they approached their particular challenge. | The Architecture of Open Source Applications: Elegance, Evolution and a Few Fearless Hacks ‚Äî examination of the structure of the software of some open-source software applications. | The Architecture of Open Source Applications: Volume II: Structure, Scale and a Few More Fearless Hacks ‚Äî the second volume in the series. | .",
            "url": "https://mlops.systems/python/skillbuilding/2021/09/18/reading-python.html",
            "relUrl": "/python/skillbuilding/2021/09/18/reading-python.html",
            "date": " ‚Ä¢ Sep 18, 2021"
        }
        
    
  
    
        ,"post50": {
            "title": "Tensors all the way down",
            "content": "#!pip install -Uqq fastbook #!pip install fastai import fastbook fastbook.setup_book() from fastai.vision.all import * from fastbook import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . . In chapter 4 of the book, we start to really get into what&#39;s going on under the hood with deep learning. Turns out, tensors are a pretty important piece. We are still in the realm of computer vision, and we are going to work on distinguishing between handwritten digits. . First we use the untar_data function to grab a sample of data from the famous MNIST data set. This function returns the path where that data was stored locally. . path = untar_data(URLs.MNIST_SAMPLE) . Path.BASE_PATH = path . path . Path(&#39;.&#39;) . Now we want to briefly inspect the contents of one of our training data folders. This is for the number 7. You can see that it&#39;s just a series of .png image files. . threes_dir = (path/&#39;train/3&#39;).ls().sorted() sevens_dir = (path/&#39;train/7&#39;).ls().sorted() sevens_dir . (#6265) [Path(&#39;train/7/10002.png&#39;),Path(&#39;train/7/1001.png&#39;),Path(&#39;train/7/10014.png&#39;),Path(&#39;train/7/10019.png&#39;),Path(&#39;train/7/10039.png&#39;),Path(&#39;train/7/10046.png&#39;),Path(&#39;train/7/10050.png&#39;),Path(&#39;train/7/10063.png&#39;),Path(&#39;train/7/10077.png&#39;),Path(&#39;train/7/10086.png&#39;)...] . In order to look at a single image, we can just open it using Image.open which comes from the Python Image Library (PIL). . im3_path = threes_dir[1] im3 = Image.open(im3_path) im3 . Jupyter knows how to display various files, so we can see that image above. But what exactly is an image made up of? If we turn that image into an array, or to a tensor (the next two cells), slicing them so you aren&#39;t just seeing zeros on the edges, then you can see that these images are made up of a matrix of values from 0 to 255. . im3_arr = array(im3)[4:10, 4:10] im3_arr . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=uint8) . im3_tns = tensor(im3)[4:10, 4:10] im3_tns . tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) . We can use the show_image function to turn those 0-255 values back into an image, like so: . show_image(im3_arr) . &lt;AxesSubplot:&gt; . A really nice way of visualising exactly what is going on is to turn this image into a pandas dataframe and then for every individual pixel value, use that value as the background gradient for that cell. Here&#39;s an example of part of an image of a handwritten number 3. . im3_t = tensor(im3) df = pd.DataFrame(im3_t[4:15,4:22]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | . 3 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | . 4 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | . 5 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | . So now we have a toolkit of ways to view the pixel values that make up an image. We also have a mental model for how we can think about images and how computers represent those images stored on our machine. . But how might we then best go about knowing whether a particular image is a 3, let&#39;s say, or a 7? . One naive approach might be just to get the average value for each individual pixel for all of the threes in our training data, and then just compare the difference between our sample image and this average representation. . Let&#39;s try that now. . Getting the average values for our images . We&#39;ll set up two lists with images of the digits converted to tensors. You can see that we have 6131 images in our &#39;threes&#39; list. . threes_tensors = [tensor(Image.open(i)) for i in threes_dir] sevens_tensors = [tensor(Image.open(i)) for i in sevens_dir] len(threes_tensors) . 6131 . We can view an individual image, as before, with the show_image function: . show_image(threes_tensors[3]) . &lt;AxesSubplot:&gt; . Now in order to get the average values for each pixels, we can use the stack method to handle the first part of this. . Think of it as basically adding an extra dimension to your data structure, such that you have a &#39;stack&#39; (it&#39;s a useful mental image) of those images. . threes_stack = torch.stack(threes_tensors) . If we look at the shape of our Pytorch stack now, we can see we have our 28x28 image, but we have a stack of 6131 of them. . threes_stack.shape . torch.Size([6131, 28, 28]) . Each individual image is still a tensor: . a_three = threes_stack[3][4:16, 4:16] a_three . tensor([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 104, 253, 253, 253, 255, 253], [ 0, 0, 0, 0, 0, 178, 248, 252, 252, 252, 253, 252], [ 0, 0, 0, 0, 0, 186, 252, 252, 252, 252, 253, 252], [ 0, 0, 0, 0, 0, 186, 252, 243, 172, 172, 39, 39], [ 0, 0, 0, 0, 0, 39, 53, 47, 0, 0, 0, 29], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 54, 208], [ 0, 0, 0, 0, 0, 0, 0, 0, 3, 41, 253, 252], [ 0, 0, 0, 0, 0, 0, 5, 41, 165, 252, 253, 252], [ 0, 0, 0, 0, 0, 109, 163, 252, 252, 252, 253, 252], [ 0, 0, 0, 0, 0, 186, 252, 252, 252, 252, 253, 252], [ 0, 0, 0, 0, 0, 187, 253, 253, 253, 253, 134, 77]], dtype=torch.uint8) . Generally speaking, for some operations (like getting the mean average) we&#39;re going to want to convert the values to floats, and it also makes sense to normalise the values at the same time. Instead of having a range of 0-255, we want a range of 0-1. . threes_stack[3][4:16, 4:16].float()/255 . tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4078, 0.9922, 0.9922, 0.9922, 1.0000, 0.9922], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6980, 0.9725, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7294, 0.9882, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7294, 0.9882, 0.9529, 0.6745, 0.6745, 0.1529, 0.1529], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.2078, 0.1843, 0.0000, 0.0000, 0.0000, 0.1137], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2118, 0.8157], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.1608, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0196, 0.1608, 0.6471, 0.9882, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4275, 0.6392, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7294, 0.9882, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7333, 0.9922, 0.9922, 0.9922, 0.9922, 0.5255, 0.3020]]) . Now that we&#39;ve done it for a single image, we can perform the same operations on our whole Pytorch stack. . threes_stack = torch.stack(threes_tensors).float()/255 sevens_stack = torch.stack(sevens_tensors).float()/255 threes_stack.shape # it&#39;s good to keep in touch with the shape of our stack . torch.Size([6131, 28, 28]) . Now we&#39;re getting closer to our desired result. We can squash the stack down into just two dimensions with a simple call to .mean(0), where 0 is the index value of the dimension through which we want to calculate the mean. You&#39;ll see now that the shape property of our threes_means variable is simply a 28x28 image. . threes_means = threes_stack.mean(0) threes_means.shape . torch.Size([28, 28]) . When we show that image, you&#39;ll see that it&#39;s a sort of blurry &#39;ideal&#39; version of a three . show_image(threes_means) . &lt;AxesSubplot:&gt; . We can do the same for the sevens: . sevens_means = sevens_stack.mean(0) show_image(sevens_means) . &lt;AxesSubplot:&gt; . Validation: Comparing our average three with a specific three . Now we have our average values, we want to compare these with a single specific digit image. We&#39;ll get the difference between those values and whichever difference is the smallest will most likely be the best answer. . Our averaged three is still threes_means and we can get a single three from our validation set like this: . threes_dir_validation = (path/&#39;valid/3&#39;).ls().sorted() sevens_dir_validation = (path/&#39;valid/7&#39;).ls().sorted() im3_validation_path = threes_dir_validation[5] im3_validation = tensor(Image.open(im3_validation_path)).float()/255 im7_validation_path = sevens_dir_validation[3] im7_validation = tensor(Image.open(im7_validation_path)).float()/255 show_image(im3_validation) . &lt;AxesSubplot:&gt; . show_image(im7_validation) . &lt;AxesSubplot:&gt; . . Note: Calculating the difference between two objects . We can use two different measurements of the difference between our mean value and the individual image: . mean absolute difference (calculated by taking the mean of the absolute difference between the two tensor values). Also known as the L1 Norm. | root mean squared error (calculated by first squaring the difference between the two tensor values, taking the mean and then square rooting those values). Also known as the L2 Norm. | . The second option, the RMSE, gives a stronger signal, you might say, for the differences because you are taking the averages from the squared values. Squaring the difference also takes care of any negative values you might have. . mean_absolute_difference_3 = (im3_validation - threes_means).abs().mean() root_mean_squared_error_3 = ((im3_validation - threes_means)**2).mean().sqrt() mean_absolute_difference_3, root_mean_squared_error_3 . (tensor(0.1188), tensor(0.2160)) . mean_absolute_difference_7 = (im7_validation - threes_means).abs().mean() root_mean_squared_error_7 = ((im7_validation - threes_means)**2).mean().sqrt() mean_absolute_difference_7, root_mean_squared_error_7 . (tensor(0.1702), tensor(0.3053)) . We can now see that our individual three image is indeed closer to the threes_means composite image than to the sevens_means composite image. A smaller value at this point is what we&#39;re looking for, and the threes have it. . It turns out that there is another way to calculate the difference that&#39;s built in to Pytorch as loss functions: . F.l1_loss(im3_validation, threes_means), F.mse_loss(im3_validation, threes_means).sqrt() . (tensor(0.1188), tensor(0.2160)) . It&#39;s a bit more concise, though it does obscure what&#39;s going on under the hood in terms of calculations. . Results of the naive approach . So this tells us that our single three is closer to an ideal 3 than an ideal 7, which is great since it reflects the ground truth of our problem. But can we get a metric to know how well we perform on average against a large number of threes and sevens from our validation set? . Yes, since we have that dataset ready for use! . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]).float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]).float()/255 valid_3_tens.shape, valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . Now we can write a helper function that will allow us to calculate the distance between two images. We use the RMSE or L1 Norm for this difference calculation: . def mnist_distance(a, b): return (a - b).abs().mean((-1, -2)) . We can use this function on our previous example: . mnist_distance(im3_validation, threes_means) . tensor(0.1188) . We can continue onwards by comparing the rank 3 tensor with the rank 2 tensor. This brings a concept called &#39;broadcasting&#39; into play. . We are comparing a tensor with 2 dimensions with a tensor with 3 dimensions, so Pytorch behaves as if both tensors have three dimensions, and (without taking extra memory) pretends as if there are multiple copies of the image in 2 dimensions. This effectively makes it as if we&#39;re comparing two 3-dimensional tensors. . From this next calculation, we see returned back a collection of the distances between all of the validation images. . mnist_distance(valid_3_tens, threes_means) . tensor([0.1328, 0.1523, 0.1245, ..., 0.1383, 0.1280, 0.1138]) . In order to check whether an image is a 3, we basically need to know whether the difference for the number 3 is larger than the difference for the number 7. . We can write a helper function for that: . def is_3(img): return mnist_distance(img, threes_means) &lt; mnist_distance(img, sevens_means) . We can now check our ground truth examples: . is_3(im3_validation), is_3(im7_validation) . (tensor(True), tensor(False)) . That&#39;s what we expected to happen. Our 3 image is a 3, and our 7 image is not a 3. . If we want to check the distance in general for our validation set, we have to convert them into floats and then get the mean, but it&#39;s really easy. Again, this uses broadcasting: . validation_accuracy_3 = is_3(valid_3_tens).float().mean() validation_accuracy_7 = 1 - is_3(valid_7_tens).float().mean() validation_accuracy_3, validation_accuracy_7 . (tensor(0.9168), tensor(0.9854)) . Overall, then, we can calculate how good our toy or baseline model is for the entire problem: . (validation_accuracy_3 + validation_accuracy_7) / 2 . tensor(0.9511) . Pretty good! . This was of course just a naive way to solve the problem. There are more advanced techniques which we&#39;ll tackle next. .",
            "url": "https://mlops.systems/pytorch/fastai/computervision/2021/09/16/ch4-tensors.html",
            "relUrl": "/pytorch/fastai/computervision/2021/09/16/ch4-tensors.html",
            "date": " ‚Ä¢ Sep 16, 2021"
        }
        
    
  
    
        ,"post51": {
            "title": "A Baseline Python Development Setup",
            "content": "The world of Python versioning (and the downstream package versioning) is wild. This StackOverflow thread gives you a sense of some of the core issues at play. (As an indication of the importance of the issue, even BDFL Guido van Rossum himself has the current second most upvoted answer.) . For a really vanilla and close-to-core-python setup, a combination of venv and pip seem to be the way to go. venv is part of the standard library and as such is pretty close to a default option. . For something a bit more involved, that handles dependencies and package installation in a slightly more deft manner, the combination of pyenv, pyenv-virtualwrapper and poetry works really well. I‚Äôll detail some of the setup gotchas and usage patterns below. . pyenv for versioning Python itself . pyenv lets you install multiple versions of Python on the same machine. The interface to switch between local versions and whatever you‚Äôve decided will be your global option is pretty intuitive. . Visit the pyenv github page for more on installation. (If you‚Äôre on a Mac you can simply do a brew install pyenv.) . To see which versions of Python you have installed locally: . pyenv versions . To see versions of Python which are available for installation: . pyenv install ‚Äîlist . Note that, as I understand it, these versions are not dynamically updated. You get an updated list of new Python versions by updating pyenv, in other words. . To install a specific version of Python, and to make it available for use: . pyenv install 3.9.1 . To set that version of Python as the global version (i.e. running python will use this version by default): . pyenv global 3.9.1 . If you are in a project directory and wish to only use a particular version of Python in that directory (and its subdirectories): . pyenv local 3.8.2 . This creates a .python-version file in that directory with the desired local version. . pyenv-virtualenv for managing virtual environments . pyenv-virtualenv is a plugin that connects the work of selecting which version of Python to use (through pyenv, which we‚Äôve previously installed) to the work of creating and running virtual environments to keep code contained in quasi-sandbox environments. When you install packages in virtual environments they don‚Äôt conflict with other locations where you might have conflicting versions of those same packages installed. . Read installation instructions and the docs here. (If you installed pyenv with homebrew, be sure to do the same with pyenv-virtualenv). . To create a virtual environment for the Python version used with pyenv, run pyenv virtualenv, specifying the Python version you want and the name of the virtual environment directory: . pyenv virtualenv 3.8.2 my-virtual-env-3.8.2 . This will create a virtual environment based on Python 3.8.2 under $(pyenv root)/versions in a folder called my-virtual-env-3.8.2. . To list what virtual environments have been created and are available to use: . pyenv virtualenvs . As a common workflow pattern, you‚Äôd create your directory and cd into it, and then you can set the virtual environment you just created as the one to use for that directory: . mkdir test-project &amp;&amp; cd test-project pyenv local my-virtual-env-3.8.2 . This should change the prompt in your terminal window and you‚Äôll thus know that you‚Äôre now working out of that virtual environment. Any time you return to that folder you‚Äôll automatically switch to that environment. . The manual way of turning on and off virtual environments is: . pyenv activate env-name pyenv deactivate env-name . To remove a virtual environment from your system: . pyenv uninstall my-virtual-env . (This is the functional equivalent of removing the directories in $(pyenv root)/versions and $(pyenv root)/versions/{version}/envs.) . poetry for handling package installation and dependencies . python-poetry is the latest standard tool for handling package installations and dependency management. . You can use poetry without the previous two tools, but really they work best all together. Follow the installation instructions documented on their page to get it going. . Then update poetry: . poetry self update . poetry is one of those tools that‚Äôs able to update itself. . For basic usage for a new project, you can follow the following workflow. There are two ways to start a new project using poetry: using new or init. For example: . poetry new some-project-name . This will kickstart your new project by creating a bunch of files and a directory structure suitable for most projects, like so: . some-project-name ‚îú‚îÄ‚îÄ pyproject.toml ‚îú‚îÄ‚îÄ README.rst ‚îú‚îÄ‚îÄ some-project-name ‚îÇ ‚îî‚îÄ‚îÄ __init__.py ‚îî‚îÄ‚îÄ tests ‚îú‚îÄ‚îÄ __init__.py ‚îî‚îÄ‚îÄ test_some-project-name.py . You might want to use a src folder (above the some-project-name in our example) which is fairly commonly used, in which case amend the command as follows: . poetry new --src some-project-name . poetry init doesn‚Äôt do all the extra work of creating a directory and file structure. It merely creates a pyproject.toml file interactively, using some smart defaults. For a minimal use of poetry, this is definitely the way to go. . The add command adds required packages to your pyproject.toml and installs them (along with all their dependencies). It does a lot under the hood to make sure that dependencies are correctly resolving before installing. For example: . poetry add zenml . To add packages only to be used in the development environment: . poetry add --dev zenml . To list all installed packages in your current environment / project: . poetry show . To uninstall a package and remove it (and its dependencies) from the project: . poetry remove zenml . To install all relevant packages and dependencies of a project that you‚Äôve newly cloned into: . poetry install . Note that it is possibly worth creating some custom scripts to handle some of the overhead of using these tools, depending on your common development workflows. .",
            "url": "https://mlops.systems/python/tools/2021/09/14/python-versioning-package-managers.html",
            "relUrl": "/python/tools/2021/09/14/python-versioning-package-managers.html",
            "date": " ‚Ä¢ Sep 14, 2021"
        }
        
    
  
    
        ,"post52": {
            "title": "Six problems TFX was trying to solve in 2017",
            "content": "TensorFlow Extended or TFX is a platform for machine learning that claims to handle pretty much everything you‚Äôd need for end-to-end model training, deployment and retraining. It was developed for Google, the successor to Sibyl, and released in public in 2017. I read the original paper that accompanied its release to understand the problems it was trying to solve, as well as to get a handle on the specific context in which it was developed. (It‚Äôs worth being wary about tools developed at places like Google; after all, hardly any of us are operating at Google-scale). . . ‚ÄòA TensorFlow-based general-purpose machine learning platform‚Äô . The engineers wanted a general-purpose tool, one that could serve many different use cases. I haven‚Äôt yet read the subsequent paper on the history of TFX, but from what I do know already there were other in-house solutions that existed before. Machine learning model training at scale, deployment and the general full-cycle behaviours are pretty involved and challenging, and it often seems like the needs of particular scenarios demand different approaches. This is as much true now as it was back int 2017, I imagine, though perhaps now we have some ideas of the broad pieces that make up the whole picture that needs to be addressed. . The problem here is that you might have certain parts that either are very compute intensive, or require special distributed computing setups, or where the models need to be trained off streaming data rather than from static stores. So with TFX they tried to make the tool sufficiently abstract that they could handle most cases someone would want to use it for. (They say at the end that there were some parts that they hadn‚Äôt anticipated, specifically sequence-to-sequence language models used in machine translation). . An end-to-end platform . The ambition for the platform and software tooling was not just to handle the smaller pieces of the training and deployment cycle, but rather to tackle the big overarching abstractions in a single approach. This of course contained some baked-in assumptions about how users would use TFX as well as what I‚Äôd say were quasi-philosophical positions on how best to approach these various parts. The paper characterises these as ‚Äòbest practices‚Äô, but certainly there hasn‚Äôt been uniform acceptance of these. . I imagine the end-to-end part was as much an attempt to encourage engineers to think of the problem in this exact way. If you are handling all the pieces of the training cycle, it‚Äôs easier to be fast and iterate and do all the things we expect of a more agile process. . Continuous training and serving . TFX was built to handle the kinds of models where the use cases demanded the ability to continuously retrain models using large quantities of streaming data. This is almost certainly not the norm, but for a company like Google I can understand that this would have been a key consideration if they wanted adoption of the tool across different teams. . In this way, certain scenarios (for example the Google Play Store case study outlined in the paper) saw a continuous retraining of models as more users used the service as well as new apps continued to be uploaded to the Play Store. If you have this kind of engineering need, and if you need to keep latency to certain boundaries (in the tens of milliseconds), it makes complete sense to have this whole structure that allows this to take place. Reading the specific example, it‚Äôs a pretty amazing feat, handling all that complexity underneath the surface. There must be many hundreds of other such services which similar levels of complexity concealed beneath the surface. . Reduce technical debt accrued via duplicated or ad hoc solutions . Prior to TFX and Sybil, it seems that there were many different approaches within Google, all addressing the same problem but in slightly different ways. . Having a series of best-practices built in to the service means that everyone can communicate about problems and about their issues using a shared language. It means that solutions discovered by one team can help other future teams. There‚Äôs a lot to be said for finding a solution that is sufficiently abstracted to work for many people. . Indeed, it seems this is the work of the MLOps community right now: find ways to abstract away problems that we all face, and to find the best abstractions that fit within the mental models we all have in our heads. The fact that there hasn‚Äôt been a grand convergence on a single solution indicates to me (at this current moment) that we haven‚Äôt found the right abstractions or flexibility within those abstractions. All the end-to-end tools handle much of the same stages of the model training and deployment process, but they each have opinions about the best practices to be employed along the way. (At least, that‚Äôs my current take on things). . Reliable serving models at scale . If you‚Äôre Google, you need to make sure that you aren‚Äôt serving garbage models to your users, or that inconsistencies in the input data aren‚Äôt polluting your retraining processes. At scale, even small mistakes compound really easily. . In the paper, two specific improvements are mentioned, tackling the challenges of low latency and high efficiency. The high efficiency example wasn‚Äôt entirely comprehensible for me, but what was clear was that they had very high expectations for how fast they wanted to make all parts of the pipelines and process. As above, the challenges of making it easy and fast to serve models ‚Äî all of which had to happen in a reliable manner ‚Äî was something that could be reused elsewhere in the company. TensorFlow Serving is what we get from their efforts in this regard. . Fast retraining with ‚Äòwarm-starting‚Äô . For the specific challenge of retraining models with streaming data, engineers were finding that they couldn‚Äôt retrain the entire model from scratch, particularly with the scale of the training data that they had. Instead, they leveraged transfer learning (reframed here as ‚Äòwarm-starting‚Äô) to take all the hard work that had already been done, and adapting this pre-existing model with the new data. This makes a lot of sense, though the reframing with the new term is a bit less comprehensible to me. . Missing pieces . There are various pieces of what I think of as the machine learning workflow (as of 2021) which seem to be missing when I read this paper. Explainability or governance of models seems somewhat of an afterthought, if it is raised at all. I think the authors might argue that many of the checks and balances are made on the data ingestion phase, and that if all that checks out then this tackles a large piece of the problem surface area. . Similarly, there is relatively little said about model versioning and data versioning. Maybe coming at this from the present moment, where it seems obvious (with tools like DVC) that data versioning is a thing you‚Äôd want to care about. . As a general response, it seems clear that if you use TensorFlow to train your models, TFX might well be a pretty neat solution that handles many of your needs, particularly if you‚Äôre operating at serious scale. If you‚Äôre a researcher (perhaps using PyTorch) with less of those specific contextual needs, it seems less than certain that TFX would suit your purposes. . A couple of other interesting observations. The data observability and validation stage seemed to place a lot of emphasis on the automation of how pre-defined schemas might get updated. I‚Äôd be interested to see how that worked in practice. I understood the challenge that if there are too many error messages about dodgy data inputs, engineers are likely to grow inured to those alerts and maybe just ignore them. But at scale, I wonder about the risks of allowing automatic updates to those schema boundaries. . Again on the validation point, I found it interesting how the authors of the paper said that users of TFX internal to Google found the option to enable this was actually a hard sell unless or until the team had experienced some kind of failure connected to poor data validation. The TFX team ended up turning on the validation parts of the pipeline by default instead of assuming that users would choose to do so manually. . I wasn‚Äôt active in the field in 2017, so it‚Äôs hard for me to be able to reconstruct exactly how prescient or not this paper was in some of its diagnoses of the problem. It doesn‚Äôt seem that TFX was the total solution that perhaps it was pitched as being, but nonetheless it seems an important engineering achievement for Google. .",
            "url": "https://mlops.systems/tfx/tensorflow/google/mlops/papers-i-read/2021/09/11/tfx-paper.html",
            "relUrl": "/tfx/tensorflow/google/mlops/papers-i-read/2021/09/11/tfx-paper.html",
            "date": " ‚Ä¢ Sep 11, 2021"
        }
        
    
  
    
        ,"post53": {
            "title": "Managing Python Environments with pyenv and pipenv",
            "content": "It‚Äôs hardly news that that managing multiple versions of Python in a development environment is hard. Adding in dependency management on top of that makes everything harder. .",
            "url": "https://mlops.systems/python/2021/09/10/python-environments.html",
            "relUrl": "/python/2021/09/10/python-environments.html",
            "date": " ‚Ä¢ Sep 10, 2021"
        }
        
    
  
    
        ,"post54": {
            "title": "Retrieval Practice with fastai chapters 1 and 2",
            "content": "Retrieval practice is when you actively try to remember something as a way of making sure that you learn it well. (Read more about it here). Today I did that with the dogs vs cats example that the first two chapters cover. . We start with installing the fastai library and importing everything from the vision library. This was hard to remember since the pattern of .all and importing * is not something I&#39;ve seen much in Python imports. . from fastai.vision.all import * . Then we create the simple function that will be used to classify the images. The pets dataset relies on the first letter of the filename for knowing whether a picture is of a cat or a dog. So the function is pretty simple: it checks whether the first letter is a capital letter or not. . The simple assert testing was a little trick that I saw mentioned somewhere this past week. It&#39;s not a full-fledged test suite, but it&#39;s at least the start of something that can later be refactored out into whatever takes its place, be it using pytest or something else. . def is_cat(string): return string[0].isupper() assert is_cat(&quot;abs&quot;) == False assert is_cat(&quot;Abs&quot;) == True . Now we have to import the data for the files and apply whatever custom transforms we want applied to them. . I had certainly forgotten that untar_data was a method when I started out with this. I also am not familiar enough with the pathlib library as I need to be. . It&#39;s interesting that we actually don&#39;t even need to do any of the batch transformations on the images in order to get excellent results. I imagine that&#39;s because the task is so close to that of the original resnet architecture. . path = untar_data(URLs.PETS)/&#39;images&#39; dls = ImageDataLoaders.from_name_func(path, get_image_files(path), label_func=is_cat, item_tfms=Resize(224)) . Then it&#39;s all about passing the dataloaders object into the cnn_learner function, along with our desired architecture. We also set the error_rate (i.e. 1 minus the accuracy at making predictions) as the metric we&#39;ll see displayed in the output. . learner = cnn_learner(dls, resnet34, metrics=error_rate) # fine-tune the model learner.fine_tune(5) . epoch train_loss valid_loss error_rate time . 0 | 0.140326 | 0.019799 | 0.008119 | 00:19 | . epoch train_loss valid_loss error_rate time . 0 | 0.046906 | 0.021923 | 0.006089 | 00:24 | . 1 | 0.041144 | 0.009382 | 0.004060 | 00:25 | . 2 | 0.028892 | 0.004109 | 0.002030 | 00:25 | . 3 | 0.008950 | 0.002290 | 0.001353 | 00:25 | . 4 | 0.004486 | 0.002822 | 0.001353 | 00:25 | . And here you can see the results. In this training run, with 5 epochs, we were able to achieve a 99.9% accuracy. Not bad! . learner.show_results() .",
            "url": "https://mlops.systems/fastai/jupyter/computervision/2021/09/10/chapter1and2recall.html",
            "relUrl": "/fastai/jupyter/computervision/2021/09/10/chapter1and2recall.html",
            "date": " ‚Ä¢ Sep 10, 2021"
        }
        
    
  
    
        ,"post55": {
            "title": "How to set a Jupyter notebook to auto-reload external libraries",
            "content": "The code to insert somewhere into your Jupyter notebook is pretty simple: . %load_ext autoreload %autoreload 2 . When you‚Äôre working on an external library or piece of Python code outside the contents of your notebook, this snippet will make sure that the updated functions and constants will always be available in their most-recently edited state. .",
            "url": "https://mlops.systems/jupyter/2021/09/09/auto-reload-external-libraries.html",
            "relUrl": "/jupyter/2021/09/09/auto-reload-external-libraries.html",
            "date": " ‚Ä¢ Sep 9, 2021"
        }
        
    
  
    
        ,"post56": {
            "title": "A Baseline Understanding of MLOps",
            "content": "Next week I‚Äôm due to begin a job as a Machine Learning Engineer at a company that works in the MLOps field. It‚Äôs a new field to me. I‚Äôve read a good deal on it in recent weeks, and listened to a few dozen episodes of the MLOps.community podcast, but I still very much consider myself a beginner in the space. To that end, I thought it worth clarifying my understanding of what MLOps is all about, the problem it is trying to solve, and where I see the opportunity there. . A top-down explanation is probably the best way to think of what we‚Äôre doing when we talk about ‚Äòdoing MLOps‚Äô: we‚Äôre doing all the things which make it possible to train, deploy and use machine learning models in the real world or ‚Äòin production‚Äô. It isn‚Äôt just a series of tools, but also a series of best practices and a community that is constantly learning and iterating to improve. . The kinds of things that you can do with machine learning models are incredibly diverse, so it stands to reason that the people who operationalise all these models have quite varied opinions and approaches to how best to do this. Even the deployment scenarios are pretty different and involve different technology stacks. There is an idea of a ‚Äòfull stack machine learning engineer‚Äô, which apparently means someone who just knows everything across the board; I hope to be able to delve into some of these areas and the key technologies represented in each space in due course on this blog. .",
            "url": "https://mlops.systems/mlops/2021/09/08/baseline-mlops-understanding.html",
            "relUrl": "/mlops/2021/09/08/baseline-mlops-understanding.html",
            "date": " ‚Ä¢ Sep 8, 2021"
        }
        
    
  
    
        ,"post57": {
            "title": "Training a classifier to detect redacted documents with fastai",
            "content": "I am working my way through the fastai course as part of an online meetup group I host.1 . This week we finished the first and second chapters of the book, during which you train a model that can recognise if an image contains a cat or a dog. Later on, you train another model that distinguishes between different types of bears (‚Äògrizzly‚Äô, ‚Äòblack‚Äô and ‚Äòteddy‚Äô). . Jeremy Howard, who is teaching the course, then prompts you to take what you learned and apply it to something that has meaning for you. (This is something that most of those who‚Äôve found any success with the course emphasise repeatedly.) . I decided to work on something adjacent to my previous life / work, where I knew there was some real-world value to be gained from such a model. I chose to train an image classifier model which would classify whether a particular image was redacted or not. . The Problem Domain: Image Redaction . Under the Freedom of Information Act (FOIA), individuals can request records and information from the US government.2 This is one collection of some of the responses to this requests, sorted into various categories. You can read, for example, responses relating to UFOs and alien visits here. . Quite often, however, these images are censored or redacted. . . Knowing that this practice exists, I thought it might be interesting to train a model that could recognise whether a particular page contained some kind of redaction. This wasn‚Äôt completely in line with what we covered during the first two chapters; I wasn‚Äôt sure if the pre-trained model we used would work for this data set and use case. . It could be useful to have such a tool, because FOIA responses can sometimes contain lots of data. In order to prepare a request for more data, you might want to be able to show that even though you were sent thousands of pages, most of those pages contained redactions and so were effectively useless. . In the ideal vision of this tool and how it would work, you could run a programme out of a particular directory and it would tell you how many pages (and what proportion) of your PDF files were redacted. . Getting the Data . The first thing I did to gather my data was to download the PDF documents available on this site. I knew that they contained examples of redactions in FOIA documents. I used Automator to split the PDF files up into individual images.3 My Automator script did some downsampling of the images as part of the process, so the images were resized to something that wasn‚Äôt prohibitively large to use for training. . Note that this stage and the next was done on my local machine. A CPU was enough for my purposes at this point, though probably I‚Äôll want to eventually port the entire process over to a single cloud machine to handle things end-to-end. . At the end of the splitting-and-resizing process, I had a little over 67,000 images (of individual pages) to train with. . Labelling the images with Prodigy . I had used Explosion.ai‚Äôs Prodigy data labelling tool in the past and so already had a license. The interface is clean and everything works pretty much as you‚Äôd hope. I had some teething issues getting it all working, but Prodigy co-creator Ines helped me work through those queries and I was up and running pretty quickly. . . It took about three hours to annotate some 4600+ images. Then I could export a .jsonl file that contained the individual annotations for whether a particular image contained a redaction or not: . . From that point it was pretty trivial to parse the file (using the json-lines package), and to resize the images down further in order to separate redacted from unredacted: . import json_lines from PIL import Image from pathlib import Path def save_resized_image_file(location_path): basewidth = 800 img = Image.open(record[&#39;image&#39;]) wpercent = (basewidth / float(img.size[0])) hsize = int((float(img.size[1]) * float(wpercent))) img = img.resize((basewidth, hsize), Image.ANTIALIAS) img.save(location_path) path = &#39;/my_projects_directory/redaction-model&#39; redacted_path = path + &quot;/redaction_training_data/&quot; + &quot;redacted&quot; unredacted_path = path + &quot;/redaction_training_data/&quot; + &quot;unredacted&quot; with open(path + &quot;/&quot; + &quot;annotations.jsonl&quot;, &quot;rb&quot;) as f: for record in json_lines.reader(f): if record[&quot;answer&quot;] == &quot;accept&quot;: save_resized_image_file(Path(redacted_path + &quot;/&quot; + record[&#39;meta&#39;][&#39;file&#39;])) else: save_resized_image_file(Path(unredacted_path + &quot;/&quot; + record[&#39;meta&#39;][&#39;file&#39;])) . Transferring the data to Paperspace with magic-wormhole . Once I had the two directories filled with the two sets of images, I zipped them up since I knew I‚Äôd want to use them on a GPU-enabled computer. . I used magic-wormhole to transfer the files over to my Paperspace Gradient machine. The files were only about 400MB in size so it took less than a minute to transfer the data. . Again, ideally I wouldn‚Äôt have this step of doing things locally first. I could certainly have done everything on the Paperspace machine from the very start, but it would have taken a bit of extra time to figure out how to process the data programatically. Moreover if I was using JupyterLab I could then use Prodigy from within my notebooks. . Using the labelled data in our training . The process of ingesting all our data (labels and raw images) is pretty easy thanks to the fastai library‚Äôs convenience classes and layered structure. We‚Äôre using the DataBlock class instead of ImageDataLoaders for extra flexibility. . path = Path(&#39;redaction_training_data&#39;) foia_documents = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(224)) dls = foia_documents.dataloaders(path) foia_documents = foia_documents.new( item_tfms=Resize(224, method=&#39;pad&#39;, pad_mode=&#39;reflection&#39;), batch_tfms=aug_transforms(max_zoom=1)) dls = foia_documents.dataloaders(path) learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(10) . The images get resized to 224x224 pixels, since this is the size that the resnet architecture expects. Since we have a good deal of labelled data, I‚Äôm comfortable using 80% of that data to train the model and the remaining 20% against which to validate. . I train it for 10 epochs as I don‚Äôt appear to reach a point where I‚Äôm overfitting. As you can see from this image, we reach an accuracy of around 96%. . . Experimenting with augmentations . Initially I had been using the RandomResizedCrop transformation on the data, but I was reminded by someone in our group (Jason) that cropping or zooming our images wouldn‚Äôt be useful since it is possible that both of those transformations would remove the small part of the image where a redaction was to be found. . In the end, I went with some settings that made sure we weren‚Äôt zooming into images or rotating them such that parts would be missing. I think there‚Äôs probably more I could squeeze out of the documentation here, particularly so that I‚Äôm not limiting myself too much in the arguments that I‚Äôm passing in. . I chose the pad method with the reflection mode since this seemed to give the best results. The zeros mode was too close to an actual redaction (i.e. a black box on the image) so I ruled that out pretty early on. . Experimenting with different architectures . The course mentions that architectures with more layers do exist. I saw that the next step up from resnet18 was resnet50. I‚Äôm certainly in the territory where I‚Äôm just turning knobs in the hope of seeing some kind of result, but I thought it was maybe worth a comparison. . The danger with having more layers (and thus more parameters) is that the model is more likely to overfit. The training process also takes much longer to execute: 44 seconds per epoch compared to 21 seconds with resnet18. It didn‚Äôt seem to measurably improve the accuracy. The best results I was able to get were still around 95%, give or take a percent or two. It seems that the real improvements are to be found in the pre-processing or augmentation stage, rather than from choosing an architecture with more layers. . Hosting the model with MyBinder . Chapter two of the course book goes into a decent amount of detail of some of the tradeoffs and issues around model deployment. Part of the exercise is to not only train a model on your own data, but go through the steps to get the model hosted online. . Using MyBinder and the voila library, alongside instructions from the book and the forums, I managed to get my model deployed. If you visit this address you‚Äôll see an interface where you should first upload an image ‚Äî i.e. a screenshot of a document. When you click ‚Äòclassify‚Äô, you‚Äôll then see a prediction of whether the image is redacted or not, as well as the confidence/probability that that prediction is true. . . Next steps . I‚Äôm at the point in the course where I know enough to be dangerous (i.e. train models), but I don‚Äôt know how to improve them from here. Some ideas I had for ways to improve the model‚Äôs accuracy: . better augmentation choices ‚Äî it‚Äôs possible that I‚Äôve misconfigured some argument or made the wrong choices in which augmentations should be applied. | more labelled data ‚Äî this one is pretty easy to fix, but I probably shouldn‚Äôt continue down this route unless I know it‚Äôs really going to help. I‚Äôm not in a position right now to be able to judge how much it‚Äôd help me. | different redaction types ‚Äî currently I have a single ‚Äòredacted‚Äô vs ‚Äòunredacted‚Äô category choice, but in reality there are several different types of redaction in the data set: some have handwritten redactions, others are square computerised boxes, and there are a couple of other types as well. I wonder whether I should train the model to recognise the different types, and then to combine those together as a ‚Äòredacted‚Äô set of categories. (I may be thinking about this wrong). | . Otherwise and for now, I‚Äôm happy with where I managed to reach with this model. I have some other ideas for how to keep going with exploring this data set. For example, even better than a slightly dumb classification model would be to have a segmentation model that was able to determine what percentage of the pixels or total area of the page that were redacted. With a reasonably accurate segmentation model of that kind, we‚Äôd then be able to provide really interesting metrics on what percentage of the information provided was redacted. . I will probably also want to go back and add in the earlier processing steps into the notebook so that things are much closer to being an ‚Äòend-to-end‚Äô solution. . Footnotes . You can find our thread in the fastai forum here.¬†&#8617; . | Other countries have variations of this law, like this from the United Kingdom.¬†&#8617; . | I realise that there is a programatic way to do this. At this early stage in the project, I was more eager to get going with the labelling, so I took the easy path by using Automator.¬†&#8617; . |",
            "url": "https://mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html",
            "relUrl": "/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html",
            "date": " ‚Ä¢ Sep 6, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I‚Äôm Alex. . I am a software engineer based in London, UK. I recently built Ekko, an open-source framework allowing developers to easily add realtime infrastructure and in-transit message processing to web applications. . I have multiple years of experience in the Ruby and JavaScript ecosystems and am comfortable working with Go, PostgreSQL, AWS cloud infrastructure and Docker. . I have a PhD in History and authored several books based on my research work in Afghanistan. . I have a long-standing blog that I will combine with this one at some point, but for now I intend to post technical posts here. .",
          "url": "https://mlops.systems/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://mlops.systems/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}