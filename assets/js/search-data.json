{
  
    
        "post0": {
            "title": "Check your security vulnerabilities with `safety`",
            "content": "safety is a tiny tool that checks your package’s dependencies for security vulnerabilities. It is free to use for open-source projects, and using it is as a pip install safety followed by safety check. . It checks a database of known security vulnerabilities. This database is only updated once every month, but if you are not open-source or you need access to the more frequently-updated database, then you can subscribe via pyup. . With that caveat, it’s not perfect, but it’s better than nothing. An easy CI win for open-source projects. . [I first learned of this tool here. Many thanks to calmcode for continuing to make these really useful videos.] .",
            "url": "https://www.mlops.systems/security/tools/calmcode/2021/11/27/safety-vulnerability-checker.html",
            "relUrl": "/security/tools/calmcode/2021/11/27/safety-vulnerability-checker.html",
            "date": " • Nov 27, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "How to set and get environment variables using Python",
            "content": "If you want to get and set environment variables using Python, simply use the relevant methods from os. To set an environment variable, do this: . import os os.environ[&#39;SOME_ENV_VARIABLE&#39;] = 13.5 . And to access an environment variable, there are actually a number of different ways. All these three are essentially the same: . os.getenv(&#39;SOME_ENV_VARIABLE&#39;) os.environ.get(&#39;SOME_ENV_VARIABLE&#39;) os.environ(&#39;SOME_ENV_VARIABLE&#39;) . For the final one (os.environ(&#39;SOME_ENV_VARIABLE&#39;)), if the variable doesn’t exist, it’ll return a KeyError, whereas the first two will just return None in that case. .",
            "url": "https://www.mlops.systems/python/2021/11/26/environment-variables.html",
            "relUrl": "/python/2021/11/26/environment-variables.html",
            "date": " • Nov 26, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "entr: a tool to run commands when files change",
            "content": "It’s a fairly common pattern that you have some code that you’re repeatedly running. Perhaps you’re fixing a failing test, and you just have to keep running it every time you make a fix. . Enter entr. This handy little tool reruns a particular command whenever changes are detected in a particular set of files. . Let’s take the example I mentioned above: you have a failing test that you’re debugging and you need to have it run every time you save a change to the file. Assuming your source code is stored in src and you’re using pytest, then you could use something like the following: . ls src/*.py | entr -c pytest test.py::test_some_feature . So now, any time you change any Python file inside the src folder, it’ll rerun your test. The -c flag will clear the terminal every time the test runs. . [Many thanks to calmcode for continuing to make these really useful videos.] .",
            "url": "https://www.mlops.systems/debugging/testing/tools/calmcode/2021/11/25/entr-reruns-tests.html",
            "relUrl": "/debugging/testing/tools/calmcode/2021/11/25/entr-reruns-tests.html",
            "date": " • Nov 25, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "On failure",
            "content": "I’ve been working as a machine learning engineer now for a few months now. If there’s one thing that I have found characterises my experience so far, it’s failure. Software fails; we even have a word for that: bugs. Learning new things might also be characterised as departing from a state of failing to understand. . There hasn’t been a week that’s gone by since I started where I didn’t encounter some kind of failure, usually my inability to understand why something was behaving in a particular way. My last post was about debugging, and finding ways to move forward in the face of failure is a key aspect of that process. . Failure isn’t fun. My initial reaction to hitting something I don’t understand is not one of glee and excitement at getting this opportunity to solve some kind of problem. But maybe it should be. It occurred to me this week that actually failure is sort of the name of the game. Solving hard problems is exactly what software engineers get paid to do. If it were just easy, it’d be a different kind of work. . Two posts by Julia Evans are pretty great on how a lot of being able to do this kind of work is about mindset. Ellen Ullman covers similar territory in ‘Life in Code’ and ‘Close to the Machine’. . The point is this: we are paid to confront this failure. This is the work. Thinking that it’s a distraction from the work — some kind of imaginary world where there are no blockers or failing tests — is the real illusion. .",
            "url": "https://www.mlops.systems/debugging/emotions/2021/11/21/on-failure.html",
            "relUrl": "/debugging/emotions/2021/11/21/on-failure.html",
            "date": " • Nov 21, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Some things I learned about debugging",
            "content": "I’ve had to deal with a whole bunch of bugs in the past few days and weeks. I thought it’d be useful to put down some thoughts about things that I’ve learned along the way. . Logging &amp; Printing . These are maybe the first things that everyone says you should do when you have a bug you need to fix: log things somewhere where you can see them. . There are some scenarios where simple print calls aren’t enough. If you’re running code through a series of tests, then the test harness will often consume all output to stdout so you won’t see any of your print statements. Luckily, test environments can usually be configured to print debug statements of loggers. . Once you can see what’s happening at a particular moment, you can see if what you expected to happen at that moment is actually happening. . Breakpoint your way to infinity! . The breakpoint() function comes built-in with Python. It’s a convenience wrapper around some pdb magic, and practically speaking it means you can set a point where you can interrupt the Python execution. Your terminal will halt at that point, and you can inspect the variables or objects available at that particular moment. . I wish I had known about this earlier on. It’s extremely useful for understanding exactly how a function or piece of code is being executed. . Come with hypotheses . If you don’t have a sense of what you expect to happen, it’s going to be hard to determine if what you’re doing is having any effect or not. . I’ve been lucky to do some pairing sessions with people as they work through bugs and problems, and I’ve had this ‘come with a hypothesis’ behaviour modelled really well for me. . It’s not a panacea; there’s still a lot of work to be done around this, but it’s sort of the foundation, particularly for non-trivial bugs. . Leave your assumptions at the door . Don’t assume what’s written is what’s actually working. This applies to the code you’re working on, the documentation, docstrings, everything. This is especially true when your codebase is rapidly changing growing, such as at a startup or a smaller company where not everything has been cemented into place. . The rapid pace of change means that things can get out of date, or people can make mistakes. This applies to packages or modules you’re importing as well. Of course, it’s probably more likely that you’re misunderstanding something vs the Python standard library has got something wrong, but for many other open-source projects, you should at least be open to the possibility that weird things might show up. . Follow the thread wherever it leads . This is something about updating your assumptions as you move through the process of testing your assumptions. If you rule out certain pathways, then you should be prepared to go down the remaining ones as far as you need. . Be systematic . I’ve found a few times now, that there are certain moments where I notice I’m far far down the road. I’ll have kept making a bunch of decisions at the various crossroads that I passed. At a certain moment, though, I need to take stock and just note down all the decisions and assumptions I’ve made in order to reach this point. . I’ll write a short note to myself (mainly), but also for teammates, where I explain all the different assumptions and pathways that I’m travelling down. I’ll specifically write down all the conditions that need to be present for this bug to present (as far as I know them). . Quite often, just writing these assumptions down will help me solve the problem outright. Even when it doesn’t, it’s extremely useful in re-grounding myself and reminding me of why I’m going down rabbit hole x or y. . Know when to stop . In an ideal world you’d get to follow every windy road and to figure out everything that doesn’t make sense. But — and this is again especially true for fast-moving startups — you might not always have time to do that. . This is somehow connected to the Pareto Principle (also known as the 80/20 rule). At a certain point you should make sure to check in with how much time you’d planned on spending on a particular bug. If you’re finding that it’s taking far longer than expected, and you have other things you’re committed to completing, then you should maybe take an opportunity to connect to your team. Alternatively, you can rescope and find a way to disable or flag a particular bug for the next sprint, or see if someone can help you with it. . Remember: this is the work . Sometimes when I’m fixing bugs I have the feeling that I’m wasting my time somehow, or that I should be doing something more productive. It’s often the case, though, that this is the work. I’m low on experience, but proxy experience that I’ve gained through reading books tells me that finding, fixing and triaging bugs is a lot of what we do as software engineers. . Know when to ask for help . Sometimes there are bugs which turn out to be bigger than you’re able to handle. It’s certainly worth pushing back against that feeling the first few times you feel it. Early on it’s often going to feel like the bug is unsolvable. . But some times there are pieces of context you don’t have, which a quick overview of what you’ve done and tried might alert someone more season to the fact that you’re going down the wrong alley. Or it might remind them of something they knew implicitly but had forgotten. The important things is to judge when is the right time to seek outside advice. .",
            "url": "https://www.mlops.systems/debugging/2021/10/25/debugging.html",
            "relUrl": "/debugging/2021/10/25/debugging.html",
            "date": " • Oct 25, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Writing Code",
            "content": "I read Daniel Roy Greenfeld’s post on how he found that coding a lot was key to improving his skills. It makes sense. Everything I’ve read so far and my previous experience at the metaskill of learning new things tells me that it is a good investment of time. . Just like you get good at writing by doing a lot of writing, on some level that is true for coding. (Of course, there are additional pieces to the puzzle: you have to develop some taste alongside the pure production side, you have to do some quality-control and refactor your code, and so on and so on.) . For me, this looks like the following: . coding at work during the week | smaller focused exercises from PythonMorsels, Exercism, LeetCode and AlgoExpert | code written while working my way through the fastai course; this will probably manifest as blog posts here as well, outlining some small project I completed along the way. | a bigger project, perhaps a package, that I’ll start building at some point. I have some ideas for things I want to implement. I’ll pick one soon. It’ll probably be related in some way to the fastai coding. I’m thinking right now of making a tool that allows you to download PDFs and use the pages of those PDFs as image files in computer vision problems; a data ingestion tool, in other words. | smaller scripts to solve daily problems in my digital life. I’ll store those on my GitHub somewhere and write up the design decisions around the more interesting ones here. | . One thing I took note of was how Daniel mentioned that it made sense to specialise and focus on one language at a time, particularly in the early days. Rather than indulging my curiosity and doing 1001 things using Go or lisp or whatever, I will try to stick to Python at least until I feel more confident with it. .",
            "url": "https://www.mlops.systems/python/skillbuilding/2021/09/18/writing-code.html",
            "relUrl": "/python/skillbuilding/2021/09/18/writing-code.html",
            "date": " • Sep 18, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Reading Python Code",
            "content": "It’s a truism of sorts that in order to improve your skills, you have to practice them. For coding, the stereotypical image is of someone typing, actually creating new things. But as often as not, you’re going to be reading code instead. This code might be something you write yesterday or last year, or it might be something that someone else wrote. . One way or another, reading code is a great way to get increasing familiarity with stylistic, syntactic patterns and to get exposed to some best practices, especially if you get to pick the code you’re reading. . I’ll be doing the same as I ramp up my Python proficiency. I wanted to gather some lists of codebases and assorted resources in one place for myself, and I hope maybe it’ll be useful for someone else as well. . Good Quality Python Code . jinja — a templating engine written in Python (and see the recommendations for supplemental reading and watching for jinja here) | howdoi — a search tool for coding answers via the command line | flask — a micro-web framework for Python | FastAPI — another web framework that’s a bit larger than flask | diamond — a Python daemon that collects and publishes system metrics | werkzeug — a web server gateway library | requests — an HTTP library, now part of the Python standard library | tablib — library for Pythonic way to work with tabular datasets | click — a Python package for creating command line interfaces | pathlib — part of the Python standard library; a module to handle filesystem paths (also the corresponding PEP proposal #428) | dataclasses — a module in the Python standard library; reduces boilerplate of writing classes (also the corresponding PEP proposal #557) | joblib — a library to support lightweight pipelining in Python | . Other Resources . 500 Lines or Less — a book in which specific small open-source projects are profiled to understand how they approached their particular challenge. | The Architecture of Open Source Applications: Elegance, Evolution and a Few Fearless Hacks — examination of the structure of the software of some open-source software applications. | The Architecture of Open Source Applications: Volume II: Structure, Scale and a Few More Fearless Hacks — the second volume in the series. | .",
            "url": "https://www.mlops.systems/python/skillbuilding/2021/09/18/reading-python.html",
            "relUrl": "/python/skillbuilding/2021/09/18/reading-python.html",
            "date": " • Sep 18, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Tensors all the way down",
            "content": "#!pip install -Uqq fastbook #!pip install fastai import fastbook fastbook.setup_book() from fastai.vision.all import * from fastbook import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . . In chapter 4 of the book, we start to really get into what&#39;s going on under the hood with deep learning. Turns out, tensors are a pretty important piece. We are still in the realm of computer vision, and we are going to work on distinguishing between handwritten digits. . First we use the untar_data function to grab a sample of data from the famous MNIST data set. This function returns the path where that data was stored locally. . path = untar_data(URLs.MNIST_SAMPLE) . Path.BASE_PATH = path . path . Path(&#39;.&#39;) . Now we want to briefly inspect the contents of one of our training data folders. This is for the number 7. You can see that it&#39;s just a series of .png image files. . threes_dir = (path/&#39;train/3&#39;).ls().sorted() sevens_dir = (path/&#39;train/7&#39;).ls().sorted() sevens_dir . (#6265) [Path(&#39;train/7/10002.png&#39;),Path(&#39;train/7/1001.png&#39;),Path(&#39;train/7/10014.png&#39;),Path(&#39;train/7/10019.png&#39;),Path(&#39;train/7/10039.png&#39;),Path(&#39;train/7/10046.png&#39;),Path(&#39;train/7/10050.png&#39;),Path(&#39;train/7/10063.png&#39;),Path(&#39;train/7/10077.png&#39;),Path(&#39;train/7/10086.png&#39;)...] . In order to look at a single image, we can just open it using Image.open which comes from the Python Image Library (PIL). . im3_path = threes_dir[1] im3 = Image.open(im3_path) im3 . Jupyter knows how to display various files, so we can see that image above. But what exactly is an image made up of? If we turn that image into an array, or to a tensor (the next two cells), slicing them so you aren&#39;t just seeing zeros on the edges, then you can see that these images are made up of a matrix of values from 0 to 255. . im3_arr = array(im3)[4:10, 4:10] im3_arr . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=uint8) . im3_tns = tensor(im3)[4:10, 4:10] im3_tns . tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) . We can use the show_image function to turn those 0-255 values back into an image, like so: . show_image(im3_arr) . &lt;AxesSubplot:&gt; . A really nice way of visualising exactly what is going on is to turn this image into a pandas dataframe and then for every individual pixel value, use that value as the background gradient for that cell. Here&#39;s an example of part of an image of a handwritten number 3. . im3_t = tensor(im3) df = pd.DataFrame(im3_t[4:15,4:22]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | . 3 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | . 4 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | . 5 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | . So now we have a toolkit of ways to view the pixel values that make up an image. We also have a mental model for how we can think about images and how computers represent those images stored on our machine. . But how might we then best go about knowing whether a particular image is a 3, let&#39;s say, or a 7? . One naive approach might be just to get the average value for each individual pixel for all of the threes in our training data, and then just compare the difference between our sample image and this average representation. . Let&#39;s try that now. . Getting the average values for our images . We&#39;ll set up two lists with images of the digits converted to tensors. You can see that we have 6131 images in our &#39;threes&#39; list. . threes_tensors = [tensor(Image.open(i)) for i in threes_dir] sevens_tensors = [tensor(Image.open(i)) for i in sevens_dir] len(threes_tensors) . 6131 . We can view an individual image, as before, with the show_image function: . show_image(threes_tensors[3]) . &lt;AxesSubplot:&gt; . Now in order to get the average values for each pixels, we can use the stack method to handle the first part of this. . Think of it as basically adding an extra dimension to your data structure, such that you have a &#39;stack&#39; (it&#39;s a useful mental image) of those images. . threes_stack = torch.stack(threes_tensors) . If we look at the shape of our Pytorch stack now, we can see we have our 28x28 image, but we have a stack of 6131 of them. . threes_stack.shape . torch.Size([6131, 28, 28]) . Each individual image is still a tensor: . a_three = threes_stack[3][4:16, 4:16] a_three . tensor([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 104, 253, 253, 253, 255, 253], [ 0, 0, 0, 0, 0, 178, 248, 252, 252, 252, 253, 252], [ 0, 0, 0, 0, 0, 186, 252, 252, 252, 252, 253, 252], [ 0, 0, 0, 0, 0, 186, 252, 243, 172, 172, 39, 39], [ 0, 0, 0, 0, 0, 39, 53, 47, 0, 0, 0, 29], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 54, 208], [ 0, 0, 0, 0, 0, 0, 0, 0, 3, 41, 253, 252], [ 0, 0, 0, 0, 0, 0, 5, 41, 165, 252, 253, 252], [ 0, 0, 0, 0, 0, 109, 163, 252, 252, 252, 253, 252], [ 0, 0, 0, 0, 0, 186, 252, 252, 252, 252, 253, 252], [ 0, 0, 0, 0, 0, 187, 253, 253, 253, 253, 134, 77]], dtype=torch.uint8) . Generally speaking, for some operations (like getting the mean average) we&#39;re going to want to convert the values to floats, and it also makes sense to normalise the values at the same time. Instead of having a range of 0-255, we want a range of 0-1. . threes_stack[3][4:16, 4:16].float()/255 . tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4078, 0.9922, 0.9922, 0.9922, 1.0000, 0.9922], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6980, 0.9725, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7294, 0.9882, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7294, 0.9882, 0.9529, 0.6745, 0.6745, 0.1529, 0.1529], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.2078, 0.1843, 0.0000, 0.0000, 0.0000, 0.1137], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2118, 0.8157], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.1608, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0196, 0.1608, 0.6471, 0.9882, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4275, 0.6392, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7294, 0.9882, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7333, 0.9922, 0.9922, 0.9922, 0.9922, 0.5255, 0.3020]]) . Now that we&#39;ve done it for a single image, we can perform the same operations on our whole Pytorch stack. . threes_stack = torch.stack(threes_tensors).float()/255 sevens_stack = torch.stack(sevens_tensors).float()/255 threes_stack.shape # it&#39;s good to keep in touch with the shape of our stack . torch.Size([6131, 28, 28]) . Now we&#39;re getting closer to our desired result. We can squash the stack down into just two dimensions with a simple call to .mean(0), where 0 is the index value of the dimension through which we want to calculate the mean. You&#39;ll see now that the shape property of our threes_means variable is simply a 28x28 image. . threes_means = threes_stack.mean(0) threes_means.shape . torch.Size([28, 28]) . When we show that image, you&#39;ll see that it&#39;s a sort of blurry &#39;ideal&#39; version of a three . show_image(threes_means) . &lt;AxesSubplot:&gt; . We can do the same for the sevens: . sevens_means = sevens_stack.mean(0) show_image(sevens_means) . &lt;AxesSubplot:&gt; . Validation: Comparing our average three with a specific three . Now we have our average values, we want to compare these with a single specific digit image. We&#39;ll get the difference between those values and whichever difference is the smallest will most likely be the best answer. . Our averaged three is still threes_means and we can get a single three from our validation set like this: . threes_dir_validation = (path/&#39;valid/3&#39;).ls().sorted() sevens_dir_validation = (path/&#39;valid/7&#39;).ls().sorted() im3_validation_path = threes_dir_validation[5] im3_validation = tensor(Image.open(im3_validation_path)).float()/255 im7_validation_path = sevens_dir_validation[3] im7_validation = tensor(Image.open(im7_validation_path)).float()/255 show_image(im3_validation) . &lt;AxesSubplot:&gt; . show_image(im7_validation) . &lt;AxesSubplot:&gt; . . Note: Calculating the difference between two objects . We can use two different measurements of the difference between our mean value and the individual image: . mean absolute difference (calculated by taking the mean of the absolute difference between the two tensor values). Also known as the L1 Norm. | root mean squared error (calculated by first squaring the difference between the two tensor values, taking the mean and then square rooting those values). Also known as the L2 Norm. | . The second option, the RMSE, gives a stronger signal, you might say, for the differences because you are taking the averages from the squared values. Squaring the difference also takes care of any negative values you might have. . mean_absolute_difference_3 = (im3_validation - threes_means).abs().mean() root_mean_squared_error_3 = ((im3_validation - threes_means)**2).mean().sqrt() mean_absolute_difference_3, root_mean_squared_error_3 . (tensor(0.1188), tensor(0.2160)) . mean_absolute_difference_7 = (im7_validation - threes_means).abs().mean() root_mean_squared_error_7 = ((im7_validation - threes_means)**2).mean().sqrt() mean_absolute_difference_7, root_mean_squared_error_7 . (tensor(0.1702), tensor(0.3053)) . We can now see that our individual three image is indeed closer to the threes_means composite image than to the sevens_means composite image. A smaller value at this point is what we&#39;re looking for, and the threes have it. . It turns out that there is another way to calculate the difference that&#39;s built in to Pytorch as loss functions: . F.l1_loss(im3_validation, threes_means), F.mse_loss(im3_validation, threes_means).sqrt() . (tensor(0.1188), tensor(0.2160)) . It&#39;s a bit more concise, though it does obscure what&#39;s going on under the hood in terms of calculations. . Results of the naive approach . So this tells us that our single three is closer to an ideal 3 than an ideal 7, which is great since it reflects the ground truth of our problem. But can we get a metric to know how well we perform on average against a large number of threes and sevens from our validation set? . Yes, since we have that dataset ready for use! . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]).float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]).float()/255 valid_3_tens.shape, valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . Now we can write a helper function that will allow us to calculate the distance between two images. We use the RMSE or L1 Norm for this difference calculation: . def mnist_distance(a, b): return (a - b).abs().mean((-1, -2)) . We can use this function on our previous example: . mnist_distance(im3_validation, threes_means) . tensor(0.1188) . We can continue onwards by comparing the rank 3 tensor with the rank 2 tensor. This brings a concept called &#39;broadcasting&#39; into play. . We are comparing a tensor with 2 dimensions with a tensor with 3 dimensions, so Pytorch behaves as if both tensors have three dimensions, and (without taking extra memory) pretends as if there are multiple copies of the image in 2 dimensions. This effectively makes it as if we&#39;re comparing two 3-dimensional tensors. . From this next calculation, we see returned back a collection of the distances between all of the validation images. . mnist_distance(valid_3_tens, threes_means) . tensor([0.1328, 0.1523, 0.1245, ..., 0.1383, 0.1280, 0.1138]) . In order to check whether an image is a 3, we basically need to know whether the difference for the number 3 is larger than the difference for the number 7. . We can write a helper function for that: . def is_3(img): return mnist_distance(img, threes_means) &lt; mnist_distance(img, sevens_means) . We can now check our ground truth examples: . is_3(im3_validation), is_3(im7_validation) . (tensor(True), tensor(False)) . That&#39;s what we expected to happen. Our 3 image is a 3, and our 7 image is not a 3. . If we want to check the distance in general for our validation set, we have to convert them into floats and then get the mean, but it&#39;s really easy. Again, this uses broadcasting: . validation_accuracy_3 = is_3(valid_3_tens).float().mean() validation_accuracy_7 = 1 - is_3(valid_7_tens).float().mean() validation_accuracy_3, validation_accuracy_7 . (tensor(0.9168), tensor(0.9854)) . Overall, then, we can calculate how good our toy or baseline model is for the entire problem: . (validation_accuracy_3 + validation_accuracy_7) / 2 . tensor(0.9511) . Pretty good! . This was of course just a naive way to solve the problem. There are more advanced techniques which we&#39;ll tackle next. .",
            "url": "https://www.mlops.systems/pytorch/fastai/computervision/2021/09/16/ch4-tensors.html",
            "relUrl": "/pytorch/fastai/computervision/2021/09/16/ch4-tensors.html",
            "date": " • Sep 16, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "A Baseline Python Development Setup",
            "content": "The world of Python versioning (and the downstream package versioning) is wild. This StackOverflow thread gives you a sense of some of the core issues at play. (As an indication of the importance of the issue, even BDFL Guido van Rossum himself has the current second most upvoted answer.) . For a really vanilla and close-to-core-python setup, a combination of venv and pip seem to be the way to go. venv is part of the standard library and as such is pretty close to a default option. . For something a bit more involved, that handles dependencies and package installation in a slightly more deft manner, the combination of pyenv, pyenv-virtualwrapper and poetry works really well. I’ll detail some of the setup gotchas and usage patterns below. . pyenv for versioning Python itself . pyenv lets you install multiple versions of Python on the same machine. The interface to switch between local versions and whatever you’ve decided will be your global option is pretty intuitive. . Visit the pyenv github page for more on installation. (If you’re on a Mac you can simply do a brew install pyenv.) . To see which versions of Python you have installed locally: . pyenv versions . To see versions of Python which are available for installation: . pyenv install —list . Note that, as I understand it, these versions are not dynamically updated. You get an updated list of new Python versions by updating pyenv, in other words. . To install a specific version of Python, and to make it available for use: . pyenv install 3.9.1 . To set that version of Python as the global version (i.e. running python will use this version by default): . pyenv global 3.9.1 . If you are in a project directory and wish to only use a particular version of Python in that directory (and its subdirectories): . pyenv local 3.8.2 . This creates a .python-version file in that directory with the desired local version. . pyenv-virtualenv for managing virtual environments . pyenv-virtualenv is a plugin that connects the work of selecting which version of Python to use (through pyenv, which we’ve previously installed) to the work of creating and running virtual environments to keep code contained in quasi-sandbox environments. When you install packages in virtual environments they don’t conflict with other locations where you might have conflicting versions of those same packages installed. . Read installation instructions and the docs here. (If you installed pyenv with homebrew, be sure to do the same with pyenv-virtualenv). . To create a virtual environment for the Python version used with pyenv, run pyenv virtualenv, specifying the Python version you want and the name of the virtual environment directory: . pyenv virtualenv 3.8.2 my-virtual-env-3.8.2 . This will create a virtual environment based on Python 3.8.2 under $(pyenv root)/versions in a folder called my-virtual-env-3.8.2. . To list what virtual environments have been created and are available to use: . pyenv virtualenvs . As a common workflow pattern, you’d create your directory and cd into it, and then you can set the virtual environment you just created as the one to use for that directory: . mkdir test-project &amp;&amp; cd test-project pyenv local my-virtual-env-3.8.2 . This should change the prompt in your terminal window and you’ll thus know that you’re now working out of that virtual environment. Any time you return to that folder you’ll automatically switch to that environment. . The manual way of turning on and off virtual environments is: . pyenv activate env-name pyenv deactivate env-name . To remove a virtual environment from your system: . pyenv uninstall my-virtual-env . (This is the functional equivalent of removing the directories in $(pyenv root)/versions and $(pyenv root)/versions/{version}/envs.) . poetry for handling package installation and dependencies . python-poetry is the latest standard tool for handling package installations and dependency management. . You can use poetry without the previous two tools, but really they work best all together. Follow the installation instructions documented on their page to get it going. . Then update poetry: . poetry self update . poetry is one of those tools that’s able to update itself. . For basic usage for a new project, you can follow the following workflow. There are two ways to start a new project using poetry: using new or init. For example: . poetry new some-project-name . This will kickstart your new project by creating a bunch of files and a directory structure suitable for most projects, like so: . some-project-name ├── pyproject.toml ├── README.rst ├── some-project-name │ └── __init__.py └── tests ├── __init__.py └── test_some-project-name.py . You might want to use a src folder (above the some-project-name in our example) which is fairly commonly used, in which case amend the command as follows: . poetry new --src some-project-name . poetry init doesn’t do all the extra work of creating a directory and file structure. It merely creates a pyproject.toml file interactively, using some smart defaults. For a minimal use of poetry, this is definitely the way to go. . The add command adds required packages to your pyproject.toml and installs them (along with all their dependencies). It does a lot under the hood to make sure that dependencies are correctly resolving before installing. For example: . poetry add zenml . To add packages only to be used in the development environment: . poetry add --dev zenml . To list all installed packages in your current environment / project: . poetry show . To uninstall a package and remove it (and its dependencies) from the project: . poetry remove zenml . To install all relevant packages and dependencies of a project that you’ve newly cloned into: . poetry install . Note that it is possibly worth creating some custom scripts to handle some of the overhead of using these tools, depending on your common development workflows. .",
            "url": "https://www.mlops.systems/python/tools/2021/09/14/python-versioning-package-managers.html",
            "relUrl": "/python/tools/2021/09/14/python-versioning-package-managers.html",
            "date": " • Sep 14, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Six problems TFX was trying to solve in 2017",
            "content": "TensorFlow Extended or TFX is a platform for machine learning that claims to handle pretty much everything you’d need for end-to-end model training, deployment and retraining. It was developed for Google, the successor to Sibyl, and released in public in 2017. I read the original paper that accompanied its release to understand the problems it was trying to solve, as well as to get a handle on the specific context in which it was developed. (It’s worth being wary about tools developed at places like Google; after all, hardly any of us are operating at Google-scale). . . ‘A TensorFlow-based general-purpose machine learning platform’ . The engineers wanted a general-purpose tool, one that could serve many different use cases. I haven’t yet read the subsequent paper on the history of TFX, but from what I do know already there were other in-house solutions that existed before. Machine learning model training at scale, deployment and the general full-cycle behaviours are pretty involved and challenging, and it often seems like the needs of particular scenarios demand different approaches. This is as much true now as it was back int 2017, I imagine, though perhaps now we have some ideas of the broad pieces that make up the whole picture that needs to be addressed. . The problem here is that you might have certain parts that either are very compute intensive, or require special distributed computing setups, or where the models need to be trained off streaming data rather than from static stores. So with TFX they tried to make the tool sufficiently abstract that they could handle most cases someone would want to use it for. (They say at the end that there were some parts that they hadn’t anticipated, specifically sequence-to-sequence language models used in machine translation). . An end-to-end platform . The ambition for the platform and software tooling was not just to handle the smaller pieces of the training and deployment cycle, but rather to tackle the big overarching abstractions in a single approach. This of course contained some baked-in assumptions about how users would use TFX as well as what I’d say were quasi-philosophical positions on how best to approach these various parts. The paper characterises these as ‘best practices’, but certainly there hasn’t been uniform acceptance of these. . I imagine the end-to-end part was as much an attempt to encourage engineers to think of the problem in this exact way. If you are handling all the pieces of the training cycle, it’s easier to be fast and iterate and do all the things we expect of a more agile process. . Continuous training and serving . TFX was built to handle the kinds of models where the use cases demanded the ability to continuously retrain models using large quantities of streaming data. This is almost certainly not the norm, but for a company like Google I can understand that this would have been a key consideration if they wanted adoption of the tool across different teams. . In this way, certain scenarios (for example the Google Play Store case study outlined in the paper) saw a continuous retraining of models as more users used the service as well as new apps continued to be uploaded to the Play Store. If you have this kind of engineering need, and if you need to keep latency to certain boundaries (in the tens of milliseconds), it makes complete sense to have this whole structure that allows this to take place. Reading the specific example, it’s a pretty amazing feat, handling all that complexity underneath the surface. There must be many hundreds of other such services which similar levels of complexity concealed beneath the surface. . Reduce technical debt accrued via duplicated or ad hoc solutions . Prior to TFX and Sybil, it seems that there were many different approaches within Google, all addressing the same problem but in slightly different ways. . Having a series of best-practices built in to the service means that everyone can communicate about problems and about their issues using a shared language. It means that solutions discovered by one team can help other future teams. There’s a lot to be said for finding a solution that is sufficiently abstracted to work for many people. . Indeed, it seems this is the work of the MLOps community right now: find ways to abstract away problems that we all face, and to find the best abstractions that fit within the mental models we all have in our heads. The fact that there hasn’t been a grand convergence on a single solution indicates to me (at this current moment) that we haven’t found the right abstractions or flexibility within those abstractions. All the end-to-end tools handle much of the same stages of the model training and deployment process, but they each have opinions about the best practices to be employed along the way. (At least, that’s my current take on things). . Reliable serving models at scale . If you’re Google, you need to make sure that you aren’t serving garbage models to your users, or that inconsistencies in the input data aren’t polluting your retraining processes. At scale, even small mistakes compound really easily. . In the paper, two specific improvements are mentioned, tackling the challenges of low latency and high efficiency. The high efficiency example wasn’t entirely comprehensible for me, but what was clear was that they had very high expectations for how fast they wanted to make all parts of the pipelines and process. As above, the challenges of making it easy and fast to serve models — all of which had to happen in a reliable manner — was something that could be reused elsewhere in the company. TensorFlow Serving is what we get from their efforts in this regard. . Fast retraining with ‘warm-starting’ . For the specific challenge of retraining models with streaming data, engineers were finding that they couldn’t retrain the entire model from scratch, particularly with the scale of the training data that they had. Instead, they leveraged transfer learning (reframed here as ‘warm-starting’) to take all the hard work that had already been done, and adapting this pre-existing model with the new data. This makes a lot of sense, though the reframing with the new term is a bit less comprehensible to me. . Missing pieces . There are various pieces of what I think of as the machine learning workflow (as of 2021) which seem to be missing when I read this paper. Explainability or governance of models seems somewhat of an afterthought, if it is raised at all. I think the authors might argue that many of the checks and balances are made on the data ingestion phase, and that if all that checks out then this tackles a large piece of the problem surface area. . Similarly, there is relatively little said about model versioning and data versioning. Maybe coming at this from the present moment, where it seems obvious (with tools like DVC) that data versioning is a thing you’d want to care about. . As a general response, it seems clear that if you use TensorFlow to train your models, TFX might well be a pretty neat solution that handles many of your needs, particularly if you’re operating at serious scale. If you’re a researcher (perhaps using PyTorch) with less of those specific contextual needs, it seems less than certain that TFX would suit your purposes. . A couple of other interesting observations. The data observability and validation stage seemed to place a lot of emphasis on the automation of how pre-defined schemas might get updated. I’d be interested to see how that worked in practice. I understood the challenge that if there are too many error messages about dodgy data inputs, engineers are likely to grow inured to those alerts and maybe just ignore them. But at scale, I wonder about the risks of allowing automatic updates to those schema boundaries. . Again on the validation point, I found it interesting how the authors of the paper said that users of TFX internal to Google found the option to enable this was actually a hard sell unless or until the team had experienced some kind of failure connected to poor data validation. The TFX team ended up turning on the validation parts of the pipeline by default instead of assuming that users would choose to do so manually. . I wasn’t active in the field in 2017, so it’s hard for me to be able to reconstruct exactly how prescient or not this paper was in some of its diagnoses of the problem. It doesn’t seem that TFX was the total solution that perhaps it was pitched as being, but nonetheless it seems an important engineering achievement for Google. .",
            "url": "https://www.mlops.systems/tfx/tensorflow/google/mlops/papers-i-read/2021/09/11/tfx-paper.html",
            "relUrl": "/tfx/tensorflow/google/mlops/papers-i-read/2021/09/11/tfx-paper.html",
            "date": " • Sep 11, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Managing Python Environments with pyenv and pipenv",
            "content": "It’s hardly news that that managing multiple versions of Python in a development environment is hard. Adding in dependency management on top of that makes everything harder. .",
            "url": "https://www.mlops.systems/python/2021/09/10/python-environments.html",
            "relUrl": "/python/2021/09/10/python-environments.html",
            "date": " • Sep 10, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Retrieval Practice with fastai chapters 1 and 2",
            "content": "Retrieval practice is when you actively try to remember something as a way of making sure that you learn it well. (Read more about it here). Today I did that with the dogs vs cats example that the first two chapters cover. . We start with installing the fastai library and importing everything from the vision library. This was hard to remember since the pattern of .all and importing * is not something I&#39;ve seen much in Python imports. . from fastai.vision.all import * . Then we create the simple function that will be used to classify the images. The pets dataset relies on the first letter of the filename for knowing whether a picture is of a cat or a dog. So the function is pretty simple: it checks whether the first letter is a capital letter or not. . The simple assert testing was a little trick that I saw mentioned somewhere this past week. It&#39;s not a full-fledged test suite, but it&#39;s at least the start of something that can later be refactored out into whatever takes its place, be it using pytest or something else. . def is_cat(string): return string[0].isupper() assert is_cat(&quot;abs&quot;) == False assert is_cat(&quot;Abs&quot;) == True . Now we have to import the data for the files and apply whatever custom transforms we want applied to them. . I had certainly forgotten that untar_data was a method when I started out with this. I also am not familiar enough with the pathlib library as I need to be. . It&#39;s interesting that we actually don&#39;t even need to do any of the batch transformations on the images in order to get excellent results. I imagine that&#39;s because the task is so close to that of the original resnet architecture. . path = untar_data(URLs.PETS)/&#39;images&#39; dls = ImageDataLoaders.from_name_func(path, get_image_files(path), label_func=is_cat, item_tfms=Resize(224)) . Then it&#39;s all about passing the dataloaders object into the cnn_learner function, along with our desired architecture. We also set the error_rate (i.e. 1 minus the accuracy at making predictions) as the metric we&#39;ll see displayed in the output. . learner = cnn_learner(dls, resnet34, metrics=error_rate) # fine-tune the model learner.fine_tune(5) . epoch train_loss valid_loss error_rate time . 0 | 0.140326 | 0.019799 | 0.008119 | 00:19 | . epoch train_loss valid_loss error_rate time . 0 | 0.046906 | 0.021923 | 0.006089 | 00:24 | . 1 | 0.041144 | 0.009382 | 0.004060 | 00:25 | . 2 | 0.028892 | 0.004109 | 0.002030 | 00:25 | . 3 | 0.008950 | 0.002290 | 0.001353 | 00:25 | . 4 | 0.004486 | 0.002822 | 0.001353 | 00:25 | . And here you can see the results. In this training run, with 5 epochs, we were able to achieve a 99.9% accuracy. Not bad! . learner.show_results() .",
            "url": "https://www.mlops.systems/fastai/jupyter/computervision/2021/09/10/chapter1and2recall.html",
            "relUrl": "/fastai/jupyter/computervision/2021/09/10/chapter1and2recall.html",
            "date": " • Sep 10, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "How to set a Jupyter notebook to auto-reload external libraries",
            "content": "The code to insert somewhere into your Jupyter notebook is pretty simple: . %load_ext autoreload %autoreload 2 . When you’re working on an external library or piece of Python code outside the contents of your notebook, this snippet will make sure that the updated functions and constants will always be available in their most-recently edited state. .",
            "url": "https://www.mlops.systems/jupyter/2021/09/09/auto-reload-external-libraries.html",
            "relUrl": "/jupyter/2021/09/09/auto-reload-external-libraries.html",
            "date": " • Sep 9, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "A Baseline Understanding of MLOps",
            "content": "Next week I’m due to begin a job as a Machine Learning Engineer at a company that works in the MLOps field. It’s a new field to me. I’ve read a good deal on it in recent weeks, and listened to a few dozen episodes of the MLOps.community podcast, but I still very much consider myself a beginner in the space. To that end, I thought it worth clarifying my understanding of what MLOps is all about, the problem it is trying to solve, and where I see the opportunity there. . A top-down explanation is probably the best way to think of what we’re doing when we talk about ‘doing MLOps’: we’re doing all the things which make it possible to train, deploy and use machine learning models in the real world or ‘in production’. It isn’t just a series of tools, but also a series of best practices and a community that is constantly learning and iterating to improve. . The kinds of things that you can do with machine learning models are incredibly diverse, so it stands to reason that the people who operationalise all these models have quite varied opinions and approaches to how best to do this. Even the deployment scenarios are pretty different and involve different technology stacks. There is an idea of a ‘full stack machine learning engineer’, which apparently means someone who just knows everything across the board; I hope to be able to delve into some of these areas and the key technologies represented in each space in due course on this blog. .",
            "url": "https://www.mlops.systems/mlops/2021/09/08/baseline-mlops-understanding.html",
            "relUrl": "/mlops/2021/09/08/baseline-mlops-understanding.html",
            "date": " • Sep 8, 2021"
        }
        
    
  
    
        ,"post14": {
            "title": "Training a classifier to detect redacted documents with fastai",
            "content": "I am working my way through the fastai course as part of an online meetup group I host.1 . This week we finished the first and second chapters of the book, during which you train a model that can recognise if an image contains a cat or a dog. Later on, you train another model that distinguishes between different types of bears (‘grizzly’, ‘black’ and ‘teddy’). . Jeremy Howard, who is teaching the course, then prompts you to take what you learned and apply it to something that has meaning for you. (This is something that most of those who’ve found any success with the course emphasise repeatedly.) . I decided to work on something adjacent to my previous life / work, where I knew there was some real-world value to be gained from such a model. I chose to train an image classifier model which would classify whether a particular image was redacted or not. . The Problem Domain: Image Redaction . Under the Freedom of Information Act (FOIA), individuals can request records and information from the US government.2 This is one collection of some of the responses to this requests, sorted into various categories. You can read, for example, responses relating to UFOs and alien visits here. . Quite often, however, these images are censored or redacted. . . Knowing that this practice exists, I thought it might be interesting to train a model that could recognise whether a particular page contained some kind of redaction. This wasn’t completely in line with what we covered during the first two chapters; I wasn’t sure if the pre-trained model we used would work for this data set and use case. . It could be useful to have such a tool, because FOIA responses can sometimes contain lots of data. In order to prepare a request for more data, you might want to be able to show that even though you were sent thousands of pages, most of those pages contained redactions and so were effectively useless. . In the ideal vision of this tool and how it would work, you could run a programme out of a particular directory and it would tell you how many pages (and what proportion) of your PDF files were redacted. . Getting the Data . The first thing I did to gather my data was to download the PDF documents available on this site. I knew that they contained examples of redactions in FOIA documents. I used Automator to split the PDF files up into individual images.3 My Automator script did some downsampling of the images as part of the process, so the images were resized to something that wasn’t prohibitively large to use for training. . Note that this stage and the next was done on my local machine. A CPU was enough for my purposes at this point, though probably I’ll want to eventually port the entire process over to a single cloud machine to handle things end-to-end. . At the end of the splitting-and-resizing process, I had a little over 67,000 images (of individual pages) to train with. . Labelling the images with Prodigy . I had used Explosion.ai’s Prodigy data labelling tool in the past and so already had a license. The interface is clean and everything works pretty much as you’d hope. I had some teething issues getting it all working, but Prodigy co-creator Ines helped me work through those queries and I was up and running pretty quickly. . . It took about three hours to annotate some 4600+ images. Then I could export a .jsonl file that contained the individual annotations for whether a particular image contained a redaction or not: . . From that point it was pretty trivial to parse the file (using the json-lines package), and to resize the images down further in order to separate redacted from unredacted: . import json_lines from PIL import Image from pathlib import Path def save_resized_image_file(location_path): basewidth = 800 img = Image.open(record[&#39;image&#39;]) wpercent = (basewidth / float(img.size[0])) hsize = int((float(img.size[1]) * float(wpercent))) img = img.resize((basewidth, hsize), Image.ANTIALIAS) img.save(location_path) path = &#39;/my_projects_directory/redaction-model&#39; redacted_path = path + &quot;/redaction_training_data/&quot; + &quot;redacted&quot; unredacted_path = path + &quot;/redaction_training_data/&quot; + &quot;unredacted&quot; with open(path + &quot;/&quot; + &quot;annotations.jsonl&quot;, &quot;rb&quot;) as f: for record in json_lines.reader(f): if record[&quot;answer&quot;] == &quot;accept&quot;: save_resized_image_file(Path(redacted_path + &quot;/&quot; + record[&#39;meta&#39;][&#39;file&#39;])) else: save_resized_image_file(Path(unredacted_path + &quot;/&quot; + record[&#39;meta&#39;][&#39;file&#39;])) . Transferring the data to Paperspace with magic-wormhole . Once I had the two directories filled with the two sets of images, I zipped them up since I knew I’d want to use them on a GPU-enabled computer. . I used magic-wormhole to transfer the files over to my Paperspace Gradient machine. The files were only about 400MB in size so it took less than a minute to transfer the data. . Again, ideally I wouldn’t have this step of doing things locally first. I could certainly have done everything on the Paperspace machine from the very start, but it would have taken a bit of extra time to figure out how to process the data programatically. Moreover if I was using JupyterLab I could then use Prodigy from within my notebooks. . Using the labelled data in our training . The process of ingesting all our data (labels and raw images) is pretty easy thanks to the fastai library’s convenience classes and layered structure. We’re using the DataBlock class instead of ImageDataLoaders for extra flexibility. . path = Path(&#39;redaction_training_data&#39;) foia_documents = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(224)) dls = foia_documents.dataloaders(path) foia_documents = foia_documents.new( item_tfms=Resize(224, method=&#39;pad&#39;, pad_mode=&#39;reflection&#39;), batch_tfms=aug_transforms(max_zoom=1)) dls = foia_documents.dataloaders(path) learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(10) . The images get resized to 224x224 pixels, since this is the size that the resnet architecture expects. Since we have a good deal of labelled data, I’m comfortable using 80% of that data to train the model and the remaining 20% against which to validate. . I train it for 10 epochs as I don’t appear to reach a point where I’m overfitting. As you can see from this image, we reach an accuracy of around 96%. . . Experimenting with augmentations . Initially I had been using the RandomResizedCrop transformation on the data, but I was reminded by someone in our group (Jason) that cropping or zooming our images wouldn’t be useful since it is possible that both of those transformations would remove the small part of the image where a redaction was to be found. . In the end, I went with some settings that made sure we weren’t zooming into images or rotating them such that parts would be missing. I think there’s probably more I could squeeze out of the documentation here, particularly so that I’m not limiting myself too much in the arguments that I’m passing in. . I chose the pad method with the reflection mode since this seemed to give the best results. The zeros mode was too close to an actual redaction (i.e. a black box on the image) so I ruled that out pretty early on. . Experimenting with different architectures . The course mentions that architectures with more layers do exist. I saw that the next step up from resnet18 was resnet50. I’m certainly in the territory where I’m just turning knobs in the hope of seeing some kind of result, but I thought it was maybe worth a comparison. . The danger with having more layers (and thus more parameters) is that the model is more likely to overfit. The training process also takes much longer to execute: 44 seconds per epoch compared to 21 seconds with resnet18. It didn’t seem to measurably improve the accuracy. The best results I was able to get were still around 95%, give or take a percent or two. It seems that the real improvements are to be found in the pre-processing or augmentation stage, rather than from choosing an architecture with more layers. . Hosting the model with MyBinder . Chapter two of the course book goes into a decent amount of detail of some of the tradeoffs and issues around model deployment. Part of the exercise is to not only train a model on your own data, but go through the steps to get the model hosted online. . Using MyBinder and the voila library, alongside instructions from the book and the forums, I managed to get my model deployed. If you visit this address you’ll see an interface where you should first upload an image — i.e. a screenshot of a document. When you click ‘classify’, you’ll then see a prediction of whether the image is redacted or not, as well as the confidence/probability that that prediction is true. . . Next steps . I’m at the point in the course where I know enough to be dangerous (i.e. train models), but I don’t know how to improve them from here. Some ideas I had for ways to improve the model’s accuracy: . better augmentation choices — it’s possible that I’ve misconfigured some argument or made the wrong choices in which augmentations should be applied. | more labelled data — this one is pretty easy to fix, but I probably shouldn’t continue down this route unless I know it’s really going to help. I’m not in a position right now to be able to judge how much it’d help me. | different redaction types — currently I have a single ‘redacted’ vs ‘unredacted’ category choice, but in reality there are several different types of redaction in the data set: some have handwritten redactions, others are square computerised boxes, and there are a couple of other types as well. I wonder whether I should train the model to recognise the different types, and then to combine those together as a ‘redacted’ set of categories. (I may be thinking about this wrong). | . Otherwise and for now, I’m happy with where I managed to reach with this model. I have some other ideas for how to keep going with exploring this data set. For example, even better than a slightly dumb classification model would be to have a segmentation model that was able to determine what percentage of the pixels or total area of the page that were redacted. With a reasonably accurate segmentation model of that kind, we’d then be able to provide really interesting metrics on what percentage of the information provided was redacted. . I will probably also want to go back and add in the earlier processing steps into the notebook so that things are much closer to being an ‘end-to-end’ solution. . Footnotes . You can find our thread in the fastai forum here. &#8617; . | Other countries have variations of this law, like this from the United Kingdom. &#8617; . | I realise that there is a programatic way to do this. At this early stage in the project, I was more eager to get going with the labelling, so I took the easy path by using Automator. &#8617; . |",
            "url": "https://www.mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html",
            "relUrl": "/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html",
            "date": " • Sep 6, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Alex. . I am a software engineer based in London, UK. I recently built Ekko, an open-source framework allowing developers to easily add realtime infrastructure and in-transit message processing to web applications. . I have multiple years of experience in the Ruby and JavaScript ecosystems and am comfortable working with Go, PostgreSQL, AWS cloud infrastructure and Docker. . I have a PhD in History and authored several books based on my research work in Afghanistan. . I have a long-standing blog that I will combine with this one at some point, but for now I intend to post technical posts here. .",
          "url": "https://www.mlops.systems/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://www.mlops.systems/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}