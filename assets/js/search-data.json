{
  
    
        "post0": {
            "title": "Six problems TFX was trying to solve in 2017",
            "content": "TensorFlow Extended or TFX is a platform for machine learning that claims to handle pretty much everything you’d need for end-to-end model training, deployment and retraining. It was developed for Google, the successor to Sibyl, and released in public in 2017. I read the original paper that accompanied its release to understand the problems it was trying to solve, as well as to get a handle on the specific context in which it was developed. (It’s worth being wary about tools developed at places like Google; after all, hardly any of us are operating at Google-scale). . ‘A TensorFlow-based general-purpose machine learning platform’ . The engineers wanted a general-purpose tool, one that could serve many different use cases. I haven’t yet read the subsequent paper on the history of TFX, but from what I do know already there were other in-house solutions that existed before. Machine learning model training at scale, deployment and the general full-cycle behaviours are pretty involved and challenging, and it often seems like the needs of particular scenarios demand different approaches. This is as much true now as it was back int 2017, I imagine, though perhaps now we have some ideas of the broad pieces that make up the whole picture that needs to be addressed. . The problem here is that you might have certain parts that either are very compute intensive, or require special distributed computing setups, or where the models need to be trained off streaming data rather than from static stores. So with TFX they tried to make the tool sufficiently abstract that they could handle most cases someone would want to use it for. (They say at the end that there were some parts that they hadn’t anticipated, specifically sequence-to-sequence language models used in machine translation). . An end-to-end platform . The ambition for the platform and software tooling was not just to handle the smaller pieces of the training and deployment cycle, but rather to tackle the big overarching abstractions in a single approach. This of course contained some baked-in assumptions about how users would use TFX as well as what I’d say were quasi-philosophical positions on how best to approach these various parts. The paper characterises these as ‘best practices’, but certainly there hasn’t been uniform acceptance of these. . I imagine the end-to-end part was as much an attempt to encourage engineers to think of the problem in this exact way. If you are handling all the pieces of the training cycle, it’s easier to be fast and iterate and do all the things we expect of a more agile process. . Continuous training and serving . TFX was built to handle the kinds of models where the use cases demanded the ability to continuously retrain models using large quantities of streaming data. This is almost certainly not the norm, but for a company like Google I can understand that this would have been a key consideration if they wanted adoption of the tool across different teams. . In this way, certain scenarios (for example the Google Play Store case study outlined in the paper) saw a continuous retraining of models as more users used the service as well as new apps continued to be uploaded to the Play Store. If you have this kind of engineering need, and if you need to keep latency to certain boundaries (in the tens of milliseconds), it makes complete sense to have this whole structure that allows this to take place. Reading the specific example, it’s a pretty amazing feat, handling all that complexity underneath the surface. There must be many hundreds of other such services which similar levels of complexity concealed beneath the surface. . Reduce technical debt accrued via duplicated or ad hoc solutions . Prior to TFX and Sybil, it seems that there were many different approaches within Google, all addressing the same problem but in slightly different ways. . Having a series of best-practices built in to the service means that everyone can communicate about problems and about their issues using a shared language. It means that solutions discovered by one team can help other future teams. There’s a lot to be said for finding a solution that is sufficiently abstracted to work for many people. . Indeed, it seems this is the work of the MLOps community right now: find ways to abstract away problems that we all face, and to find the best abstractions that fit within the mental models we all have in our heads. The fact that there hasn’t been a grand convergence on a single solution indicates to me (at this current moment) that we haven’t found the right abstractions or flexibility within those abstractions. All the end-to-end tools handle much of the same stages of the model training and deployment process, but they each have opinions about the best practices to be employed along the way. (At least, that’s my current take on things). . Reliable serving models at scale . If you’re Google, you need to make sure that you aren’t serving garbage models to your users, or that inconsistencies in the input data aren’t polluting your retraining processes. At scale, even small mistakes compound really easily. . In the paper, two specific improvements are mentioned, tackling the challenges of low latency and high efficiency. The high efficiency example wasn’t entirely comprehensible for me, but what was clear was that they had very high expectations for how fast they wanted to make all parts of the pipelines and process. As above, the challenges of making it easy and fast to serve models — all of which had to happen in a reliable manner — was something that could be reused elsewhere in the company. TensorFlow Serving is what we get from their efforts in this regard. . Fast retraining with ‘warm-starting’ . For the specific challenge of retraining models with streaming data, engineers were finding that they couldn’t retrain the entire model from scratch, particularly with the scale of the training data that they had. Instead, they leveraged transfer learning (reframed here as ‘warm-starting’) to take all the hard work that had already been done, and adapting this pre-existing model with the new data. This makes a lot of sense, though the reframing with the new term is a bit less comprehensible to me. . Missing pieces . There are various pieces of what I think of as the machine learning workflow (as of 2021) which seem to be missing when I read this paper. Explainability or governance of models seems somewhat of an afterthought, if it is raised at all. I think the authors might argue that many of the checks and balances are made on the data ingestion phase, and that if all that checks out then this tackles a large piece of the problem surface area. . Similarly, there is relatively little said about model versioning and data versioning. Maybe coming at this from the present moment, where it seems obvious (with tools like DVC) that data versioning is a thing you’d want to care about. . As a general response, it seems clear that if you use TensorFlow to train your models, TFX might well be a pretty neat solution that handles many of your needs, particularly if you’re operating at serious scale. If you’re a researcher (perhaps using PyTorch) with less of those specific contextual needs, it seems less than certain that TFX would suit your purposes. . A couple of other interesting observations. The data observability and validation stage seemed to place a lot of emphasis on the automation of how pre-defined schemas might get updated. I’d be interested to see how that worked in practice. I understood the challenge that if there are too many error messages about dodgy data inputs, engineers are likely to grow inured to those alerts and maybe just ignore them. But at scale, I wonder about the risks of allowing automatic updates to those schema boundaries. . Again on the validation point, I found it interesting how the authors of the paper said that users of TFX internal to Google found the option to enable this was actually a hard sell unless or until the team had experienced some kind of failure connected to poor data validation. The TFX team ended up turning on the validation parts of the pipeline by default instead of assuming that users would choose to do so manually. . I wasn’t active in the field in 2017, so it’s hard for me to be able to reconstruct exactly how prescient or not this paper was in some of its diagnoses of the problem. It doesn’t seem that TFX was the total solution that perhaps it was pitched as being, but nonetheless it seems an important engineering achievement for Google. .",
            "url": "https://www.mlops.systems/tfx/tensorflow/google/mlops/2021/09/11/tfx-paper.html",
            "relUrl": "/tfx/tensorflow/google/mlops/2021/09/11/tfx-paper.html",
            "date": " • Sep 11, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Managing Python Environments with pyenv and pipenv",
            "content": "It’s hardly news that that managing multiple versions of Python in a development environment is hard. Adding in dependency management on top of that makes everything harder. .",
            "url": "https://www.mlops.systems/python/2021/09/10/python-environments.html",
            "relUrl": "/python/2021/09/10/python-environments.html",
            "date": " • Sep 10, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Retrieval Practice with fastai chapters 1 and 2",
            "content": "Retrieval practice is when you actively try to remember something as a way of making sure that you learn it well. (Read more about it here). Today I did that with the dogs vs cats example that the first two chapters cover. . We start with installing the fastai library and importing everything from the vision library. This was hard to remember since the pattern of .all and importing * is not something I&#39;ve seen much in Python imports. . from fastai.vision.all import * . Then we create the simple function that will be used to classify the images. The pets dataset relies on the first letter of the filename for knowing whether a picture is of a cat or a dog. So the function is pretty simple: it checks whether the first letter is a capital letter or not. . The simple assert testing was a little trick that I saw mentioned somewhere this past week. It&#39;s not a full-fledged test suite, but it&#39;s at least the start of something that can later be refactored out into whatever takes its place, be it using pytest or something else. . def is_cat(string): return string[0].isupper() assert is_cat(&quot;abs&quot;) == False assert is_cat(&quot;Abs&quot;) == True . Now we have to import the data for the files and apply whatever custom transforms we want applied to them. . I had certainly forgotten that untar_data was a method when I started out with this. I also am not familiar enough with the pathlib library as I need to be. . It&#39;s interesting that we actually don&#39;t even need to do any of the batch transformations on the images in order to get excellent results. I imagine that&#39;s because the task is so close to that of the original resnet architecture. . path = untar_data(URLs.PETS)/&#39;images&#39; dls = ImageDataLoaders.from_name_func(path, get_image_files(path), label_func=is_cat, item_tfms=Resize(224)) . Then it&#39;s all about passing the dataloaders object into the cnn_learner function, along with our desired architecture. We also set the error_rate (i.e. 1 minus the accuracy at making predictions) as the metric we&#39;ll see displayed in the output. . learner = cnn_learner(dls, resnet34, metrics=error_rate) # fine-tune the model learner.fine_tune(5) . epoch train_loss valid_loss error_rate time . 0 | 0.140326 | 0.019799 | 0.008119 | 00:19 | . epoch train_loss valid_loss error_rate time . 0 | 0.046906 | 0.021923 | 0.006089 | 00:24 | . 1 | 0.041144 | 0.009382 | 0.004060 | 00:25 | . 2 | 0.028892 | 0.004109 | 0.002030 | 00:25 | . 3 | 0.008950 | 0.002290 | 0.001353 | 00:25 | . 4 | 0.004486 | 0.002822 | 0.001353 | 00:25 | . And here you can see the results. In this training run, with 5 epochs, we were able to achieve a 99.9% accuracy. Not bad! . learner.show_results() .",
            "url": "https://www.mlops.systems/fastai/jupyter/computervision/2021/09/10/chapter1and2recall.html",
            "relUrl": "/fastai/jupyter/computervision/2021/09/10/chapter1and2recall.html",
            "date": " • Sep 10, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "How to set a Jupyter notebook to auto-reload external libraries",
            "content": "The code to insert somewhere into your Jupyter notebook is pretty simple: . %load_ext autoreload %autoreload 2 . When you’re working on an external library or piece of Python code outside the contents of your notebook, this snippet will make sure that the updated functions and constants will always be available in their most-recently edited state. .",
            "url": "https://www.mlops.systems/jupyter/2021/09/09/auto-reload-external-libraries.html",
            "relUrl": "/jupyter/2021/09/09/auto-reload-external-libraries.html",
            "date": " • Sep 9, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "A Baseline Understanding of MLOps",
            "content": "Next week I’m due to begin a job as a Machine Learning Engineer at a company that works in the MLOps field. It’s a new field to me. I’ve read a good deal on it in recent weeks, and listened to a few dozen episodes of the MLOps.community podcast, but I still very much consider myself a beginner in the space. To that end, I thought it worth clarifying my understanding of what MLOps is all about, the problem it is trying to solve, and where I see the opportunity there. . A top-down explanation is probably the best way to think of what we’re doing when we talk about ‘doing MLOps’: we’re doing all the things which make it possible to train, deploy and use machine learning models in the real world or ‘in production’. It isn’t just a series of tools, but also a series of best practices and a community that is constantly learning and iterating to improve. . The kinds of things that you can do with machine learning models are incredibly diverse, so it stands to reason that the people who operationalise all these models have quite varied opinions and approaches to how best to do this. Even the deployment scenarios are pretty different and involve different technology stacks. There is an idea of a ‘full stack machine learning engineer’, which apparently means someone who just knows everything across the board; I hope to be able to delve into some of these areas and the key technologies represented in each space in due course on this blog. .",
            "url": "https://www.mlops.systems/mlops/2021/09/08/baseline-mlops-understanding.html",
            "relUrl": "/mlops/2021/09/08/baseline-mlops-understanding.html",
            "date": " • Sep 8, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Training a classifier to detect redacted documents with fastai",
            "content": "I am working my way through the fastai course as part of an online meetup group I host.1 . This week we finished the first and second chapters of the book, during which you train a model that can recognise if an image contains a cat or a dog. Later on, you train another model that distinguishes between different types of bears (‘grizzly’, ‘black’ and ‘teddy’). . Jeremy Howard, who is teaching the course, then prompts you to take what you learned and apply it to something that has meaning for you. (This is something that most of those who’ve found any success with the course emphasise repeatedly.) . I decided to work on something adjacent to my previous life / work, where I knew there was some real-world value to be gained from such a model. I chose to train an image classifier model which would classify whether a particular image was redacted or not. . The Problem Domain: Image Redaction . Under the Freedom of Information Act (FOIA), individuals can request records and information from the US government.2 This is one collection of some of the responses to this requests, sorted into various categories. You can read, for example, responses relating to UFOs and alien visits here. . Quite often, however, these images are censored or redacted. . . Knowing that this practice exists, I thought it might be interesting to train a model that could recognise whether a particular page contained some kind of redaction. This wasn’t completely in line with what we covered during the first two chapters; I wasn’t sure if the pre-trained model we used would work for this data set and use case. . It could be useful to have such a tool, because FOIA responses can sometimes contain lots of data. In order to prepare a request for more data, you might want to be able to show that even though you were sent thousands of pages, most of those pages contained redactions and so were effectively useless. . In the ideal vision of this tool and how it would work, you could run a programme out of a particular directory and it would tell you how many pages (and what proportion) of your PDF files were redacted. . Getting the Data . The first thing I did to gather my data was to download the PDF documents available on this site. I knew that they contained examples of redactions in FOIA documents. I used Automator to split the PDF files up into individual images.3 My Automator script did some downsampling of the images as part of the process, so the images were resized to something that wasn’t prohibitively large to use for training. . Note that this stage and the next was done on my local machine. A CPU was enough for my purposes at this point, though probably I’ll want to eventually port the entire process over to a single cloud machine to handle things end-to-end. . At the end of the splitting-and-resizing process, I had a little over 67,000 images (of individual pages) to train with. . Labelling the images with Prodigy . I had used Explosion.ai’s Prodigy data labelling tool in the past and so already had a license. The interface is clean and everything works pretty much as you’d hope. I had some teething issues getting it all working, but Prodigy co-creator Ines helped me work through those queries and I was up and running pretty quickly. . . It took about three hours to annotate some 4600+ images. Then I could export a .jsonl file that contained the individual annotations for whether a particular image contained a redaction or not: . . From that point it was pretty trivial to parse the file (using the json-lines package), and to resize the images down further in order to separate redacted from unredacted: . import json_lines from PIL import Image from pathlib import Path def save_resized_image_file(location_path): basewidth = 800 img = Image.open(record[&#39;image&#39;]) wpercent = (basewidth / float(img.size[0])) hsize = int((float(img.size[1]) * float(wpercent))) img = img.resize((basewidth, hsize), Image.ANTIALIAS) img.save(location_path) path = &#39;/my_projects_directory/redaction-model&#39; redacted_path = path + &quot;/redaction_training_data/&quot; + &quot;redacted&quot; unredacted_path = path + &quot;/redaction_training_data/&quot; + &quot;unredacted&quot; with open(path + &quot;/&quot; + &quot;annotations.jsonl&quot;, &quot;rb&quot;) as f: for record in json_lines.reader(f): if record[&quot;answer&quot;] == &quot;accept&quot;: save_resized_image_file(Path(redacted_path + &quot;/&quot; + record[&#39;meta&#39;][&#39;file&#39;])) else: save_resized_image_file(Path(unredacted_path + &quot;/&quot; + record[&#39;meta&#39;][&#39;file&#39;])) . Transferring the data to Paperspace with magic-wormhole . Once I had the two directories filled with the two sets of images, I zipped them up since I knew I’d want to use them on a GPU-enabled computer. . I used magic-wormhole to transfer the files over to my Paperspace Gradient machine. The files were only about 400MB in size so it took less than a minute to transfer the data. . Again, ideally I wouldn’t have this step of doing things locally first. I could certainly have done everything on the Paperspace machine from the very start, but it would have taken a bit of extra time to figure out how to process the data programatically. Moreover if I was using JupyterLab I could then use Prodigy from within my notebooks. . Using the labelled data in our training . The process of ingesting all our data (labels and raw images) is pretty easy thanks to the fastai library’s convenience classes and layered structure. We’re using the DataBlock class instead of ImageDataLoaders for extra flexibility. . path = Path(&#39;redaction_training_data&#39;) foia_documents = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(224)) dls = foia_documents.dataloaders(path) foia_documents = foia_documents.new( item_tfms=Resize(224, method=&#39;pad&#39;, pad_mode=&#39;reflection&#39;), batch_tfms=aug_transforms(max_zoom=1)) dls = foia_documents.dataloaders(path) learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(10) . The images get resized to 224x224 pixels, since this is the size that the resnet architecture expects. Since we have a good deal of labelled data, I’m comfortable using 80% of that data to train the model and the remaining 20% against which to validate. . I train it for 10 epochs as I don’t appear to reach a point where I’m overfitting. As you can see from this image, we reach an accuracy of around 96%. . . Experimenting with augmentations . Initially I had been using the RandomResizedCrop transformation on the data, but I was reminded by someone in our group (Jason) that cropping or zooming our images wouldn’t be useful since it is possible that both of those transformations would remove the small part of the image where a redaction was to be found. . In the end, I went with some settings that made sure we weren’t zooming into images or rotating them such that parts would be missing. I think there’s probably more I could squeeze out of the documentation here, particularly so that I’m not limiting myself too much in the arguments that I’m passing in. . I chose the pad method with the reflection mode since this seemed to give the best results. The zeros mode was too close to an actual redaction (i.e. a black box on the image) so I ruled that out pretty early on. . Experimenting with different architectures . The course mentions that architectures with more layers do exist. I saw that the next step up from resnet18 was resnet50. I’m certainly in the territory where I’m just turning knobs in the hope of seeing some kind of result, but I thought it was maybe worth a comparison. . The danger with having more layers (and thus more parameters) is that the model is more likely to overfit. The training process also takes much longer to execute: 44 seconds per epoch compared to 21 seconds with resnet18. It didn’t seem to measurably improve the accuracy. The best results I was able to get were still around 95%, give or take a percent or two. It seems that the real improvements are to be found in the pre-processing or augmentation stage, rather than from choosing an architecture with more layers. . Hosting the model with MyBinder . Chapter two of the course book goes into a decent amount of detail of some of the tradeoffs and issues around model deployment. Part of the exercise is to not only train a model on your own data, but go through the steps to get the model hosted online. . Using MyBinder and the voila library, alongside instructions from the book and the forums, I managed to get my model deployed. If you visit this address you’ll see an interface where you should first upload an image — i.e. a screenshot of a document. When you click ‘classify’, you’ll then see a prediction of whether the image is redacted or not, as well as the confidence/probability that that prediction is true. . . Next steps . I’m at the point in the course where I know enough to be dangerous (i.e. train models), but I don’t know how to improve them from here. Some ideas I had for ways to improve the model’s accuracy: . better augmentation choices — it’s possible that I’ve misconfigured some argument or made the wrong choices in which augmentations should be applied. | more labelled data — this one is pretty easy to fix, but I probably shouldn’t continue down this route unless I know it’s really going to help. I’m not in a position right now to be able to judge how much it’d help me. | different redaction types — currently I have a single ‘redacted’ vs ‘unredacted’ category choice, but in reality there are several different types of redaction in the data set: some have handwritten redactions, others are square computerised boxes, and there are a couple of other types as well. I wonder whether I should train the model to recognise the different types, and then to combine those together as a ‘redacted’ set of categories. (I may be thinking about this wrong). | . Otherwise and for now, I’m happy with where I managed to reach with this model. I have some other ideas for how to keep going with exploring this data set. For example, even better than a slightly dumb classification model would be to have a segmentation model that was able to determine what percentage of the pixels or total area of the page that were redacted. With a reasonably accurate segmentation model of that kind, we’d then be able to provide really interesting metrics on what percentage of the information provided was redacted. . I will probably also want to go back and add in the earlier processing steps into the notebook so that things are much closer to being an ‘end-to-end’ solution. . Footnotes . You can find our thread in the fastai forum here. &#8617; . | Other countries have variations of this law, like this from the United Kingdom. &#8617; . | I realise that there is a programatic way to do this. At this early stage in the project, I was more eager to get going with the labelling, so I took the easy path by using Automator. &#8617; . |",
            "url": "https://www.mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html",
            "relUrl": "/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html",
            "date": " • Sep 6, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Alex. . I am a software engineer based in London, UK. I recently built Ekko, an open-source framework allowing developers to easily add realtime infrastructure and in-transit message processing to web applications. . I have multiple years of experience in the Ruby and JavaScript ecosystems and am comfortable working with Go, PostgreSQL, AWS cloud infrastructure and Docker. . I have a PhD in History and authored several books based on my research work in Afghanistan. . I have a long-standing blog that I will combine with this one at some point, but for now I intend to post technical posts here. .",
          "url": "https://www.mlops.systems/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://www.mlops.systems/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}