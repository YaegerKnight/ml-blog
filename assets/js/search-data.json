{
  
    
        "post0": {
            "title": "A painless way to create an MVP demo using computer vision models",
            "content": "üö¶ Motivation . After the second class of the fastai course, we‚Äôre encouraged to create mini-projects that result in models we can deploy online. Deployment is a huge field with its own complexities, of course, but having an option to get something out in the world that‚Äôs visible and usable is extremely useful. . In this post, I will walk you through how I built a super quick MVP of my redacted document detector project. I used: . fastai to classify and extract redacted pages extracted from PDFs | icevision (@ai_fast_track) to detect the redacted areas | HuggingFace Spaces (with Gradio and Streamlit) to deploy my MVP | . The post shows how I went about thinking through the task, showcasing some examples of small prototypes I built along the way, including the final stage where I built: . an app including everything that would be needed by a final ‚Äòdeployed‚Äô use case of my model | two models working in tandem in the same app (one classification, one object detection) | optional PDF generation of items detected by the model (!) | . I also explore why you might want to have a minimal deployed version of your application in the first place! . üêæ Step by step, iteration by iteration . This week I chose to use my previous work on redacted images to leverage a dataset I‚Äôd previously collected. I wanted to showcase something useful and interesting and I ended up slightly blocked as to what I was going to build. After discussing it with the study group briefly, I was reminded not to try to bite off too much: start small with the smallest possible next version of what you want, and then continue from there. . Since I already had a large dataset of redacted and unredacted images (extracted from PDF documents available online), I used this to train a classification model that could tell whether a page contained redactions or not. . With that model exported, it was then easy to get a simple Gradio app demo up and running, particularly with the suggestions in Tanishq Abraham‚Äôs really useful tutorial blogpost. . It‚Äôs an easy step to go from having a Gradio app deployed to then hosting that same demo as a Huggingface Space, so I then did that. You can access the demo here at strickvl/fastai_redaction_classifier. . . At this first stage I had the exported model itself uploaded inside the Spaces repository, but this useful blog by Omar Espejel showed how I could just upload my model directly to the Huggingface model hub. Instead of calling learn.export(&#39;model.pkl&#39;) and uploading the model file itself, I could just run the following code after authentication: . from huggingface_hub import push_to_hub_fastai repo_id = &quot;MY_USERNAME/MY_LEARNER_NAME&quot; push_to_hub_fastai(learner=learn, repo_id=repo_id) . My model lives here on the Huggingface model hub and can be directly saved or just used via the hosted Inference API. . ‚ö°Ô∏è Using the inference API for more flexibility . Buoyed on by Tanishq‚Äôs blog and the workflow we‚Äôd seen in the lecture that week, I thought it might be worth running my inference requests through the HTTP API instead of letting Huggingface handle all that. . Thanks to a really simple and comprehensible example made by @Nuvic I was quickly able to get something up and running. The forked source code is available here and the main website where you can try out the tool is here: https://strickvl.github.io/predict_redaction_classification/. . If you search for ‚Äòredacted document‚Äô images and save one of them do your local computer you can use those to try it out. It uses simple Javascript code to pass the image you upload into the inference API using a simple HTTP request. It parses the results and displays them as shown here: . . While the demo gives a sense of the model‚Äôs capabilities, in reality you would probably not find it very helpful to use a web app that required you to feed a document‚Äôs pages to it one by one. I started to think about a more complex application where you could upload a PDF and it would split the PDF for you and do all the inference behind the scenes. . üöÄ Building an MVP of a redaction detection application . I spent a brief half-hour considering deploying a simple Flask web app hosted somewhere for free before realising I didn‚Äôt even need to go that far to create a proof of concept that would have the required functionality. I returned back to Huggingface Spaces hoping that I‚Äôd be able to build everything out. . You can access the demo / MVP app that I created here: https://huggingface.co/spaces/strickvl/redaction-detector . . This MVP app runs two models to mimic the experience of what a final deployed version of the project might look like. . The first model (a classification model trained with fastai, available on the Huggingface Hub here and testable as a standalone demo here), classifies and determines which pages of the PDF are redacted. I‚Äôve written about how I trained this model here. | The second model (an object detection model trained using IceVision (itself built partly on top of fastai)) detects which parts of the image are redacted. This is a model I‚Äôve been working on for a while and I described my process in a series of blog posts. | . This MVP app does several things: . it extracts any pages it considers to contain redactions and displays that subset as an image carousel. It also displays some text alerting you to which specific pages were redacted. | if you click the ‚ÄúAnalyse and extract redacted images‚Äù checkbox, it will: | pass the pages it considered redacted through the object detection model | calculate what proportion of the total area of the image was redacted as well as what proportion of the actual content (i.e. excluding margins etc where there is no content) | create a PDF that you can download that contains only the redacted images, with an overlay of the redactions that it was able to identify along with the confidence score for each item. | . Converting a Gradio app over to Streamlit . I was curious about the differences between the main two options enabled by Huggingface Spaces, so I then worked a little on converting my Gradio app to a Streamlit app. The process of conversion was fairly easy for the most part; the only difference is the style of programming expected by Streamlit: . Streamlit is a less declarative style of creating your app. It runs your code from top to bottom, rendering whatever elements you specify. | This seems to result in more verbose code (e.g. compare this with this). | . There are two easy ways to deploy a Streamlit app: either host it natively on Streamlit itself, or host it on Huggingface Spaces. The advantage of hosting natively on Streamlit is that you essentially have what looks and feels like a custom website that is 100% your application. In the end, I didn‚Äôt go down this route for two reasons: . Hosting via Huggingface Spaces keeps the connection between your demo app and your username. You can click through to view all of my demos and applications here, for example. On Streamlit there is currently no concept of a user‚Äôs portfolio. If you‚Äôre trying to showcase your work, Huggingface Spaces is the clear winner in this regard. | Hosting on Streamlit seems to have restrictive memory constraints. I frequently ran into restrictions on the machine that was running my application and would quite often be encouraged to reboot my app, clearing its cache, and instructed to refer to docs on how to make my application more efficient. The docs were useful, but I ran into issues using the Streamlit cache (the main solution offered) because of the models I was using. Luckily, Huggingface Spaces‚Äô backend instances seem far more generous in terms of resources. For small / trivial apps not doing much you‚Äôll be fine with Streamlit, but for anything more involved there‚Äôs more of a decision to be made. | I didn‚Äôt convert all the various parts of my Gradio app over to work on Streamlit ‚Äî in particular extraction of images and display as a carousel was non-trivial ‚Äî but you can get a sense of the flexibility with this image: . . (Alternatively, you can try it out over on Huggingface Spaces here.) . You can see that I was able to insert a chart to display the proportion calculations. This is much more pleasant than the pure text version. Streamlit‚Äôs documentation is pretty great and their basic ‚ÄòGet started‚Äô tutorial should indeed be your first port of call. . ü§î Lessons learned . . In this post you learned: . 1Ô∏è‚É£ to start with simple prototypes . 2Ô∏è‚É£ how to easily deploy fastai models on Huggingface Spaces and the Hub and . 3Ô∏è‚É£ that you can create functional MVP demos of real products and applications . I was ‚Äî and continue to be ‚Äî surprised that the free Huggingface Spaces environment has no problem running all this fairly compute-intensive inference on their backend. (That said, if you try to upload a document containing dozens or hundreds of pages and you‚Äôll quickly hit up against the edge of what they allow.) . I became very familiar with the Gradio interface docs while creating this app and was impressed by how customisable the final application could be. You don‚Äôt have as much freedom as a web application written from scratch, but you still do have a lot of freedom. . üìê When to use Gradio: . if you have a simple use case that you want to highlight | if your inputs and outputs are clearly defined | if you have a single model to showcase | if you want to get something quickly deployed | . üåä When to use Streamlit: . if your use case is more interactive or less simple than just basic input-then-output | if you want more control on how your demo application is displayed | if you enjoy a more imperative style of programming | . Given how much inference is going on behind the scenes, I‚Äôm surprised that these applications run as fast as it does. For a document with 4 or 5 redacted pages, it takes around 10 seconds to do all the steps described above. 10 seconds is still far too long for a scenario where you wanted to run inference over millions of pages, but in that scenario you wouldn‚Äôt be manually uploading them on a web app either. . It‚Äôs extremely gratifying to have these kinds of tools available to use for free, and really exciting that you get to build out prototypes of this kind after just two weeks of study on the fastai course. .",
            "url": "https://mlops.systems/fastai/computervision/redactionmodel/tools/2022/05/07/redaction-mvp-huggingface.html",
            "relUrl": "/fastai/computervision/redactionmodel/tools/2022/05/07/redaction-mvp-huggingface.html",
            "date": " ‚Ä¢ May 7, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "How my pet cat taught me a lesson about validation data for image classification",
            "content": "I‚Äôm participating in the latest iteration of the fastai course as taught by Jeremy Howard. This past week we got a very high-level overview of some of the ways deep learning is proving very powerful in solving problems as well as how we can use its techniques to fairly quickly get great results on image classification problems. . I‚Äôve done the earlier parts of the course before, so some of these demonstrations were less mind-blowing than the first time I saw them. For this iteration of the course, Jeremy showcased a Kaggle notebook which trains a model to distinguish whether an image is of a bird or not. . Last time I did the course, I trained an image classifier model to distinguish whether an image was redacted or not to around 95% accuracy. (This actually was the genesis of my larger redaction object detection project that I‚Äôve been blogging about for the past few months.) . The key ingredients: what goes into a model? . The course teaches things top-down, so we start off with both the practical experience of training state-of-the-art models as well as the overall context to what goes into these high-level functions. These pieces include: . your input data ‚Äî this style of programming differs from traditional software engineering where your functions take data in order to ‚Äòlearn‚Äô how to make their predictions | the ‚Äòweights‚Äô ‚Äî when we‚Äôre using pre-trained models, you can think of these as an initial set of variables that are already pretty useful in that configuration and can do a lot of things. | your model ‚Äî this is what you‚Äôre training and, once trained, you can think of it as a function in and of itself that takes in inputs and outputs predictions. | the predictions ‚Äî these are the guesses that your model makes, based on whatever you pass in as inputs. So if you pass in an image of a cat to a model (see below), the prediction could be whether that cat is one particular kind or another. | your ‚Äòloss‚Äô ‚Äî this is a measure of checking how well your model is doing as it trains. | a means of updating your weights ‚Äî depending on how well (or badly) the training goes, you‚Äôll want a way of updating the weights so that each time it gets a bit better at optimising for whatever you‚Äôve set up your model to do. In lesson one we learn about stochastic gradient descent, a way of optimising and updating these weights automatically. | your labels ‚Äî these are the ground truth assertions that get used to determine how well the model is doing as it trains. | transformations &amp; augmentations ‚Äî more on this will come in lesson two, but these allow you to squeeze more value out of your data. This is especially valuable when you‚Äôre fine-tuning a model and don‚Äôt have massive amounts of data to use for training. | . Represented in code, the classic fastai example where you train a model to distinguish between cats and dogs is as follows: . from fastai.vision.all import * path = untar_data(URLs.PETS)/&#39;images&#39; def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func( path, get_image_files(path), valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) learn = vision_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(1) . This small code snippet contains all the various parts just mentioned. The high-level API and abstractions that fastai provides allows you to work with these concepts in a way that is fast and flexible, though if you need to dive into the details you can do so as well. . Image classification isn‚Äôt just about images . One of the parts of the first chapter I enjoy the most is the examples of projects where image classification was applied to problems or scenarios where it doesn‚Äôt first appear that the problem has anything to do with computer vision. . We see malware converted into images and distinguished using classification. We see sounds in an urban environment converted into images and classified with fastai. In the study group I host for some student on the course, one of our members presented an initial proof of concept of using images of music to distinguish genre: . Saturday evening side-project: Trained a baseline ML model to classify audio files to identify their music genre using @fastdotai based on a kaggle dataset.Acheived only 50% accuracy, probably because problem is hard. Next job is to check what @DienhoaT has done to win a GPU. pic.twitter.com/EahvgtYBDL . &mdash; Kurian Benoy (@kurianbenoy2) April 30, 2022 I like the creativity needed to think of how to turn problems and data into a form such that they can become computer vision problems. . My own efforts: classifying my cat . True story: a few years ago my cat escaped from the vet and a reward was mentioned for anyone who found our cute ginger cat. Throughout the course of the day, the vets were perplexed to see people coming in with random ginger cats that they‚Äôd found in the neighborhood, but none of them were ours! With this iteration of the course, therefore, I was curious to try out this simple but slightly silly example and see how well a deep learning model could do at recognising distinguishing Mr Blupus ‚Äî don‚Äôt ask! ‚Äî from other random photos of ginger cats. . Training the model was pretty easy. Like any cat owner, I have thousands of photos of our cat so an initial dataset to use was quick to assemble. I downloaded a few hundred random ginger cat photos via DuckDuckGo using some code Jeremy had used in his bird vs forest Kaggle notebook. A few minutes and ten epochs later, I had achieved 96.5% accuracy on my validation data after fine-tuning resnet50! . Little Saturday afternoon side-project: training an image classification model using @fastdotai to distinguish my cat (&#39;Mr Blupus&#39; -- don&#39;t ask!) from other random ginger cat photos.Achieved 96.5% accuracy on the validation set within a few minutes! pic.twitter.com/by5ZlM0Kkp . &mdash; Alex Strick van Linschoten (@strickvl) April 30, 2022 After the initial excitement died down, I realised that the result was probably an illusion. Our cat is an indoor cat and we have a relatively small house. Couple that with the fact that the backdrops to the photos of Mr Blupus are relatively distinctive (particular kinds of sheets or carpets) and it seems pretty clear that the model wasn‚Äôt learning how to identify our cat, but rather it was learning how to distinguish photos of our house or our carpets. . ‚òπÔ∏è . Luckily, chapter one gets into exactly this problem, showing an example of how exactly this validation issue can give you a false sense of confidence in your model. When I evaluated my model on the validation data it wasn‚Äôt a fair test, since in all likeliness may model had already seen a similar backdrop to whatever was found inside the validation set. . I discussed this when I presented this to those at the study group / meetup yesterday and we agreed that it‚Äôd be best if I held out some settings or locations from the training entirely. I took 30 minutes to do that in the evening and had a third ‚Äòtest‚Äô dataset which consisted of 118 images of our cat in certain locations that the model wasn‚Äôt trained on and thus couldn‚Äôt use to cheat. I added a few more photos to the training data so that there were enough examples from which to learn. . . I was supposedly getting 98% accuracy now, but I knew that number to be false. I then needed to figure out how to get the accuracy for my held-out test set. With a lot of help from Francesco and a really useful blogpost on doing batch inference with fastai, I first got the predictions for my test data: . test_files = [fn for fn in sorted((Path(&quot;/path/to/test_set_blupus_photos&quot;)).glob(&#39;**/*&#39;)) if fn.is_file()] test_dl = learn.dls.test_dl(test_files) preds, _ = learn.get_preds(dl=test_dl) . I then created a tensor with the ground truth predictions for my test set and compared them with what my model had predicted: . gts = torch.tensor([0 for _ in range(118)]) accuracy = (gts == preds.argmax(dim=1)) . At this point, getting the final accuracy was as simple as getting the proportion of correct guesses: . sum([1 for item in accuracy if item]) / len(preds) . This gave me an accuracy on my held-out test set of 93.2% which was surprisingly good. . I half wonder whether there is still some cheating going on somehow, some quality of the photos or the iPhone camera I used to take them that is being used to distinguish the photos of my cat vs other ginger cats. . Nevertheless, this was a useful lesson for me to learn. I realised while working with the tensors in the final step above that I‚Äôm not at all comfortable manipulating data with PyTorch so luckily that‚Äôll get covered in future lessons. . UPDATE: . Following some discussion in the fastai forums, it was suggested that I take a look at Grad-CAM in chapter 18. This is a technique to visualise the activations which allows you to see which parts of the image it is paying the most attention to (sort of). I ran the code using a sample Blupus image and this was the result. I don‚Äôt understand how most (any?) of this works, but it was really cool to have a working result of sorts nonetheless! . .",
            "url": "https://mlops.systems/fastai/computervision/2022/05/02/pet-cat-image-classifier-fastai.html",
            "relUrl": "/fastai/computervision/2022/05/02/pet-cat-image-classifier-fastai.html",
            "date": " ‚Ä¢ May 2, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "How to trust the data you feed your model: alternative data validation solutions in a computer vision context (part 3)",
            "content": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.) . The previous two posts in this series have made the case for why you might want to consider adding a Great Expectations step or stage to your computer vision project, particularly once it becomes something you‚Äôre going to want to iterate on a few times. . This post begins by showcasing how you can use Evidently‚Äôs open-source library to calculate and visualise comparisons between your data. I list some of the lighter alternatives to Great Expectations and Evidently, concluding with some thoughts on when you might use it as part of your computer vision pipeline. . TL;DR: Alternatives for data validation using Python . üõ† Data validation tools come in many flavours, from full-featured libraries like Great Expectations down to the humble assert statement in Python. . | ‚ö†Ô∏è The tool you choose should be appropriate to your particular use case and situation. You might not need or want to add a large dependency or take on extra code / project complexity, in which case there are alternative options available to you. . | ‚è∞ You‚Äôll also want to think about when you‚Äôre doing your validation. Two key moments stand out for machine learning projects: when you‚Äôre ingesting data prior to training or fine-tuning a model, and at the moment where you‚Äôre doing inference on a trained model. . | üìÉ For my project, I‚Äôm using a variety of tools as part of my process because I‚Äôve found it gives me confidence in the predictions my model is making and it gives me freedom to experiment and iterate, without needing to also worry that I‚Äôm silently breaking something with downstream effects on my model performance. . | . Alternatives: Using Evidently for drift detection . I‚Äôve previously written about why Evidently is a great tool to use for drift detection and data monitoring over on the ZenML blog. At its core, Evidently takes two chunks of data and compares them. The statistical comparisons going on under the hood are quite sophisticated, but as an interface to be used it is extremely trivial to get going. . In the case of my redaction project data, I did the work of transforming my annotation and image metadata into Pandas DataFrames for Great Expectations already, so using it with Evidently at this point is trivial: . from evidently.dashboard import Dashboard from evidently.dashboard.tabs import DataDriftTab from evidently.pipeline.column_mapping import ColumnMapping real_annotations = main_annotations_df[[&#39;area&#39;, &#39;category_name&#39;, &#39;top_left_x&#39;, &#39;top_left_y&#39;, &#39;width&#39;, &#39;height&#39;, &#39;orientation&#39;]] easy_synth_annotations = easy_synth_annotations_df[[&#39;area&#39;, &#39;category_name&#39;, &#39;top_left_x&#39;, &#39;top_left_y&#39;, &#39;width&#39;, &#39;height&#39;, &#39;orientation&#39;]] hard_synth_annotations = hard_synth_annotations_df[[&#39;area&#39;, &#39;category_name&#39;, &#39;top_left_x&#39;, &#39;top_left_y&#39;, &#39;width&#39;, &#39;height&#39;, &#39;orientation&#39;]] column_mapping = ColumnMapping( numerical_features=[&quot;area&quot;, &quot;width&quot;, &quot;height&quot;, &#39;top_left_x&#39;, &#39;top_left_y&#39;], categorical_features=[&quot;category_name&quot;, &#39;orientation&#39;], ) drift_report = Dashboard(tabs=[DataDriftTab()]) drift_report.calculate(real_annotations, hard_synth_annotations, column_mapping=column_mapping) drift_report.save(&quot;reports/my_report.html&quot;) . In this code, I‚Äôm comparing between the real (i.e. manually annotated) annotations and the ‚Äòhard‚Äô synthetic annotations that I created (and blogged about recently). I choose the columns I care about, tell Evidently which columns are numerical vs categorical features and save the report. (I can also display the report directly within a Jupyter notebook.) When I open the report, I see this: . . You can unfold the graphs to dive into the details for specific features, as in the following example where I take a look at the orientation of my annotations and see the difference between my manual annotations and the synthetically generated ‚Äòhard‚Äô batch: . . It doesn‚Äôt surprise me too much that we have this disparity, since the only annotations that are portrait in the synthetically-generated set are those for the content box around the whole page. All the rest are landscape, and that‚Äôs by design. (Note: you can make the comparisons using different statistical tests depending on your use case. I‚Äôm told that the next Evidently release will increase the number of available options for this.) . I can repeat the same test for the image DataFrame. I‚Äôve included some metadata for each image such as how many annotations are associated with the image, or how many redaction vs content annotations are associated and so on. The code is basically the same, except now taking into account the different columns and their types: . # comparing between real images and hard_synth images column_mapping = ColumnMapping( numerical_features=[&quot;area&quot;, &quot;width&quot;, &quot;height&quot;, &#39;annotation_count&#39;, &#39;content_annotation_count&#39;, &#39;redaction_annotation_count&#39;, &#39;area&#39;, &#39;file_size_bytes&#39;], categorical_features=[&#39;orientation&#39;, &#39;format&#39;, &#39;mode&#39;], ) drift_report = Dashboard(tabs=[DataDriftTab()]) drift_report.calculate(main_images, hard_synth_images, column_mapping=column_mapping) drift_report.save(&quot;reports/my_report-real-vs-hard-images.html&quot;) . And we get this report: . . You can immediately see how certain things like the number of annotations and the number of redactions in an image was a bit different when comparing the two. We also seem to have a far more even distribution of file sizes in the synthetically generated images and that makes sense since that was essentially randomly determined. . Note that all the data that goes into making these reports can be accessed programatically as a Python object or JSON through Evidently‚Äôs Profile feature, which is probably what you‚Äôre going to want when assessing for drift as part of a continuous training / continuous deployment cycle. . If you change just a few things once more, you get a really useful data quality report showing distributions, correlations, and various other features of your data at a single glance: . # profiling data quality from evidently.dashboard.tabs import DataQualityTab quality_dashboard = Dashboard(tabs=[DataQualityTab()]) quality_dashboard.calculate(main_images, hard_synth_images, column_mapping=images_column_mapping) quality_dashboard.save(&quot;reports/quality-report.html&quot;) . You can get an idea of the report that it produces in the following screen recording from my browser: . . As a place to get started with understanding a dataset, this is a pretty nice visualisation and report to have in your toolkit, but even after immersion in your data it can be useful to take a step back with something like this data quality overview. For instance, it reveals quite clearly how the average number of annotations in my manually annotated dataset is quite a bit lower than that of my synthetically generated examples. Of course, that was by intention, but it is nice to see that confirmed in the data. . Once you have your model ready, there are other reports that Evidently offers which perhaps I‚Äôll return to in a subsequent blogpost but for now I hope this has given you a flavour of the tool and how easy it is to get going with it. . (As a side-note, Evidently‚Äôs community is friendly, welcoming and filled with interesting people thinking about these issues. I find it a welcome breath of fresh air when compared with some other tools‚Äô forums or chat platforms, so it also has that going for it!) . Alternatives: some other options . With Evidently, we drifted a little into the ‚Äòvisualise your data‚Äô territory which wasn‚Äôt really the point of this post, but you can see how they combined clear visualisation with the statistical validation working underneath to calculate whether data was drifting. The following are some other tools I‚Äôve come across that might help you in validating data in a computer vision context. I haven‚Äôt found a use for them in my project, but it‚Äôs possible that they might gel with what you‚Äôre doing: . TensorFlow Data Validation (TFDV) ‚Äî This is a part of TensorFlow and tfx which uses schemas to validate your data. If you‚Äôre using TensorFlow, you might have heard of this and might even be using it already, but I don‚Äôt get the sense that this is often much recommended. I include it as it is a prominent option available to you. | Deepchecks ‚Äî Deepchecks is adjacent to what Great Expectations offers, albeit with an emphasis on the kinds of tests you might want to do for ML model training code. It has some features and documented use cases for computer vision (object detection and classification) but I haven‚Äôt used it myself. Feels like a tool worth keeping your eye on, however. (Works on Pandas dataframes and numpy arrays.) | pandera ‚Äî This is a statistical tool for validating data inside dataframes, and it overlaps quite a bit in its functionality with Great Expectations, particularly with the hypothesis testing functionality. Worth checking out. | Cerberus ‚Äî Offers a lightweight schema-based validation functionality for Python objects. | jsonschema ‚Äî similar in approach to Cerberus, above, this is a lightweight way to test your JSON files based on how they conform to a defined schema. Useful in the case of annotations files, perhaps, if you really want something minimal. | schema ‚Äî More of the same: a Pythonic way to validate JSON or YAML files based on schema. | assert ‚Äî We shouldn‚Äôt forget the humble assert statement, which I have sprinkled in various places within my code where it makes sense to make sure that data flowing through conforms to whatever implicit or explicit contracts exist. | . I mention these various options not to suggest that you should use them all, but rather to state that you have options ranging the whole spectrum of complexity and dependency. . When to do data validation in your project . Regularly! I‚Äôve written previously about how you can think about data validation as testing for your data. Just like many (most?) engineering teams run their tests every time you add a new commit to the codebase, it‚Äôs worth thinking of these kinds of tests as something that get run at any point where the underlying data gets updated. . There are three points where it might make sense to do some data validation: . at the point of data ingestion | at the point just prior to training a model, i.e. after your data has been split into training and validation sets | at the point of inference (i.e. using the data being passed into the trained model) | . . The first (at data ingestion) is essential, especially if you have any kind of continuous training or continuous deployment loop going on. You don‚Äôt want to be training on data that clearly is unsuitable for training, or where the distribution has shifted so much that it‚Äôs going to cause hidden problems down the line. . The second (at training-validation split time) may or may not be important depending on your use case. For my redaction project I don‚Äôt think there is a great deal of benefit from this and so haven‚Äôt incorporated it as such. . The third (at inference time) is quite important to have, even though the behaviour when an anomaly is detected might be different from if you were to detect issues earlier on in the process. You might choose to just log the result of your validation check internally, or you could potentially also feed the result back to a user in the terms of some sort of warning (i.e. if the image that they were uploading was a very different kind of image from the data that had been used to train the model). . What I‚Äôm using for my redaction project . I don‚Äôt have any general advice as to which tool you should use as part of your computer vision model training pipeline. It‚Äôs likely to be heavily context-dependent and will differ based on the particular use case or problem you‚Äôre trying to solve. For my own project, however, I can be more specific. . I‚Äôm using plain assert statements liberally scattered through my code, in part leftover from when writing the code but also as a failsafe should strange or anomalous data make its way into my functions. I‚Äôm not sure if this is a best practice or not ‚Äî I could imagine someone telling me that it‚Äôs not advised ‚Äî but for now it‚Äôs helpful, especially as I continue to change things in the innards of various parts of the process. . I‚Äôm using Great Expectations as a general-purpose validation tool to lay out my ‚Äòexpectations‚Äô of my data in a assertive and declarative way, and even though it took a little longer to wrap my head round how it worked, I‚Äôm glad I made the effort as it seems really helpful. . I‚Äôm using Evidently to do similar things as Great Expectations, but I find they have different strengths and benefits even as they serve the same purpose. Evidently is a bit more of a lighter piece in the process, I feel, and as such it‚Äôs a bit more flexible and you can iterate faster with it. I am not quite at the point where I‚Äôm serving my model to accept inference requests from outside, but Evidently will be part of that process when I do, for sure. . Finally, FiftyOne is also somehow part of the validation process. (I‚Äôve written about that previously.) Having visual tools that allow you to quickly test out a hypothesis or debug something unexpected in your training results is an essential part of the work of developing computer vision models. . This brings my short series on data validation for computer vision to a close. I‚Äôm fully conscious that I might have missed some obvious opportunities, tricks or workflows that may be widely used in this field, so I welcome any comments and feedback that you might have. .",
            "url": "https://mlops.systems/tools/redactionmodel/computervision/datavalidation/2022/04/28/data-validation-great-expectations-part-3.html",
            "relUrl": "/tools/redactionmodel/computervision/datavalidation/2022/04/28/data-validation-great-expectations-part-3.html",
            "date": " ‚Ä¢ Apr 28, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "How to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 2)",
            "content": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.) . In the first part of this series, I made the case for why you might want to include some kind of data validation if you‚Äôre working on training a model in general, and if your working on object detection in specific. There are many things that can go wrong with your data inputs and you ought to have some kind of safeguards in place to prevent some tricky failures and bugs. . TL;DR for data validation with Great Expectations . üëÄ Data validation helps give you confidence in the raw ingredients that feed into your models, especially in scenarios where you retrain or fine-tune regularly. . | ‚úÖ For object detection problems, there are many ways your data can fail in some silent way. You should want to be aware of when your training data isn‚Äôt meeting your assumptions of what it should look like. . | üõ† Great Expectations is a general purpose data validation tool that goes a long way to restoring trust in your data, and their automatic profiling feature is really useful when getting started. . | üí™ In this second post on data validation for the computer vision context, I show how you can use the automatic profiling feature of Great Expectations to get you started with increasing your confidence in your object detection annotations. I will show you a concrete example where I created some validation rules for my manually-annotated dataset. I then applied those rules to my synthetic dataset in order to validate it. . | . Initial notebook-based setup . In the last post I showed how you can easily use the Great Expectations library directly on a Pandas DataFrame, manually specifying values you expect to be the case for your data. For example, perhaps your data should always have certain columns, or the values of a certain column should always be a certain type or mostly range between certain values. You can define all these fairly easily, leveraging your domain knowledge of the data. . If you know you‚Äôre going to want to use Great Expectations as a more fully-fledged part of your pipeline or workflow, you‚Äôll probably want to go through the more extensive setup stages and create a dedicated ‚Äòcontext‚Äô which can be longer-lasting than just length of your script runtime. Think of the ‚Äòcontext‚Äô as somewhere all your expectations and configuration of how to access your data is stored. . Full instructions on how to set all this up can be found in the docs, but for the most part it‚Äôs a matter of pip installing Great Expectations, running great_expectations init , and then great_expectations datasource new. . That final command will take you through an interactive setup that has you fill in and amend Jupyter notebooks. (I‚Äôm not fully sold on the prominence of this workflow that has you spinning up a Jupyter runtime, dynamically editing notebooks and so on, but I found doing it for my project wasn‚Äôt as inconvenient as I‚Äôd expected. Plus, there are non-interactive and pure Pythonic ways to get everything configured if you need or prefer that.) . Once you have your context created and your data sources connected, you can move on to the main course: using the Profiler. . Using the Great Expectations Profiler . Setting up your validations (i.e. your ‚Äòexpectations‚Äô) for your data can be done in a number of different ways. We saw last time how you can define these manually, but in this post I want to show how you can follow another recommended workflow by allowing the profiler to review your data and to make an initial set of assumptions about the boundaries and patterns embedded in those values. . Note, as the docs mention, the expectations that are automatically generated from your dataset are ‚Äúdeliberately over-fitted on your data‚Äù. This means that if your DataFrame has 10,321 rows, one of the expectations generated will be that datasets due for validation with this suite of expectations will also have exactly 10,321 rows: . ‚ÄúThe intention is for this Expectation Suite to be edited and updated to better suit your specific use case - it is not specifically intended to be used as is.‚Äù (source) . You‚Äôll want and have to do a decent amount of manual checking through, amending and updating any expectations that get created during this process. That said, I am finding that it makes a lot of sense to start with some kind of initial baseline of assumptions that can be corrected versus starting from complete zero and building things up purely based on your domain knowledge of the data. . Needless to say, this whole process assumes you have a decent grasp on the domain context and have explored your data already. You probably wouldn‚Äôt go to the trouble of setting up Great Expectations if you were doing something that required only a quick solution, but it bears repeating that the expectations you define are only as good as your understanding of the limits and underlying realities of your data. This is probably why something like Great Expectations lends itself quite well to a data-centric approach. . Getting the profiler to work requires a few interlocking abstractions to be created or instantiated: . expectation_suite_name = &quot;redaction_annotations_suite&quot; main_batch_request = RuntimeBatchRequest( datasource_name=&quot;redaction_data&quot;, data_connector_name=&quot;default_runtime_data_connector_name&quot;, data_asset_name=&quot;main_annotations_df&quot;, # This can be anything that identifies this data_asset for you runtime_parameters={&quot;batch_data&quot;: main_annotations_df}, # df is your dataframe batch_identifiers={&quot;default_identifier_name&quot;: &quot;default_identifier&quot;}, ) context.create_expectation_suite( expectation_suite_name=expectation_suite_name, overwrite_existing=True # toggle this as appropriate ) validator = context.get_validator( batch_request=main_batch_request, expectation_suite_name=expectation_suite_name ) profiler = UserConfigurableProfiler(profile_dataset=validator) suite = profiler.build_suite() context.save_expectation_suite(suite) # use this to save your suite in the context for reuse . The above code perhaps seems like a lot, but really all you‚Äôre doing is getting your data, making the relevant connections between Great Expectations and your context, and then running the profiler so it can work its magic. . . You can‚Äôt yet see the specific values that were imputed from your data, but even this high-level output shows you some of the expectations that it‚Äôs thinking would be useful to create. . At this stage, you‚Äôll want to take some time to review the specific expectations. You‚Äôll want to: . ensure that they make sense for your dataset | remove any of the really rigid expectations (e.g. that any dataset must have exactly the same number of rows) | use the inputed expectations as a springboard for any other ideas that might come to mind | . Note that this is an essential step to complete before moving forward. You could use the unedited auto-generated expectations suite as your data validation, but it would almost certainly have little use or value for you. The auto-generated suite is a starting place that you need to amend and tailor to your specific situation. . In my case, I was able to amend some of the min / max values to more suitable defaults. (You amend these expectations in the .json file that was created inside the expectations subfolder within your context.) I also included some other domain-driven expectations that the profiler couldn‚Äôt have known to include. For example, I know from having immersed myself in this data for several months now that most annotations should have a ‚Äòhorizontal‚Äô or ‚Äòsquare‚Äô orientation. Great Expectations doesn‚Äôt create this expectation automatically, so I add it to the list of basic assumptions already generated. . Viewing Data Docs reports on validated data . Once you have a suite of expectations set up to your liking, you can run a checkpoint against your original data just to make sure you haven‚Äôt introduced or amended something that doesn‚Äôt match up with the original data. You should get no errors at this point. . checkpoint_config = { &quot;name&quot;: &quot;my_checkpoint&quot;, &quot;config_version&quot;: 1, &quot;class_name&quot;: &quot;SimpleCheckpoint&quot;, &quot;validations&quot;: [ { &quot;batch_request&quot;: { &quot;datasource_name&quot;: &quot;redaction_data&quot;, &quot;data_connector_name&quot;: &quot;default_runtime_data_connector_name&quot;, &quot;data_asset_name&quot;: &quot;main_annotations_df&quot;, }, &quot;expectation_suite_name&quot;: expectation_suite_name, } ], } context.add_checkpoint(**checkpoint_config) results = context.run_checkpoint( checkpoint_name=&quot;my_checkpoint&quot;, batch_request={ &quot;runtime_parameters&quot;: {&quot;batch_data&quot;: main_annotations_df}, &quot;batch_identifiers&quot;: { &quot;default_identifier_name&quot;: &quot;default_identifier&quot; }, }, ) context.build_data_docs() # builds data docs to inspect the results . What you really want, however, is to run your expectations suite against new data. That‚Äôs the real value of what Great Expectations brings, i.e. to check that incoming data due to be added to your larger base dataset conforms to the broad realities of that base dataset. . In my case, the first thing I was interested to check was whether the synthetic images I created would match the expectations suite I‚Äôd created based off my core hand-annotated data. (Quick context if you haven‚Äôt been following the project so far: I have a core dataset which is manually annotated for the objects inside images. I also created two sets of synthetic data to supplement the manual annotations, which boosted my model performance considerably.) . . The web UI is where you can go to get a visual overview of where your data is passing and failing to meet your (great) expectations. You will want (and I will need) to configure your expectations suite to meet the core assumptions you make about your data derived from your particular domain. . For my case, some expectations I will add that are specific to my use case: . redaction annotations should mostly be of horizontal orientation | content annotations should mostly be of portrait orientation | most images should have only one content annotation | annotations shouldn‚Äôt be larger than the associated image, or positioned outside the boundaries of that image. (Because of how you define them, in reality this is several expectations, but conceptually it‚Äôs just one or two). | the area taken up by most annotations should be less than half that taken up by the total image | . ‚Ä¶and so on. I hope it‚Äôs clear now how Great Expectations can be a tremendous asset that can give you confidence in your data. . In the next and final post of the series, I will explore some other tools that you can consider when performing these kinds of validation. I will also offer my take on when each tool would be appropriate, as well as where they would be appropriate to use within the machine learning workflow and lifecycle. .",
            "url": "https://mlops.systems/tools/redactionmodel/computervision/datavalidation/2022/04/26/data-validation-great-expectations-part-2.html",
            "relUrl": "/tools/redactionmodel/computervision/datavalidation/2022/04/26/data-validation-great-expectations-part-2.html",
            "date": " ‚Ä¢ Apr 26, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "How to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 1)",
            "content": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.) . Data validation is a process of verifying that data is accurate and consistent. It plays a crucial role in end-to-end machine learning pipelines. . There is a lack of validation tools in Computer Vision (CV) due the complexity of the data used by the domain. . In this series of articles, I will show you how to leverage the Great Expectations open-source library to validate object detection data. This will help you to feed your model with data less prone to break your model performance. . When something goes wrong with a newly trained or newly deployed version of your model, where do you look first? Where does your gut tell you the bug or issue is likely to be found? For me, knee deep into my redaction model project, I immediately think of my data. For sure, I could probably have more in the way of testing to be sure my code is working how I expect it to work, but issues with the data are far more likely to be silent killers. Issues with data are unlikely to raise a loud exception and suddenly bring my training to a stop. Instead, my training will continue, but I‚Äôll either get really unsatisfactory results or I‚Äôll get results that are underperforming the real potential of the data I‚Äôm feeding into my model. That‚Äôs the scary part: I most likely won‚Äôt even know that my data is broken or faulty. . For software engineers, testing your code is a tried-and-tested way to find some confidence in what you‚Äôre trying to accomplish. (I am exploring some of these best practices in my review series about Patrick Viafore‚Äôs excellent Robust Python book, which covers testing along with typing and various other patterns.) For critical systems, testing is one of the things that allows you to sleep soundly. For those living in the world of machine learning or data science, data validation is like writing tests for your data. You can be confident that your data looks and has the shape of what you feel it should when you address the data quality issue head-on. . If you think of your model training workflow as a pipeline, there are certain places where it makes sense to do some kind of data validation: . at the very beginning, when you‚Äôre seeing your data for the first time: a lot of exploration and basic analysis really helps at this point. It will help you build up intuition for the general patterns and boundaries of the data you‚Äôre going to use to train your model. | any time you do some kind of conversion: perhaps you have to ‚Äî as I do with my project ‚Äî convert from one image annotation format into another and you‚Äôre juggling x and y coordinates constantly, or maybe you‚Äôre using different image formats at different points? | prior to training your model: ‚Äògarbage in, garbage out‚Äô as the saying goes‚Ä¶ You probably want to make sure that you only have high quality data passing through into your model training pipeline step. | as part of a continuous training loop: perhaps you‚Äôve trained and deployed a model, but now a few months have passed, you have more data and you want to retrain your model. Are you confident that the new data retains the same characteristics and qualities of your original data? | . As you can see, there are many different approaches that you might take. To discuss where you might want to validate your data is to discuss where your processes might be flawed in some way. For most projects of any size or significance, you probably will find that taking the care with your data inputs will pay dividends. . Data validation and computer vision . It often seems like computer vision exists in a world unto its own, particularly when it comes to the data used to train models. These idiosyncrasies amount to a strong case for some kind of data validation: . image data isn‚Äôt always easily introspectable, especially on the aggregate level (i.e. what is the ‚Äòaverage‚Äô of a series of images, or how to think of the standard deviation of your images?) | for something like object detection, the annotations are stored in a separate location from the images to which they correspond, leaving the door open for a creeping data drift between the original image locations and what is listed in the annotations. | For massive data sets, the original data will likely not be stored in the environment where you‚Äôre doing some fine-tuning with new data. | Different model architectures require different kinds of pre-processing for your data and sometimes annotations need converting into slightly different formats (perhaps for your evaluation metric) | The pure images (or image-adjacent objects like medical scans) contain a lot of sub-surface metadata that isn‚Äôt easily accessed and isn‚Äôt automatically used as criteria for comparison or error detection. | . In short, there are lots of ways that training a computer vision model can go wrong, and implementing even basic safeguards against this can give you confidence in the data you‚Äôre using. Unfortunately, the landscape of tooling for data validation in the computer vision space feels like it‚Äôs lagging behind what exists for tabular data, for example, but that‚Äôs almost certainly because it‚Äôs just a harder problem. The big data validation libraries don‚Äôt really cater towards computer vision as a core domain, and (as you‚Äôll see below) you‚Äôll probably have to crowbar your data into the formats they expect. . Big picture: what might this look like for my project? . As I outlined above, there are lots of different places where you might want to use some kind of data validation strategy. At the level of code, you might want to make your input and output validation a bit more solid by using type annotations and a type checker like mypy. You can add tests to ensure that edge cases are being handled, and that your assumptions about your code behaviour are proven. You also have your tests to ensure that changes in one function or area of your codebase don‚Äôt break something somewhere else. . At the level of your data, you can of course use simple assert statements within the functional meat of your codebase. For example, at the point where you‚Äôre ingesting data pre-training you could assert that each image is of a certain format and size, and perhaps even that annotations associated with that image ‚Äòmake sense‚Äô as per the context of whatever problem you‚Äôre solving. You can handle some of these assertions and checks with simple conditionals, perhaps, earlier on in the process when you are ingesting or pre-processing your data. . A significant benefit of having these simple assertions inside your core functions is that you are handling the ways things can go wrong at the same time as you‚Äôre writing the functionality itself. A disadvantage is that your code can easily become cluttered with all this non-core behaviour. It feels a little like the validation can become an afterthought in this scenario. For this reason, it seems to make sense to me that you‚Äôd want to have one or more dedicated checkpoints where your data undergoes some kind of validation process. In the context of a pipeline, this means you probably will want one or more steps where this happens. . Tradeoffs . For tiny throwaway projects, or for proof-of-concept experimentation, it might not make sense to start off by working up a massive data validation suite. A really rigorous validation process early on might slow you down more than is useful. Instead, simple assert statements coupled with type annotations on your functions might be the way to go for safeguards and will-this-be-readable-in-the-future sanity checks. . Ideally, you‚Äôll want to create some kind of end-to-end pipeline or workflow at the beginning of your process, since this will allow you to iterate faster in a manner that‚Äôs meaningful for whatever you‚Äôre trying to solve. With a basic pipeline in place, data validation can be added as a stage of its own without too much disruption once you have an initial working prototype. As with most things in life, investing for the longer term is going to take a bit more upfront effort but that shouldn‚Äôt be too much an issue as long as your project has that kind of a horizon to it. . What kind of validation does Great Expectations offer? . Great Expectations is an open-source data validation tool. It is somewhat agnostic as to what specific use case you have, but I don‚Äôt think it‚Äôd be wrong to say that it isn‚Äôt primarily developed for those working on computer vision problems; tabular data seems to be a much cosier fit. . I stated above that Great Expectations could be used as if you were adding tests for your data. At a very high level, you can think of it as a fancier way of adding assertions about your data. The ‚Äòexpectations‚Äô in the title are like those assertion statements, only in this case there are dozens of different pre-made ‚Äòexpectations‚Äô you can choose from. For example, you could assert that you expect that the values of a particular column of a Pandas DataFrame be between 0 and 100, and that if they exceeded those boundaries then it would be only a very small proportion that did so. . Your expectations make up a ‚Äòsuite‚Äô, and you run your suite of expectations against a batch or data asset. There are another 10 or 20 concepts or terms that I‚Äôd need to define and connect together in a mental map before we covered everything about how Great Expectations works. Unfortunately, this is one of the things I found most confusing about getting to know the library through its documentation. From the outside, it appears that they had one set of terminology, but now it‚Äôs partially changed to a different set of terms or abstractions. Presumably for reasons of backwards compatibility, some of the old abstractions remain in the documentation and explanations, which makes it not always clear to understand how the various pieces fit together. . . You can read the glossary over at their documentation site if you want to learn more, but for now everything I explained above should suffice. . There seem to be two main ways of setting up and using Great Expectations. One is heavily interactive and driven by executing cells in a series of notebooks. The other is as you‚Äôd expect ‚Äî code-based using a Python library, backed by some external configuration files and templates. I didn‚Äôt find the notebook-based configuration and setup very compelling, but it is the one emphasised in the documentation and in online materials, so I will give it due attention in the next part of this blog series. For now, it might suffice to show a very simple version of how the code-based use works: . A simple example of using Great Expectations for data validation . The first thing I did was to convert my annotations data into a Pandas DataFrame. You can use Pandas, SQL and Apache Spark as sources for your data to be validated through Great Expectations, and luckily my COCO annotations file was just a JSON file so it was easily converted. While doing the conversion, I made sure to add some extra metadata along the way: a column noting whether an image or a redaction was horizontal or vertical in its orientation, for example, or splitting the bbox array into its four constituent parts. . import great_expectations as ge annotations_df = ge.from_pandas(pd.DataFrame(annotations)) feature_columns = [&#39;area&#39;, &#39;iscrowd&#39;, &#39;image_id&#39;, &#39;category_id&#39;, &#39;id&#39;, &#39;synthetically_generated&#39;, &#39;category_name&#39;] for col in feature_columns: annotations_df.expect_column_to_exist(col) annotations_df.expect_column_values_to_be_in_set( &quot;category_name&quot;, [&quot;content&quot;, &quot;redaction&quot;] ) . Great Expectations wraps the Pandas library, so importing the data was easy. Then adding the expectations (methods beginning with expect‚Ä¶) was trivial. Below you can see the result from the second of the expectations. All of the column values were in that set, so the test passed. . { &quot;success&quot;: true, &quot;result&quot;: { &quot;element_count&quot;: 6984, &quot;missing_count&quot;: 0, &quot;missing_percent&quot;: 0.0, &quot;unexpected_count&quot;: 0, &quot;unexpected_percent&quot;: 0.0, &quot;unexpected_percent_total&quot;: 0.0, &quot;unexpected_percent_nonmissing&quot;: 0.0, &quot;partial_unexpected_list&quot;: [] }, &quot;meta&quot;: {}, &quot;exception_info&quot;: { &quot;raised_exception&quot;: false, &quot;exception_traceback&quot;: null, &quot;exception_message&quot;: null } } . In the second part of this series, I‚Äôll explore how the interactive way of using Great Expectations works, and I‚Äôll also show the web results interface for your expectations suite. It‚Äôs much fancier than the dictionary / object that was output above, and what‚Äôs even better is that you can have Great Expectations make some of its own guesses about what the right expectations for your particular dataset might be. . I hope for now that I‚Äôve made the case for why data validation is probably worth doing, and started you thinking about how that might apply to a computer vision use case. .",
            "url": "https://mlops.systems/tools/redactionmodel/computervision/datavalidation/2022/04/19/data-validation-great-expectations-part-1.html",
            "relUrl": "/tools/redactionmodel/computervision/datavalidation/2022/04/19/data-validation-great-expectations-part-1.html",
            "date": " ‚Ä¢ Apr 19, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "'I guess this is what data-centric AI is!': Performance boosts after training with synthetic data",
            "content": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.) . A clean and focused dataset is probably at the top of the list of things that would be nice to have when starting to tackle a machine learning problem. For object detection, there are some useful starting points, but for many use cases you‚Äôre probably going to have to start from scratch. This is what I‚Äôve been doing for the past few months: working to bootstrap my way into a dataset that allows me to get decent performance training a model that can recognise redactions made on documents. . As part of that journey so far, some of the big things that I‚Äôve taken time to do include: . manually annotating 1000+ images | using a model-in-the-loop to help bootstrap that annotation process by pre-filling annotation suggestions on an image that I could then correct | creating synthetic images to increase the size of my dataset used in training | spending time looking at what the model found difficult, or what it got wrong | . At the end of my synthetic data creation blogpost, I mentioned that the next step would be to test the effect of adding in the new synthetic examples. Well‚Ä¶ the results are in! . A failed attempt to train with synthetic data . I wasn‚Äôt sure exactly how much synthetic data would be appropriate or performant to use, so created a loose experiment where I started with 20% of the total images and increasing up until I reached 50%. (I figured that more than 50% synthetic data probably wasn‚Äôt a great idea and would probably not help my model perform out in the real world.) . . As you can see above: my initial experiment did not show great results. In fact, in several places, if I added synthetic data my model actually performed worse. This was a strong repudiation of my intuition of what would happen. After all, the whole point of adding the synthetic data was to get the model more of a chance to learn / train and thus improve its ability to recognise redaction object in documents. . I dug into the data that I‚Äôd generated and the data I‚Äôd been using to train, and discovered a nasty bug which was tanking the performance. A week of debugging mislabelled bounding boxes in evenings after work and I was back with results that finally made sense. . Performance boosts after adding synthetic data . . In this chart, at the bottom you can see how training the model without the synthetic data (no-synthetic-batch16) performed. Ok, not great. Then the next best performing (combined-75real-25synthetic-randomsplit)was when 25% of the total number of images was synthetic, and the rest were real manually annotated images. At the top, with around an 81% COCO score, was the model where I used 50% synthetic and 50% real images. This seemed to fit what my intuition said would happen. . More synthetic data helped. I guessed that if I had millions of labelled images then the synthetic data would perhaps have been less useful, but starting from scratch it was really supporting the process. . I was curious what would happen when I returned to FiftyOne to carry out some error analysis on the new model‚Äôs performance. Even before I had reached those results, I had a hunch that the synthetic images I‚Äôd created were perhaps too generic. I think they probably were helping boost some baseline performance of my model, but I knew they weren‚Äôt helping with the hard parts of detecting redactions. . ‚ÄòHard examples‚Äô: creating targeted synthetic data . As a reminder, this is the kind of image that is ‚Äòhard‚Äô for my model (or even a human) to be able to identify all the redactions: . . The FiftyOne visualisations of what was and wasn‚Äôt working validated my hunch: yes, synthetic data helped somewhat, but the model‚Äôs low performance seemed much more vulnerable to misrecognition of the hard examples. Even with a 50/50 split between synthetic data and real manually annotated data, the hard examples were still hard! (And the converse was also true: the model was already pretty good at identifying ‚Äòeasy‚Äô redactions (e.g. of the black box type). . If we look back at the example of a ‚Äòhard‚Äô redaction above, two things stood out: . They‚Äôre hard, even for a human! This was borne out in the way I needed to take special care not to forget or mislabel when I was adding manual annotations. | There are lots of redactions on a single page/image. | The second point was probably important, not only in the sense that there were more chances of getting something wrong on a single page, but also in the sense that the redactions were (relatively) small. The detection of small objects is almost its own field in the world of computer vision and I don‚Äôt know too much about it, but I do know it‚Äôs somewhat an unsolved problem. That said, finding a way to boost the performance of the models on these ‚Äòhard‚Äô examples (there were a few other types of hard image) seemed like it might tackle a significant shortcoming of my model. . I decided to try creating a separate batch of synthetic image data, this time fully tailored to tackling some of the hardness mentioned above: it would have many small redactions on a single page, they would all be white boxes and there might also be things like tables with white box-like shapes coexisting next to redactions. . Luckily, the work I‚Äôd done previously on creating synthetic data helped me get started quickly. I returned to borb, an open-source tool for quickly creating PDF documents that allows for a pretty flexible prototyping of layouts with all sorts of bells and whistles added. These were some of the documents I generated: . . The hard images were hard, and I had created some synthetic chimeras that (I believed) approximated some of the features of the original hard images. I did not want to overbalance my training data, however, and took care not to create too many of this type of image. . My script ‚Äî as with the previous synthetic data ‚Äî also required me to create the annotation files at the same time as creating the document. With borb it was relatively trivial to get the bounding box data for objects created, and there was even in-built functionality to create and apply redactions onto a document. (I‚Äôm moving fairly quickly over the mechanics of how this all worked, but it‚Äôs not too far distant from how I described it in my previous post so I‚Äôd refer you there for more details). . Once the images were created and added to my datasets, it was time to retrain the model and see what benefit it brought. . . As you can see, the model jumped up from around 80.5 to 84% when I aded the hard synthetic examples in. That‚Äôs a pretty nice jump as far as I‚Äôm concerned, especially given that I only added in 300 images to the training data. I still had a little over a thousand of the original basic synthetic images that I was using, but this result showed me that tackling the badly performing parts of the model head-on seemed to have a positive outcome. . At this point, I did some more experiments around the edges, applying other things I knew would probably boost the performance even more, notably first checking what would happen if I increased the image size from 512 to 640. I got up to an 86% COCO score with that improvement alone. . In a final twist, I second-guessed myself and wondered whether the original synthetic data was even helping at all‚Ä¶ I removed the thousand or so ‚Äòbasic‚Äô synthetic images from the data and retrained the model. To my surprise, I achieved more or less the same COCO score as I had with the basic synthetic images. I‚Äôm taking this as a strong suggestion that my basic synthetic images aren‚Äôt actually helping as much as I‚Äôd thought, and that probably a smaller number of them as a % of the total would be beneficial. . Reflections on experimenting with synthetic data . So, what can I conclude from this whole excursion into the world of synthetic image creation as a way of boosting model performance? . adding synthetic data really can help! | the world of synthetic data creation is a huge rabbit hole and potentially you can get lost trying to create the perfect synthetic versions of your original data. (I mean this both in the sense of ‚Äòthere‚Äôs lots to learn‚Äô as well as ‚Äòyou can spend or lose a ton of time here‚Äô.) | Targeted synthetic data designed to clear up issues where the model has been identified as underperforming is probably best. (Conversely, and I‚Äôll be careful how much I generalise here, middle-of-the-road synthetic data that doesn‚Äôt resemble the original dataset may not be worth your time.) | Knowing your original data and domain really well helps. A lot. My intuition about what things the model would stumble on was fuelled by this knowledge of the documents and the domain, as well as by the experience of having done manual annotations for many hours. | . There are probably many (many) more things I can do to continually tinker away at this model to improve it: . continue down the path of more error analysis, which would fuel more targeted addition of annotations, and so on. | create better versions of synthetic data with more variation to encompass the various kinds of documents out in the real world. | more self-training with the model in the loop to fuel my manual annotation process. | further increases to the image size (perhaps in conjunction with progressive resizing). | increasing the backbone from resnet50 to resnet101. | . In general, improving the quality of the data used to train my model seems to have been (by far) the best way to improve my model performance. Hyper-parameter tuning of the sort that is often referenced in courses or in blog posts does not seem to have had much of a benefit. . It is probably (mostly) good enough for my use case and for where I want to be heading with this project. There are other things that need addressing around the edges, notably parts of the project that could be made more robust and ‚Äòproduction-ready‚Äô. More about that in due course, but for now please do comment below if you have suggestions for things that I haven‚Äôt thought of that might improve my model performance! .",
            "url": "https://mlops.systems/tools/redactionmodel/computervision/2022/04/06/synthetic-data-results.html",
            "relUrl": "/tools/redactionmodel/computervision/2022/04/06/synthetic-data-results.html",
            "date": " ‚Ä¢ Apr 6, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Some characteristics of best-in-class ML portfolio projects",
            "content": "Ekko was the last time I worked on a big project that would be presented publicly. An open-source framework that provided realtime infrastructure and in-transit message processing for web applications was a group project that I worked on together with three other colleagues, and we took the time to really make the how and the why really explicit. We made animations, diagrams, charts, and I learned a lot about what‚Äôs hard when explaining technical projects, even when the audience is expected to be (mostly) technically literate. . . I‚Äôve been working on my redaction project since December and slowly but surely I‚Äôm tying the ends together and getting ready for it to come to a close. As part of the final touches, I want to offer something equivalent to how we presented Ekko. From reading around and exposure to various projects over the years, it seems to me that machine learning projects sometimes have different emphases and conventions. This blog post is my attempt to list some of the characteristics of really great ML portfolio projects, with an emphasis on how the project is presented. . Some top projects . Healthsea by Edward Schmuhl (@aestheticedwar1) is my current favourite project writeup, blending amazing visuals, full explanation and a clear overview | This project (by @ahmed_besbes_) was recommended to me and although it‚Äôs more of a step-through of how the project works and was created, it also is clearly presented and very visual. | For computer vision projects, Hugging Face Spaces is a great place to find interesting Gradio demos, though after a while they blend into each other a little. HF Spaces also doesn‚Äôt seem like it gets used for full project explanation that often. | . Characteristics of top projects . Some things I think make a great portfolio project stand out: . visual design ‚Äî looks count for a lot, for better or for worse. | interactivity ‚Äî if there is some kind of a demo or application that I can play around with in order to relate to concepts being written about, that‚Äôd be great. | visual explanations alongside pure text ‚Äî a diagram or animation can really help bring explanations to life. | a clear overview ‚Äî the structure of the writeup should be clear and readers should be able to get a high-level overview first without necessarily needing to read through every last detail. | explain what problem you‚Äôre solving ‚Äî spend (probably) more time than you think is necessary to explain what problem you‚Äôre solving and set up the context for the work you did. | code snippets are ok, but don‚Äôt just dump your source code. | present your dead ends ‚Äî don‚Äôt just present the happy path; feel free to present things that didn‚Äôt work out as well. Readers will want to know that you encountered difficulties and there are benefits from seeing how you made decisions along the way. | present further work and next steps ‚Äî offer hints at what other work could be done on the project, even if you‚Äôre done with it for now. | don‚Äôt lose track of the use case ‚Äî show that you thought about the specific problem you were solving and not just as a technical problem in a void. (Real-world use cases have constraints, and your solution should live within a universe where those constraints directed you). | Feel free to link out ‚Äî you can easily link to other places where you‚Äôve gone into the details about a particular problem you encountered. No need to cram every single last detail into the project portfolio. | Don‚Äôt forget the purpose of the portfolio ‚Äî it doesn‚Äôt need to be an exhaustive catalogue of every last detail; it just needs to offer a compelling overview that is understandable as an independent entity. | . There are other aspects which are more table stakes for anything you write online ‚Äî no typos, clear writing and so on. . I took the time to step back from the project to write this down as I move into a phase where I‚Äôll increasingly focus on the full writeup and I wanted to have a list to remind me of the things I valued in these kinds of projects. . If you have good examples of ML portfolio projects (or really great blog write-ups with interactivity and so on), please let me know in the comments! .",
            "url": "https://mlops.systems/computervision/skillbuilding/2022/04/04/ml-portfolio-best-practices.html",
            "relUrl": "/computervision/skillbuilding/2022/04/04/ml-portfolio-best-practices.html",
            "date": " ‚Ä¢ Apr 4, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Building my own image to use IceVision with Paperspace",
            "content": "I‚Äôve been using Paperspace right to fuel my ML/Deep Learning experimentation since more or less the beginning. It was one of the recommended platforms that offered GPUs for the fastai course and when I started working on my redaction project I chose to keep going since I had little reason to change. . Fast-forward a few months, and I‚Äôve had a few issues along the way. Paperspace works by provisioning a Docker image, connecting it to a fixed filesystem / storage backend and then serving this up to you in a web interface as a Jupyter notebook. I found that sometimes there were issues with dependencies breaking, or special pip install magic I had to include in my notebook so that things would work again. . Included in this is the reality that a full install of IceVision ‚Äî an amazing library for computer vision that handles a lot of the pain around integrating various libraries and use cases ‚Äî simply takes a while as it has to download and setup some pretty hefty dependencies. I had found that going from zero to working on the day‚Äôs specific issue took around 20 minutes when you factored in all the setup, updates from the Github repo, syncing data and so on. . Inspired by my reading and study of Docker ‚Äî and with a tip from a Paperspace engineer about how I could get started ‚Äî I set out to build a custom image that handled most of the setup upfront and automatically updated with the latest changes and data. . Amazingly, it worked more or less immediately! I created a new Dockerfile based of the original suggestion and the core additions were the following: . RUN wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh &amp;&amp; bash icevision_install.sh cuda11 &amp;&amp; rm icevision_install.sh RUN pip install torchtext==0.11.0 --upgrade RUN pip install opencv-python ipywidgets icevision-dashboards RUN apt update &amp;&amp; apt install -y libsm6 libxext6 RUN apt-get install -y libxrender-dev CMD make lfs &amp;&amp; git lfs pull . In order to set this up with Paperspace, you first have to go to your notebooks inside a project and click to create a new Paperspace notebook. . . Once there, you can ignore the suggestion to ‚Äúselect a runtime‚Äù, but rather select your machine from the available GPUs. I usually choose the RTX5000 and set it up for an auto-shutdown after 6 hours. . . Then you want to click the ‚ÄòAdvanced Options‚Äô toggle so you can add in all the details of the image being used. . . This is what worked for me. In order to use JupyterLab, the container command should be: . jupyter lab --allow-root --ip=0.0.0.0 --no-browser --ServerApp.trust_xheaders=True --ServerApp.disable_check_xsrf=False --ServerApp.allow_remote_access=True --ServerApp.allow_origin=&#39;*&#39; --ServerApp.allow_credentials=True . I enter my private GitHub repo (along with my username and a custom token generated to allow Paperspace to download the repo) in the ‚ÄòWorkspace‚Äô section. . Then when I click ‚ÄòStart Notebook‚Äô, it works! No more hanging around for IceVision to install. My Docker image already has this! . I realise that I‚Äôm probably a little late to the party in terms of using Docker and seeing how it can bring some real improvements in terms of reproducibility of environments as well as these little quality-of-life perks like not hanging around to install everything each time you want to use it. This was a really useful experience for me to learn from, and I‚Äôll certainly be using this going forward in other projects I work on. .",
            "url": "https://mlops.systems/tools/docker/computervision/2022/03/25/paperspace-docker-icevision.html",
            "relUrl": "/tools/docker/computervision/2022/03/25/paperspace-docker-icevision.html",
            "date": " ‚Ä¢ Mar 25, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Starting Docker In A Month Of Lunches",
            "content": "As far as software engineers go, I‚Äôm still barely a spring chicken, six months into my job with ZenML. Working at a startup is fairly fast-paced and the ability to get going quickly with any number of libraries and tools is a requirement of the work. The number of things I could study, learn or practice is vastly larger than the amount of time I have. Accordingly, it helps to try to pick things that will be long-lasting, teach a cross-cutting skill or that are somehow fundamental. . Two closely-connected technologies that I‚Äôve realised I can no longer avoid are Docker and Kubernetes. I have some high-level knowledge of both, having worked with Docker images on Ekko and having encountered Kubernetes in recent months, but it‚Äôs become clear in the last few weeks that they aren‚Äôt going away. More than that, it seems that not having a better (practical) grasp of some of the ins and outs of both is holding me back from grasping more complex decisions that are being made at work. . [Side-note: I‚Äôm very curious about Podman as a Docker-adjacent alternative, but I need to understand Docker better first before I can make comparisons. I‚Äôd also note that I‚Äôm pretty sure that there are lots of cases where Kubernetes is overkill, and where it doesn‚Äôt make much sense to add all that complexity, particularly for smaller teams and projects. It‚Äôs nevertheless a feature of life in the MLOps space, it seems, so I must understand it.] . I‚Äôve had my eye on two Manning books by Elton Stoneman for a while, and now seems the perfect time to dive in. Learn Docker in a Month of Lunches and Learn Kubernetes in a Month of Lunches are very practical introductions to their subjects, come with good reviews and feedback and were published relatively recently. I‚Äôm especially happy that both books are extremely hands-on and even though I won‚Äôt in any way be an expert in either technology by the end, I‚Äôll at least have some experience of having encountered the core use cases of both and maybe have a strong idea of what I do and don‚Äôt know. . I‚Äôm not sure whether I‚Äôll complete each one in exactly a month, but I‚Äôll try to fast-track my reading. The chapters are written in such a way as to be digestible (including exercises) in around an hour. Stoneman says in the introduction to the Kubernetes book that it‚Äôs best to start with the Docker one, which I suppose makes sense given that one builds on the other. . Just like my posts as I read through Robust Python (which I haven‚Äôt stopped doing), I‚Äôll write up various things that I learn along the way, mainly as notes for myself but perhaps it will have value beyond this limited goal. So far I‚Äôve read through the first three chapters of the Docker book, so what follows are some notes on the key points from that. . Core Docker Commands . The book has you running a lot of examples. Two commands mentioned (specific to this book) to help clean up the images and containers that you were using: . # clean up containers and application packages docker container rm -f $(docker container ls -aq) # to reclaim disk space docker image rm -f $(docker image ls -f reference=&#39;dial/*&#39; -q) . Some core commands for interacting with a container: . # run a container docker container run CONTAINERNAME # run an interactive container with a connected terminal session docker container run --interactive --tty CONTAINERNAME # list running containers docker container ls # list all containers with any status docker container ls --all # list processes running in a container # CONTAINERNAME could be part of the container ID as well docker container top CONTAINERNAME # show logs for a container docker container logs CONTAINERNAME # view all details about a container docker container inspect CONTAINERNAME # get stats on a particular container docker container stats CONTAINERNAME # special flags # --detach starts the container in the background # --publish publishes a port from the container to the computer # delete containers # the force flag shuts it down if still running docker container rm --force CONTAINERNAME # the nuclear option docker container rm --force $(docker container ls --all --quiet) . Building your own images with Dockerfiles . Some commands which are useful for making your own images: . # gets the image from DockerHub registry docker image pull IMAGENAME . Key mental models for Docker images: . images are made up of ‚Äòlayers‚Äô | Docker images are stored as lots of small files, brought together as one image when you build with a Dockerfile. | each layer of an image corresponds to a line in your Dockerfile | layers are successively built on top of each other | the order of the layers determines the cache invalidation. If something changes in a lower layer, then all subsequent layers are regenerated. It‚Äôs for this reason that it pays to be careful about the order in which you write the commands that make up your Dockerfile. | . It seems to be considered a good practice (at least where I am right now in the book) to pass in environment variables from the outside into your Docker image. This way you can keep configuration separate from how you actually run it. So you‚Äôd have a command something like this: . docker container run --env EPOCHS=30 SOMEUSER/CONTAINERNAME . which would pass the EPOCHS environment variable into the runtime of the Docker image if it had been set up with something like this as a line inside the Dockerfile: . ENV EPOCHS=&quot;1&quot; . Note that only the environment variables that you specifically select to be passed into the container get passed in. . Dockerfile layout . Dockerfiles seem to have some commonalities in terms of the structure: . you start with a base image on which you‚Äôre building. These seem usually or often to be a base image containing a runtime for whatever language your application uses | Then there‚Äôs often environment variables afterwards | Then you can specify a WORKDIR which is the working directory for the application | Then you can COPY files from your local filesystem into that working directory | Then at the end you specify which CMD needs to be run in order to execute the application. | . Once you‚Äôre done with writing your Dockerfile, use the following command to build your image: . docker image build --tag SOMENAME . . Note that final . trailing that command. The . states that the current directory is the ‚Äòcontext‚Äô and thus is used for when you‚Äôre copying in files using the COPY command. . You can view the layers of your Docker image with the docker image history IMAGENAME command (which will output them to the terminal). . To see how much disk space your containers and images are taking up, type docker system df. . When you rebuild an image, you can specify this with a :v2 after the name, as in this command: . docker image build -t web-app:v2 . . When it comes to optimising your Dockerfile, bear the following in mind: . ‚ÄúAny Dockerfile you write should be optimised so that the instructions are ordered by how frequently they change ‚Äî with instructions that are unlikely to change at the start of the Dockerfile, and instructions most likely to change at the end. The goal is for most builds to only need to execute the last instruction, using the cache for everything else. That saves time, disk space, and network bandwidth when you start sharing your images.‚Äù (pp. 42-43) . Some command tips and tricks: . combine multiple commands onto the same line | put the CMD instruction early on as it‚Äôs unlikely to change | .",
            "url": "https://mlops.systems/tools/dockerinamonthoflunches/books-i-read/2022/03/21/docker-in-a-month.html",
            "relUrl": "/tools/dockerinamonthoflunches/books-i-read/2022/03/21/docker-in-a-month.html",
            "date": " ‚Ä¢ Mar 21, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Figuring out why my object detection model is underperforming with FiftyOne, a great tool you probably haven't heard of",
            "content": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.) . So you‚Äôve trained a computer vision model, but you think it could do better. What do you do next? This is a common scenario, especially for computer vision problems where fine-tuning someone else‚Äôs pre-trained model is a pretty normal initial step that gets taken. You emerge with a decent score on whatever metric you care about, but it also isn‚Äôt great. . One part of the solution is certainly ‚Äòmore data‚Äô. This approach was recently highlighted by Boris Dayma on Twitter: . Easy recipe to get quickly a cool classification model on your own dataset ü§ì‚úÖ spend 1-2h to sort part of your data‚úÖ split in train/val as 90/10‚úÖ fine-tune a model (HuggingFace makes it easy)‚úÖ use that model to sort faster more data‚úÖ train again &amp; repeat until happy! . &mdash; Boris Dayma ü•ë (@borisdayma) March 11, 2022 In my case, I currently have a little over 1200 images that have been annotated, but of those some 600 of them don‚Äôt contain any redactions at all (i.e. they just have content boxes). I did mention that I was using a similar approach early on, where I‚Äôd use the model to help pre-annotate images, but I haven‚Äôt been using that recently. . I‚Äôm realising that more important than pure volume of data is to annotate types of images that are the hardest for the model to learn. So what I really want to know at this point is where I should place my focus when it comes to supplementing the training data. My images aren‚Äôt currently divided into separate classes, but I have a proxy (the filename) which will be really helpful once I‚Äôve identified which types I need to supplement. . When seeking to improve computer vision models with error analysis, some kind of visual inspection is essential. fastai had a number of utility methods that helped in the interpretation of where a model was underperforming, but for object detection I think you do need something that was built to purpose, where you can really dive into the specific ways each object was or wasn‚Äôt detected. . Enter FiftyOne. . FiftyOne is an open-source tool built specifically to support the curation and creation of datasets for computer vision models. It is almost two years old in its open-source incarnation, and (or but?) it feels very solid and robust in its implementation. Voxel51, the company behind it, has taken great pains to write excellent documentation and guides, and they have a supportive community behind the scenes, too. . FiftyOne Basics . FiftyOne is a Python library that offers a visual interface to your data. For my redaction model, the base interface looks something like this: . . You need to convert your dataset such that FiftyOne can interpret the structure of where images are stored as well as the annotations themselves, but many commonly-used formats are supported. In my case, COCO annotations are supported out of the box, so it was trivial to import the data to generate the above visualisation. . You can use the FiftyOne application inside a Jupyter Notebook, or you can have it open in a separate tab. A separate tab is my preference as it allows for a larger interface. (There is also a completely separate Desktop app interface you can use, but I think not all functionality works there so you might want to stick to a separate tab). . Luckily for me, my computer vision framework of choice is IceVision, and they recently integrated with FiftyOne which makes creating datasets a breeze. . So how did FiftyOne help me understand how my model was performing? (Note: the sections that follow were significantly helped by following this, this and this part of the FiftyOne docs.) . Comparing ground truth with predictions . The first thing I did was visualise the ground truth annotations alongside the predictions of my model. (This is the model mentioned in my last blogpost, which had a COCO score of almost 80%.) . This requires performing inference on a slice of our images. Unfortunately, I had to do that inference on my local (CPU) machine because FiftyOne doesn‚Äôt work on Paperspace cloud machines on account of port forwarding choices that Paperspace make. This makes for a slightly slower iteration cycle, but once the inference is done you don‚Äôt have to do it again. . . You can see here that it‚Äôs possible to selectively turn off and on the various overlaid annotations. If you want to compare how redactions are detected (and not see the content box), then this is an easy way to toggle between. . Viewing only high-confidence predictions . Not all predictions are created equal, too, so it would be useful to view only those predictions where the confidence was higher than 75%. FiftyOne makes this kind of conditional view easy. You can do it in code, as in the following snippet, or you can do it via the GUI inside the app. . from fiftyone import ViewField as F # Only contains detections with confidence &gt;= 0.75 # `dataset` is the FiftyOne core object that was created before high_conf_view = dataset.filter_labels(&quot;prediction&quot;, F(&quot;confidence&quot;) &gt; 0.75) . . ‚ÄòPatches‚Äô: detailed views for detected objects . For a more fine-grained understanding on the ways our model is predicting redactions, we can create what are called ‚Äòpatches‚Äô to view and scroll through prediction-by-prediction. . . This is an excellent way to view things through the eyes of your model. These are all the objects it considers to be redactions. We‚Äôll get to finding the ones where it doesn‚Äôt do as well in a bit, but this view allows us to immerse ourselves in the reality of how our model is predicting redaction boxes. We can see that certain types of boxes are well-represented in our dataset: coloured or shaded rectangles in particular. . Understanding how our model performs for separate classes . We only have two classes in our training data: redaction and content, so doing a class analysis doesn‚Äôt help us too much for this problem, but using the mean average precision (MAP) calculation we can see the difference between how well our model does on redactions vs content: . . We can also easily plot an interactive chart that quite clearly displays these differences: . . Viewing the false positives and false negatives . The previous calculations also added some metadata to each image, denoting whether it was considered a true positive, false positive or false negative. It‚Äôs really useful to be able to easily switch between these views, and identifying the images with the largest numbers of false positives and false negatives will help appreciate what our model struggles with. . This view is sorted by total number of false positives in an image. False positives are where the model confidently has predicted something to be a redaction box, for example, that is not actually a redaction box. . . In this image you can see that the model predicts a redaction box with 82% confidence that is clearly not a redaction. Note, too, how the smaller redactions to the right and the large partial redaction to the left were not detected. . False negatives are where there were some redactions to be predicted, but our model never made those predictions (or was very unconfident in doing so). . . In this image excerpt, you can see that some predictions were made, but many were also missed. This image shows the ground truth reality of what should have been predicted: . . Scrolling through the examples with high numbers of false positives and false negatives gives me a really useful indication of which kinds of redactions with which I need to annotate and supplement my training data. I already had a sense of this from my own intuition, but it‚Äôs excellent to see this confirmed in the data. . Finding detection mistakes with FiftyOne Brain‚Äôs mistakenness calculation . FiftyOne is not only the visual interface, but it also has something called the FiftyOne brain. . . It&#39;s worth being aware of the distinction between the two: FiftyOne itself is open-source and free to use. The brain is closed-source and free for non-commercial uses. The brain allows you to perform various calculations on your dataset to determine (among other things): . visual similarity | uniqueness | mistakenness | hardness | . (You can also visualise embeddings to cluster image or annotation types, but I haven‚Äôt used that feature yet so can‚Äôt comment as its effectiveness.) . For my dataset, visualising similarity and uniqueness revealed what I already knew: that lots of the images were similar. Knowing the context of the documents well means I‚Äôm familiar with how a lot of the documents look the same. Not much of a revelation there. . The mistakenness calculation is useful, however. It compares between the ground truth and the predictions to get a sense of which images it believes contains annotations that might be wrong. I can filter these such that we only show images where it is more than 80% confident mistakes have been made. Instantly it reveals a few examples where there have been annotation mistakes. To take one example, here you can see the ground truth annotations: . . And here you can see what was predicted: . . In this example, it was even clear from the beginning that redactions had been missed, and that the single annotation that had been made (a content box) was incorrect. . Finding missing annotations . We can also view images that the FiftyOne brain tagged as containing missing annotations: . session.view = dataset.match(F(&quot;possible_missing&quot;) &gt; 0) . . Unfortunately the compute_hardness method only works for classification models currently, but regardless I think we have a lot to work with already. . Conclusions and Next Steps . I hope this practical introduction to FiftyOne has given you a high-level overview of the ways the tool can be useful in evaluating your computer vision models. . For my redaction project, I‚Äôm taking some clear action steps I need to work on as a result of some of this analysis. . I need do annotate more of the kinds of images it struggles with. Specifically, this means images containing redactions that are just white boxes, with a bonus for those white redaction boxes being superimposed on top of a page filed with white boxes (i.e. some sort of table or form). | I need to remove some of the bad/false ground truth annotations that the FiftyOne brain helpfully identified. | I will probably want to repeat this process together in a model that was trained together with the synthetic data to see what differences can be observed. | As a general point, I probably want to incorporate visual inspection of the data at various points in the training pipeline, not just after the model has been trained. | . If you know any other tools that help with this kind of visual analysis of model performance and how to improve in a data-driven approach, please do let me know! .",
            "url": "https://mlops.systems/redactionmodel/computervision/tools/debugging/jupyter/2022/03/12/fiftyone-computervision.html",
            "relUrl": "/redactionmodel/computervision/tools/debugging/jupyter/2022/03/12/fiftyone-computervision.html",
            "date": " ‚Ä¢ Mar 12, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Incremental Improvements to my Redaction Detection Model",
            "content": "(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.) . Last time I wrote about my work on this project, I‚Äôd just finished creating synthetic image data to supplement my manual annotations. Before integrating those into the model training, I wanted to make changes to a few hyper parameters to ensure that I‚Äôm getting as much out of the current configuration as possible. . I focused on three ways of improving the model‚Äôs performance, each of which ended up having an effect albeit in different ways. . Increasing the image size . When I started the project, I set the size of the images that would be passed into the training as the training dataset to 384x384 pixels. (It is a convention relating to how some of the older pre-trained models (like EfficientDet) were such that the image size must be divisible by 128.) This turned out to be too small. . The next steps up were 512 and 640. The GPU / hardware on which I was training my model seemed to have no problem with either of these image sizes and the performance increased as I worked with either 512 or 640 as the base image sizes. . Increasing the batch size . Another important lever at my disposal was either to increase the batch size (the number of images that are used in each epoch) or to decrease the learning rate. (A useful Twitter thread by Boris Dayma explains some of the tradeoffs for one versus the other, along with some references to things to read). . I had started off with a batch size of 8, but increasing to 16 and then 32 had a big effect on my model‚Äôs performance: . . Batch sizes of both 16 and 32 eventually converged on more or less the same COCOMetric score of around 74%. The validation loss rate showed pretty clearly that the highest (32) batch size overfit far faster than for 16. It seems that 16 is the best choice for now. . Backbone model size . The way I‚Äôve set things up to train this object detection model requires two main choices in terms of architecture: a particular pre-trained model and a backbone. VFNet (as mentioned previously) outperformed basically everything else I‚Äôve tried and I think it seems to be a clear best choice for the model. In terms of the backbone, I‚Äôd been using resnet50 until now, but following some of the above experiments, it made sense to try increasing the backbone size as well. (An obvious disadvantage to this approach was slower training times and a larger final model size.) . . In this image you can see the stages of improvements we made throughout this whole process. vfnet-pre-synthetic-base was the lowest performer at the beginning, then doubling the batch size gave another boost of almost 8% to our model performance. Then the final increase to the backbone size added another 4% increase bringing us to a score of around 78% for the COCOMetric. . It remains to be seen how much of these changes will make sense when I introduce the synthetic data, or if there are more effective boosters to the model performance in the form of adding annotations to areas where the model struggles the most. .",
            "url": "https://mlops.systems/redactionmodel/computervision/tools/2022/03/03/model-improvements.html",
            "relUrl": "/redactionmodel/computervision/tools/2022/03/03/model-improvements.html",
            "date": " ‚Ä¢ Mar 3, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "Three Python Helpers for Parsing Inputs",
            "content": "I continue to slowly work my way through the calmcode back catalogue. This week I learned about three tiny utility packages that make certain parsing tasks less painful. . parse (introduced here) is a way of turning simple text patterns into restructured data. Take the following example as an illustration: . from parse import parse url = &quot;https://github.com/strickvl/some-repo/&quot; parse(&quot;https://github.com/{owner}/{repo}/&quot;, url).named # returns {&#39;owner&#39;: &#39;strickvl&#39;, &#39;repo&#39;: &#39;some-repo&#39;} . As Vincent explains, it‚Äôs sort of the inverse or opposite operation to what happens with an f-string. . For URLs of various kinds that you want to decompose easily, yarl (introduced here) is a great way to approach that in Python. . For dates stored in some kind of a string format, you might want to try datefinder (introduced here), an elegant if not always perfect way for converting date strings into datetime.datetime objects. .",
            "url": "https://mlops.systems/python/tools/2022/02/27/python-parsers.html",
            "relUrl": "/python/tools/2022/02/27/python-parsers.html",
            "date": " ‚Ä¢ Feb 27, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "It's raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model",
            "content": ". This blog outlines my process (and a few false starts) for generating a series of synthetic images (and corresponding annotations) to supplement training data used in a machine learning project. This problem is one for which there aren‚Äôt many (any?) pre-existing data sets that I can repurpose so I‚Äôve been trying to find ways to bootstrap and improve the performance of the model I‚Äôm training. . Before I dive into the details, I wanted to include a little context on the wider project and what I‚Äôm seeking to accomplish. It is a relatively common practice for documents released as part of FOIA requests to contain redactions. With so many documents being released ‚Äî and perhaps in specific cases where legal teams are dealing with huge numbers of those redacted documents ‚Äî it can be useful to identify which documents are redacted and/or to get a sense of just how much has been redacted. If you have 10 or 20 documents you can fairly easily get that overview, but if you have 10,000 or a million documents? That‚Äôs where my project comes in: I want to train a model to make it easy to detect redactions in a document and to generate statistics on what proportion of a document or documents have been redacted. . You can read more about the problem domain, about my initial forays into annotating a dataset for this problem, as well as view some examples of these redactions (and perhaps why they‚Äôre not as easy to identify as you might think). You can even try out a demo showing some of what the model can identify here. Note that this isn‚Äôt the latest version of the model so it‚Äôs not the absolute best performance. . What‚Äôs the deal with synthetic images? . It‚Äôs a truism that in computer vision projects you probably need or want a lot of data to get good results. For the Facebooks and Bytedances of the world this perhaps isn‚Äôt an issue: they have access to a ton of data, for better or for worse. But for me, I don‚Äôt have teams of data annotators or millions of users generating all this data. This is probably the norm for small- to medium-sized computer vision problems being solved out in the world, especially with more non-traditional entrants into the field who are just trying to do things with the skills instead of generating research and so on. . Instead of using huge amounts of data, we need to be smarter about how we work, levelling ourselves up with whatever tricks of the trade we can muster. The fastai course contains a great number of these best practices, perhaps unsurprisingly since it is in some way targeted at individuals seeking to solve their domain-specific problems. One of the key insights I took away from earlier parts of the fastai book was the benefits of using pre-trained models. With a wealth of these models available and accessible, you don‚Äôt need to start your work from scratch. Instead, fine-tune your model and benefit from the expertise and hard work of others. . You do need some data to get started with fine-tuning a pre-trained model, however. That‚Äôs why I took a bit of time to make some initial annotations. I currently have annotated 2097 images, labelling where I have found redactions on the images as well as a box to show which parts of the image contain text or content. That approach has done pretty well so far, with in the low to mid seventies in terms of a % COCO score. (This is a commonly-used metric to assess the performance for object detection problems.) I want to go further, though, which is where synthetic images come in. . The big bottleneck in the annotation process is, of course, me. Depending on how many redactions any particular image contains, it could take me 5-10 minutes for a single image‚Äôs worth of annotations. This does not scale. Part of the speedup for this process is to use self-training, but I‚Äôll write about that separately. Another option that has is often used is to generate images which approximate (to a greater or lesser degree) the actual real images. The useful thing about generating the images yourself is that you know where you placed the redactions, so you have the annotations at the same time. . My overall goal here was to boost my model‚Äôs performance. I didn‚Äôt know how how well these synthetic images would contribute, or even if they‚Äôd contribute to any boost at all. I was also quite conscious of the fact that you could probably spend a year generating pixel-perfect synthetic redacted documents. I didn‚Äôt want to waste too much time doing that, so at various points I had to make decisions as to whether a particular stage was good enough. . Phase 1: Get a Baseline / Naive Trial . When I started this, I didn‚Äôt know how hard or easy it was going to be, so I set myself a low bar. I knew it was theoretically possible to create images with Python, but I‚Äôd never done it before so didn‚Äôt have a sense of the range of possibilities. . In situations like this, I find Jupyter notebooks really reveal their strengths. Experimentation is easy and the pace of iteration can be really high. A few minutes of searching around and it seemed like Pillow (aka ‚ÄòPIL‚Äô) was probably the best option to go with. I noted that you could edit, resize, copy and paste images. For my basic version of a synthetic image generator, that‚Äôs most of what I needed to do: . Take an image that we know contains no redactions. | Get a separate image file that is of a redaction box / squiggle or shape. | Randomly resize the redaction shape. | Paste the redaction shape at a random location on top of the base unredacted image. | . And voila! Finding unredacted images was easy since I had previously used fastai to build a model that could detect to ~95% accuracy whether an image contained a redaction or not. For the redactions, it took me about an hour with Pixelmator Pro and its ‚Äòquick selection‚Äô tool to extract 100 examples of various kinds of redaction that I knew were commonly found in the data set. You can see some of this variety in the illustration that follows, though note that each individual redaction snippet was its own separate image for the purposes of my synthetic generation. . . I found that it was pretty trivial to generate images of the kind I proposed above. The placement of the redactions didn‚Äôt always make sense, and sometimes the random resize that the redaction underwent meant that it was either far too small or far too large. I also hadn‚Äôt included any steps to capture the annotation in this prototype, but I knew it was possible so continued onwards. . Detour: Get Stuck Pretty Quickly, Experience bbox Sprawl . Buoyed by my success in the prototype stage, I immediately added a bunch of improvements and features to what I wanted to achieve. I knew I wanted to make sure that the redaction stayed within the boundaries of the original base image. I also wanted to ensure that it stayed within the boundaries of the content of the base image ‚Äî i.e. redactions generally tend to be made on top of content which tends not to be right on the outer margins. . I rushed into things too fast without thinking the problem through and quite quickly got into deep waters as all the various pieces started to overlap. I was somehow still in notebook mode, passing various objects through various other custom libraries, not sure what I was passing where. In short: it was a mess. . One thing that tripped me up really fast was bboxes. (A bbox, in case this means nothing to you, is a data structure or type that allows you to represent where a box is positioned if you were to paste it on top of a base image (for example). It seems that there are different conventions about how to represent this concept of the location of a box on top of some other larger space. Some people represented it with pairs of coordinates, such that for each of the four corners of the box you‚Äôd have an [x, y] pair to represent each point. Others took this bbox type to contain references to the xmin, ymin, xmax, and ymax values of the box. In this way you could reconstruct the various corners since you had two opposite corners specified. Another option was that used by COCO, which was [xmin, ymin, width, height]. And yet another option was to represent a bounding box by [x_center, y_center, width, height]. (This is a useful article that details some of these representation differences.) . I‚Äôm sure there are people who are really good at keeping multiple types of x and y coordinates, each with slightly different nuances, in their heads. I am not such a person and after an hour or two of struggling in these deep waters I realised I needed to regroup. . My notebook experiments had been good for uncovering the range of possibility, but now that I had a better sense of the edges of the problem ‚Äî and the twists and turns of dealing with bounding boxes ‚Äî I had to take a more systematic approach. I spent some time with pen and paper thinking through the flow that this synthetic generation process would have to include. I thought through what the various independent parts of this could be, and how data would flow through this set of steps. . Phase 2: Generate My Own Base Images . The first part of this process was to generate my own base images. In general, the types of base unredacted images in the core data set were relatively unremarkable. These were mostly letters, reports or some kind of form / table. I figured I could approximate this pretty quickly. By chance, that very weekend I happened to listen to an episode of the Real Python podcast which interviewed the creator of borb, a Python package for creating and manipulating PDFs. I knew I wanted images in the end, but I had already created a tool to extract images from PDFs and I figured borb would probably save me time, even if it meant I had to do some converting back and forth between images and PDF files. . The great thing about borb is that it offers an easy abstraction with which to reason about creating PDF documents. Have some text and want it to be displayed on a page? Done. Want that text to be displayed in three columns? Done. Want do insert some images and have the text flow round it? Done. Have styling requirements? Done. And on and on. I figured that this was just the level of abstraction I needed ‚Äî rather than staying in the world of pixel primitives like lines and boxes. . Once I got going it was easy to generate base images with multi-column text and some random coloured shapes thrown in here and there. (I used lorem-text to generate random Latin texts.) After I created the PDF I then had to convert it into an image format for use elsewhere in the generator pipeline but I think that speed hit was a price worth paying. . Phase 3: Generate My Own Redactions . The redactions weren‚Äôt quite as easy as the base images. The easiest version of a redaction box was literally that: a black box that sits on top of the base image. That much was easy to create. Pillow had some useful interfaces that I could use to quickly create randomly sized boxes. I could even add text to them in the upper left corner as I‚Äôd noticed that many of the real redactions did that. . It was less clear to me how I‚Äôd go about generating the other kinds of redactions, particularly ones that resembled a handwritten mark in thick black marker over the top of a document. In the end, I decided not to go any further with anything that wasn‚Äôt a box, but I did make the redaction boxes more varied. I set it such that the box would be filled with a random colour. If the colour was dark enough, I made sure that the text was in a light (contrasting) colour. And ensure that there wasn‚Äôt always a text on the box. . Not perfect, but still it gave me a way to move forward. . The Big Picture: Bringing It All Together . With these pieces complete, I had the basics of the next version of my synthetic image generation. You can see the flow and progression of my script in the following diagram: . . You‚Äôll note that there were a number of other steps that supported the image creation. I did again descend into bbox hell when calculating exactly where to paste the redaction image, but with a much more modularised approach to my code I didn‚Äôt get lost. Type hints also kept me honest about what variables I was passing in and out of the functions I‚Äôd created. . I ended up using the initial model I‚Äôd trained so far in the step that figured out where the content of the image was. You‚Äôll recall that this was one of the annotations I‚Äôd already been generating when I annotated my data, and since it‚Äôs a fairly simple computer vision task I was already seeing excellent performance from that specific class in object detection. IceVision, a library that I‚Äôm using for the computer vision and deep learning parts of this project, allowed me to fairly easily make this inference on the images and extract the bbox coordinates for the content box. . I made sure to include a lot of random variation in the first two steps where the base and redaction images were created. I didn‚Äôt remove the original naive approach completely. Instead, I made it 50% likely that we‚Äôd generate an image versus just picking one of the unredacted images from our store. Then I gave the same chance for the redaction as to whether we‚Äôd use an actual redaction snippet or one of the computer-generated boxes. There was lots of resizing and colouring and various other randomisation that was also included. . Phase 5: Make The Images Look Old and Worn . Only one step remained. I realised that when I generated the images completely from scratch, not using any of the real base images or redaction snippets, that they looked very new and unrealistic. A significant proportion of the documents in the collection looked like they‚Äôd been photocopied a thousand times and in general had seen better days. Sometimes the quality was such to make them unreadable. I realised if I was going to get good results with the overall goal (i.e. improve my model‚Äôs performance) I‚Äôd have to make the synthetic creations look old somehow. . After some exploration I settled on augraphy as how I‚Äôd process the newly generated images to look old and worn. Luckily for me, this package seems to have been created explicitly to support machine learning workflows for synthetic data creation, and it seemed to be (somewhat) actively maintained. There was a default set of so-called ‚Äòaugmentations‚Äô that Augraphy suggested I apply to my image. Unfortunately it was simply too aggressive. I guess for some workflows it would have been great, but the page ended up looking somewhat unintelligible by the end. Compare these two examples: . . Not only did the default Augraphy transforms often make the redaction indistinguishable, it shifted parts of the image around on the page for these crinkle and scrunch effects, which would have rendered my annotations inaccurate. . That said, as you can see from the left image, it was pretty easy to switch out the default for a set of random transforms to be applied that wasn‚Äôt quite so aggressive. I‚Äôm thankful that tools like this exist out in the open-source space and that allow me to get on with the work of solving the actual problem I‚Äôm interested in working on. . Final Results: 2097 Synthetic Images . . This gif gives you a brief sense of some of the images I generated as a result of the process I‚Äôve detailed above. They‚Äôre not perfect, and as I write I currently don‚Äôt know how well they will perform when training my model. . I have 2097 real annotated images, so I‚Äôm going to combine them with a maximum of an equal number of synthetic images. I‚Äôll try out different proportions of real to synthetic, but that‚Äôs also a topic for another blogpost to follow. Stay tuned! . It took about three and a half hours to create these 2000+ images on my laptop. There are LOTS of places where I could have made speed improvements, notably all the conversion between PDF and image objects, the inference for the content box and also the fact that the pipeline wasn‚Äôt performed in parallel on all my CPU cores. I spent about 30 minutes exploring Ray as a means to getting this process to be executed in parallel but it ended up being not as simple as I‚Äôd initially thought so I‚Äôve left that to one side for now. In any case, I won‚Äôt be creating so many synthetic images at once so often, so it wasn‚Äôt a real blocking point for my work. . Note, too, that the annotations get created as part of the same script. I append them to a synthetic annotations file at the same time as the synthetic images is generated, and the file is subject to being combined with the real annotations at a later stage. . There are obviously lots of ways this synthetic data creation process could be optimised, but I was recently reminded that it‚Äôs also important not to lose momentum and not to let the perfect be the enemy of the good. . The next step is to carry out an experiment to see the effect of adding in the synthetic annotations on model performance. There are a bunch of really tricky aspects to this (most notably finding ways to make sure not to allow my training data to leak into the validation data) but I‚Äôll save all that for my next blogpost. . (If you got all the way to the end, well done!) .",
            "url": "https://mlops.systems/redactionmodel/computervision/python/tools/2022/02/10/synthetic-image-data.html",
            "relUrl": "/redactionmodel/computervision/python/tools/2022/02/10/synthetic-image-data.html",
            "date": " ‚Ä¢ Feb 10, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "What are invariants and how can they help make your Python classes more robust?",
            "content": "We‚Äôve read about enumerations and we‚Äôve read about data classes. Now it‚Äôs the turn of classes. Chapter 10 of Patrick Viafore‚Äôs excellent book, ‚ÄòRobust Python‚Äô, is the last of the user-defined types to be covered. Early on he makes a good point that classes are often taught really early to those new to Python and/or programming, and that maybe the story is a bit more complicated. As I‚Äôve mentioned before, things like enums and data classes are more or less unmentioned in such educational materials and as such I found this book really helped me fill in the conceptual gaps. . First off, for someone who has just learned about data classes, how would you explain what is new or distinct when it comes to classes? They‚Äôre slightly different syntactically, with classes requiring you to write a bit more boilerplate. Compare the following: . from dataclasses import dataclass import datetime from typing import Literal # data class definition @dataclass class Cat: name: str breed: CatBreed birth_date: datetime.date gender: Literal[&#39;male&#39;, &#39;female&#39;] # class definition class Dog: def __init__(self, name: str, breed: CatBreed, birth_date: datetime.date, gender: Literal[&#39;male&#39;, &#39;female&#39;]): self.name = name self.breed = breed self.birth_date = birth_date self.gender = gender . You can note how it seems like the data class version is much more readable and involves less boilerplate to achieve the same effect, and for a simple example like this you‚Äôre probably right. The difference, and where classes make sense and shine, is when you have a conceptual grouping or type that includes some notion of invariants. . What is an invariant? . Most of this chapter is about invariants and how they relate to classes, and I‚Äôll admit I had never heard of the concept before reading in this book. An invariant is defined as ‚Äúa property of an entity that remains unchanged throughout the lifetime of that entity.‚Äù You can think of it as some kind of context or a property about that particular type that you need to encode and that won‚Äôt change. . The book gives a pizza example (where a Pizza object could encode that in its list of toppings, the cheese could only be the final topping (i.e. on top) of the pizza). An alternative might be some kind of rule relating to an ID number, where either it must be unique to some kind of specification, or where the ID must conform to some kind of specification. . Even with this rudimentary definition, you can see how there might be some advantages to being able to account for these rules and properties of the object type. (With data classes, you don‚Äôt have as much flexibility to specify all these nuances.) So what happens when you‚Äôre instantiating a class and you hit one of those scenarios where your contextual rules dictate that something can‚Äôt happen? (i.e. someone tries to create a Pizza object that has cheese as the bottom-layer topping) The book offers up two options: . Throw an exception ‚Äî this will break you out of the code flow and prevent the object from being constructed | Do something to make the data fit ‚Äî you can perform some kind of transformation which sees the cheese ingredient as being forced onto the top layer of the pizza toppings (or whatever is the equivalent for your specific scenario) | Note that the kinds of restrictions posed by these invariants are things that can‚Äôt fully be captured by the typing system. We‚Äôve covered type hints and how they can help make your code more robust, but types don‚Äôt help much when it comes to the order of a list, for example. . Why code around invariants? . So why go to all of this trouble in the first place? How does it benefit to code with the invariants in mind? To start with, it‚Äôll probably help you think through edge cases and exceptions that you could do well to be wary of. The invariants alert you to the fact that arguments passed into functions and methods will not always be in the form that you would ideally like. (As a side note, this might also encourage you to add unit tests.) . It will help you keep the code that handles the invariants together instead of mixing it in with the code that instantiates the objects. In general, it will enhance your ability to reason about the code and the concepts that your code reflects. This is important not only for the implementation in code, but for how you think about any particular part and how it relates to the rest of your code base. . The goal for all of this: fewer bugs and a more robust system. Yes, it takes a bit more effort to think whether there are implicit or explicit invariants, but doing so makes your code and your system more reliable. In Viafore‚Äôs words: . ‚ÄúYou‚Äôre making an easier API for people to think about, and you reduce the risk of people using your objects incorrectly. [‚Ä¶] You never want someone to be surprised when using your code.‚Äù (p. 141) . Invariants and class consumers . The rest of the chapter is about the implementation consequences of thinking about classes in this invariants-first way. For consumers of the class, how should you ensure that the invariants handled are clear? Aside from the implementation itself (in the constructor), docstrings and code comments are suggested as a means to this end. Of course, README files and documentation in general can serve the same purpose, but it‚Äôs best if the context and information about invariants is as close to the code as possible. . Invariants and class maintainers . For (future) maintainers of the class, unit tests are the way to go. Make sure that the relevant scenarios and invariants are covered by testing code and you will have extra confidence that your object instantiation really does do what you intend. Your code should already be doing the checking for invariants on the instantiation side, but unit tests are a way of ensuring that this is actually the case (and also that these invariants remain covered as the code base continues to evolve. . (The book offers one way of doing such tests for invariants with contextlib.contextmanager on page 145.) . Encapsulation and classes . As the final chunk of the chapter, we learn about private, protected and public access to the properties and methods of a class, and how they relate to the maintenance of invariants. . This is an important part of the story. As users interface with your class and API, encapsulation is a way to ensure that they update and interact with the these properties in a way that is under your control. For example, even if at instantiation you enforce the Pizza object having cheese as the top-layer topping, what do we have in place to ensure that the user doesn‚Äôt just amend the toppings property such that the cheese is the bottom-layer topping (i.e. AFTER instantiation)? Encapsulation ‚Äî having an entity hide or restrict access to certain properties and actions ‚Äî is how you handle that. . The book goes into a fair amount of detail on the uses of these different levels of access, and introduces the idea of ‚Äòaccessors‚Äô and ‚Äòmutators‚Äô as an alternative to the more commonly-used ‚Äògetters‚Äô and ‚Äòsetters‚Äô. . Remember, ‚Äúyou use invariants to allow users to reason about your objects and reduce cognitive load.‚Äù (p. 151) . So what am I supposed to use? . . The end of the chapter offers this really helpful flowchart diagram which summarises the choices that we‚Äôve covered during the previous three chapters. I really want to highlight that this chapter helped me think about classes in a way I hadn‚Äôt, despite having been through courses, having read numerous articles and of course coded in this class-oriented fashion for several years. . The next few chapters continue onwards by thinking about how to design your interfaces such that they make sense for your users and allow your code base to grow with as few headaches as possible. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/02/08/robust-python-10.html",
            "relUrl": "/robustpython/python/books-i-read/2022/02/08/robust-python-10.html",
            "date": " ‚Ä¢ Feb 8, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "Upgrade your Python dicts with data classes",
            "content": "I‚Äôve been curious about data classes since more or less my first day at work when someone mentioned to me that Pydantic was built on the shoulders of data classes. I hadn‚Äôt taken the opportunity to dive into all the details of what data classes do until now, prompted by their being part of Patrick Viafore‚Äôs book, ‚ÄòRobust Python‚Äô, specifically chapter nine. . An example upfront might help ground the conversation. Here is a data class in action: . import datetime from dataclasses import dataclass from typing import Literal @dataclass class CatPassport: name: str breed: CatBreed issue_date: datetime.date expiry_date: datetime.date gender: Literal[&#39;male&#39;, &#39;female&#39;] aria = CatPassport(&quot;Aria&quot;, CatBreed(&#39;bengal&#39;), datetime.date(2022, 01, 05), datetime.date(2025, 01, 04), &#39;female&#39;) print(aria.name) # prints &#39;Aria&#39; . From this you can see that it‚Äôs an easy way to represent structured data made up of different types. Where it excels over simply using a dict or a class you write yourself is the fact that it auto-generates a number of __ dunder helper methods. You get __str__ and __repr__ to handle what this object looks like when you try to print() it. It also creates an __eq__ method which allows you to check for equality between two objects of the same type with the == comparison operator. . (If you want to add a way to compare between your data class objects, you can add arguments to the @dataclass decorator like @dataclass(eq=True, order=True) which will handle the creation of the relevant dunder methods. . The fact that data classes are just classes at heart mean that you can also add behaviours to these collections of values, something that isn‚Äôt possible with a plain dict. . You can specify that your data class should be frozen (@dataclass(frozen=True)) which effectively makes it an immutable data store, though taking note that objects stored as values on the data class‚Äô properties might themselves not be immutable (think lists and dicts). . After reading the chapter in ‚ÄòRobust Python‚Äô, I read around a little to get a sense of this concept. I read the official docs which were fairly helpful, but in fact it was the PEP document (557) that I found most interesting. I haven‚Äôt previously taken the time to dive into the specifics of PEP specifications before, but I discovered that they are pretty readable and you get a real sense of what problem a particular feature or addition to the language was trying to solve. . PEP 557 explains some of the alternatives and why it might be useful to include this new feature. I also learned about the attrs package and how data classes are actually just a subset of what attrs offers. (As a side note, I was surprised that attrs seems to have been mentioned nowhere in ‚ÄòRobust Python‚Äô, even in the context of the upcoming Pydantic chapter. Perhaps it was just too confusing to have all these things alongside one another.) . Other options to consider alongside data classes when dealing with heterogenous data inside a single object or structure include TypedDict and namedtuple, but it seems like the default for this kind of scenario should probably just be a data class, though I should add that it is only part of the standard library for Python 3.7 and above. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/02/05/robust-python-9.html",
            "relUrl": "/robustpython/python/books-i-read/2022/02/05/robust-python-9.html",
            "date": " ‚Ä¢ Feb 5, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "How and where to use enums in Python",
            "content": "The second part of Viafore‚Äôs ‚ÄòRobust Python‚Äô is all about user-created types. We start simple in chapter eight and consider the Enum type as a way of defining a particular restricted set of values. An example might help get us started: . from enum import Enum class TrafficLightsState(Enum): RED = &quot;red&quot; YELLOW = &quot;yellow&quot; GREEN = &quot;green&quot; OFF = &quot;off&quot; current_state = TrafficLightsState.GREEN print(current_state.value) # prints &#39;green&#39; . We subclass off Enum and define the pairings of values that belong together. I hope you can see already that this is a readable way to define these values and show that they are part of the same semantic grouping. . If we‚Äôre using these definitions not because we care about the values themselves but because we want to be able to evaluate whether the state of one particular traffic light is the same as a different traffic light, we can use auto to automatically assign values (ascending integers, by default) in the following way: . from enum import Enum, auto class TrafficLightsState(Enum): RED = auto() YELLOW = auto() GREEN = auto() OFF = auto() current_state = TrafficLightsState.GREEN print(current_state.value) # prints 3 . You can iterate through your enums or get their length just as if it was a list, too. . While writing the above text, I realised that I was getting confused about the difference between types and classes in Python. It turns out that whatever differences once existed, they aren‚Äôt much of a thing any more and to all intents and purposes they‚Äôre practically the same thing. . A lot of the enum-related definitions at work are defined in this file. You can see that we tend not to use auto, though I‚Äôm not really sure why. (We don‚Äôt ever seem to compare against actual values.) . If you want to make sure that the actual values assigned to these grouped constants are unique, you can add the @unique decorator which will enforce that you aren‚Äôt duplicating values. . Better still for the readability of your code, you can use this collective type in your type annotations. For sure the difference between these two options should be clear: . def get_status(some_input: str) -&gt; str: # code goes here def get_status(some_input: str) -&gt; TrafficLightsState: # code goes here . In the first case, it is far less clear what‚Äôs going on. . Note that if you‚Äôre purely looking for a way to restrict the assignation to a particular variable, you can also use the Literal type, introduced in Python 3.8, though remember that it doesn‚Äôt help with iteration, runtime checking or map values from name to value. For all that, you‚Äôll want to be using Enum.‚Äù . If you want a way to combine Enums together, you can subclass from enum.Flag. Consider the case of when you have a list of enums for days of the week, but you want to represent the weekend as a pairing of Saturday and Sunday (if you were in Europe, e.g.). You could do the following: . from enum import Flag, auto class Weekday(Flag): MONDAY = auto() TUESDAY = auto() WEDNESDAY = auto() THURSDAY = auto() FRIDAY = auto() SATURDAY = auto() SUNDAY = auto() weekend = Weekday.SATURDAY | Weekday.SUNDAY . You can perform bitwise operations on these combined groupings, but note that the values must support bitwise operations. (Strings don‚Äôt support them, while integers do.) . Finally, the chapter covers the special case of IntEnum and IntFlag which allows for the conversion of integer values. This can be confusing and lead to non-robust behaviours, so the book discourages this particular usage. . Next up is Data Classes, something I‚Äôm extremely interested in getting to grips with as it comes up in our codebase at work a decent amount. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/01/30/robust-python-8.html",
            "relUrl": "/robustpython/python/books-i-read/2022/01/30/robust-python-8.html",
            "date": " ‚Ä¢ Jan 30, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "Using mypy for Python type checking",
            "content": "The final two chapters of part one of Patrick Viafore‚Äôs ‚ÄòRobust Python‚Äô cover more practical advice on how to actually use and implement type checking in either a new project or a legacy codebase. . mypy is the most commonly used option for type checking in Python and it does most of what you probably need it for. You can run it via the command line, inline as part of your IDE, or as part of a CI/CD pipeline. At work we do all three. . You can configure mypy to your heart‚Äôs desire either with inline comments in your code, or via a configuration file. A configuration file is probably the way to go, particularly if you‚Äôre versioning your code and sharing these kinds of settings across a team. . Chapter 6 goes into detail about some of the specific options or settings you can tweak to make mypy more or less sensitive to certain kinds of errors. For example, in a previous post we mentioned how you can implicitly accept None as a type with the Optional type annotation wrapper. But maybe you don‚Äôt want to allow this behaviour because it‚Äôs generally not a great idea: if so, you can use the ‚Äîstrict-optional flag to get notified whenever you‚Äôre using that particular construction. . mypy also allows for the export of its results to html and xml, and you can run it in the background as a daemon which (particularly for large code bases) might speed it up. . We also learn about some alternatives to mypy, namely Pyre and Pyright. Pyre runs as a daemon in the background and allows you to run queries relating to type usage in your codebase. It also includes a static code analyser called Pysa that runs a kind of security analysis on your code called ‚Äòtaint analysis‚Äô. A quick summary of this would be to say that you can specify specific kinds of security flaws that you want to address and/or prevent being part of your codebase. . Pyright is interesting since it has a useful VS Code integration (via the Pylance extension). You get all sorts of autocompletion and tooltip goodness by using Pyright/Pylance. . Finally, chapter 7 thinks through how you might want to approach actually using type checking and type hints in a larger codebase, perhaps one that already exists. It‚Äôs useful this was included as I imagine these sorts of practicalities are much more of a blocker to adoption than any technical issues. After a brief discussion of tradeoffs, we learn about some different options for where you might want to start with introducing types to a legacy codebase. . Focusing on the pain points ‚Äî i.e. where the lack of type hints has already seen bugs emerge in the past | or perhaps adding them to new code only | or perhaps type annotating the pieces of the codebase that actually drive the product or business‚Äô profits | or maybe whatever is complex to understand | . All of these are options and it will definitely depend on your particular situation. . We also learn about two tools that might help get you started with type annotation: MonkeyType and Pytype. Both auto-generate type hints for your codebase. MonkeyType does so dynamically, so it only generates type hints for parts of your code that it accesses while running the code. Pytype does so by static analysis. Both deliver some kind of output that you can then use (perhaps) as the basis of some annotations of your codebase. My instinct is that these two tools feel like they might lead to some faulty assumptions or errors if you rely on them too much and that in fact it would be better to just methodically go through your code and incrementally add type hints as suggested above. . This concludes the type hints part of the book. I feel like I really got a solid overview of why type hints are used in large or complex Python codebases as well as how to implement this practically. I will be writing separately about how we use mypy and type hinting at ZenML as I think it offers an interesting case study on some of the benefits and tradeoffs that we‚Äôve observed on a day-to-day basis. . Next up in Robust Python: defining your own types with Enums, data classes, classes and how this fits into libraries like Pydantic. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/01/22/robust-python-6.html",
            "relUrl": "/robustpython/python/books-i-read/2022/01/22/robust-python-6.html",
            "date": " ‚Ä¢ Jan 22, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "Using type annotation with collections in Python",
            "content": "The fifth chapter of ‚ÄòRobust Python‚Äô continues on from where we left off last time. We saw how to apply type annotations when simple things like strings, integers and floats were involved. This chapter deals with the different ways you annotate your types when collections get involved. . We start with the context for why this is even something that requires a separate chapter to deal with. This involves the difference between homogenous and heterogeneous types. For a Python list, we could say it had homogenous types if all the items were of the same type (strings, e.g.). If this list contains multiple different types (a mix of strings and integers, e.g.) then we‚Äôd have to say it contained heterogenous types. This is of importance given that the presence of multiple types in a single list is going to require you to handle the types differently. Even in the most trivial of examples (as with strings and integers being together), the interfaces for both are different. Try adding a string to an integer in Python and see what happens. . So it‚Äôs actually not quite true to say that a collection of homogenous types have to all be exactly the same type, but they must share common interfaces and ideally be handled using the same logic. If you think about it, in the real world heterogenous types are pretty common occurrences. There are often situations where, for example, you have to handle the output of API calls or data that doesn‚Äôt derive from code that‚Äôs in yous control and then you‚Äôll perhaps be dealing with a dictionary that contains all sorts of types. . In Python we do have the typing.Any annotation, but it‚Äôs pretty clear ‚Äî and the book emphasises this ‚Äî that isn‚Äôt really useful in the vast majority of cases. You might as well not bother with type annotations if you‚Äôre going to liberally be using Any. . The first of our collection type helpers: TypedDict . TypedDict was introduced in Python 3.8 and allows you to communicate intent when it comes to the types that are being passed through your code. Note that, as with a lot of what we‚Äôre talking about here, this is all information that‚Äôs useful for a type checker and isn‚Äôt something that is dynamically checked. . You can use TypedDict to define structures that specify the types of fields of your dictionary in a way that is easier to parse as a human reader than just using dict. See this example, adapted from one in the book: . from typing import TypedDict class Range(TypedDict): min: float max: float class Stats(TypedDict): value: int unit: str confidenceRange: Range our_stats = Stats(value=3, unit=&quot;some_name&quot;, confidenceRange=Range(min=1.3, max=5.5)) print(our_stats) # returns {&#39;value&#39;: 3, &#39;unit&#39;: &#39;some_name&#39;, &#39;confidenceRange&#39;: {&#39;min&#39;: 1.3, &#39;max&#39;: 5.5}} . If TypedDict doesn‚Äôt do everything you need it to, we have some other options. . Custom Collections with TypeVar . TypeVar in Python is how you can implement generics. Generics, as I learned while reading, are ways of representing things that are the same, like when you don‚Äôt care what specific type is being used. Take this example from the book, where you want to reverse items in a list, but only if the items are all of the same type. You could write the following: . from typing import TypeVar T = TypeVar(&#39;T&#39;) def reverse(coll: list[T]) -&gt; list[T]: return coll[::-1] . You can use generics in other ways to create new kinds of collections or groupings. For example, again this one is adapted from the book, if you were writing a series of methods that returned either something useful or a particular error message: . def get_weather_data(location: str) -&gt; Union[WeatherData, APIError]: # ‚Ä¶ def get_financial_data(transaction: str) -&gt; Union[FinancialData, APIError]: # ‚Ä¶ . ‚Ä¶and so on, you could use generics as a way of simplifying how this gets presented: . T = TypeVar(&#39;T&#39;) APIResponse = Union[T, APIError] def get_weather_data(location: str) -&gt; APIResponse[WeatherData]: # ‚Ä¶ def get_financial_data(transaction: str) -&gt; APIResponse[FinancialData]: # ‚Ä¶ . That looks and feels so much cleaner! . Tweaking existing functionality with collections . If you‚Äôre just making slight changes to the behaviour of collections, instead of subclassing dictionaries or lists or whatever, it‚Äôs better to override the methods of collections.UserDict, collections.UserString and/or collections.UserList. . You‚Äôll run into fewer problems when you actually implement this. Of course, there is a slight performance cost to importing these collections, so it‚Äôs worth making sure this cost isn‚Äôt too high. . You‚Äôll maybe have noticed that there isn‚Äôt a collections.UserSet in the list above. For sets we‚Äôll have to use abstract base classes which are found in collections.abc. The big difference between the User* pattern of classes, there is no built-in storage for the abc classes. You have to provide your own storage if you need it. So for sets, we‚Äôd use collections.abc.Set and then implement whatever group of methods are required for that particular class. . In the set example, we have to implement __contains__, __iter__ and __len__, and then the other set operations will automatically work. There are currently (as of Python 3.10.2) 25 different ABCs available to use. I definitely will be exploring those as they seem really useful. . Even though this chapter got into the weeds of collections a little, I learned a lot and I‚Äôm already finding places in the ZenML codebase where all of this is being used. . Typeguard . Before I leave, since we‚Äôre still thinking about types, I wanted to share this little package I discovered the other day: typeguard. You can use it in a bunch of different ways, but a useful short video from calmcode.io showed how a simple decorator can simplify code and catch type errors. . Consider the following example code: . def calculate_risk(risk_factor: float) -&gt; str: &quot;&quot;&quot;Calculates how much risk you took&quot;&quot;&quot; return risk_factor * 3 # arbitrary return value :) . What if someone passes in a wrong type into this function? It‚Äôll fail. So maybe we want to handle that particular situation: . def calculate_risk(risk_factor: float) -&gt; str: &quot;&quot;&quot;Calculates how much risk you took&quot;&quot;&quot; if not isinstance(risk_factor, float): raise ValueError(&quot;Wrong type for risk_factor&quot;) return risk_factor * 3 . If you have lots of parameters in your function and you have to handle them all, this could get messy quite quickly. Instead, we can pip install typeguard and do the following: . from type guard import typechecked @typechecked def calculate_risk(risk_factor: float) -&gt; str: &quot;&quot;&quot;Calculates how much risk you took&quot;&quot;&quot; return risk_factor * 3 . Now that‚Äôs a handy little decorator! It‚Äôll handle all the raising of appropriate errors above based on whether you passed in the right type or not. It works for classes as well. You‚Äôre welcome, and thanks Vincent for making the introductory video! .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/01/18/robust-python-5.html",
            "relUrl": "/robustpython/python/books-i-read/2022/01/18/robust-python-5.html",
            "date": " ‚Ä¢ Jan 18, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "A Midway Report on my Computer Vision Project",
            "content": "(This post is adapted from a twitter thread, so is a bit more terse than usual.) . I recently switched what I spend the majority of my professional life doing (history -&gt; software engineering). I‚Äôm currently working as an ML Engineer at ZenML and really enjoying this new world of MLOps, filled as it is with challenges and opportunities. . I wanted to get some context for the wider work of a data scientist to help me appreciate the problem we are trying to address at ZenML, so looked around for a juicy machine learning problem to work on as a longer project. . I was also encouraged by Jeremy Howard‚Äôs advice to ‚Äúbuild one project and make it great‚Äù. This approach seems like it has really paid off for those who‚Äôve studied the fastai course and I wanted to really go deep on something myself. . Following some previous success working with other mentors from SharpestMinds on a previous project, I settled on Computer Vision and was lucky to find Farid AKA @ai_fast_track to mentor me through the work. . In the last 6 weeks, I‚Äôve made what feels like good progress on the problem. This image offers an overview of the pieces I‚Äôve been working on, to the point where the ‚Äòsolution‚Äô to my original problem feels on the verge of being practically within reach. . . After just a few lessons of the FastAI course, I trained a classification model to ~95% accuracy to help me sort redacted images from unredacted images. . I used Explosion‚Äôs Prodigy to annotate an initial round of data to pass into the next step, enjoying how the labelling process brought me into greater contact with the dataset along the way. . I switched to using IceVision to help me with the more complicated object detection problem, using MMDetection and VFNet to get pretty good results early on. . I‚Äôm currently in the process of creating my own synthetic images to boost the annotations I‚Äôve manually made. (I‚Äôll be writing about this process soon as well, as I‚Äôm learning a lot about why this is so important for these kinds of computer vision problems.) . I‚Äôve also been amazed at the effectiveness of self-training (i.e. using my initial model in my annotation loop to generate an initial set of annotations which I can easily amend as appropriate, then feeding those annotations in to create a better model and so on). More to follow on that step, too. . I started using Evidently to do some drift detection, inspired by some work I was doing for ZenML on adding Evidently as an integration to our own tool. This helped me think about how new data was affecting the model and the training cycle. I feel like there‚Äôs a lot of depth here to understand, and am looking forward to diving in. . I made a tiny little demo on HuggingFace Spaces to show off the current inference capabilities and to see the model in a setting that feels close to reality. This is a simple little Gradio app but I liked how easy this was to put together (a couple of hours, mainly involving some build issues and a dodgy requirements.txt file) . Along the way, I found it sometimes quite painful or fiddly to handle the PDF files that are the main data source for the project, so I built my own Python package to handle the hard work. I used fastai‚Äôs nbdev to very quickly get the starters of what I‚Äôm hoping might be a useful tool for others using PDF data for ML projects. . Throughout all this, Farid has been patiently helping guide me forward. He saved me from going down some dark rabbit holes, from spending too long studying skills and parts of the problem that needed relatively little mastery in order to get to where I am. . Farid has been a consistently enthusiastic and kind advocate for my work, moreover, and this has really helped me stay the course for this project that takes a decent chunk of my time (especially seeing as I do it completely aside / separately from my day job). . I feel like I‚Äôm consistently making progress and learning the skills of a data scientist working in computer vision, even though I have so much left to learn! My project still has a ways to go before it‚Äôs ‚Äòdone‚Äô, but I‚Äôm confident that I‚Äôll get there with Farid‚Äôs support. (Thank you!) .",
            "url": "https://mlops.systems/python/fastai/tools/redactionmodel/2022/01/16/midway-report-redaction-project.html",
            "relUrl": "/python/fastai/tools/redactionmodel/2022/01/16/midway-report-redaction-project.html",
            "date": " ‚Ä¢ Jan 16, 2022"
        }
        
    
  
    
        ,"post19": {
            "title": "Different ways to constrain types in Python",
            "content": "The fourth chapter of ‚ÄòRobust Python‚Äô continues on from where we left off last time. We had previously learned about the benefits of type annotations in general terms, as well as started to understand how we might apply these annotations to simple code examples. But what if things are a bit more complicated? Then we have a few more options at our disposal. . Note that you can assign all of these type assignments to variables (‚Äòtype aliases‚Äô), which might just make your code that much more readable. . Optional to catch None references . Optional as a type annotation is where you want to allow a specific type or None to be passed in to a particular function: . from typing import Optional def some_function(value: Optional[int]) -&gt; int: # your code goes here . Note that you‚Äôll probably want (and mypy will remind you if you forget) to handle what happens in both those cases inside your function. (You may need to specifically pass in the ‚Äîstrict-optional flag to catch this when using mypy.) . Union to group types together . This is used when multiple different types can be used for the same variable: . from typing import Union def returns_the_input(input: Union[str, int]) -&gt; Union[str, int]: return input . This function doesn‚Äôt really do anything, but you get the idea. Note, too, that Optional[int] is really a version of Union[int, None]. (The book gets into exactly why we might care about reducing the number of possible options by way of a little detour into set theory.) . Literal to include only specific values . A little like what I believe enumerations do, we also have the Literal type. It restricts you to whatever specific values are defined: . from typing import Literal def some_function(input: Literal[1, 2, 3]) -&gt; int: return input . Here the function is restricted to inputs that are either 1, 2 or 3. Note that these are a feature that applies to Python 3.8 and above. . Annotated for more complicated restrictions . These are available, but not really useful since they only function as a communication method. You can specify specific restrictions such as the following (example is taken from the book, p. 56: . from typing import Annotated x: Annotated[int, ValueRange(3,5)] y: Annotated[str, MatchesRegex(&#39;[abc]{2}&#39;) . Read more about it here. The book doesn‚Äôt spend much time on it and it seems like it‚Äôs probably best left alone for the moment. . NewType to cover different contexts applied to the same type . NewType, on the other hand, is quite useful. You can create new types which are identical to some other type, and those new values made with the new type will have access to all the methods and properties as the original type. . from typing import NewType class Book: # you implement the class here NewBook = NewType(&quot;NewBook&quot;, Book) def process_new_book(book: NewBook): # here you handle what happens to the new book . You can achieve something like the same thing with classes and inheritance, I believe, but this is a lightweight version which might be useful to achieve the same end goal. . Final to prevent reassignment / rebinding . You can specify that a particular variable should have a single value and that value only. (Note that mutations of an object etc are all still possible, but reassignment to a new memory address is not possible. . from typing import Final NAME: Final = &quot;Alex&quot; . If you tried to subsequently change this to a different name, mypy would catch that you‚Äôd tried to do this. This can be valuable across very large codebases, where the potential for someone to reassign a variable might be not insignificant. . So there you have it: a bunch of different ways to handle combinations of types and/or more complicated annotation scenarios. The next chapter will cover what happens when we throw collections into the mix, and what type annotation challenges are raised. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/01/08/robust-python-4.html",
            "relUrl": "/robustpython/python/books-i-read/2022/01/08/robust-python-4.html",
            "date": " ‚Ä¢ Jan 8, 2022"
        }
        
    
  
    
        ,"post20": {
            "title": "Learning about 'nbdev' while building a Python package for PDF machine learning datasets",
            "content": "While working to develop a computer vision model that detects redactions in documents obtained as a result of FOIA requests, I have encountered some tasks that I end up repeating over and over again. Most of the raw data in the problem domain exists in the form of PDFs. These PDF files contain scanned images of various government documents. I use these images as the training data for my model. . The things I have to do as part of the data acquisition and transformation process include the following: . downloading all the PDF files linked to from a particular website, or series of web pages | converting and splitting all the downloaded PDF files into appropriately sized individual image files suitable for use in a computer vision model | generating statistics on the data being downloaded and processed, as well as (further down the line) things like detecting data drift for incoming training data | splitting up data as appropriate for train / validation / test data sets | extracting text data from the images via an OCR process | versioning, syncing and uploading those images to an S3 bucket or some other cloud equivalent for use in the overall workflow | . It‚Äôs not hard to see that many of these things likely apply to multiple machine learning data acquisition scenarios. While writing the code to handle these elements in my specific use case, I realised it might be worth gathering this functionality together in an agnostic tool that can handle some of these scenarios. . I had wanted to try out nbdev ever since it was announced back in 2019. The concept was different to what I was used, but there were lots of benefits to be had. I chose this small project to give it an initial trial run. I didn‚Äôt implement all of the above features. The two notable missing parts are text extraction and data versioning and/or synchronisation. . pdfsplitter is the package I created to scratch that itch. It‚Äôs still very much a work in progress, but I think I did enough with nbdev to have an initial opinion. . I think I had postponed trying it out because I was worried about a steep learning curve. It turned out that an hour or two was all it took before I was basically up and running, with an understanding of all the relevant pieces that you generally use during the development lifecycle. . Built in to nbdev in general is the ability to iterate quickly and driven by short, small experiments. This is powered by Jupyter notebooks, which are sort of the core of everything that nbdev is about. If you don‚Äôt like notebooks, you won‚Äôt like nbdev. It‚Äôs a few years since it first saw the light of day as a tool, and as such it felt like a polished way of working, and most of the pieces of a typical development workflow were well accounted for. In fact, a lot of the advantages come from convenience helpers of various kinds. Automatic parallelised testing, easy submission to Anaconda and PyPi package repositories, automatic building of documentation and standardising locations for making configuration changes. All these parts were great. . Perhaps the most sneakily pleasant part of using nbdev was how it encouraged best practices. There‚Äôs no concept of keeping test and documentation code in separate silos away from the source notebooks. Following the best traditions of literate programming, nbdev encourages you to do that as you develop. Write a bit of code here, write some narrative explanation and documentation there, and write some tests over there to confirm that it‚Äôs working in the way you expected. When Jeremy speaks of the significant boost in productivity, I believe that a lot of it comes from the fact that so much is happening in one place. . While working on pdfsplitter, I had the feeling that I could just focus on the problem at hand, building something to help speed up the process of importing and generating images from PDF data for machine learning projects. . Not everything was peaches and roses, however. I ran into a weird mismatch with the documentation pages generated and my GitHub fork of nbdev since I was using main as the default branch but nbdev still uses master. I will be submitting an issue to their repository, and it was an easy fix, but it was confusing to struggle with that early on in my process. I‚Äôm also not sure how well nbdev will gel with large teams of developers, especially when they‚Äôre working on the same notebooks / modules. I know reviewnb exists now and even is used within fastai for code reviews, but I would imagine an uphill battle trying to persuade a team to take a chance with that. . I‚Äôve been using VSCode at work, supercharged with GitHub Copilot and various other goodies, so it honestly felt like a bit of a step back to be forced to develop inside the Jupyter notebook interface, absent all of my tools. I also found the pre-made CLI functions a little fiddly to use ‚Äî fiddly in the sense that I wish I‚Äôd set up some aliases for them early on as you end up calling them all the time. In fact, any time I made a change I would find myself making all these calls to build the library and then the documentation, not forgetting to run the tests and so on. That part felt a bit like busy work and I wish some of those steps could be combined together. Maybe I‚Äôm using it wrong. . All in all, I enjoyed this first few hours of contact with nbdev and I will continue to use it while developing pdfsplitter. The experience was also useful to reflect back into my current development workflow and environment, especially when it comes to keeping that close relationship between the code, documentation and tests. . [Photo by Laura Ockel on Unsplash] .",
            "url": "https://mlops.systems/python/jupyter/fastai/tools/2022/01/06/nbdev-early-impressions.html",
            "relUrl": "/python/jupyter/fastai/tools/2022/01/06/nbdev-early-impressions.html",
            "date": " ‚Ä¢ Jan 6, 2022"
        }
        
    
  
    
        ,"post21": {
            "title": "Getting practical with type annotations and `mypy`",
            "content": "The third chapter of ‚ÄòRobust Python‚Äô offers a quick introduction to the practicalities of type annotations in Python. We also see tools like mypy being used to catch places where the reality of your code doesn‚Äôt necessarily match the type annotations that you‚Äôve stated. . For the first, a quick example can suffice: . name: str = &quot;alex&quot; def some_function(some_number: int, some_text: str = &quot;some text&quot;) -&gt; str: # your code goes here return &quot;&quot; # returns a string . You can see the different places that type annotations might appear. You can annotate variables in your code. I‚Äôve seen this one less often, but it‚Äôs possible. Then you can have type annotations for the parameters when defining functions (some even with default values assigned). You can also have type annotations for the return value of those functions. . Note that type hints are not used at runtime, so in that sense they are completely optional and don‚Äôt affect how your code runs when it‚Äôs passed through the Python interpreter. (Type hints were introduced in Python 3.5, though there is a way to achieve the same effect using comments and a standard way of listing type annotations that way if you are stuck with a 2.7 codebase, for example.) . With some type annotations added to our code, we can use a typechecker like mypy to see whether things are really as we imagine. In Viafore‚Äôs own words: . ‚Äútype checkers are what allow the type annotations to transcend from communication method to a safety net. It is a form of static analysis.‚Äù . If your codebase uses type annotations to communicate intent, and you‚Äôre using mypy to catch any of those type errors, remember that typecheckers only catch this certain type of errors. You still need to be doing testing and all the other best practices to help catch the rest. . One forward-looking benefit covered by this chapter was how having code covered with type annotations and type checking could give you the confidence to change things in the codebase that otherwise you would have hesitated to even approach. There are, of course, also some tradeoffs and disadvantages to adding this in: particularly around speed of iteration and possibly flexibility, but the book makes a strong case for why most large Python codebases could probably use type checking as part of their arsenal. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2022/01/03/robust-python-3.html",
            "relUrl": "/robustpython/python/books-i-read/2022/01/03/robust-python-3.html",
            "date": " ‚Ä¢ Jan 3, 2022"
        }
        
    
  
    
        ,"post22": {
            "title": "Counter: a shortcut to counting iterables in Python",
            "content": "I came across this special dictionary type while reading an earlier chapter of ‚ÄòRobust Python‚Äô the other day. It‚Äôs perhaps best illustrated with an example: . from collections import Counter Counter([1,1,2,3]) # returns Counter({1: 2, 2: 1, 3: 1}) Counter(&#39;The Netherlands&#39;.lower()) # returns Counter({&#39;e&#39;: 3, &#39;t&#39;: 2, &#39;h&#39;: 2, &#39;n&#39;: 2, &#39; &#39;: 1, &#39;r&#39;: 1, &#39;l&#39;: 1, &#39;a&#39;: 1, &#39;d&#39;: 1, &#39;s&#39;: 1}) . I had no idea this existed, and of course usually default to some kind of a cookie-cutter loop when trying get counts of elements and put those counts into a dict. . To get the inividual elements, just call the elements method on the Counter object. To get the most common n elements, call the most_common(n) method. To get the total number of counts inside the dictionary, use the total method. To reset all the counts, use the clear method. . Just a nice little set of functionality, hiding in plain sight inside the Python standard library. . Photo by Ibrahim Rifath on Unsplash .",
            "url": "https://mlops.systems/python/2022/01/01/counter.html",
            "relUrl": "/python/2022/01/01/counter.html",
            "date": " ‚Ä¢ Jan 1, 2022"
        }
        
    
  
    
        ,"post23": {
            "title": "What's special about types in Python?",
            "content": "The first section of Robust Python dives into types. We begin by taking a step back to think about what exactly types are being used for, and what they might bring us. Python was not (until v3.5) a language with which you could easily use typing. I remember going to the Pylondinium conference in London in 2018 and going to a talk by Bernat Gabor about type hints in Python. Back then I didn‚Äôt have much of a sense of how new they were to many people, but even now I don‚Äôt get the feeling that they‚Äôve been universally adopted. Hence Patrick‚Äôs book, I suppose‚Ä¶ . A type is defined in the book as being ‚Äúa communication method‚Äù, both to / for computers (‚Äúmechanical representation‚Äù) as well as for humans (‚Äúsemantic representation‚Äù). For the computer, when a variable is of a certain type this determines what methods can be called on that particular object. As such, though I‚Äôm straying into territory I don‚Äôt fully understand, I believe it also helps with compilation efficiency. (Python is a dynamically-typed language so any errors or type mismatches will only become apparent at runtime, however). . For humans, types can help signal intent. This connects with my previous chapter summary from this book where I stated that code should communicate intent well to be considered ‚Äòrobust‚Äô. Take the following simple code snippet: . dates = [...] def process_date(input): date = extract_date(input) dates.append(date) return date . We have an extract_date function (defined elsewhere in the code), but we have no real sense of what this input parameter would be. Are we taking in strings as input? Are we taking in datetime.datetime objects? Does the extract_date function accept both, or do we need to ensure that we are only taking a specific type? All these questions could be cleared up with a simple type hint as part of the function definition, like so: . dates = [...] def process_date(input: datetime.datetime): date = extract_date(input) dates.append(date) return date . Now we know what the input should be, and we can also add a type hint to the extract_date function as well which will help communicate our intent. . We also learn how Python is more towards the ‚Äòstrongly-typed‚Äô side of things on the language spectrum. If you try to concatenate a list with a dict in Python using the + operator, Python will throw a TypeError and fail. If you try to do the same in Javascript you get two different answers depending on the order of the two operands: . &gt;&gt;&gt; [] + {} &quot;[object Object]&quot; &gt;&gt;&gt; {} + [] 0 . For our purposes, using Python, we can use the strong typing to our advantage. . Python is dynamically typed, though, which takes a bit more caution to handle in a robust manner. Any type mismatches will only be found at runtime ‚Äî at least using just the vanilla install of the language without any extra imports or modules. . The chapter ends with a brief discussion of duck typing, defined as ‚Äúthe ability to use objects and entities in a programming language as long as they adhere to some interface‚Äù. We gain a lot in terms of increased composability, but if you rely on this feature of the language too much then it can become a hindrance in terms of communicating intent. . This chapter didn‚Äôt add too many new concepts or skills to my current understanding of the benefits of types, but it was useful to have this concept of ‚Äòcommunicating intent‚Äô to be reiterated. When I think back to how I‚Äôve heard types mentioned in the past, they often get cast in a technical sense, whereas thinking about communication between developers I think is a more motivating framing. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2021/12/30/robust-python-2.html",
            "relUrl": "/robustpython/python/books-i-read/2021/12/30/robust-python-2.html",
            "date": " ‚Ä¢ Dec 30, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "What makes code robust?",
            "content": "We use a lot of modern Python idioms, libraries and patterns at work, so I‚Äôve been wanting to get up to speed on that and maybe even actively contribute to this general direction. A recently-published book, Robust Python: Write Clean and Maintainable Code by Patrick Viafore, seems like it answers many of the questions I have around this topic. It is quite dense in terms of the amount of new things per chapter, so I‚Äôll be working my way through it in the coming months and reflecting on things as I encounter them. . The first chapter is mainly about setting the scene for all the technical pieces that follow. Patrick asks the core questions: what is robust code and why do we even care? What problems does it solve to think about code in this way. . What I took away was that a robust codebase emphasises good communication as well as avoiding accidental complexity. A lot has been written about ‚Äòclean code‚Äô and how to achieve this, but it seems that ‚ÄòRobust Python‚Äô is arguing for looking a bit further into the future, when you have to come back to refactor your code three months after you wrote it, or when your colleague needs to do the same. . ‚ÄúWriting robust code means deliberately thinking about the future.‚Äù (p. 3) . You write robust code, in other words, because you know that the codebase is going to be changing and shifting and that whatever you write today may need to be modified at a later date: . ‚ÄúA robust codebase is resilient and error-free in spite of constant change.‚Äù (p. 4) . We‚Äôre trying to solve for the way that code is often hard to reason about or understand when you‚Äôre outside the original moment when it was written. Accordingly, it pays dividends to take a bit of extra time upfront to write code such that it does communicate intent well, and that you haven‚Äôt made things more complicated than they need to be. . Moreover, the communication of intent needs to be done in a way that is asynchronous. The book goes into a bit more detail about why communication practices that require minimal cost and minimal proximity are to be preferred. These include: the code itself, in-code comments, tests, version control history, wikis, and in-project documentation. . The first part of the book is all about type annotation, using mypy, and how working with types helps makes your code more robust. We use a lot of this at work so I‚Äôm excited to take a deep dive into this. .",
            "url": "https://mlops.systems/robustpython/python/books-i-read/2021/12/29/robust-python-1.html",
            "relUrl": "/robustpython/python/books-i-read/2021/12/29/robust-python-1.html",
            "date": " ‚Ä¢ Dec 29, 2021"
        }
        
    
  
    
        ,"post25": {
            "title": "Exploring J, an array programming language",
            "content": "I‚Äôve long wanted to explore the J programming language. I think I probably first heard about it from Jeremy Howard amidst one of the early iterations of the fastai course. He‚Äôs since spoken about it in other places. . It is part of the family of programming languages that includes APL, K and Q. These can broadly be categorised as array-programming languages, where arrays are generally the core data structure and mental model to keep in mind. They used to be extremely popular in the 1970s and 1980s, particularly among institutions or businesses with a requirement for performant calculation / computation. One of these, Q, continues to live on (as a closed-source language) in the world of finance and trading. (Q is popular alongside the proprietary database kdb+). . You‚Äôre probably wondering why someone would want to use this fairly specialised and niche language. When you look at examples of J code ‚Äî like the ones here, for example ‚Äî it‚Äôs easy to simply dismiss it as an unreadable (‚Äòwrite-only‚Äô) language. Indeed, many do dismiss it for this reason. Code is often compact, with single letters or symbols doing all the work. Defenders of J hold this up as a feature, not a problem. The compactness of the language means that you can fit the entirety of the solution (space) of a complex problem on a single screen, whereas in many (most?) other languages you would have to be scrolling up and down through dozens or even hundreds of lines of code. . The array languages seem to come at solving problems from a particular perspective. The symbols and letters that transform the arrays in J function as a pattern language. For a simple example, think of what you have to do when you want to find the count of a particular element from within an array/list. The array language paradigm argues that you don‚Äôt want to waste your time and screen space writing out boilerplate code to carry out this calculation, when it‚Äôs a common pattern that you can just use from the language itself. When problem-solving, therefore, spend your time thinking about the problem and not messing around with syntax or repeating yourself. . J and its cousins are extremely efficient. It is written in C, and I recently heard someone quote one of the early J pioneers as having said that ‚Äúit is not theoretically possible to write J code that is more performant than C, but it often ends up being so‚Äù. For some math- or statistics-heavy domains (think the world of finance), it is extremely helpful to have this highly abstracted language that works performantly on large datasets. Moreover, it seems to be even more helpful when you have a hard problem to work on that isn‚Äôt fully understood. . Kenneth Iverson‚Äôs wrote a paper (‚ÄúNotation as a Tool of Thought‚Äù) that is a classic in computer science and gets into some of the above arguments. (It is written using APL, but it also applies to J). I will probably return to that at a future date, because it often comes up and is recommended as a particularly rich document worth taking time to explore in depth. . Very much as a project to indulge my curiosity, I will be exploring J over the coming months. I have been listening to the back catalogue of The Array Cast podcast, and I will be slowly working my way through some of the resources listed on the official J site. Let me know if you have experience working with J! .",
            "url": "https://mlops.systems/j/2021/12/29/j-language.html",
            "relUrl": "/j/2021/12/29/j-language.html",
            "date": " ‚Ä¢ Dec 29, 2021"
        }
        
    
  
    
        ,"post26": {
            "title": "A Taxonomy of Redaction",
            "content": "One of the things that makes it hard to train a model to detect redactions in documents is the fact that there are lots of kinds of redactions. Not only were different tools or methods used at different times, but even organisations and agencies from the same country or government didn‚Äôt always share redaction practices. . I took a bit of time to try to understand the different kinds of redactions in my (pretty huge) data set. I didn‚Äôt have any special process for selecting these images; I randomly sorted the immediate ~70,000 images I have collected and looked through to try to identify some patterns. . Taking a close look at the actual parts of images that contain redactions gives me a better sense of the challenges involved in detecting those redactions. As I iterate through my collection of images, I can start to build up an intuitive sense of where class imbalances might exist. Among the images that contain redactions, for example, which ones are most represented and which contain fewer examples? In general, where do I need to focus my efforts when it comes to improving my model? . The first easy distinction to draw is that between digital and hand-applied redactions. . . It seems that the trend in this is towards digital redactions. Perhaps it is seen as less reliable, or perhaps it‚Äôs more time consuming to attach the reasons for redactions having happened. Perhaps, too, there are some legal reasons why each redaction needed to start having a specific reason applied to it. . At first glance, no pun intended, it would appear that digital redactions are much easier to recognise. They‚Äôre often uniform in how they are applied and are usually pretty blunt in their appearance. There are some non-redaction uses for totally black or grey boxes laid on top of text, but they aren‚Äôt common and it‚Äôs a pretty strong feature to have to predict. . Handwritten redactions are also easy to recognise, but potentially the borders are harder to make out. Sometimes having a thinner pen with which redactions are applied might make it slightly less accurate. . It is more practically important to distinguish between redactions that are easy to recognise vs ones that take some time to notice. I can use my own speed at noticing the redaction on a page as a gauge. It‚Äôs not a perfect analogy, but Jeremy Howard‚Äôs adage that if a human can reliably do some kind of classification or object detection, then probably a computer can as well. I guess the inverse is also true: if a human will find it hard to recognise a particular feature in an image, then a computer will probably also find it hard. . There isn‚Äôt much point spending too long with the ‚Äòeasy‚Äô redactions. These are usually whatever is boxy and blunt. It‚Äôs the stereotype of a redacted document, one like what was used as the cover art on the (much-censored) Guant√°namo Diary by Mohamedou Ould Slahi. . . Sometimes you see that the entire page has been redacted with some kind of a coloured box. Other times entire columns of information has been redacted from a table. These definitely feel like they are the more recent types of redactions. . One thing that makes detecting redactions hard, on the other hand, is if the number of redactions is small. It stands to reason that lots of small redactions can stand out at first glance, whereas a single small redaction on one corner of the page is maybe harder to notice. . The hardest of redactions seems like it is in examples like this: . . A white box on top of other white boxes! I often have to look quite closely at these to distinguish what is normal text and what is a redaction box. Some of them have a faint thin grey boundary box around them, which I guess ends up being pretty useful as a way to make that distinction. Surprisingly, the model that I‚Äôve trained so far is not terrible at making these kinds of distinctions. . . I have a few hundred annotated images so far, but I now have an intuitive sense of the hard parts of the object detection test. I also have a sense of how represented I feel like those hard parts are ‚Äî not very. . As I wrote in my previous update on my progress in this project, the next step is very much to find ways to increase the volume of good training data that I‚Äôm using to train my model. Part of that will involve creating synthetic data, part of that will be using self-training to speed up my annotation, and of course another part will just be doing more manual annotation. I‚Äôve already started work on creating the synthetic data. More on that next time! .",
            "url": "https://mlops.systems/redactionmodel/2021/12/15/redaction-taxonomy.html",
            "relUrl": "/redactionmodel/2021/12/15/redaction-taxonomy.html",
            "date": " ‚Ä¢ Dec 15, 2021"
        }
        
    
  
    
        ,"post27": {
            "title": "73% accuracy for redaction object detection",
            "content": "Last time I wrote about my redaction model training project, I explained how I used Prodigy to annotate and label a bunch of images. I subsequently spent a long evening going through the process, getting to know my data. I managed to make 663 annotations, though quite a few of those were negative annotations: I was stating that a certain document contained no redactions at all. . Once I had my redactions, I needed to convert the files from a Prodigy format into a .coco annotation format. I am using IceVision, a really useful computer vision library, for which it is easier if I pass in the annotations in the .coco format. . From that point, it was fairly easy to follow the steps of the object detection tutorial outlined in the IceVision documentation. I ran into some problems with Paperspace Gradient not easily installing and importing IceVision. For some reason files don‚Äôt get unzipped on Paperspace, but it‚Äôs possible to just do this manually: . Do the basic install, including the import of icevision.all. Wait for the error to get raised, then open up a terminal and enter: | . cd /root/.icevision/mmdetection_configs/ rm v2.16.0.zip wget https://github.com/airctic/mmdetection_configs/archive/refs/tags/v2.16.0.zip unzip v2.16.0.zip . Then run it again as normal. Later on, another error will get raised. Fix it with this (again in the terminal): . jupyter nbextension enable --py widgetsnbextension . This enables ipywidgets in the notebook, I think. . Once through all of that, I was able to fine-tune a model based on the annotations which I currently have. I selected VFNet as the model I wanted to use as the pertained model. After training for 40 epochs, I reached an accuracy of 73%: . . If we look at some of the results (using model_type.show_results()) we can get a sense of the parts it found easy and the parts which it found hard. (All the boxes below are what it as predicted, not the ground truth annotations.) Some identification of boxes went as you might expect: . . I was surprised that something like this worked as well as it did: . . It wasn‚Äôt perfect, but I don‚Äôt remember having annotated too many of this specific redaction type, so I‚Äôm fairly happy with how it worked out. You can see it still makes a number of mistakes and isn‚Äôt always precise about where the boxes should go. I hope that‚Äôll improve as I add more examples of this type of redaction. . My next steps for this project include the following: . create synthetic data. The redactions are probably easy enough to mimic where we‚Äôll get a lot of value from the use of synthetic data (fake redactions on not-real document backgrounds). It‚Äôll be an easy way to boost my training data set by a good amount, hopefully leading to big improvements in my model accuracy. | potentially add in either active learning (to help speed up my annotation process) or self-training (using the model to make annotation suggestions on unlabelled data and using only the suggestions with really high confidence estimates). | think through the augmentations that I use as part of my workflow. I basically want augmentations that are similar to however the production use case will be: i.e. the kinds of redacted images that it might see when being given real-world data at inference time post-training. | add in experiment tracking. I‚Äôve never used something like Weights &amp; Biases, so I‚Äôm excited to try that out and have a real process for tracking my progress throughout this project. | cleaning up and refactoring (a bit) my repository where the code lives for processing the input data. It‚Äôs starting to get a bit unwieldy and I‚Äôm worried I‚Äôll start to forget the order things were done and some of those small details. | .",
            "url": "https://mlops.systems/redactionmodel/computervision/progressreport/2021/12/11/redaction-progress-week-one.html",
            "relUrl": "/redactionmodel/computervision/progressreport/2021/12/11/redaction-progress-week-one.html",
            "date": " ‚Ä¢ Dec 11, 2021"
        }
        
    
  
    
        ,"post28": {
            "title": "What is VFNet?",
            "content": "VFNet is short for VariFocalNet. This method of object detection was first released in 2008 and it scored 55.1 on the COCO test-dev benchmark, state-of-the-art at the time. There have since been other improvements. . . The original paper is here. The implementation of this model is here. . The problem it solves is that when we‚Äôre training a model, we have a large number of possible options for objects detected in an image. What we need to do is rank these options in order of likelihood of being a correct bounding of a box. . It is based on and draws on the MMDetection model/toolbox. MMDetection is a Pytorch library for object detection. It is modular, allowing for greater customisability. . Other resources . Airctic Presentation on VFNet .",
            "url": "https://mlops.systems/redactionmodel/computervision/2021/11/30/vfnet-basics.html",
            "relUrl": "/redactionmodel/computervision/2021/11/30/vfnet-basics.html",
            "date": " ‚Ä¢ Nov 30, 2021"
        }
        
    
  
    
        ,"post29": {
            "title": "How to annotate image data for object detection with Prodigy",
            "content": "I‚Äôm back to working on the redaction model, though this time with a slightly more focused objective: object detection. . Object detection is when you put bounding boxes around the specific object that you are trying to locate within an image. The end goal for my project is to be able to identify ‚Äî for an arbitrary image ‚Äî which parts of the image are redacted, and then to be able to calculate what proportion of the image is redacted. . For this, I need annotations. Annotations are the data that I will use as the fuel for the model I hope to train. We need a lot of annotations of specific redactions in order for the computer to be able to learn to detect what is a redaction and what is just an empty box, for example. . I showed in an earlier post how I trained a model to detect whether there was any kind of redaction inside an image (to around 95% accuracy). For this next stage, it isn‚Äôt enough to offer a binary ‚Äòyes‚Äô or ‚Äòno‚Äô for whether it has been redacted. I need to specify the coordinates of a bounding box which encompasses each redaction. . In terms of the final output of the annotations, there are two main ways that this could go. I could either: . get x and y coordinates for the centre of the bounding box, and then a height and a width of the box around this centre point | get the four coordinates for each of the corners of the bounding box. | The COCO dataset format will eventually want datasets in the second format, but Prodigy has its own way of storing the data which I just left for now. Once I have a better handle on the annotation flow I will write a custom recipe which will save the data in exactly the format that I want. For now, it‚Äôs good enough. . Installing Prodigy into your development environment is a breeze now that you can do it with pip: . pip install prodigy -f https://XXXX-XXXX-XXXX-XXXX@download.prodi.gy # where the XXXs are your license code . Getting going with the image training was as easy as the following CLI command: . prodigy image.manual redaction-object-detection /path/to/image/data --label CONTENT,REDACTION --remove-base64 . Note that the --remove-base64 is to ensure that Prodigy doesn‚Äôt store the raw binary image data inside the database alongside the annotations. Prodigy (and their sister tool Spacy) is a little more focused on textual data, where storing the original data alongside the annotation doesn‚Äôt pose too much of an issue, but for image files this probably is a bit of an anti-pattern and could lead to a very large database. . You get a local URL to go visit and you see an interface where you can make the necessary annotations: . . You can see that I am distinguishing between two different classes: redactions and content. Redactions are what we‚Äôve been talking about above. Content, however, is a bounding box for the content on a page. Remember that at the end of all of this we want a percentage of the page that has been redacted. Some images have reduced sized images, where the actual content which could have been redacted only takes up half of the A4 page. If that whole section was redacted, I‚Äôd want a final amount closer to 100% for that image rather than the 50% I‚Äôd get if I just went with the total percentage of redacted pixels on the whole image file. . Doing a few annotations, I ran into a couple of issues almost immediately. What do I do with a page like this: . . The whole text of the page is annotated, but the text only extended half-way down the page. There was only 50% of the page that could have been redacted, but should the content boundary box encompass more of the page, or just the only full-section redaction? . And for the following image, what is the right way to think about how to make the annotation? . . This redaction encompasses multiple lines, so to some extent it doesn‚Äôt make a difference whether we have overlapping annotations or two adjoining boundary boxes. But for the purposes of training our model, will this contribute to a less accurate model? Should I be using polygon boundaries (which Prodigy can also use for annotations)? . . As an aside, this is why annotating your own data is so valuable. You get to see the limits of the annotations, and you get to really own the decisions that are being made. It is a bit early for me to know which approach is the best solution to these two problems, but being aware of them is important. Once we‚Äôre done with our annotations, we can easily export our data to a jsonl file with the following CLI command: . prodigy db-out redaction-object-detection &gt; ./redaction-object-detection-annotations.jsonl . This gives us a file containing all our annotations. A sample for one image gives the idea: . { &quot;image&quot;: &quot;sample/path/redacted/04-F-0269_Global_Screening_Guidance-03.jpg&quot;, &quot;text&quot;: &quot;04-F-0269_Global_Screening_Guidance-03&quot;, &quot;meta&quot;: { &quot;file&quot;: &quot;04-F-0269_Global_Screening_Guidance-03.jpg&quot; }, &quot;path&quot;: &quot;sample/path/redacted/04-F-0269_Global_Screening_Guidance-03.jpg&quot;, &quot;_is_binary&quot;: false, &quot;_input_hash&quot;: 1413334570, &quot;_task_hash&quot;: 1588323116, &quot;_view_id&quot;: &quot;image_manual&quot;, &quot;width&quot;: 800, &quot;height&quot;: 1035, &quot;spans&quot;: [ { &quot;id&quot;: &quot;0ef6ccd0-4a79-471d-9aa1-9c903c83801e&quot;, &quot;label&quot;: &quot;CONTENT&quot;, &quot;color&quot;: &quot;yellow&quot;, &quot;x&quot;: 76.5, &quot;y&quot;: 112.5, &quot;height&quot;: 786.1, &quot;width&quot;: 587.6, &quot;center&quot;: [370.3, 505.55], &quot;type&quot;: &quot;rect&quot;, &quot;points&quot;: [ [76.5, 112.5], [76.5, 898.6], [664.1, 898.6], [664.1, 112.5] ] }, { &quot;id&quot;: &quot;cd05d521-8efb-416b-87df-4624f16ca7f3&quot;, &quot;label&quot;: &quot;REDACTION&quot;, &quot;color&quot;: &quot;cyan&quot;, &quot;x&quot;: 80.3, &quot;y&quot;: 786.2, &quot;height&quot;: 20.2, &quot;width&quot;: 428.4, &quot;center&quot;: [294.5, 796.3], &quot;type&quot;: &quot;rect&quot;, &quot;points&quot;: [ [80.3, 786.2], [80.3, 806.4], [508.7, 806.4], [508.7, 786.2] ] }, { &quot;id&quot;: &quot;3e268e33-4eba-457d-8d17-8271a79ee589&quot;, &quot;label&quot;: &quot;REDACTION&quot;, &quot;color&quot;: &quot;magenta&quot;, &quot;x&quot;: 108.1, &quot;y&quot;: 772.3, &quot;height&quot;: 15.1, &quot;width&quot;: 400.6, &quot;center&quot;: [308.4, 779.85], &quot;type&quot;: &quot;rect&quot;, &quot;points&quot;: [ [108.1, 772.3], [108.1, 787.4], [508.7, 787.4], [508.7, 772.3] ] } ], &quot;answer&quot;: &quot;accept&quot;, &quot;_timestamp&quot;: 1638214078 } . Everything we‚Äôre interested in is inside the spans attribute, and it actually contains both kinds of the annotation that I mentioned above. . As you can see, annotating images in this way is fairly painless, and it brings you in closer contact with your raw data which is an added bonus. .",
            "url": "https://mlops.systems/redactionmodel/computervision/datalabelling/2021/11/29/prodigy-object-detection-training.html",
            "relUrl": "/redactionmodel/computervision/datalabelling/2021/11/29/prodigy-object-detection-training.html",
            "date": " ‚Ä¢ Nov 29, 2021"
        }
        
    
  
    
        ,"post30": {
            "title": "Check your security vulnerabilities with `safety`",
            "content": "safety is a tiny tool that checks your package‚Äôs dependencies for security vulnerabilities. It is free to use for open-source projects, and using it is as a pip install safety followed by safety check. . It checks a database of known security vulnerabilities. This database is only updated once every month, but if you are not open-source or you need access to the more frequently-updated database, then you can subscribe via pyup. . With that caveat, it‚Äôs not perfect, but it‚Äôs better than nothing. An easy CI win for open-source projects. . [I first learned of this tool here. Many thanks to calmcode for continuing to make these really useful videos.] .",
            "url": "https://mlops.systems/security/tools/calmcode/2021/11/27/safety-vulnerability-checker.html",
            "relUrl": "/security/tools/calmcode/2021/11/27/safety-vulnerability-checker.html",
            "date": " ‚Ä¢ Nov 27, 2021"
        }
        
    
  
    
        ,"post31": {
            "title": "Launching a podcast about MLOps",
            "content": "I‚Äôll be co-hosting a new podcast about MLOps, with new episodes out every fortnight. Pipeline Conversations: A Machine Learning Podcast by ZenML is the new podcast from the company where I work. (We build an open-source tool for data scientists to empower them to take control of how their models live in production.) . Our first episode gets into some of the background for why ZenML exists in the first place. Upcoming episodes will be discussions with guests from the data science and MLOps space. . I‚Äôm excited to get the opportunity to talk with so many interesting and smart people. .",
            "url": "https://mlops.systems/zenml/podcast/appearances/2021/11/27/pipeline-conversations.html",
            "relUrl": "/zenml/podcast/appearances/2021/11/27/pipeline-conversations.html",
            "date": " ‚Ä¢ Nov 27, 2021"
        }
        
    
  
    
        ,"post32": {
            "title": "How to set and get environment variables using Python",
            "content": "If you want to get and set environment variables using Python, simply use the relevant methods from os. To set an environment variable, do this: . import os os.environ[&#39;SOME_ENV_VARIABLE&#39;] = 13.5 . And to access an environment variable, there are actually a number of different ways. All these three are essentially the same: . os.getenv(&#39;SOME_ENV_VARIABLE&#39;) os.environ.get(&#39;SOME_ENV_VARIABLE&#39;) os.environ(&#39;SOME_ENV_VARIABLE&#39;) . For the final one (os.environ(&#39;SOME_ENV_VARIABLE&#39;)), if the variable doesn‚Äôt exist, it‚Äôll return a KeyError, whereas the first two will just return None in that case. .",
            "url": "https://mlops.systems/python/2021/11/26/environment-variables.html",
            "relUrl": "/python/2021/11/26/environment-variables.html",
            "date": " ‚Ä¢ Nov 26, 2021"
        }
        
    
  
    
        ,"post33": {
            "title": "entr: a tool to run commands when files change",
            "content": "It‚Äôs a fairly common pattern that you have some code that you‚Äôre repeatedly running. Perhaps you‚Äôre fixing a failing test, and you just have to keep running it every time you make a fix. . Enter entr. This handy little tool reruns a particular command whenever changes are detected in a particular set of files. . Let‚Äôs take the example I mentioned above: you have a failing test that you‚Äôre debugging and you need to have it run every time you save a change to the file. Assuming your source code is stored in src and you‚Äôre using pytest, then you could use something like the following: . ls src/*.py | entr -c pytest test.py::test_some_feature . So now, any time you change any Python file inside the src folder, it‚Äôll rerun your test. The -c flag will clear the terminal every time the test runs. . [Many thanks to calmcode for continuing to make these really useful videos.] .",
            "url": "https://mlops.systems/debugging/testing/tools/calmcode/2021/11/25/entr-reruns-tests.html",
            "relUrl": "/debugging/testing/tools/calmcode/2021/11/25/entr-reruns-tests.html",
            "date": " ‚Ä¢ Nov 25, 2021"
        }
        
    
  
    
        ,"post34": {
            "title": "On failure",
            "content": "I‚Äôve been working as a machine learning engineer now for a few months now. If there‚Äôs one thing that I have found characterises my experience so far, it‚Äôs failure. Software fails; we even have a word for that: bugs. Learning new things might also be characterised as departing from a state of failing to understand. . There hasn‚Äôt been a week that‚Äôs gone by since I started where I didn‚Äôt encounter some kind of failure, usually my inability to understand why something was behaving in a particular way. My last post was about debugging, and finding ways to move forward in the face of failure is a key aspect of that process. . Failure isn‚Äôt fun. My initial reaction to hitting something I don‚Äôt understand is not one of glee and excitement at getting this opportunity to solve some kind of problem. But maybe it should be. It occurred to me this week that actually failure is sort of the name of the game. Solving hard problems is exactly what software engineers get paid to do. If it were just easy, it‚Äôd be a different kind of work. . Two posts by Julia Evans are pretty great on how a lot of being able to do this kind of work is about mindset. Ellen Ullman covers similar territory in ‚ÄòLife in Code‚Äô and ‚ÄòClose to the Machine‚Äô. . The point is this: we are paid to confront this failure. This is the work. Thinking that it‚Äôs a distraction from the work ‚Äî some kind of imaginary world where there are no blockers or failing tests ‚Äî is the real illusion. .",
            "url": "https://mlops.systems/debugging/emotions/2021/11/21/on-failure.html",
            "relUrl": "/debugging/emotions/2021/11/21/on-failure.html",
            "date": " ‚Ä¢ Nov 21, 2021"
        }
        
    
  
    
        ,"post35": {
            "title": "Some things I learned about debugging",
            "content": "I‚Äôve had to deal with a whole bunch of bugs in the past few days and weeks. I thought it‚Äôd be useful to put down some thoughts about things that I‚Äôve learned along the way. . Logging &amp; Printing . These are maybe the first things that everyone says you should do when you have a bug you need to fix: log things somewhere where you can see them. . There are some scenarios where simple print calls aren‚Äôt enough. If you‚Äôre running code through a series of tests, then the test harness will often consume all output to stdout so you won‚Äôt see any of your print statements. Luckily, test environments can usually be configured to print debug statements of loggers. . Once you can see what‚Äôs happening at a particular moment, you can see if what you expected to happen at that moment is actually happening. . Breakpoint your way to infinity! . The breakpoint() function comes built-in with Python. It‚Äôs a convenience wrapper around some pdb magic, and practically speaking it means you can set a point where you can interrupt the Python execution. Your terminal will halt at that point, and you can inspect the variables or objects available at that particular moment. . I wish I had known about this earlier on. It‚Äôs extremely useful for understanding exactly how a function or piece of code is being executed. . Come with hypotheses . If you don‚Äôt have a sense of what you expect to happen, it‚Äôs going to be hard to determine if what you‚Äôre doing is having any effect or not. . I‚Äôve been lucky to do some pairing sessions with people as they work through bugs and problems, and I‚Äôve had this ‚Äòcome with a hypothesis‚Äô behaviour modelled really well for me. . It‚Äôs not a panacea; there‚Äôs still a lot of work to be done around this, but it‚Äôs sort of the foundation, particularly for non-trivial bugs. . Leave your assumptions at the door . Don‚Äôt assume what‚Äôs written is what‚Äôs actually working. This applies to the code you‚Äôre working on, the documentation, docstrings, everything. This is especially true when your codebase is rapidly changing growing, such as at a startup or a smaller company where not everything has been cemented into place. . The rapid pace of change means that things can get out of date, or people can make mistakes. This applies to packages or modules you‚Äôre importing as well. Of course, it‚Äôs probably more likely that you‚Äôre misunderstanding something vs the Python standard library has got something wrong, but for many other open-source projects, you should at least be open to the possibility that weird things might show up. . Follow the thread wherever it leads . This is something about updating your assumptions as you move through the process of testing your assumptions. If you rule out certain pathways, then you should be prepared to go down the remaining ones as far as you need. . Be systematic . I‚Äôve found a few times now, that there are certain moments where I notice I‚Äôm far far down the road. I‚Äôll have kept making a bunch of decisions at the various crossroads that I passed. At a certain moment, though, I need to take stock and just note down all the decisions and assumptions I‚Äôve made in order to reach this point. . I‚Äôll write a short note to myself (mainly), but also for teammates, where I explain all the different assumptions and pathways that I‚Äôm travelling down. I‚Äôll specifically write down all the conditions that need to be present for this bug to present (as far as I know them). . Quite often, just writing these assumptions down will help me solve the problem outright. Even when it doesn‚Äôt, it‚Äôs extremely useful in re-grounding myself and reminding me of why I‚Äôm going down rabbit hole x or y. . Know when to stop . In an ideal world you‚Äôd get to follow every windy road and to figure out everything that doesn‚Äôt make sense. But ‚Äî and this is again especially true for fast-moving startups ‚Äî you might not always have time to do that. . This is somehow connected to the Pareto Principle (also known as the 80/20 rule). At a certain point you should make sure to check in with how much time you‚Äôd planned on spending on a particular bug. If you‚Äôre finding that it‚Äôs taking far longer than expected, and you have other things you‚Äôre committed to completing, then you should maybe take an opportunity to connect to your team. Alternatively, you can rescope and find a way to disable or flag a particular bug for the next sprint, or see if someone can help you with it. . Remember: this is the work . Sometimes when I‚Äôm fixing bugs I have the feeling that I‚Äôm wasting my time somehow, or that I should be doing something more productive. It‚Äôs often the case, though, that this is the work. I‚Äôm low on experience, but proxy experience that I‚Äôve gained through reading books tells me that finding, fixing and triaging bugs is a lot of what we do as software engineers. . Know when to ask for help . Sometimes there are bugs which turn out to be bigger than you‚Äôre able to handle. It‚Äôs certainly worth pushing back against that feeling the first few times you feel it. Early on it‚Äôs often going to feel like the bug is unsolvable. . But some times there are pieces of context you don‚Äôt have, which a quick overview of what you‚Äôve done and tried might alert someone more season to the fact that you‚Äôre going down the wrong alley. Or it might remind them of something they knew implicitly but had forgotten. The important things is to judge when is the right time to seek outside advice. .",
            "url": "https://mlops.systems/debugging/2021/10/25/debugging.html",
            "relUrl": "/debugging/2021/10/25/debugging.html",
            "date": " ‚Ä¢ Oct 25, 2021"
        }
        
    
  
    
        ,"post36": {
            "title": "Writing Code",
            "content": "I read Daniel Roy Greenfeld‚Äôs post on how he found that coding a lot was key to improving his skills. It makes sense. Everything I‚Äôve read so far and my previous experience at the metaskill of learning new things tells me that it is a good investment of time. . Just like you get good at writing by doing a lot of writing, on some level that is true for coding. (Of course, there are additional pieces to the puzzle: you have to develop some taste alongside the pure production side, you have to do some quality-control and refactor your code, and so on and so on.) . For me, this looks like the following: . coding at work during the week | smaller focused exercises from PythonMorsels, Exercism, LeetCode and AlgoExpert | code written while working my way through the fastai course; this will probably manifest as blog posts here as well, outlining some small project I completed along the way. | a bigger project, perhaps a package, that I‚Äôll start building at some point. I have some ideas for things I want to implement. I‚Äôll pick one soon. It‚Äôll probably be related in some way to the fastai coding. I‚Äôm thinking right now of making a tool that allows you to download PDFs and use the pages of those PDFs as image files in computer vision problems; a data ingestion tool, in other words. | smaller scripts to solve daily problems in my digital life. I‚Äôll store those on my GitHub somewhere and write up the design decisions around the more interesting ones here. | . One thing I took note of was how Daniel mentioned that it made sense to specialise and focus on one language at a time, particularly in the early days. Rather than indulging my curiosity and doing 1001 things using Go or lisp or whatever, I will try to stick to Python at least until I feel more confident with it. .",
            "url": "https://mlops.systems/python/skillbuilding/2021/09/18/writing-code.html",
            "relUrl": "/python/skillbuilding/2021/09/18/writing-code.html",
            "date": " ‚Ä¢ Sep 18, 2021"
        }
        
    
  
    
        ,"post37": {
            "title": "Reading Python Code",
            "content": "It‚Äôs a truism of sorts that in order to improve your skills, you have to practice them. For coding, the stereotypical image is of someone typing, actually creating new things. But as often as not, you‚Äôre going to be reading code instead. This code might be something you write yesterday or last year, or it might be something that someone else wrote. . One way or another, reading code is a great way to get increasing familiarity with stylistic, syntactic patterns and to get exposed to some best practices, especially if you get to pick the code you‚Äôre reading. . I‚Äôll be doing the same as I ramp up my Python proficiency. I wanted to gather some lists of codebases and assorted resources in one place for myself, and I hope maybe it‚Äôll be useful for someone else as well. . Good Quality Python Code . jinja ‚Äî a templating engine written in Python (and see the recommendations for supplemental reading and watching for jinja here) | howdoi ‚Äî a search tool for coding answers via the command line | flask ‚Äî a micro-web framework for Python | FastAPI ‚Äî another web framework that‚Äôs a bit larger than flask | diamond ‚Äî a Python daemon that collects and publishes system metrics | werkzeug ‚Äî a web server gateway library | requests ‚Äî an HTTP library, now part of the Python standard library | tablib ‚Äî library for Pythonic way to work with tabular datasets | click ‚Äî a Python package for creating command line interfaces | pathlib ‚Äî part of the Python standard library; a module to handle filesystem paths (also the corresponding PEP proposal #428) | dataclasses ‚Äî a module in the Python standard library; reduces boilerplate of writing classes (also the corresponding PEP proposal #557) | joblib ‚Äî a library to support lightweight pipelining in Python | . Other Resources . 500 Lines or Less ‚Äî a book in which specific small open-source projects are profiled to understand how they approached their particular challenge. | The Architecture of Open Source Applications: Elegance, Evolution and a Few Fearless Hacks ‚Äî examination of the structure of the software of some open-source software applications. | The Architecture of Open Source Applications: Volume II: Structure, Scale and a Few More Fearless Hacks ‚Äî the second volume in the series. | .",
            "url": "https://mlops.systems/python/skillbuilding/2021/09/18/reading-python.html",
            "relUrl": "/python/skillbuilding/2021/09/18/reading-python.html",
            "date": " ‚Ä¢ Sep 18, 2021"
        }
        
    
  
    
        ,"post38": {
            "title": "Tensors all the way down",
            "content": "#!pip install -Uqq fastbook #!pip install fastai import fastbook fastbook.setup_book() from fastai.vision.all import * from fastbook import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . . In chapter 4 of the book, we start to really get into what&#39;s going on under the hood with deep learning. Turns out, tensors are a pretty important piece. We are still in the realm of computer vision, and we are going to work on distinguishing between handwritten digits. . First we use the untar_data function to grab a sample of data from the famous MNIST data set. This function returns the path where that data was stored locally. . path = untar_data(URLs.MNIST_SAMPLE) . Path.BASE_PATH = path . path . Path(&#39;.&#39;) . Now we want to briefly inspect the contents of one of our training data folders. This is for the number 7. You can see that it&#39;s just a series of .png image files. . threes_dir = (path/&#39;train/3&#39;).ls().sorted() sevens_dir = (path/&#39;train/7&#39;).ls().sorted() sevens_dir . (#6265) [Path(&#39;train/7/10002.png&#39;),Path(&#39;train/7/1001.png&#39;),Path(&#39;train/7/10014.png&#39;),Path(&#39;train/7/10019.png&#39;),Path(&#39;train/7/10039.png&#39;),Path(&#39;train/7/10046.png&#39;),Path(&#39;train/7/10050.png&#39;),Path(&#39;train/7/10063.png&#39;),Path(&#39;train/7/10077.png&#39;),Path(&#39;train/7/10086.png&#39;)...] . In order to look at a single image, we can just open it using Image.open which comes from the Python Image Library (PIL). . im3_path = threes_dir[1] im3 = Image.open(im3_path) im3 . Jupyter knows how to display various files, so we can see that image above. But what exactly is an image made up of? If we turn that image into an array, or to a tensor (the next two cells), slicing them so you aren&#39;t just seeing zeros on the edges, then you can see that these images are made up of a matrix of values from 0 to 255. . im3_arr = array(im3)[4:10, 4:10] im3_arr . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=uint8) . im3_tns = tensor(im3)[4:10, 4:10] im3_tns . tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) . We can use the show_image function to turn those 0-255 values back into an image, like so: . show_image(im3_arr) . &lt;AxesSubplot:&gt; . A really nice way of visualising exactly what is going on is to turn this image into a pandas dataframe and then for every individual pixel value, use that value as the background gradient for that cell. Here&#39;s an example of part of an image of a handwritten number 3. . im3_t = tensor(im3) df = pd.DataFrame(im3_t[4:15,4:22]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | . 3 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | . 4 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | . 5 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | . So now we have a toolkit of ways to view the pixel values that make up an image. We also have a mental model for how we can think about images and how computers represent those images stored on our machine. . But how might we then best go about knowing whether a particular image is a 3, let&#39;s say, or a 7? . One naive approach might be just to get the average value for each individual pixel for all of the threes in our training data, and then just compare the difference between our sample image and this average representation. . Let&#39;s try that now. . Getting the average values for our images . We&#39;ll set up two lists with images of the digits converted to tensors. You can see that we have 6131 images in our &#39;threes&#39; list. . threes_tensors = [tensor(Image.open(i)) for i in threes_dir] sevens_tensors = [tensor(Image.open(i)) for i in sevens_dir] len(threes_tensors) . 6131 . We can view an individual image, as before, with the show_image function: . show_image(threes_tensors[3]) . &lt;AxesSubplot:&gt; . Now in order to get the average values for each pixels, we can use the stack method to handle the first part of this. . Think of it as basically adding an extra dimension to your data structure, such that you have a &#39;stack&#39; (it&#39;s a useful mental image) of those images. . threes_stack = torch.stack(threes_tensors) . If we look at the shape of our Pytorch stack now, we can see we have our 28x28 image, but we have a stack of 6131 of them. . threes_stack.shape . torch.Size([6131, 28, 28]) . Each individual image is still a tensor: . a_three = threes_stack[3][4:16, 4:16] a_three . tensor([[ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 104, 253, 253, 253, 255, 253], [ 0, 0, 0, 0, 0, 178, 248, 252, 252, 252, 253, 252], [ 0, 0, 0, 0, 0, 186, 252, 252, 252, 252, 253, 252], [ 0, 0, 0, 0, 0, 186, 252, 243, 172, 172, 39, 39], [ 0, 0, 0, 0, 0, 39, 53, 47, 0, 0, 0, 29], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 54, 208], [ 0, 0, 0, 0, 0, 0, 0, 0, 3, 41, 253, 252], [ 0, 0, 0, 0, 0, 0, 5, 41, 165, 252, 253, 252], [ 0, 0, 0, 0, 0, 109, 163, 252, 252, 252, 253, 252], [ 0, 0, 0, 0, 0, 186, 252, 252, 252, 252, 253, 252], [ 0, 0, 0, 0, 0, 187, 253, 253, 253, 253, 134, 77]], dtype=torch.uint8) . Generally speaking, for some operations (like getting the mean average) we&#39;re going to want to convert the values to floats, and it also makes sense to normalise the values at the same time. Instead of having a range of 0-255, we want a range of 0-1. . threes_stack[3][4:16, 4:16].float()/255 . tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4078, 0.9922, 0.9922, 0.9922, 1.0000, 0.9922], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6980, 0.9725, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7294, 0.9882, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7294, 0.9882, 0.9529, 0.6745, 0.6745, 0.1529, 0.1529], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.2078, 0.1843, 0.0000, 0.0000, 0.0000, 0.1137], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2118, 0.8157], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.1608, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0196, 0.1608, 0.6471, 0.9882, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4275, 0.6392, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7294, 0.9882, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882], [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7333, 0.9922, 0.9922, 0.9922, 0.9922, 0.5255, 0.3020]]) . Now that we&#39;ve done it for a single image, we can perform the same operations on our whole Pytorch stack. . threes_stack = torch.stack(threes_tensors).float()/255 sevens_stack = torch.stack(sevens_tensors).float()/255 threes_stack.shape # it&#39;s good to keep in touch with the shape of our stack . torch.Size([6131, 28, 28]) . Now we&#39;re getting closer to our desired result. We can squash the stack down into just two dimensions with a simple call to .mean(0), where 0 is the index value of the dimension through which we want to calculate the mean. You&#39;ll see now that the shape property of our threes_means variable is simply a 28x28 image. . threes_means = threes_stack.mean(0) threes_means.shape . torch.Size([28, 28]) . When we show that image, you&#39;ll see that it&#39;s a sort of blurry &#39;ideal&#39; version of a three . show_image(threes_means) . &lt;AxesSubplot:&gt; . We can do the same for the sevens: . sevens_means = sevens_stack.mean(0) show_image(sevens_means) . &lt;AxesSubplot:&gt; . Validation: Comparing our average three with a specific three . Now we have our average values, we want to compare these with a single specific digit image. We&#39;ll get the difference between those values and whichever difference is the smallest will most likely be the best answer. . Our averaged three is still threes_means and we can get a single three from our validation set like this: . threes_dir_validation = (path/&#39;valid/3&#39;).ls().sorted() sevens_dir_validation = (path/&#39;valid/7&#39;).ls().sorted() im3_validation_path = threes_dir_validation[5] im3_validation = tensor(Image.open(im3_validation_path)).float()/255 im7_validation_path = sevens_dir_validation[3] im7_validation = tensor(Image.open(im7_validation_path)).float()/255 show_image(im3_validation) . &lt;AxesSubplot:&gt; . show_image(im7_validation) . &lt;AxesSubplot:&gt; . . Note: Calculating the difference between two objects . We can use two different measurements of the difference between our mean value and the individual image: . mean absolute difference (calculated by taking the mean of the absolute difference between the two tensor values). Also known as the L1 Norm. | root mean squared error (calculated by first squaring the difference between the two tensor values, taking the mean and then square rooting those values). Also known as the L2 Norm. | . The second option, the RMSE, gives a stronger signal, you might say, for the differences because you are taking the averages from the squared values. Squaring the difference also takes care of any negative values you might have. . mean_absolute_difference_3 = (im3_validation - threes_means).abs().mean() root_mean_squared_error_3 = ((im3_validation - threes_means)**2).mean().sqrt() mean_absolute_difference_3, root_mean_squared_error_3 . (tensor(0.1188), tensor(0.2160)) . mean_absolute_difference_7 = (im7_validation - threes_means).abs().mean() root_mean_squared_error_7 = ((im7_validation - threes_means)**2).mean().sqrt() mean_absolute_difference_7, root_mean_squared_error_7 . (tensor(0.1702), tensor(0.3053)) . We can now see that our individual three image is indeed closer to the threes_means composite image than to the sevens_means composite image. A smaller value at this point is what we&#39;re looking for, and the threes have it. . It turns out that there is another way to calculate the difference that&#39;s built in to Pytorch as loss functions: . F.l1_loss(im3_validation, threes_means), F.mse_loss(im3_validation, threes_means).sqrt() . (tensor(0.1188), tensor(0.2160)) . It&#39;s a bit more concise, though it does obscure what&#39;s going on under the hood in terms of calculations. . Results of the naive approach . So this tells us that our single three is closer to an ideal 3 than an ideal 7, which is great since it reflects the ground truth of our problem. But can we get a metric to know how well we perform on average against a large number of threes and sevens from our validation set? . Yes, since we have that dataset ready for use! . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]).float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]).float()/255 valid_3_tens.shape, valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . Now we can write a helper function that will allow us to calculate the distance between two images. We use the RMSE or L1 Norm for this difference calculation: . def mnist_distance(a, b): return (a - b).abs().mean((-1, -2)) . We can use this function on our previous example: . mnist_distance(im3_validation, threes_means) . tensor(0.1188) . We can continue onwards by comparing the rank 3 tensor with the rank 2 tensor. This brings a concept called &#39;broadcasting&#39; into play. . We are comparing a tensor with 2 dimensions with a tensor with 3 dimensions, so Pytorch behaves as if both tensors have three dimensions, and (without taking extra memory) pretends as if there are multiple copies of the image in 2 dimensions. This effectively makes it as if we&#39;re comparing two 3-dimensional tensors. . From this next calculation, we see returned back a collection of the distances between all of the validation images. . mnist_distance(valid_3_tens, threes_means) . tensor([0.1328, 0.1523, 0.1245, ..., 0.1383, 0.1280, 0.1138]) . In order to check whether an image is a 3, we basically need to know whether the difference for the number 3 is larger than the difference for the number 7. . We can write a helper function for that: . def is_3(img): return mnist_distance(img, threes_means) &lt; mnist_distance(img, sevens_means) . We can now check our ground truth examples: . is_3(im3_validation), is_3(im7_validation) . (tensor(True), tensor(False)) . That&#39;s what we expected to happen. Our 3 image is a 3, and our 7 image is not a 3. . If we want to check the distance in general for our validation set, we have to convert them into floats and then get the mean, but it&#39;s really easy. Again, this uses broadcasting: . validation_accuracy_3 = is_3(valid_3_tens).float().mean() validation_accuracy_7 = 1 - is_3(valid_7_tens).float().mean() validation_accuracy_3, validation_accuracy_7 . (tensor(0.9168), tensor(0.9854)) . Overall, then, we can calculate how good our toy or baseline model is for the entire problem: . (validation_accuracy_3 + validation_accuracy_7) / 2 . tensor(0.9511) . Pretty good! . This was of course just a naive way to solve the problem. There are more advanced techniques which we&#39;ll tackle next. .",
            "url": "https://mlops.systems/pytorch/fastai/computervision/2021/09/16/ch4-tensors.html",
            "relUrl": "/pytorch/fastai/computervision/2021/09/16/ch4-tensors.html",
            "date": " ‚Ä¢ Sep 16, 2021"
        }
        
    
  
    
        ,"post39": {
            "title": "A Baseline Python Development Setup",
            "content": "The world of Python versioning (and the downstream package versioning) is wild. This StackOverflow thread gives you a sense of some of the core issues at play. (As an indication of the importance of the issue, even BDFL Guido van Rossum himself has the current second most upvoted answer.) . For a really vanilla and close-to-core-python setup, a combination of venv and pip seem to be the way to go. venv is part of the standard library and as such is pretty close to a default option. . For something a bit more involved, that handles dependencies and package installation in a slightly more deft manner, the combination of pyenv, pyenv-virtualwrapper and poetry works really well. I‚Äôll detail some of the setup gotchas and usage patterns below. . pyenv for versioning Python itself . pyenv lets you install multiple versions of Python on the same machine. The interface to switch between local versions and whatever you‚Äôve decided will be your global option is pretty intuitive. . Visit the pyenv github page for more on installation. (If you‚Äôre on a Mac you can simply do a brew install pyenv.) . To see which versions of Python you have installed locally: . pyenv versions . To see versions of Python which are available for installation: . pyenv install ‚Äîlist . Note that, as I understand it, these versions are not dynamically updated. You get an updated list of new Python versions by updating pyenv, in other words. . To install a specific version of Python, and to make it available for use: . pyenv install 3.9.1 . To set that version of Python as the global version (i.e. running python will use this version by default): . pyenv global 3.9.1 . If you are in a project directory and wish to only use a particular version of Python in that directory (and its subdirectories): . pyenv local 3.8.2 . This creates a .python-version file in that directory with the desired local version. . pyenv-virtualenv for managing virtual environments . pyenv-virtualenv is a plugin that connects the work of selecting which version of Python to use (through pyenv, which we‚Äôve previously installed) to the work of creating and running virtual environments to keep code contained in quasi-sandbox environments. When you install packages in virtual environments they don‚Äôt conflict with other locations where you might have conflicting versions of those same packages installed. . Read installation instructions and the docs here. (If you installed pyenv with homebrew, be sure to do the same with pyenv-virtualenv). . To create a virtual environment for the Python version used with pyenv, run pyenv virtualenv, specifying the Python version you want and the name of the virtual environment directory: . pyenv virtualenv 3.8.2 my-virtual-env-3.8.2 . This will create a virtual environment based on Python 3.8.2 under $(pyenv root)/versions in a folder called my-virtual-env-3.8.2. . To list what virtual environments have been created and are available to use: . pyenv virtualenvs . As a common workflow pattern, you‚Äôd create your directory and cd into it, and then you can set the virtual environment you just created as the one to use for that directory: . mkdir test-project &amp;&amp; cd test-project pyenv local my-virtual-env-3.8.2 . This should change the prompt in your terminal window and you‚Äôll thus know that you‚Äôre now working out of that virtual environment. Any time you return to that folder you‚Äôll automatically switch to that environment. . The manual way of turning on and off virtual environments is: . pyenv activate env-name pyenv deactivate env-name . To remove a virtual environment from your system: . pyenv uninstall my-virtual-env . (This is the functional equivalent of removing the directories in $(pyenv root)/versions and $(pyenv root)/versions/{version}/envs.) . poetry for handling package installation and dependencies . python-poetry is the latest standard tool for handling package installations and dependency management. . You can use poetry without the previous two tools, but really they work best all together. Follow the installation instructions documented on their page to get it going. . Then update poetry: . poetry self update . poetry is one of those tools that‚Äôs able to update itself. . For basic usage for a new project, you can follow the following workflow. There are two ways to start a new project using poetry: using new or init. For example: . poetry new some-project-name . This will kickstart your new project by creating a bunch of files and a directory structure suitable for most projects, like so: . some-project-name ‚îú‚îÄ‚îÄ pyproject.toml ‚îú‚îÄ‚îÄ README.rst ‚îú‚îÄ‚îÄ some-project-name ‚îÇ ‚îî‚îÄ‚îÄ __init__.py ‚îî‚îÄ‚îÄ tests ‚îú‚îÄ‚îÄ __init__.py ‚îî‚îÄ‚îÄ test_some-project-name.py . You might want to use a src folder (above the some-project-name in our example) which is fairly commonly used, in which case amend the command as follows: . poetry new --src some-project-name . poetry init doesn‚Äôt do all the extra work of creating a directory and file structure. It merely creates a pyproject.toml file interactively, using some smart defaults. For a minimal use of poetry, this is definitely the way to go. . The add command adds required packages to your pyproject.toml and installs them (along with all their dependencies). It does a lot under the hood to make sure that dependencies are correctly resolving before installing. For example: . poetry add zenml . To add packages only to be used in the development environment: . poetry add --dev zenml . To list all installed packages in your current environment / project: . poetry show . To uninstall a package and remove it (and its dependencies) from the project: . poetry remove zenml . To install all relevant packages and dependencies of a project that you‚Äôve newly cloned into: . poetry install . Note that it is possibly worth creating some custom scripts to handle some of the overhead of using these tools, depending on your common development workflows. .",
            "url": "https://mlops.systems/python/tools/2021/09/14/python-versioning-package-managers.html",
            "relUrl": "/python/tools/2021/09/14/python-versioning-package-managers.html",
            "date": " ‚Ä¢ Sep 14, 2021"
        }
        
    
  
    
        ,"post40": {
            "title": "Six problems TFX was trying to solve in 2017",
            "content": "TensorFlow Extended or TFX is a platform for machine learning that claims to handle pretty much everything you‚Äôd need for end-to-end model training, deployment and retraining. It was developed for Google, the successor to Sibyl, and released in public in 2017. I read the original paper that accompanied its release to understand the problems it was trying to solve, as well as to get a handle on the specific context in which it was developed. (It‚Äôs worth being wary about tools developed at places like Google; after all, hardly any of us are operating at Google-scale). . . ‚ÄòA TensorFlow-based general-purpose machine learning platform‚Äô . The engineers wanted a general-purpose tool, one that could serve many different use cases. I haven‚Äôt yet read the subsequent paper on the history of TFX, but from what I do know already there were other in-house solutions that existed before. Machine learning model training at scale, deployment and the general full-cycle behaviours are pretty involved and challenging, and it often seems like the needs of particular scenarios demand different approaches. This is as much true now as it was back int 2017, I imagine, though perhaps now we have some ideas of the broad pieces that make up the whole picture that needs to be addressed. . The problem here is that you might have certain parts that either are very compute intensive, or require special distributed computing setups, or where the models need to be trained off streaming data rather than from static stores. So with TFX they tried to make the tool sufficiently abstract that they could handle most cases someone would want to use it for. (They say at the end that there were some parts that they hadn‚Äôt anticipated, specifically sequence-to-sequence language models used in machine translation). . An end-to-end platform . The ambition for the platform and software tooling was not just to handle the smaller pieces of the training and deployment cycle, but rather to tackle the big overarching abstractions in a single approach. This of course contained some baked-in assumptions about how users would use TFX as well as what I‚Äôd say were quasi-philosophical positions on how best to approach these various parts. The paper characterises these as ‚Äòbest practices‚Äô, but certainly there hasn‚Äôt been uniform acceptance of these. . I imagine the end-to-end part was as much an attempt to encourage engineers to think of the problem in this exact way. If you are handling all the pieces of the training cycle, it‚Äôs easier to be fast and iterate and do all the things we expect of a more agile process. . Continuous training and serving . TFX was built to handle the kinds of models where the use cases demanded the ability to continuously retrain models using large quantities of streaming data. This is almost certainly not the norm, but for a company like Google I can understand that this would have been a key consideration if they wanted adoption of the tool across different teams. . In this way, certain scenarios (for example the Google Play Store case study outlined in the paper) saw a continuous retraining of models as more users used the service as well as new apps continued to be uploaded to the Play Store. If you have this kind of engineering need, and if you need to keep latency to certain boundaries (in the tens of milliseconds), it makes complete sense to have this whole structure that allows this to take place. Reading the specific example, it‚Äôs a pretty amazing feat, handling all that complexity underneath the surface. There must be many hundreds of other such services which similar levels of complexity concealed beneath the surface. . Reduce technical debt accrued via duplicated or ad hoc solutions . Prior to TFX and Sybil, it seems that there were many different approaches within Google, all addressing the same problem but in slightly different ways. . Having a series of best-practices built in to the service means that everyone can communicate about problems and about their issues using a shared language. It means that solutions discovered by one team can help other future teams. There‚Äôs a lot to be said for finding a solution that is sufficiently abstracted to work for many people. . Indeed, it seems this is the work of the MLOps community right now: find ways to abstract away problems that we all face, and to find the best abstractions that fit within the mental models we all have in our heads. The fact that there hasn‚Äôt been a grand convergence on a single solution indicates to me (at this current moment) that we haven‚Äôt found the right abstractions or flexibility within those abstractions. All the end-to-end tools handle much of the same stages of the model training and deployment process, but they each have opinions about the best practices to be employed along the way. (At least, that‚Äôs my current take on things). . Reliable serving models at scale . If you‚Äôre Google, you need to make sure that you aren‚Äôt serving garbage models to your users, or that inconsistencies in the input data aren‚Äôt polluting your retraining processes. At scale, even small mistakes compound really easily. . In the paper, two specific improvements are mentioned, tackling the challenges of low latency and high efficiency. The high efficiency example wasn‚Äôt entirely comprehensible for me, but what was clear was that they had very high expectations for how fast they wanted to make all parts of the pipelines and process. As above, the challenges of making it easy and fast to serve models ‚Äî all of which had to happen in a reliable manner ‚Äî was something that could be reused elsewhere in the company. TensorFlow Serving is what we get from their efforts in this regard. . Fast retraining with ‚Äòwarm-starting‚Äô . For the specific challenge of retraining models with streaming data, engineers were finding that they couldn‚Äôt retrain the entire model from scratch, particularly with the scale of the training data that they had. Instead, they leveraged transfer learning (reframed here as ‚Äòwarm-starting‚Äô) to take all the hard work that had already been done, and adapting this pre-existing model with the new data. This makes a lot of sense, though the reframing with the new term is a bit less comprehensible to me. . Missing pieces . There are various pieces of what I think of as the machine learning workflow (as of 2021) which seem to be missing when I read this paper. Explainability or governance of models seems somewhat of an afterthought, if it is raised at all. I think the authors might argue that many of the checks and balances are made on the data ingestion phase, and that if all that checks out then this tackles a large piece of the problem surface area. . Similarly, there is relatively little said about model versioning and data versioning. Maybe coming at this from the present moment, where it seems obvious (with tools like DVC) that data versioning is a thing you‚Äôd want to care about. . As a general response, it seems clear that if you use TensorFlow to train your models, TFX might well be a pretty neat solution that handles many of your needs, particularly if you‚Äôre operating at serious scale. If you‚Äôre a researcher (perhaps using PyTorch) with less of those specific contextual needs, it seems less than certain that TFX would suit your purposes. . A couple of other interesting observations. The data observability and validation stage seemed to place a lot of emphasis on the automation of how pre-defined schemas might get updated. I‚Äôd be interested to see how that worked in practice. I understood the challenge that if there are too many error messages about dodgy data inputs, engineers are likely to grow inured to those alerts and maybe just ignore them. But at scale, I wonder about the risks of allowing automatic updates to those schema boundaries. . Again on the validation point, I found it interesting how the authors of the paper said that users of TFX internal to Google found the option to enable this was actually a hard sell unless or until the team had experienced some kind of failure connected to poor data validation. The TFX team ended up turning on the validation parts of the pipeline by default instead of assuming that users would choose to do so manually. . I wasn‚Äôt active in the field in 2017, so it‚Äôs hard for me to be able to reconstruct exactly how prescient or not this paper was in some of its diagnoses of the problem. It doesn‚Äôt seem that TFX was the total solution that perhaps it was pitched as being, but nonetheless it seems an important engineering achievement for Google. .",
            "url": "https://mlops.systems/tfx/tensorflow/google/mlops/papers-i-read/2021/09/11/tfx-paper.html",
            "relUrl": "/tfx/tensorflow/google/mlops/papers-i-read/2021/09/11/tfx-paper.html",
            "date": " ‚Ä¢ Sep 11, 2021"
        }
        
    
  
    
        ,"post41": {
            "title": "Managing Python Environments with pyenv and pipenv",
            "content": "It‚Äôs hardly news that that managing multiple versions of Python in a development environment is hard. Adding in dependency management on top of that makes everything harder. .",
            "url": "https://mlops.systems/python/2021/09/10/python-environments.html",
            "relUrl": "/python/2021/09/10/python-environments.html",
            "date": " ‚Ä¢ Sep 10, 2021"
        }
        
    
  
    
        ,"post42": {
            "title": "Retrieval Practice with fastai chapters 1 and 2",
            "content": "Retrieval practice is when you actively try to remember something as a way of making sure that you learn it well. (Read more about it here). Today I did that with the dogs vs cats example that the first two chapters cover. . We start with installing the fastai library and importing everything from the vision library. This was hard to remember since the pattern of .all and importing * is not something I&#39;ve seen much in Python imports. . from fastai.vision.all import * . Then we create the simple function that will be used to classify the images. The pets dataset relies on the first letter of the filename for knowing whether a picture is of a cat or a dog. So the function is pretty simple: it checks whether the first letter is a capital letter or not. . The simple assert testing was a little trick that I saw mentioned somewhere this past week. It&#39;s not a full-fledged test suite, but it&#39;s at least the start of something that can later be refactored out into whatever takes its place, be it using pytest or something else. . def is_cat(string): return string[0].isupper() assert is_cat(&quot;abs&quot;) == False assert is_cat(&quot;Abs&quot;) == True . Now we have to import the data for the files and apply whatever custom transforms we want applied to them. . I had certainly forgotten that untar_data was a method when I started out with this. I also am not familiar enough with the pathlib library as I need to be. . It&#39;s interesting that we actually don&#39;t even need to do any of the batch transformations on the images in order to get excellent results. I imagine that&#39;s because the task is so close to that of the original resnet architecture. . path = untar_data(URLs.PETS)/&#39;images&#39; dls = ImageDataLoaders.from_name_func(path, get_image_files(path), label_func=is_cat, item_tfms=Resize(224)) . Then it&#39;s all about passing the dataloaders object into the cnn_learner function, along with our desired architecture. We also set the error_rate (i.e. 1 minus the accuracy at making predictions) as the metric we&#39;ll see displayed in the output. . learner = cnn_learner(dls, resnet34, metrics=error_rate) # fine-tune the model learner.fine_tune(5) . epoch train_loss valid_loss error_rate time . 0 | 0.140326 | 0.019799 | 0.008119 | 00:19 | . epoch train_loss valid_loss error_rate time . 0 | 0.046906 | 0.021923 | 0.006089 | 00:24 | . 1 | 0.041144 | 0.009382 | 0.004060 | 00:25 | . 2 | 0.028892 | 0.004109 | 0.002030 | 00:25 | . 3 | 0.008950 | 0.002290 | 0.001353 | 00:25 | . 4 | 0.004486 | 0.002822 | 0.001353 | 00:25 | . And here you can see the results. In this training run, with 5 epochs, we were able to achieve a 99.9% accuracy. Not bad! . learner.show_results() .",
            "url": "https://mlops.systems/fastai/jupyter/computervision/2021/09/10/chapter1and2recall.html",
            "relUrl": "/fastai/jupyter/computervision/2021/09/10/chapter1and2recall.html",
            "date": " ‚Ä¢ Sep 10, 2021"
        }
        
    
  
    
        ,"post43": {
            "title": "How to set a Jupyter notebook to auto-reload external libraries",
            "content": "The code to insert somewhere into your Jupyter notebook is pretty simple: . %load_ext autoreload %autoreload 2 . When you‚Äôre working on an external library or piece of Python code outside the contents of your notebook, this snippet will make sure that the updated functions and constants will always be available in their most-recently edited state. .",
            "url": "https://mlops.systems/jupyter/2021/09/09/auto-reload-external-libraries.html",
            "relUrl": "/jupyter/2021/09/09/auto-reload-external-libraries.html",
            "date": " ‚Ä¢ Sep 9, 2021"
        }
        
    
  
    
        ,"post44": {
            "title": "A Baseline Understanding of MLOps",
            "content": "Next week I‚Äôm due to begin a job as a Machine Learning Engineer at a company that works in the MLOps field. It‚Äôs a new field to me. I‚Äôve read a good deal on it in recent weeks, and listened to a few dozen episodes of the MLOps.community podcast, but I still very much consider myself a beginner in the space. To that end, I thought it worth clarifying my understanding of what MLOps is all about, the problem it is trying to solve, and where I see the opportunity there. . A top-down explanation is probably the best way to think of what we‚Äôre doing when we talk about ‚Äòdoing MLOps‚Äô: we‚Äôre doing all the things which make it possible to train, deploy and use machine learning models in the real world or ‚Äòin production‚Äô. It isn‚Äôt just a series of tools, but also a series of best practices and a community that is constantly learning and iterating to improve. . The kinds of things that you can do with machine learning models are incredibly diverse, so it stands to reason that the people who operationalise all these models have quite varied opinions and approaches to how best to do this. Even the deployment scenarios are pretty different and involve different technology stacks. There is an idea of a ‚Äòfull stack machine learning engineer‚Äô, which apparently means someone who just knows everything across the board; I hope to be able to delve into some of these areas and the key technologies represented in each space in due course on this blog. .",
            "url": "https://mlops.systems/mlops/2021/09/08/baseline-mlops-understanding.html",
            "relUrl": "/mlops/2021/09/08/baseline-mlops-understanding.html",
            "date": " ‚Ä¢ Sep 8, 2021"
        }
        
    
  
    
        ,"post45": {
            "title": "Training a classifier to detect redacted documents with fastai",
            "content": "I am working my way through the fastai course as part of an online meetup group I host.1 . This week we finished the first and second chapters of the book, during which you train a model that can recognise if an image contains a cat or a dog. Later on, you train another model that distinguishes between different types of bears (‚Äògrizzly‚Äô, ‚Äòblack‚Äô and ‚Äòteddy‚Äô). . Jeremy Howard, who is teaching the course, then prompts you to take what you learned and apply it to something that has meaning for you. (This is something that most of those who‚Äôve found any success with the course emphasise repeatedly.) . I decided to work on something adjacent to my previous life / work, where I knew there was some real-world value to be gained from such a model. I chose to train an image classifier model which would classify whether a particular image was redacted or not. . The Problem Domain: Image Redaction . Under the Freedom of Information Act (FOIA), individuals can request records and information from the US government.2 This is one collection of some of the responses to this requests, sorted into various categories. You can read, for example, responses relating to UFOs and alien visits here. . Quite often, however, these images are censored or redacted. . . Knowing that this practice exists, I thought it might be interesting to train a model that could recognise whether a particular page contained some kind of redaction. This wasn‚Äôt completely in line with what we covered during the first two chapters; I wasn‚Äôt sure if the pre-trained model we used would work for this data set and use case. . It could be useful to have such a tool, because FOIA responses can sometimes contain lots of data. In order to prepare a request for more data, you might want to be able to show that even though you were sent thousands of pages, most of those pages contained redactions and so were effectively useless. . In the ideal vision of this tool and how it would work, you could run a programme out of a particular directory and it would tell you how many pages (and what proportion) of your PDF files were redacted. . Getting the Data . The first thing I did to gather my data was to download the PDF documents available on this site. I knew that they contained examples of redactions in FOIA documents. I used Automator to split the PDF files up into individual images.3 My Automator script did some downsampling of the images as part of the process, so the images were resized to something that wasn‚Äôt prohibitively large to use for training. . Note that this stage and the next was done on my local machine. A CPU was enough for my purposes at this point, though probably I‚Äôll want to eventually port the entire process over to a single cloud machine to handle things end-to-end. . At the end of the splitting-and-resizing process, I had a little over 67,000 images (of individual pages) to train with. . Labelling the images with Prodigy . I had used Explosion.ai‚Äôs Prodigy data labelling tool in the past and so already had a license. The interface is clean and everything works pretty much as you‚Äôd hope. I had some teething issues getting it all working, but Prodigy co-creator Ines helped me work through those queries and I was up and running pretty quickly. . . It took about three hours to annotate some 4600+ images. Then I could export a .jsonl file that contained the individual annotations for whether a particular image contained a redaction or not: . . From that point it was pretty trivial to parse the file (using the json-lines package), and to resize the images down further in order to separate redacted from unredacted: . import json_lines from PIL import Image from pathlib import Path def save_resized_image_file(location_path): basewidth = 800 img = Image.open(record[&#39;image&#39;]) wpercent = (basewidth / float(img.size[0])) hsize = int((float(img.size[1]) * float(wpercent))) img = img.resize((basewidth, hsize), Image.ANTIALIAS) img.save(location_path) path = &#39;/my_projects_directory/redaction-model&#39; redacted_path = path + &quot;/redaction_training_data/&quot; + &quot;redacted&quot; unredacted_path = path + &quot;/redaction_training_data/&quot; + &quot;unredacted&quot; with open(path + &quot;/&quot; + &quot;annotations.jsonl&quot;, &quot;rb&quot;) as f: for record in json_lines.reader(f): if record[&quot;answer&quot;] == &quot;accept&quot;: save_resized_image_file(Path(redacted_path + &quot;/&quot; + record[&#39;meta&#39;][&#39;file&#39;])) else: save_resized_image_file(Path(unredacted_path + &quot;/&quot; + record[&#39;meta&#39;][&#39;file&#39;])) . Transferring the data to Paperspace with magic-wormhole . Once I had the two directories filled with the two sets of images, I zipped them up since I knew I‚Äôd want to use them on a GPU-enabled computer. . I used magic-wormhole to transfer the files over to my Paperspace Gradient machine. The files were only about 400MB in size so it took less than a minute to transfer the data. . Again, ideally I wouldn‚Äôt have this step of doing things locally first. I could certainly have done everything on the Paperspace machine from the very start, but it would have taken a bit of extra time to figure out how to process the data programatically. Moreover if I was using JupyterLab I could then use Prodigy from within my notebooks. . Using the labelled data in our training . The process of ingesting all our data (labels and raw images) is pretty easy thanks to the fastai library‚Äôs convenience classes and layered structure. We‚Äôre using the DataBlock class instead of ImageDataLoaders for extra flexibility. . path = Path(&#39;redaction_training_data&#39;) foia_documents = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(224)) dls = foia_documents.dataloaders(path) foia_documents = foia_documents.new( item_tfms=Resize(224, method=&#39;pad&#39;, pad_mode=&#39;reflection&#39;), batch_tfms=aug_transforms(max_zoom=1)) dls = foia_documents.dataloaders(path) learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(10) . The images get resized to 224x224 pixels, since this is the size that the resnet architecture expects. Since we have a good deal of labelled data, I‚Äôm comfortable using 80% of that data to train the model and the remaining 20% against which to validate. . I train it for 10 epochs as I don‚Äôt appear to reach a point where I‚Äôm overfitting. As you can see from this image, we reach an accuracy of around 96%. . . Experimenting with augmentations . Initially I had been using the RandomResizedCrop transformation on the data, but I was reminded by someone in our group (Jason) that cropping or zooming our images wouldn‚Äôt be useful since it is possible that both of those transformations would remove the small part of the image where a redaction was to be found. . In the end, I went with some settings that made sure we weren‚Äôt zooming into images or rotating them such that parts would be missing. I think there‚Äôs probably more I could squeeze out of the documentation here, particularly so that I‚Äôm not limiting myself too much in the arguments that I‚Äôm passing in. . I chose the pad method with the reflection mode since this seemed to give the best results. The zeros mode was too close to an actual redaction (i.e. a black box on the image) so I ruled that out pretty early on. . Experimenting with different architectures . The course mentions that architectures with more layers do exist. I saw that the next step up from resnet18 was resnet50. I‚Äôm certainly in the territory where I‚Äôm just turning knobs in the hope of seeing some kind of result, but I thought it was maybe worth a comparison. . The danger with having more layers (and thus more parameters) is that the model is more likely to overfit. The training process also takes much longer to execute: 44 seconds per epoch compared to 21 seconds with resnet18. It didn‚Äôt seem to measurably improve the accuracy. The best results I was able to get were still around 95%, give or take a percent or two. It seems that the real improvements are to be found in the pre-processing or augmentation stage, rather than from choosing an architecture with more layers. . Hosting the model with MyBinder . Chapter two of the course book goes into a decent amount of detail of some of the tradeoffs and issues around model deployment. Part of the exercise is to not only train a model on your own data, but go through the steps to get the model hosted online. . Using MyBinder and the voila library, alongside instructions from the book and the forums, I managed to get my model deployed. If you visit this address you‚Äôll see an interface where you should first upload an image ‚Äî i.e. a screenshot of a document. When you click ‚Äòclassify‚Äô, you‚Äôll then see a prediction of whether the image is redacted or not, as well as the confidence/probability that that prediction is true. . . Next steps . I‚Äôm at the point in the course where I know enough to be dangerous (i.e. train models), but I don‚Äôt know how to improve them from here. Some ideas I had for ways to improve the model‚Äôs accuracy: . better augmentation choices ‚Äî it‚Äôs possible that I‚Äôve misconfigured some argument or made the wrong choices in which augmentations should be applied. | more labelled data ‚Äî this one is pretty easy to fix, but I probably shouldn‚Äôt continue down this route unless I know it‚Äôs really going to help. I‚Äôm not in a position right now to be able to judge how much it‚Äôd help me. | different redaction types ‚Äî currently I have a single ‚Äòredacted‚Äô vs ‚Äòunredacted‚Äô category choice, but in reality there are several different types of redaction in the data set: some have handwritten redactions, others are square computerised boxes, and there are a couple of other types as well. I wonder whether I should train the model to recognise the different types, and then to combine those together as a ‚Äòredacted‚Äô set of categories. (I may be thinking about this wrong). | . Otherwise and for now, I‚Äôm happy with where I managed to reach with this model. I have some other ideas for how to keep going with exploring this data set. For example, even better than a slightly dumb classification model would be to have a segmentation model that was able to determine what percentage of the pixels or total area of the page that were redacted. With a reasonably accurate segmentation model of that kind, we‚Äôd then be able to provide really interesting metrics on what percentage of the information provided was redacted. . I will probably also want to go back and add in the earlier processing steps into the notebook so that things are much closer to being an ‚Äòend-to-end‚Äô solution. . Footnotes . You can find our thread in the fastai forum here.¬†&#8617; . | Other countries have variations of this law, like this from the United Kingdom.¬†&#8617; . | I realise that there is a programatic way to do this. At this early stage in the project, I was more eager to get going with the labelling, so I took the easy path by using Automator.¬†&#8617; . |",
            "url": "https://mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html",
            "relUrl": "/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html",
            "date": " ‚Ä¢ Sep 6, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I‚Äôm Alex. . I am a software engineer based in London, UK. I recently built Ekko, an open-source framework allowing developers to easily add realtime infrastructure and in-transit message processing to web applications. . I have multiple years of experience in the Ruby and JavaScript ecosystems and am comfortable working with Go, PostgreSQL, AWS cloud infrastructure and Docker. . I have a PhD in History and authored several books based on my research work in Afghanistan. . I have a long-standing blog that I will combine with this one at some point, but for now I intend to post technical posts here. .",
          "url": "https://mlops.systems/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  

  
  

  
      ,"page13": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://mlops.systems/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}