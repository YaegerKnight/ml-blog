<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Training a classifier to detect redacted documents with fastai | mlops.systems</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Training a classifier to detect redacted documents with fastai" />
<meta name="author" content="Alex Strick van Linschoten" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="How I trained a model to detect redactions in FOIA requests, using Prodigy for data labelling and the fastai library for model training" />
<meta property="og:description" content="How I trained a model to detect redactions in FOIA requests, using Prodigy for data labelling and the fastai library for model training" />
<link rel="canonical" href="https://mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html" />
<meta property="og:url" content="https://mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html" />
<meta property="og:site_name" content="mlops.systems" />
<meta property="og:image" content="https://mlops.systems/images/redacted_section.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-09-06T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2021-09-06T00:00:00-05:00","url":"https://mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html","@type":"BlogPosting","dateModified":"2021-09-06T00:00:00-05:00","image":"https://mlops.systems/images/redacted_section.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html"},"author":{"@type":"Person","name":"Alex Strick van Linschoten"},"headline":"Training a classifier to detect redacted documents with fastai","description":"How I trained a model to detect redactions in FOIA requests, using Prodigy for data labelling and the fastai library for model training","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://mlops.systems/feed.xml" title="mlops.systems" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script
  defer
  data-domain="mlops.systems"
  src="https://plausible.io/js/plausible.js"
></script>


<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">mlops.systems</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Training a classifier to detect redacted documents with fastai</h1><p class="page-description">How I trained a model to detect redactions in FOIA requests, using Prodigy for data labelling and the fastai library for model training</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-09-06T00:00:00-05:00" itemprop="datePublished">
        Sep 6, 2021
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Alex Strick van Linschoten</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      9 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#fastai">fastai</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#redactionmodel">redactionmodel</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#computervision">computervision</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#datalabelling">datalabelling</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#the-problem-domain-image-redaction">The Problem Domain: Image Redaction</a></li>
<li class="toc-entry toc-h2"><a href="#getting-the-data">Getting the Data</a></li>
<li class="toc-entry toc-h2"><a href="#labelling-the-images-with-prodigy">Labelling the images with Prodigy</a></li>
<li class="toc-entry toc-h2"><a href="#transferring-the-data-to-paperspace-with-magic-wormhole">Transferring the data to Paperspace with magic-wormhole</a></li>
<li class="toc-entry toc-h2"><a href="#using-the-labelled-data-in-our-training">Using the labelled data in our training</a></li>
<li class="toc-entry toc-h2"><a href="#experimenting-with-augmentations">Experimenting with augmentations</a></li>
<li class="toc-entry toc-h2"><a href="#experimenting-with-different-architectures">Experimenting with different architectures</a></li>
<li class="toc-entry toc-h2"><a href="#hosting-the-model-with-mybinder">Hosting the model with MyBinder</a></li>
<li class="toc-entry toc-h2"><a href="#next-steps">Next steps</a></li>
<li class="toc-entry toc-h2"><a href="#footnotes">Footnotes</a></li>
</ul><p>I am working my way through <a href="https://course.fast.ai">the fastai course</a> as part of an <a href="https://www.meetup.com/delft-fast-ai-study-group">online meetup group</a> I host.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<p>This week we finished the first and second chapters of the book, during which you train a model that can recognise if an image contains a cat or a dog. Later on, you train another model that distinguishes between different types of bears (‘grizzly’, ‘black’ and ‘teddy’).</p>

<p><a href="https://twitter.com/jeremyphoward">Jeremy Howard</a>, who is teaching the course, then prompts you to take what you learned and apply it to something that has meaning for you. (This is something that most of those who’ve found any success with the course <a href="https://sanyambhutani.com/how-not-to-do-fast-ai--or-any-ml-mooc-/">emphasise repeatedly</a>.)</p>

<p>I decided to work on something adjacent to my previous life / work, where I knew there was some real-world value to be gained from such a model. I chose to train an image classifier model which would classify whether a particular image was redacted or not.</p>

<h2 id="the-problem-domain-image-redaction">
<a class="anchor" href="#the-problem-domain-image-redaction" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Problem Domain: Image Redaction</h2>

<p>Under the <a href="https://en.wikipedia.org/wiki/Freedom_of_Information_Act_(United_States)">Freedom of Information Act</a> (FOIA), individuals can request records and information from the US government.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> This is <a href="https://www.esd.whs.mil/FOIA/Reading-Room/Reading-Room-List_2/">one collection</a> of some of the responses to this requests, sorted into various categories. You can read, for example, responses relating to UFOs and alien visits <a href="https://www.esd.whs.mil/FOIA/Reading-Room/Reading-Room-List/UFO/">here</a>.</p>

<p>Quite often, however, these images are censored or redacted.</p>

<p><img src="/images/redacted_sample.png" alt="" title="A name and an email address have been redacted here"></p>

<p>Knowing that this practice exists, I thought it might be interesting to train a model that could recognise whether a particular page contained some kind of redaction. This wasn’t completely in line with what we covered during the first two chapters; I wasn’t sure if the pre-trained model we used would work for this data set and use case.</p>

<p>It could be useful to have such a tool, because FOIA responses can sometimes contain lots of data. In order to prepare a request for more data, you might want to be able to show that even though you were sent thousands of pages, most of those pages contained redactions and so were effectively useless.</p>

<p>In the ideal vision of this tool and how it would work, you could run a programme out of a particular directory and it would tell you how many pages (and what proportion) of your PDF files were redacted.</p>

<h2 id="getting-the-data">
<a class="anchor" href="#getting-the-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Getting the Data</h2>

<p>The first thing I did to gather my data was to download the PDF documents available on <a href="https://www.esd.whs.mil/FOIA/Reading-Room/Reading-Room-List_2/">this site</a>. I knew that they contained examples of redactions in FOIA documents. I used <a href="https://support.apple.com/en-gb/guide/automator/welcome/mac">Automator</a> to split the PDF files up into individual images.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup> My Automator script did some downsampling of the images as part of the process, so the images were resized to something that wasn’t prohibitively large to use for training.</p>

<p>Note that this stage and the next was done on my local machine. A CPU was enough for my purposes at this point, though probably I’ll want to eventually port the entire process over to a single cloud machine to handle things end-to-end.</p>

<p>At the end of the splitting-and-resizing process, I had a little over 67,000 images (of individual pages) to train with.</p>

<h2 id="labelling-the-images-with-prodigy">
<a class="anchor" href="#labelling-the-images-with-prodigy" aria-hidden="true"><span class="octicon octicon-link"></span></a>Labelling the images with Prodigy</h2>

<p>I had used Explosion.ai’s <a href="https://prodi.gy">Prodigy data labelling tool</a> in the past and so already had a license. The interface is clean and everything works pretty much as you’d hope. I had some teething issues getting it all working, but <a href="https://twitter.com/_inesmontani">Prodigy co-creator Ines</a> helped me <a href="https://support.prodi.gy/t/labelling-a-set-of-images-classification/4608/1">work through those queries</a> and I was up and running pretty quickly.</p>

<p><img src="/images/prodigy-interface.png" alt="" title="The interface for image classification looked like this"></p>

<p>It took about three hours to annotate some 4600+ images. Then I could export a <code class="language-plaintext highlighter-rouge">.jsonl</code> file that contained the individual annotations for whether a particular image contained a redaction or not:</p>

<p><img src="/images/annotations_jsonl.png" alt=""></p>

<p>From that point it was pretty trivial to parse the file (using the <a href="https://pypi.org/project/json-lines/"><code class="language-plaintext highlighter-rouge">json-lines</code> package</a>), and to resize the images down further in order to separate redacted from unredacted:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">json_lines</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>

<span class="k">def</span> <span class="nf">save_resized_image_file</span><span class="p">(</span><span class="n">location_path</span><span class="p">):</span>
    <span class="n">basewidth</span> <span class="o">=</span> <span class="mi">800</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="n">record</span><span class="p">[</span><span class="s">'image'</span><span class="p">])</span>
    <span class="n">wpercent</span> <span class="o">=</span> <span class="p">(</span><span class="n">basewidth</span> <span class="o">/</span> <span class="nb">float</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">hsize</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="nb">float</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="n">size</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">*</span> <span class="nb">float</span><span class="p">(</span><span class="n">wpercent</span><span class="p">)))</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">img</span><span class="p">.</span><span class="n">resize</span><span class="p">((</span><span class="n">basewidth</span><span class="p">,</span> <span class="n">hsize</span><span class="p">),</span> <span class="n">Image</span><span class="p">.</span><span class="n">ANTIALIAS</span><span class="p">)</span>
    <span class="n">img</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">location_path</span><span class="p">)</span>

<span class="n">path</span> <span class="o">=</span> <span class="s">'/my_projects_directory/redaction-model'</span>

<span class="n">redacted_path</span> <span class="o">=</span> <span class="n">path</span> <span class="o">+</span> <span class="s">"/redaction_training_data/"</span> <span class="o">+</span> <span class="s">"redacted"</span>
<span class="n">unredacted_path</span> <span class="o">=</span> <span class="n">path</span> <span class="o">+</span> <span class="s">"/redaction_training_data/"</span> <span class="o">+</span> <span class="s">"unredacted"</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span> <span class="o">+</span> <span class="s">"/"</span> <span class="o">+</span> <span class="s">"annotations.jsonl"</span><span class="p">,</span> <span class="s">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">record</span> <span class="ow">in</span> <span class="n">json_lines</span><span class="p">.</span><span class="n">reader</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">record</span><span class="p">[</span><span class="s">"answer"</span><span class="p">]</span> <span class="o">==</span> <span class="s">"accept"</span><span class="p">:</span>
            <span class="n">save_resized_image_file</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">redacted_path</span> <span class="o">+</span> <span class="s">"/"</span> <span class="o">+</span> <span class="n">record</span><span class="p">[</span><span class="s">'meta'</span><span class="p">][</span><span class="s">'file'</span><span class="p">]))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">save_resized_image_file</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">unredacted_path</span> <span class="o">+</span> <span class="s">"/"</span> <span class="o">+</span> <span class="n">record</span><span class="p">[</span><span class="s">'meta'</span><span class="p">][</span><span class="s">'file'</span><span class="p">]))</span>
</code></pre></div></div>

<h2 id="transferring-the-data-to-paperspace-with-magic-wormhole">
<a class="anchor" href="#transferring-the-data-to-paperspace-with-magic-wormhole" aria-hidden="true"><span class="octicon octicon-link"></span></a>Transferring the data to Paperspace with <code class="language-plaintext highlighter-rouge">magic-wormhole</code>
</h2>

<p>Once I had the two directories filled with the two sets of images, I zipped them up since I knew I’d want to use them on a GPU-enabled computer.</p>

<p>I used <a href="https://magic-wormhole.readthedocs.io"><code class="language-plaintext highlighter-rouge">magic-wormhole</code></a> to transfer the files over to my <a href="https://gradient.paperspace.com">Paperspace Gradient</a> machine. The files were only about 400MB in size so it took less than a minute to transfer the data.</p>

<p>Again, ideally I wouldn’t have this step of doing things locally first. I could certainly have done everything on the Paperspace machine from the very start, but it would have taken a bit of extra time to figure out how to process the data programatically. Moreover if I was using JupyterLab I could then <a href="https://prodi.gy/docs/install#jupyterlab">use Prodigy from within my notebooks</a>.</p>

<h2 id="using-the-labelled-data-in-our-training">
<a class="anchor" href="#using-the-labelled-data-in-our-training" aria-hidden="true"><span class="octicon octicon-link"></span></a>Using the labelled data in our training</h2>

<p>The process of ingesting all our data (labels and raw images) is pretty easy thanks to the fastai library’s convenience classes and layered structure. We’re using the <code class="language-plaintext highlighter-rouge">DataBlock</code> class instead of <code class="language-plaintext highlighter-rouge">ImageDataLoaders</code> for extra flexibility.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s">'redaction_training_data'</span><span class="p">)</span>

<span class="n">foia_documents</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span>
    <span class="n">blocks</span><span class="o">=</span><span class="p">(</span><span class="n">ImageBlock</span><span class="p">,</span> <span class="n">CategoryBlock</span><span class="p">),</span>
    <span class="n">get_items</span><span class="o">=</span><span class="n">get_image_files</span><span class="p">,</span>
    <span class="n">splitter</span><span class="o">=</span><span class="n">RandomSplitter</span><span class="p">(</span><span class="n">valid_pct</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span>
    <span class="n">get_y</span><span class="o">=</span><span class="n">parent_label</span><span class="p">,</span>
    <span class="n">item_tfms</span><span class="o">=</span><span class="n">Resize</span><span class="p">(</span><span class="mi">224</span><span class="p">))</span>

<span class="n">dls</span> <span class="o">=</span> <span class="n">foia_documents</span><span class="p">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

<span class="n">foia_documents</span> <span class="o">=</span> <span class="n">foia_documents</span><span class="p">.</span><span class="n">new</span><span class="p">(</span>
    <span class="n">item_tfms</span><span class="o">=</span><span class="n">Resize</span><span class="p">(</span><span class="mi">224</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s">'pad'</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="s">'reflection'</span><span class="p">),</span>
    <span class="n">batch_tfms</span><span class="o">=</span><span class="n">aug_transforms</span><span class="p">(</span><span class="n">max_zoom</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">foia_documents</span><span class="p">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>

<span class="n">learn</span> <span class="o">=</span> <span class="n">cnn_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">resnet18</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="n">error_rate</span><span class="p">)</span>
<span class="n">learn</span><span class="p">.</span><span class="n">fine_tune</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>

<p>The images get resized to 224x224 pixels, since this is the size that the <code class="language-plaintext highlighter-rouge">resnet</code> architecture expects. Since we have a good deal of labelled data, I’m comfortable using 80% of that data to train the model and the remaining 20% against which to validate.</p>

<p>I train it for 10 epochs as I don’t appear to reach a point where I’m overfitting. As you can see from this image, we reach an accuracy of around 96%.</p>

<p><img src="/images/training_results.png" alt=""></p>

<h2 id="experimenting-with-augmentations">
<a class="anchor" href="#experimenting-with-augmentations" aria-hidden="true"><span class="octicon octicon-link"></span></a>Experimenting with augmentations</h2>

<p>Initially I had been using the <code class="language-plaintext highlighter-rouge">RandomResizedCrop</code> transformation on the data, but I was reminded by someone in our group (<a href="http://fabacademy.org/2021/labs/ulb/students/jason-pettiaux/fastai/">Jason</a>) that cropping or zooming our images wouldn’t be useful since it is possible that both of those transformations would remove the small part of the image where a redaction was to be found.</p>

<p>In the end, I went with some settings that made sure we weren’t zooming into images or rotating them such that parts would be missing. I think there’s probably more I could squeeze out of <a href="https://docs.fast.ai/vision.augment.html#Resize">the documentation</a> here, particularly so that I’m not limiting myself too much in the arguments that I’m passing in.</p>

<p>I chose the <code class="language-plaintext highlighter-rouge">pad</code> method with the <code class="language-plaintext highlighter-rouge">reflection</code> mode since this seemed to give the best results. The <code class="language-plaintext highlighter-rouge">zeros</code> mode was too close to an actual redaction (i.e. a black box on the image) so I ruled that out pretty early on.</p>

<h2 id="experimenting-with-different-architectures">
<a class="anchor" href="#experimenting-with-different-architectures" aria-hidden="true"><span class="octicon octicon-link"></span></a>Experimenting with different architectures</h2>

<p>The course mentions that architectures with more layers do exist. I saw that the next step up from <code class="language-plaintext highlighter-rouge">resnet18</code> was <code class="language-plaintext highlighter-rouge">resnet50</code>. I’m certainly in the territory where I’m just turning knobs in the hope of seeing some kind of result, but I thought it was maybe worth a comparison.</p>

<p>The danger with having more layers (and thus more parameters) is that the model is more likely to overfit. The training process also takes much longer to execute: 44 seconds per epoch compared to 21 seconds with <code class="language-plaintext highlighter-rouge">resnet18</code>. It didn’t seem to measurably improve the accuracy. The best results I was able to get were still around 95%, give or take a percent or two. It seems that the real improvements are to be found in the pre-processing or augmentation stage, rather than from choosing an architecture with more layers.</p>

<h2 id="hosting-the-model-with-mybinder">
<a class="anchor" href="#hosting-the-model-with-mybinder" aria-hidden="true"><span class="octicon octicon-link"></span></a>Hosting the model with MyBinder</h2>

<p>Chapter two of the course book goes into a decent amount of detail of some of the tradeoffs and issues around model deployment. Part of the exercise is to not only train a model on your own data, but go through the steps to get the model hosted online.</p>

<p>Using <a href="https://mybinder.org">MyBinder</a> and the <a href="https://voila.readthedocs.io"><code class="language-plaintext highlighter-rouge">voila</code> library</a>, alongside instructions from the book and the forums, I managed to get my model deployed. If you visit <a href="https://hub.gke2.mybinder.org/user/strickvl-binder-redaction-s1nr4p8k/voila/render/binder-redaction-classifier.ipynb?token=kReM2K-iSkmSjud5N28o8Q">this address</a> you’ll see an interface where you should first upload an image — i.e. a screenshot of a document. When you click ‘classify’, you’ll then see a prediction of whether the image is redacted or not, as well as the confidence/probability that that prediction is true.</p>

<p><img src="/images/mybinder_interface.png" alt="" title="Part of the MyBinder interface following a successful inference"></p>

<h2 id="next-steps">
<a class="anchor" href="#next-steps" aria-hidden="true"><span class="octicon octicon-link"></span></a>Next steps</h2>

<p>I’m at the point in the course where I know enough to be dangerous (i.e. train models), but I don’t know how to improve them from here. Some ideas I had for ways to improve the model’s accuracy:</p>

<ul>
  <li>better augmentation choices — it’s possible that I’ve misconfigured some argument or made the wrong choices in which augmentations should be applied.</li>
  <li>more labelled data — this one is pretty easy to fix, but I probably shouldn’t continue down this route unless I know it’s really going to help. I’m not in a position right now to be able to judge how much it’d help me.</li>
  <li>different redaction types — currently I have a single ‘redacted’ vs ‘unredacted’ category choice, but in reality there are several different types of redaction in the data set: some have handwritten redactions, others are square computerised boxes, and there are a couple of other types as well. I wonder whether I should train the model to recognise the different types, and then to combine those together as a ‘redacted’ set of categories. (I may be thinking about this wrong).</li>
</ul>

<p>Otherwise and for now, I’m happy with where I managed to reach with this model. I have some other ideas for how to keep going with exploring this data set. For example, even better than a slightly dumb classification model would be to have a segmentation model that was able to determine what percentage of the pixels or total area of the page that were redacted. With a reasonably accurate segmentation model of that kind, we’d then be able to provide really interesting metrics on what percentage of the information provided was redacted.</p>

<p>I will probably also want to go back and add in the earlier processing steps into the notebook so that things are much closer to being an ‘end-to-end’ solution.</p>

<h2 id="footnotes">
<a class="anchor" href="#footnotes" aria-hidden="true"><span class="octicon octicon-link"></span></a>Footnotes</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>You can find our thread in the fastai forum <a href="https://forums.fast.ai/t/virtual-study-group-delft-the-netherlands-europe-time-zone/90521">here</a>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>Other countries have variations of this law, like <a href="https://www.gov.uk/make-a-freedom-of-information-request">this</a> from the United Kingdom. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>I realise that there is a programatic way to do this. At this early stage in the project, I was more eager to get going with the labelling, so I took the easy path by using Automator. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">↩</a></p>
    </li>
  </ol>
</div>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="strickvl/ml-blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A place to share my technical learnings.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/strickvl" target="_blank" title="strickvl"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/strickvl" target="_blank" title="strickvl"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
