<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Some foundations for machine learning with PyTorch | mlops.systems</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Some foundations for machine learning with PyTorch" />
<meta name="author" content="Alex Strick van Linschoten" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I outline the basic process that a computer uses when training a model, greatly simplified and all explained through the lens of PyTorch and how it calculates gradients. These are some pre-requisite foundations that we will later apply to our Fashion MNIST dataset." />
<meta property="og:description" content="I outline the basic process that a computer uses when training a model, greatly simplified and all explained through the lens of PyTorch and how it calculates gradients. These are some pre-requisite foundations that we will later apply to our Fashion MNIST dataset." />
<link rel="canonical" href="https://mlops.systems/fastai/computervision/pytorch/2022/05/12/seven-steps-gradient-calculations.html" />
<meta property="og:url" content="https://mlops.systems/fastai/computervision/pytorch/2022/05/12/seven-steps-gradient-calculations.html" />
<meta property="og:site_name" content="mlops.systems" />
<meta property="og:image" content="https://mlops.systems/images/ml-training-big-picture/seven-steps.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-05-12T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2022-05-12T00:00:00-05:00","url":"https://mlops.systems/fastai/computervision/pytorch/2022/05/12/seven-steps-gradient-calculations.html","@type":"BlogPosting","dateModified":"2022-05-12T00:00:00-05:00","image":"https://mlops.systems/images/ml-training-big-picture/seven-steps.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://mlops.systems/fastai/computervision/pytorch/2022/05/12/seven-steps-gradient-calculations.html"},"author":{"@type":"Person","name":"Alex Strick van Linschoten"},"headline":"Some foundations for machine learning with PyTorch","description":"I outline the basic process that a computer uses when training a model, greatly simplified and all explained through the lens of PyTorch and how it calculates gradients. These are some pre-requisite foundations that we will later apply to our Fashion MNIST dataset.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://mlops.systems/feed.xml" title="mlops.systems" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script
  defer
  data-domain="mlops.systems"
  src="https://plausible.io/js/plausible.js"
></script>


<link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">mlops.systems</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Some foundations for machine learning with PyTorch</h1><p class="page-description">I outline the basic process that a computer uses when training a model, greatly simplified and all explained through the lens of PyTorch and how it calculates gradients. These are some pre-requisite foundations that we will later apply to our Fashion MNIST dataset.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-05-12T00:00:00-05:00" itemprop="datePublished">
        May 12, 2022
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Alex Strick van Linschoten</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#fastai">fastai</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#computervision">computervision</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#pytorch">pytorch</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/strickvl/ml-blog/tree/master/_notebooks/2022-05-12-seven-steps-gradient-calculations.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/strickvl/ml-blog/master?filepath=_notebooks%2F2022-05-12-seven-steps-gradient-calculations.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/strickvl/ml-blog/blob/master/_notebooks/2022-05-12-seven-steps-gradient-calculations.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#Seven-steps-for-a-machine-to-learn">Seven steps for a machine to learn </a></li>
<li class="toc-entry toc-h1"><a href="#Calculating-gradients-with-PyTorch">Calculating gradients with PyTorch </a>
<ul>
<li class="toc-entry toc-h2"><a href="#1.-Setup:-add-.requires_grad_()-to-a-tensor">1. Setup: add .requires_grad_() to a tensor </a></li>
<li class="toc-entry toc-h2"><a href="#2.-Use-.backward()-to-calculate-the-gradient">2. Use .backward() to calculate the gradient </a></li>
<li class="toc-entry toc-h2"><a href="#3.-Access-the-gradient-via-the-.grad-attribute">3. Access the gradient via the .grad attribute </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#'Stepping':-using-learning-rates-to-figure-out-how-much-to-step">&#39;Stepping&#39;: using learning rates to figure out how much to step </a></li>
<li class="toc-entry toc-h1"><a href="#Takeaways-from-the-seven-step-process">Takeaways from the seven-step process </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-05-12-seven-steps-gradient-calculations.ipynb
-->

<div class="container" id="notebook-container">
        
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">!</span>pip install -Uqq fastbook nbdev torch
<span class="kn">import</span> <span class="nn">fastbook</span>
<span class="n">fastbook</span><span class="o">.</span><span class="n">setup_book</span><span class="p">()</span>

<span class="kn">from</span> <span class="nn">fastai.vision.all</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">fastbook</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms</span> <span class="kn">import</span> <span class="n">ToTensor</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>

    </div>
</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In the <a href="https://mlops.systems/fastai/computervision/pytorch/2022/05/12/fashion-mnist-pixel-similarity.html">previous post</a> I showed a naive approach to calculating the similarity or difference between images, and how that could be used to create a function that did pretty well at estimating whether any particular image was a pullover or a dress.</p>
<p><a href="https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb">Chapter 4 of the fastbook</a> then takes us on a journey showing a smarter approach where the computer can make even better estimations and predictions. The broad strokes of this approach are simple to grasp, but of course the individual details are where the nuances of machine learning are to be found.</p>
<h1 id="Seven-steps-for-a-machine-to-learn">
<a class="anchor" href="#Seven-steps-for-a-machine-to-learn" aria-hidden="true"><span class="octicon octicon-link"></span></a>Seven steps for a machine to learn<a class="anchor-link" href="#Seven-steps-for-a-machine-to-learn"> </a>
</h1>
<p><img src="/images/copied_from_nb/nb_images/ml-training-big-picture/seven-steps.png" alt=""></p>
<p><strong>1. Initialise a set of weights</strong></p>
<p>We use random values start with. There could potentially be more elaborate /
fancy ways to calculate some starting values that are closer to our end goal,
but in practice it is unnecessary because we have a process that we are going to
use to update our values.</p>
<p><strong>2. Use the weights to make a prediction</strong></p>
<p>We check whether the sets of weights we have currently set as part of our
function are the right ones. We check the prediction (i.e. what came out the
other end of our model / function) and get a sense of how well our model did.</p>
<p><strong>3. Loss: see how well we did with our predictions</strong></p>
<p>We calculate the loss for our data. How well did we do at predicting what we
were trying to predict? We use a number that will be small if our function is
doing well. (Note: this is just a convention and otherwise there's no special
reason for this.)</p>
<p><strong>4. Calculate the gradients across all the weights</strong></p>
<p>We are calculating the gradient for lots of numbers, not just a single value.
For every stage, we get back the gradient for all of the numbers. This is done
sequentially, where we calculate the gradient for one weight / number, keeping
all the other numbers constant, and then we repeat this for all the other
weights.</p>
<p>Gradient calculation relates to calculating derivatives and with this we are
stepping firmly into the space of calculus. I don't fully understand how all
this works, but intuitively, the important thing to know is this: we are
calculating the change of the value, not the value itself. I.e. we want to know
how things will change (by how much, and in what direction) if we shift this
value slightly.</p>
<p>Note that the process of calculating the gradients is a performance
optimisation. We could just as well have done this with a (slower) manual
process where we adjust a little bit each time. With the gradient calculations
we can take bigger steps in the direction we want, with more precision guiding
our guesses about the direction and distance we want to go.</p>
<p><strong>5. 'Step': Update the weights</strong></p>
<p>This is where we increase or decrease our own weights by a small amount and see
whether the loss goes up or down. As hinted in step 4, we use calculus to figure
out:</p>
<ul>
<li>which direction to go in</li>
<li>how much we should increase or decrease our own weights</li>
</ul>
<p><strong>6. Repeat starting at step 2</strong></p>
<p>This is an iterative process.</p>
<p><strong>7. Iterate until we decide to stop</strong></p>
<p>There are various criteria determining when we should stop. Some possible
stopping points might include:</p>
<ul>
<li>when the model is 'good enough' for our use case</li>
<li>when we have run out of time (or money!)</li>
<li>when the accuracy starts getting worse (i.e. the model isn't performing as well)</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Calculating-gradients-with-PyTorch">
<a class="anchor" href="#Calculating-gradients-with-PyTorch" aria-hidden="true"><span class="octicon octicon-link"></span></a>Calculating gradients with PyTorch<a class="anchor-link" href="#Calculating-gradients-with-PyTorch"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For those of us who don't bring a strong mathematics foundation into this space,
the mention of calculus, derivatives and gradients isn't especially reassuring,
but rest assured that PyTorch can help us out during this process.</p>
<p>There are three main parts to using PyTorch to calculate gradients (i.e. step
four of the seven steps listed above.)</p>
<h2 id="1.-Setup:-add-.requires_grad_()-to-a-tensor">
<a class="anchor" href="#1.-Setup:-add-.requires_grad_()-to-a-tensor" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. Setup: add <code>.requires_grad_()</code> to a tensor<a class="anchor-link" href="#1.-Setup:-add-.requires_grad_()-to-a-tensor"> </a>
</h2>
<p>For any Tensor where we know we're going to want to calculate the gradients of
values, we call <code>.require_grad()</code> on that Tensor.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span>

<span class="n">x_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>

<span class="n">y_tensor</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x_tensor</span><span class="p">)</span>

<span class="n">y_tensor</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(9., grad_fn=&lt;PowBackward0&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here we can see that 3 squared is indeed 9, and we can see the <code>grad_fn</code> as part
of the Tensor.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="2.-Use-.backward()-to-calculate-the-gradient">
<a class="anchor" href="#2.-Use-.backward()-to-calculate-the-gradient" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. Use <code>.backward()</code> to calculate the gradient<a class="anchor-link" href="#2.-Use-.backward()-to-calculate-the-gradient"> </a>
</h2>
<p>This actually refers to backpropagation, something which is explained much later
in the book. This step is also known as the 'backward pass'. Note, that this is
again another piece of jargon that we just have to learn. In reality this method
might as well have been called <code>.calculate_gradients()</code>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_tensor</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="3.-Access-the-gradient-via-the-.grad-attribute">
<a class="anchor" href="#3.-Access-the-gradient-via-the-.grad-attribute" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. Access the gradient via the <code>.grad</code> attribute<a class="anchor-link" href="#3.-Access-the-gradient-via-the-.grad-attribute"> </a>
</h2>
<p>We view the gradient by checking this <code>.grad</code> attribute.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">x_tensor</span><span class="o">.</span><span class="n">grad</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(6.)</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>I can't explain why this is the case, since I've never learned how to calculate
gradients or derivatives (or anything about calculus, for that matter!) but in
any case it's not really important.</p>
<p>Note that we can do this whole process over Tensors that are more complex than
illustrated in the above simple example:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">complex_x</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">([</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">12.</span><span class="p">])</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="n">complex_y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">complex_x</span><span class="p">)</span>
<span class="n">complex_y</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">complex_x</span><span class="o">.</span><span class="n">grad</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([ 6., 10., 24.])</pre>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Something else I discovered while doing this was that gradients can only be
calculated on floating point values, so this is why when we create <code>x_tensor</code>
and <code>complex_x</code> we create them with floating point values (<code>3.</code> etc) instead of
just integers. In reality, I think there will be some kind of normalisation of
our values as part of the process, so they would probably <em>already</em> be floats,
but it's worth noting.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="'Stepping':-using-learning-rates-to-figure-out-how-much-to-step">
<a class="anchor" href="#'Stepping':-using-learning-rates-to-figure-out-how-much-to-step" aria-hidden="true"><span class="octicon octicon-link"></span></a>'Stepping': using learning rates to figure out how much to step<a class="anchor-link" href="#'Stepping':-using-learning-rates-to-figure-out-how-much-to-step"> </a>
</h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we know how to calculate gradients, the key question for the fifth step
in our seven-step process is the following:</p>
<blockquote>
<p>"How much should we shift our values based on what we get back from our
gradient calculations?</p>
</blockquote>
<p>We call this amount the 'learning rate', and it is usually a value between 0.001
and 0.1. Very small, in other words :)
We adjust our weights / parameters by this basic equation:</p>
<div class="highlight"><pre><span></span><span class="n">weights</span> <span class="o">-=</span> <span class="n">weights</span><span class="o">.</span><span class="n">grad</span> <span class="o">*</span> <span class="n">learning_rate</span>
</pre></div>
<p>This process is called "stepping the parameters" and it uses an optimisation
step.</p>
<p>The learning rate shouldn't be too high, else our loss can get higher / worse or
otherwise we can just bounce around within the boundaries of our function
without ever reaching the optimum (== lowest) loss.</p>
<p>At the same time, we shouldn't use a very tiny learning rate (i.e. even tinier
than the 0.001-0.1 mentioned above) since then the process will take a really
long time and while we might reach the optimum loss at some point, it might not
be the fastest way to get there.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Takeaways-from-the-seven-step-process">
<a class="anchor" href="#Takeaways-from-the-seven-step-process" aria-hidden="true"><span class="octicon octicon-link"></span></a>Takeaways from the seven-step process<a class="anchor-link" href="#Takeaways-from-the-seven-step-process"> </a>
</h1>
<p>Most of this makes a lot of intuitive sense to me. The parts that don't are what
is going on with the gradient calculations and finding of derivatives and so on.
For now, it appears that I can get away without understanding precisely how that
works. It is enough to appreciate that we have a way to make these calculations,
and those calculations are optimised for us</p>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="strickvl/ml-blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/fastai/computervision/pytorch/2022/05/12/seven-steps-gradient-calculations.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A place to share my technical learnings.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/strickvl" target="_blank" title="strickvl"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/strickvl" target="_blank" title="strickvl"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
