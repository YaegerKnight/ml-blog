<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://mlops.systems/feed.xml" rel="self" type="application/atom+xml" /><link href="https://mlops.systems/" rel="alternate" type="text/html" /><updated>2022-10-21T13:49:00-05:00</updated><id>https://mlops.systems/feed.xml</id><title type="html">mlops.systems</title><subtitle>A place to share my technical learnings.</subtitle><entry><title type="html">Using J for simple calculation</title><link href="https://mlops.systems/j/mathematics/q31/mu123/2022/10/21/maths-using-j.html" rel="alternate" type="text/html" title="Using J for simple calculation" /><published>2022-10-21T00:00:00-05:00</published><updated>2022-10-21T00:00:00-05:00</updated><id>https://mlops.systems/j/mathematics/q31/mu123/2022/10/21/maths-using-j</id><content type="html" xml:base="https://mlops.systems/j/mathematics/q31/mu123/2022/10/21/maths-using-j.html">&lt;p&gt;I’m curious to see if I can improve my command of &lt;a href=&quot;https://www.jsoftware.com&quot;&gt;some J idioms and syntax&lt;/a&gt; while working on &lt;a href=&quot;https://www.open.ac.uk/courses/maths/degrees/bsc-mathematics-q31&quot;&gt;my mathematics study&lt;/a&gt; at the Open University. People sometimes (pejoratively) say that J is just a calculator on steroids. I happen to need a calculator from time to time doing &lt;a href=&quot;https://www.open.ac.uk/courses/modules/mu123&quot;&gt;MU123&lt;/a&gt;, so I figure I’ll just learn as I go. (This comes with a huge caveat to everything I write in these blogs: I could be completely wrong in how they’re working, or the language I use to describe things etc. I welcome corrections and comments below.)&lt;/p&gt;

&lt;p&gt;I &lt;a href=&quot;https://mlops.systems/j/mathematics/mu123/q31/notation/2022/10/16/notational-precedence.html&quot;&gt;wrote last week&lt;/a&gt; about orders of precedence and notation in J (and for mathematics in general) so I won’t repeat myself here. The summary of that all is that J evaluates from right to left in the order that expressions are encountered.&lt;/p&gt;

&lt;h2 id=&quot;some-ultra-basics&quot;&gt;Some ultra basics&lt;/h2&gt;

&lt;p&gt;Negative numbers are denoted by the underscore (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_&lt;/code&gt;) symbol:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   3-4
_1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that there can’t be any space in between the underscore and the number that’s negative.&lt;/p&gt;

&lt;p&gt;Multiplication is handled by the asterisk (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*&lt;/code&gt;) symbol, much like elsewhere in the world of computers:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   3&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;5
15
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Division is handled by the percentage (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;%&lt;/code&gt;) symbol:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   15 % 5
3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;While we’re here at simple operators, we can specify power operations with the caret:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   3^2
9
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The square root is calculated by using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;%:&lt;/code&gt; as in the following calculation:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   %:9
3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;fractions-and-rational-numbers&quot;&gt;Fractions and Rational Numbers&lt;/h2&gt;

&lt;p&gt;In J, the letter &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;r&lt;/code&gt; is used for the notation of rational numbers (i.e. the numbers which represent a ratio of the two integers). For example, to represent two-thirds, you would write &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;2r3&lt;/code&gt;. J is smart about interactions between rationals (i.e. fractions), so you can use them in calculations:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   1r2 &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; 6r4
3r4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you want to turn a decimal number into a fraction / rational number, use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x:&lt;/code&gt; as in the following example:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   x:0.3
3r10
   x:0.97
97r100
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;assigning-variables&quot;&gt;Assigning variables&lt;/h2&gt;

&lt;p&gt;Variables and algebra hasn’t come up too much so far in MU123, but as a sneak peek, in J you assign variables using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;=.&lt;/code&gt; as in:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;.4
&lt;span class=&quot;nv&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;.0.6
&lt;span class=&quot;nv&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;._0.3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;open-questions&quot;&gt;Open Questions&lt;/h2&gt;

&lt;p&gt;I’m still looking for the J way to do rounding (i.e. decimal places and significant figures). I did see &lt;a href=&quot;https://code.jsoftware.com/wiki/Fifty_Shades_of_J/Chapter_28#Grouping_and_Rounding&quot;&gt;one example on the J wiki&lt;/a&gt; which went like this:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;   &lt;span class=&quot;nv&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;: &amp;lt;.@&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;0.5&amp;amp;+&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
   R 9.5
10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So that rounds numbers up. On the first line some kind of function is defined and then on the second line we’re applying it to the number 9.5 which rounds up to 10. I assume the logic of that function is ‘round down to the nearest integer if the decimal value is less than 0.5, but round up if it’s greater than 0.5’.&lt;/p&gt;

&lt;p&gt;I’m also trying to figure out how exactly to use J in Jupyter notebooks. I use &lt;a href=&quot;https://fastpages.fast.ai&quot;&gt;fastpages&lt;/a&gt; for this blog and one feature is that you can publish your notebooks and they get converted into blog pages. If I had a way to write J-backed notebooks, that’d be great. (At the moment, it seems &lt;a href=&quot;https://github.com/tmcguirefl/J-Jupyter&quot;&gt;these files&lt;/a&gt; are the main options and guides.)&lt;/p&gt;

&lt;h2 id=&quot;acknowledgements&quot;&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;I learned a lot from the ‘Numbers’ section in &lt;a href=&quot;https://www.jsoftware.com/help/learning/19.htm&quot;&gt;Learning J&lt;/a&gt; while figuring out how to do these things.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="j" /><category term="mathematics" /><category term="q31" /><category term="mu123" /><summary type="html">I’m curious to see if I can improve my command of some J idioms and syntax while working on my mathematics study at the Open University. People sometimes (pejoratively) say that J is just a calculator on steroids. I happen to need a calculator from time to time doing MU123, so I figure I’ll just learn as I go. (This comes with a huge caveat to everything I write in these blogs: I could be completely wrong in how they’re working, or the language I use to describe things etc. I welcome corrections and comments below.)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/mu123-unit1/jblue.png" /><media:content medium="image" url="https://mlops.systems/images/mu123-unit1/jblue.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Deep learning tricks all the way down, with a bit of mathematics for good measure</title><link href="https://mlops.systems/computervision/fastai/parttwo/2022/10/17/fastai-part-2-lesson-9-stable-diffusion.html" rel="alternate" type="text/html" title="Deep learning tricks all the way down, with a bit of mathematics for good measure" /><published>2022-10-17T00:00:00-05:00</published><updated>2022-10-17T00:00:00-05:00</updated><id>https://mlops.systems/computervision/fastai/parttwo/2022/10/17/fastai-part-2-lesson-9-stable-diffusion</id><content type="html" xml:base="https://mlops.systems/computervision/fastai/parttwo/2022/10/17/fastai-part-2-lesson-9-stable-diffusion.html">&lt;p&gt;&lt;em&gt;(This is part of a series of blog posts relating to and responding to the live FastAI course (part 2) being taught October-December 2022. To read others, see the ones listed for &lt;a href=&quot;https://mlops.systems/categories/#parttwo&quot;&gt;the ‘parttwo’ tag&lt;/a&gt;.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Much awaited and anticipated, the &lt;a href=&quot;https://www.fast.ai/posts/part2-2022.html&quot;&gt;second part of the FastAI course&lt;/a&gt; is being taught live again. If part one is about getting solid foundations and learning how to get going in a practical/useful way, part two is about approaching things from the foundations but with research or ‘impractical’ questions kept in mind. The backdrop of the current iteration is the breakthroughs happening in the world of generative computer vision models like &lt;a href=&quot;https://huggingface.co/CompVis/stable-diffusion-v1-4&quot;&gt;Stable Diffusion&lt;/a&gt;, which we’ll explore and deconstruct (and reconstruct?!) over the coming weeks.&lt;/p&gt;

&lt;p&gt;Diving into the details of how things work means that along the way we’re much more likely to encounter (legitimate) specialist vocabulary and techniques as well as a decent dose of jargon. Whereas bringing up the intricacies of particular algorithms, architectures or mathematical methods was unnecessary during part one, it seems like part two is a little bit more of a venue for that kind of material. I will use these blogs as a way of reviewing materials and concepts introduced during the lectures as well as keeping track of the big questions I have.&lt;/p&gt;

&lt;p&gt;In this blog, in particular, I’ll keep a glossary at the bottom for some new terms which were introduced. I may repeat this for subsequent blog reviews, depending on what’s covered in those lessons. I’ll also keep a section containing new mathematical symbols that are introduced. (This blog mainly relates to the core lecture given during week 1. I’ll update it later with some small extras that came up from the 9A and 9B videos, or expand those into separate posts on their own.)&lt;/p&gt;

&lt;p&gt;Stable Diffusion isn’t, in itself, a model that I’m especially interested in, except insofar as it teaches me fundamental principles about the craft of deep learning or about doing research in this field. As such, my plan and current intention is to stick to documenting core mental models or bigger-picture lessons that I’m taking away from the lessons rather than each individual step that Jeremy made along the way. (This seems to be the motivation behind including it in the course at all. Stable Diffusion touches so many topics (big and small) and getting to grips with this one thing will help understand many other things about machine learning and the world of research.)&lt;/p&gt;

&lt;h1 id=&quot;-stable-diffusion-101&quot;&gt;💬🌄 Stable Diffusion 101&lt;/h1&gt;

&lt;p&gt;If you’ve been on the internet at all during the past 6-12 months, you’ll almost certainly have been exposed to examples of images that have been generated using techniques grounded in deep learning. Here is one that I generated just now:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/fastai-lesson-9/sd-dutch-nn.png&quot; alt=&quot;&quot; title=&quot;This used the prompt 'a Dutch male AI researcher trying to understand how neural networks work. looking at a computer screen on his desk'&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These images are generated by passing in a prompt into the model which then uses that to come up with something that represents the text you passed in. (Shoutout to the creators and maintainers of &lt;a href=&quot;https://github.com/fastai/diffusion-nbs&quot;&gt;the tremendously useful &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;diffusion-nbs&lt;/code&gt; notebooks&lt;/a&gt;.) The interface between the text and the image is still not as seamless as might be hoped, and a discipline of ‘prompt engineering’ has grown around finding the best ways to coax certain kinds of images out of the model. (See &lt;a href=&quot;http://dallery.gallery/wp-content/uploads/2022/07/The-DALL·E-2-prompt-book-v1.02.pdf&quot;&gt;this book&lt;/a&gt; to learn more about what works with DALL-E 2, for example. Or visit &lt;a href=&quot;https://lexica.art&quot;&gt;Lexica&lt;/a&gt; to search images based on the prompts that were used to create them.)&lt;/p&gt;

&lt;p&gt;There are some obvious interim questions that result from the existence of such models and their outputs, notably what this allows in terms of creativity and how it might transform the kinds of tools we use for image editing and creation. The advances are certainly impressive, but outside the field, (and without being moving on too quickly from the unquestioned achievement of these types of models) what does it mean for the rest of deep learning?&lt;/p&gt;

&lt;p&gt;The first thing that is maybe interesting for the field is the way that these models are multi-modal, or in other words they aren’t stuck in the silo of being text-only, or image-only, and so on. We are able to translate (to a greater or lesser degree) between language and images with these models, which seems like it might open up a whole universe of interactions and behaviours that are interesting to explore.&lt;/p&gt;

&lt;p&gt;At a very (very) high level, what’s going on with stable diffusion is that it starts with generating an image that is more or less purely random noise, and then (with subsequent iterations) slowly reveals an image and coherence that was contained within the random noise. Similar to how Michelangelo said of sculpture (“It is already there, I just have to chisel away the superfluous material”), what happens here is that we have to remove the superfluous noise.&lt;/p&gt;

&lt;h1 id=&quot;-core-takeaways-how-does-it-work&quot;&gt;🛠 Core Takeaways: How does it work?&lt;/h1&gt;

&lt;p&gt;Those of you not taking the course live will have to wait a few months for the lectures to be released and in any case I don’t want to parrot the order and progression of how Jeremy explained how Stable Diffusion works. With that said, I was pleasantly surprised by how much I was able to follow along given what is a fairly technically involved topic. (Note to self: the fundamentals continue to be important!)&lt;/p&gt;

&lt;h2 id=&quot;fundamentals-still-count&quot;&gt;Fundamentals still count&lt;/h2&gt;

&lt;p&gt;Even though there are a hundred and one small innovations and technologies which make something like Stable Diffusion possible, in the end we’re still dealing with Deep Learning and we’re still dealing with finding ways of converting things into numbers which can be used by machines to update weights by way of evaluating loss functions. So many of the individual pieces that make up how you build something like Stable Diffusion amount to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;figure out how to get this non-number-like thing into a numeric representation (ideally a vector of some kind)&lt;/li&gt;
  &lt;li&gt;do all the usual deep learning things that we’ve done a thousand times and that we know work&lt;/li&gt;
  &lt;li&gt;at the end, maybe find a way to convert the numeric representation that our model learned into some kind of form that is useful to us&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Obviously the details are important and nobody is creating magical generative art with this very high-level hand-wavy explanation, but for someone at the earlier end of their journey into deep learning it is reassuring that the fundamentals continue to have relevance and that those mental models remain useful as a way of thinking about new developments.&lt;/p&gt;

&lt;h2 id=&quot;the-tricks-are-the-way&quot;&gt;The tricks &lt;em&gt;are&lt;/em&gt; the way&lt;/h2&gt;

&lt;p&gt;The other pleasant surprise was the enduring relevance of ‘tricks’. In chapter one of the FastAI book, Jeremy &amp;amp; Sylvain showcase a number of examples where clever approaches are taken to solve problems with Deep Learning:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;a malware classification program is made by converting malware code into an image which is used to train a model&lt;/li&gt;
  &lt;li&gt;a fraud detection algorithm is trained by converting images of computer mouse movements&lt;/li&gt;
  &lt;li&gt;…and so on&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Even amongst the Delft FastAI study group, Kurian trained &lt;a href=&quot;https://kurianbenoy.com/ml-blog/fastai/fastaicourse/2022/05/01/AudioCNNDemo.html&quot;&gt;a classifier to detect genre in music samples&lt;/a&gt; using a similar method (i.e. using images as an intermediary form for the samples which were used in training). The book emphasises:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“In general, you’ll find that a small number of general approaches in deep learning can go a long way, if you’re a bit creative in how you represent your data! You shouldn’t think of approaches like the ones described here as “hacky workarounds,” because actually they often (as here) beat previously state-of-the-art results. These really are the right ways to think about these problem domains.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Most of these ‘tricks’ seem to relate to either performance improvements (i.e. how can we get this training to happen faster, or with fewer compute needs) or ways of getting your problem domain into a form that we can use deep learning techniques on them. In the case of Stable Diffusion, one of the problems we have to address is how to work in this multi-modal manner, where text is used to represent a particular idea (which in turn needs a vector/numeric representation) but where we also want to represent that same idea in image form.&lt;/p&gt;

&lt;p&gt;At the same time, we have the whole autoencoder part of the story — whereby we use an encoder to turn a large image into a (smaller-sized) latent representation which can be used in training, and then we use a decoder to turn a noisy latent into a full-sized image — which seems to mainly be about making the training process more efficient.&lt;/p&gt;

&lt;p&gt;Each of these techniques come with their own complexities and histories, but it’s just notable to me how the story of the development of machine learning techniques seems somehow to be a succession of these small incremental innovations that progressively accrue. That’s not to say that there aren’t big breakthroughs in either understanding why things work the way they do, or in the more tactical method space, but it just seemed very apparent in the unpacking of Stable Diffusion that a great deal of creative stitching together of ideas had taken place.&lt;/p&gt;

&lt;p&gt;The historian in me is fascinated by the different pathways that the field has explored, or the reasons why certain techniques emerged when they did, or how hardware improvements gave tried-and-rejected techniques a new lease of life, but I’m guessing that probably doesn’t help much with the work of research.&lt;/p&gt;

&lt;h2 id=&quot;-what-happens-when-we-train-the-diffusion-model&quot;&gt;💪 What happens when we train the diffusion model&lt;/h2&gt;

&lt;p&gt;A diffusion model is a neural network that we train. The way it works is that it removes noise from an image (passed in as input along with a text prompt) such that the output more closely resembles the prompt. When we are training our network, we pass in the vectorised words along with the latent forms of the images (since those are much smaller file sizes and thus faster / more efficient to train). We use the encoder to get a latent representation of the image that we use for training.&lt;/p&gt;

&lt;p&gt;For the text caption, we want a way to represent the association of images with text captions in vector space. In other words, if there are various phrases that all represent more or less the same image if you were to translate those phrases into an image, then those should be similar when represented as a vector. The technique or trick for this is to use ‘contrastive loss’, a particular kind of loss function which allows us to calculate the relative similarity of two vectors. This contrastive loss is what gives us the first two letters of &lt;a href=&quot;https://openai.com/blog/clip/&quot;&gt;‘CLIP’&lt;/a&gt;, a neural network developed by OpenAI.&lt;/p&gt;

&lt;p&gt;The CLIP model takes some text and outputs an embedding, i.e. some features in vector form that our unet can use for training along with the images in their latent representation form.&lt;/p&gt;

&lt;h2 id=&quot;-what-happens-when-we-generate-an-image&quot;&gt;🎨 What happens when we generate an image&lt;/h2&gt;

&lt;p&gt;When generating our image we can use the neural network we trained to progressively remove noise from our candidate image. We start off with a more or less completely noisy image, then apply the unet to it and it returns the noise that it calculates is sitting on top of a latent representation that approximates the vectorised version of our prompt. We take a fraction of that, remove it, and repeat a few times. (Currently that can take as many as 50 iterations before we reach a really impressive image, but new techniques are in review which would dramatically reduce the need for so many iterations.)&lt;/p&gt;

&lt;p&gt;Note that it is during the inference stage where we need the decoder part of our (VAE) encoder to turn a latent tensor representation of an image into a fully-fledged large picture.&lt;/p&gt;

&lt;h1 id=&quot;-how-to-play--practice-for-part-ii&quot;&gt;🎺 How to play &amp;amp; practice for part II&lt;/h1&gt;

&lt;p&gt;I also wanted to briefly take a second to reflect on what might be useful as ways to get practically involved during the coming weeks. In part one, the instruction was fairly simple: “train lots of models”. In part two, the practicality is initially still there, it seems, but there will be other areas of emphasis. The things that seem to make sense to me currently are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;continue to blog as a way of reflecting and developing my understanding&lt;/li&gt;
  &lt;li&gt;understand and digest the core concepts that are introduced&lt;/li&gt;
  &lt;li&gt;whenever the ‘code everything from scratch’ part of the course starts, make sure to at least attempt this on my own alongside whatever is being showcased in lectures&lt;/li&gt;
  &lt;li&gt;discuss areas where concepts are unclear during the weekly &lt;a href=&quot;https://www.meetup.com/delft-fast-ai-study-group/&quot;&gt;Delft FastAI study group&lt;/a&gt; calls that I organise&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I suspect that the discipline of the coding will be most instructive, once we
get to it, though by extension probably also one that comes with the most
struggle.&lt;/p&gt;

&lt;p&gt;Following a session of the Delft Study Group, I gathered some more suggestions for how to get the most out of this part 2:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;get hands-on as much as possible&lt;/li&gt;
  &lt;li&gt;‘the details matter’ and try to go above and beyond with the course and you’ll be rewarded&lt;/li&gt;
  &lt;li&gt;blog and explain what you’re learning&lt;/li&gt;
  &lt;li&gt;answer questions on the forums as a way of cementing your learning&lt;/li&gt;
  &lt;li&gt;don’t let yourself get blocked by ideas that you don’t understand along the way. Keep following along with the course and more likely than not these things will clear themselves up&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;-glossary-of-core-terms&quot;&gt;📖 Glossary of Core Terms&lt;/h1&gt;

&lt;p&gt;(Listed alphabetically, not in the order of exposition. Also these reflect my current understanding which is not always complete, so I’ll keep this updated as my understanding grows.)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Analytic derivatives&lt;/strong&gt; — This is a faster way of calculating the gradients for our image, such that we calculate the whole set at once. This is what PyTorch uses under the hood in conjunction with a GPU to speed up the training process.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Autoencoder (model)&lt;/strong&gt; — This is a combination of an encoder and a decoder. This model is a neural network with a series of layers that progressively ‘compress’ an image (through convolutions) until the point where it is much smaller. At this point the representation is called a ‘latent’. Then (in the full autoencoder) the image is progressively scaled back up into its full version.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;CLIP&lt;/strong&gt; — This is a model that turns text into images, powered by ‘contrastive loss’. It was developed by OpenAI.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Contrastive loss&lt;/strong&gt; — This is a loss function that allows us to compare the similarity of two vectors. We multiply them together and sum up all the values. (This process is also known as the dot product.) If the two vectors are similar, we would expect the number to be large. Contrastive loss is used in CLIP.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Convolutional layer&lt;/strong&gt; — This is a key part of computer vision and it is a way of representing images at different resolutions. Images are either scaled up or down through a convolutional layer (which seems to be some way of averaging the values of an image). Convolutional is the C in CNN.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Decoder&lt;/strong&gt; — This is the part of an autoencoder that takes a latent representation and scales it back up (i.e. ‘decompresses’ it) to its full representation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Differential equations&lt;/strong&gt; — This is a part of mathematics which is really important for Stable Diffusion and whose language forms the context and backdrop for discussions around this technique, but it is a fairly different set of vocabulary from what we use for deep learning.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Diffusion sampler&lt;/strong&gt; — This is the part of the process which relates to adding or subtracting noise from an image.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dot product&lt;/strong&gt; — This is the process by which we multiply two vectors by each other and sum up the values. It is used when we are calculating contrastive loss, but it is a common linear algebra calculation.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Embedding&lt;/strong&gt; — This is a representation of something as a vector, particularly useful for deep learning. In particularly, it’s useful for areas like text where we might, for example have semantic fields that we want to represent as being similar to each other, but we need to do so in such a way as is comprehensible and processable by a machine. Embeddings allow us to do this in vector space.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Encoder&lt;/strong&gt; — This is one part of an autoencoder that takes a full sized image and passes it through a series of convolutions such that at the end we have a significantly reduced tensor that is known as a latent.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Finite differentiation&lt;/strong&gt; — This is one way of calculating the gradients for our image, but it is done pixel by pixel. It is quite slow. (Contrast with analytic derivative.)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Guidance + guidance scale&lt;/strong&gt; — This is the prompt that we pass into our Stable Diffusion model. The guidance scale is what we can pass in to our generation function call to specify how much we want the prompt to be strictly followed.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Latent representation(s)&lt;/strong&gt; — This is the intermediate product in the middle of an autoencoder. It is what is produced by an encoder, and it is what is consumed by a decoder. It is sort of a compressed version of all the important pieces of information relating to a particular image, for example.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Momentum&lt;/strong&gt; — This is a technique used by optimizers in which if we increase the same parameters (or weights) several times in a row, then it seems likely that we’ll do that again so we can increase the learning rate for those parameters so that we don’t have to make so many iterations.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Negative prompts&lt;/strong&gt; — You can pass in a negative prompt along with your prompt and it is a way somehow of ensuring that the resulting image does not correspond to whatever was in the negative prompt. (Think of it as ‘subtracting’ from the main prompt, which seems to be what is going on under the hood, in vector space).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Noise&lt;/strong&gt; — Noise is random data, with no meaning as such. It is important in the world of Stable Diffusion because the work of generating the image is the work of removing noise.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Perceptual loss&lt;/strong&gt; — This is another kind of loss function that may play a role in Stable Diffusion going forward int he course.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Pipeline&lt;/strong&gt; — This is the concept that is used by the &lt;a href=&quot;https://github.com/huggingface/diffusers&quot;&gt;HuggingFace Diffusers library&lt;/a&gt;, out of which our images are generated.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Score function&lt;/strong&gt; — This is another way of stating the gradients for our image. I.e. the representation of what needs adjusting (and by how much) in order to remove noise from our image.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Step&lt;/strong&gt; — This is one iteration of the inference process.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Textual Inversion&lt;/strong&gt; — This is the process of creating a new embedding for a single specific concept or item. (i.e. the Indian watercolour example in the course)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Time step&lt;/strong&gt; — This is a concept from the way the original Stable Diffusion creators thought about things. It represents a way to go from a value to an amount of noise that gets added to an image.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;VAE&lt;/strong&gt; — This is the specific kind of autoencoder used in Stable Diffusion.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;️-new-mathematical-symbols&quot;&gt;✖️➗ New Mathematical Symbols&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;∑ — means to sum up&lt;/li&gt;
  &lt;li&gt;∇ — we use this symbol instead of something else (d something?) in representing the gradient because we are talking about the gradients of many pixel values and not just a single one&lt;/li&gt;
  &lt;li&gt;β — (beta) — used in relation to the time steps to represent the amount of noise or variance. I think this is used instead of the letter σ (sigma), but I might be wrong on that.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;-enduring-questions&quot;&gt;❓ Enduring Questions&lt;/h1&gt;

&lt;p&gt;Some of the questions which I have in my mind following the class include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;These ‘generative’ models intersect with the field of art and creativity, at least nominally, but to what extent can we even say that they are generating something versus simply repeating things that they’ve already seen? (see also, &lt;a href=&quot;https://dl.acm.org/doi/10.1145/3442188.3445922&quot;&gt;the ‘stochastic parrots’ paper&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;SKILL: are there tricks or best practices when deciphering jargon-rich papers down to their core and, in doing so, being able to see the parts of the paper that are new (versus the parts that are just standard practice)?&lt;/li&gt;
  &lt;li&gt;SKILL: what does it mean to be ‘impractical’ i.e. do research in this field? What is involved and how is it generally or most usefully done?&lt;/li&gt;
  &lt;li&gt;What are the useful or fundamental innovations involved in Stable Diffusion?&lt;/li&gt;
  &lt;li&gt;What are the parts of ML/DL and/or Stable Diffusion that we do because of time or hardware or cost limitations as opposed to the things that we do because they are the right way to approach this particular problem? (provoked by the whole detour down into autoencoders that seems mainly to be there to improve iteration speed.)&lt;/li&gt;
  &lt;li&gt;What’s the bigger takeaway for the field as a whole? In other words, what things can we think about doing now with these new techniques?&lt;/li&gt;
  &lt;li&gt;Why does the whole ‘time steps’ conversion stage happen at all? i.e. why can’t we just choose a random number to represent how much or little noise we apply to an image for our training data.&lt;/li&gt;
  &lt;li&gt;The autoencoder / compression step seems like an amazing technique all to its
own. Is it really lossless, or is some information lost along the way?&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="computervision" /><category term="fastai" /><category term="parttwo" /><summary type="html">(This is part of a series of blog posts relating to and responding to the live FastAI course (part 2) being taught October-December 2022. To read others, see the ones listed for the ‘parttwo’ tag.)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/fastai-lesson-9/sd-dutch-nn.png" /><media:content medium="image" url="https://mlops.systems/images/fastai-lesson-9/sd-dutch-nn.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Avoiding BIDMAS, or how J does notation</title><link href="https://mlops.systems/j/mathematics/mu123/q31/notation/2022/10/16/notational-precedence.html" rel="alternate" type="text/html" title="Avoiding BIDMAS, or how J does notation" /><published>2022-10-16T00:00:00-05:00</published><updated>2022-10-16T00:00:00-05:00</updated><id>https://mlops.systems/j/mathematics/mu123/q31/notation/2022/10/16/notational-precedence</id><content type="html" xml:base="https://mlops.systems/j/mathematics/mu123/q31/notation/2022/10/16/notational-precedence.html">&lt;p&gt;One of the topics that comes up early on in Open University’s &lt;a href=&quot;https://www.open.ac.uk/courses/modules/mu123&quot;&gt;MU123 mathematics course&lt;/a&gt; is precedence. Those who grew up in English-speaking countries will probably know this as &lt;a href=&quot;https://www.bbc.co.uk/bitesize/topics/zxqnsk7/articles/znm8cmn&quot;&gt;BODMAS or BIDMAS&lt;/a&gt;. The order of precedence for execution of a mathematical expression gives us an idea for how to resolve expressions that don’t make sense. For example, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;3 + 1 x 4&lt;/code&gt; can  either amount to 7 or 16, depending on how when you do the multiplication step.&lt;/p&gt;

&lt;p&gt;Brackets are one way to make things more precise, and that’s probably why they’re the B in BIDMAS and that they go first. We could write &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;3 + (1 x 4)&lt;/code&gt; to make it really clear that we wanted the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1 x 4&lt;/code&gt; sub-expression to be evaluated first.&lt;/p&gt;

&lt;p&gt;With the rules of precedence, we technically wouldn’t need to add in any brackets because we could (likely) assume that people would follow the standard rules and they would know that we have to evaluate multiplications before we evaluate the additions. So we have a way, but it maybe feels a bit unsatisfactory.&lt;/p&gt;

&lt;p&gt;Some languages or domains, however, have notational rules which don’t rely on a meta-schema of precedence rules like BIDMAS to tell you which expressions should be evaluated first. Instead, the order is determined in other ways, with the option of brackets when needed.&lt;/p&gt;

&lt;p&gt;Several of the languages in the APL family, like J, simply evaluate from right to left in the order that expressions are encountered. See this example in J:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-j&quot;&gt;   3 + 1 * 4
7
   4 * 3 + 1
16
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The order in which the expressions are evaluated determines the answer.&lt;/p&gt;

&lt;p&gt;Thinking and reading a bit about these orders of precedence brought me to learn a bit about other traditions of mathematical notation. The one most used and that you’ll be most familiar with is called &lt;em&gt;infix notation&lt;/em&gt; i.e. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;3 + 4&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Prefix notation (AKA Polish notation) is when we write &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;+ 3 4&lt;/code&gt; (to the same end) and postfix notation (AKA reverse Polish notation) is when we write &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;3 4 +&lt;/code&gt;. (The Polish part relates back to &lt;a href=&quot;https://en.wikipedia.org/wiki/Jan_Łukasiewicz&quot;&gt;Jan Łukasiewicz&lt;/a&gt;, who invented it in 1924.) These kinds of notation are used in Lisp and Clojure, for example.&lt;/p&gt;

&lt;p&gt;Why would you want to use a notation style like this? Some possible reasons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the operands in the expressions can handle arbitrary numbers of arguments, making them more efficient to write&lt;/li&gt;
  &lt;li&gt;they are consistent with the syntax used for functions in computer programming (which can be easier to get your mind round)&lt;/li&gt;
  &lt;li&gt;they’re clearer to read and (mostly) unambiguous, unlike infix notation which (see above) requires a whole order of precedence if you’re not using brackets&lt;/li&gt;
  &lt;li&gt;there’s no confusion or need for precedence rules&lt;/li&gt;
  &lt;li&gt;it’s faster for a machine to evaluate, since the way expressions are formulated is much easier to translate into computer code.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So there you go. I’m unclear whether there are more fundamental benefits to living in the world of post-/prefix notation, and perhaps it’s a little like the people who argue that &lt;a href=&quot;https://gizmodo.com/why-we-should-switch-to-a-base-12-counting-system-5977095&quot;&gt;we’d all be better off&lt;/a&gt; if we lived in a base-12 world instead of base-10, but that’s beside the point for now.&lt;/p&gt;

&lt;p&gt;I’ll try to share some more diversions from my mathematics study along the way, hopefully powered by &lt;a href=&quot;https://www.jsoftware.com&quot;&gt;J&lt;/a&gt; which I’m trying to get back into.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="j" /><category term="mathematics" /><category term="mu123" /><category term="q31" /><category term="notation" /><summary type="html">One of the topics that comes up early on in Open University’s MU123 mathematics course is precedence. Those who grew up in English-speaking countries will probably know this as BODMAS or BIDMAS. The order of precedence for execution of a mathematical expression gives us an idea for how to resolve expressions that don’t make sense. For example, 3 + 1 x 4 can either amount to 7 or 16, depending on how when you do the multiplication step.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/notational-precedence/j-precedence.png" /><media:content medium="image" url="https://mlops.systems/images/notational-precedence/j-precedence.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Storing Bytes: what data serialisation is and why you need it for machine learning</title><link href="https://mlops.systems/redactionmodel/computervision/mlops/python/tools/zenml/2022/09/07/serialisation.html" rel="alternate" type="text/html" title="Storing Bytes: what data serialisation is and why you need it for machine learning" /><published>2022-09-07T00:00:00-05:00</published><updated>2022-09-07T00:00:00-05:00</updated><id>https://mlops.systems/redactionmodel/computervision/mlops/python/tools/zenml/2022/09/07/serialisation</id><content type="html" xml:base="https://mlops.systems/redactionmodel/computervision/mlops/python/tools/zenml/2022/09/07/serialisation.html">&lt;p&gt;&lt;em&gt;(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out &lt;a href=&quot;https://mlops.systems/categories/#redactionmodel&quot;&gt;the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;redactionmodel&lt;/code&gt; taglist&lt;/a&gt;.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Serialisation and deserialisation. I ran headfirst into these two words on my first day in my new job. From the way my colleagues discussed them, it seemed like this was something I should have learned from a computer science degree; foundational concepts with practical applications throughout most places that computers touched.&lt;/p&gt;

&lt;p&gt;A few months in, I’ve come to appreciate a little more about what the underlying concept is about as well as some of the reasons why it remains both relevant and something that pops up regularly. I’ll begin by setting out some of this context before showing an example of where I encountered it recently in my own project. By the end, you’ll understand why this is such an important (and practical) concept and why you’ll encounter it a lot while doing machine learning.&lt;/p&gt;

&lt;h2 id=&quot;-the-basics&quot;&gt;🔢 The Basics&lt;/h2&gt;

&lt;p&gt;In the common definition, serialisation is the process by which you convert something into a sequence of bytes, and deserialisation is when you convert the other way (i.e. &lt;em&gt;from&lt;/em&gt; bytes). In some domains it is also known as marshalling or pickling.&lt;/p&gt;

&lt;p&gt;This commonly is encountered when you need to store some data on disk (i.e. &lt;em&gt;not&lt;/em&gt; or no longer in memory). Perhaps you need some kind of permanent storage of that data, or you need to make the data available to another process. The process through which you transform the data (from something that is comprehensible to whatever environment or language you’re working on) is serialisation.&lt;/p&gt;

&lt;p&gt;To give another example, in a language like Python we often think in and deal through a series of ‘objects’: think dictionaries or even classes in an OOP context. In order to save this to disk, we have to convert it to some other format that firstly is in some format that is stable when saved as a file. We might want to send that data across the network, or have it opened by a different process or a programme running in a different language. Serialisation is the process by which something context and perhaps language-specific gets transformed into this universal substrate (i.e. a sequence of bytes).&lt;/p&gt;

&lt;h2 id=&quot;-common-ways-to-serialise-data-in-python&quot;&gt;🍏 Common ways to serialise data in Python&lt;/h2&gt;

&lt;p&gt;In the past, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pickle&lt;/code&gt; was a commonly-used way of making this conversion. It has a lot of shortcomings, two of which sit at the top of the list:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;there isn’t (as far as I’m aware) much interoperability for objects that are serialised with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pickle&lt;/code&gt;. If you want to load an object that has been ‘pickled’, the entity doing the ‘unpickling’ will have to be running the exact same version of Python as the one that did the pickling. (If I’m not mistaken, there might even be some cross platform interoperability issues as well.)&lt;/li&gt;
  &lt;li&gt;security concerns are serious when it comes to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pickle&lt;/code&gt;: when you &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;load(...)&lt;/code&gt; some pickled object, this will run whatever code is inside with the assumption that it is ‘trusted’. As such, it is unsuitable for use with untrusted data and generally people tend to turn their nose at &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pickle&lt;/code&gt;. (If you &lt;em&gt;do&lt;/em&gt; have to interact with some pickled data, &lt;a href=&quot;https://docs.python.org/3/library/pickletools.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pickletools&lt;/code&gt;&lt;/a&gt; is a handy tool that allows you to inspect and interact with the file &lt;strong&gt;without&lt;/strong&gt; running the arbitrary code packaged inside. While we’re at the library recommendations, it’s also worth checking out &lt;a href=&quot;https://github.com/trailofbits/fickling&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fickling&lt;/code&gt;&lt;/a&gt; which overlaps in functionality somewhat.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;JSON has become a commonly-used format for serialising data (or its cousin &lt;a href=&quot;https://jsonlines.org&quot;&gt;JSONL&lt;/a&gt;, for too-much-to-load-into-memory-at-once data). This is a common format with many uses, but it does come with a serious shortcoming which is that it only supports certain data types. If you’re saving some custom object of your own creation, you’ll first need to convert that into a format that can be transformed into a JSON object/file. If you don’t, then your object will not be able to be rehydrated from the on-disk representation.&lt;/p&gt;

&lt;p&gt;Note that the Python &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pickle&lt;/code&gt; module serialises data into a binary format, whereas the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;json&lt;/code&gt; module converts it into a text format (i.e. readable and comprehensible to someone browsing files or displaying their contents with something like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cat&lt;/code&gt;). Moreover, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pickle&lt;/code&gt; does handle many (most?) objects and types that you can throw at it, though with all the caveats mentioned above.&lt;/p&gt;

&lt;p&gt;I haven’t explored it at all, but while reading a bit about this area I was consistently pointed to Google’s &lt;a href=&quot;https://developers.google.com/protocol-buffers&quot;&gt;Protobuf&lt;/a&gt; format / library which is another way to serialise structured data. I am unable to properly evaluate the extent to which this is an improvement on existing protocols.&lt;/p&gt;

&lt;h2 id=&quot;-serialisation-and-deserialisation-in-machine-learning&quot;&gt;🔐 Serialisation and deserialisation in Machine Learning&lt;/h2&gt;

&lt;p&gt;I mentioned earlier that this concept and operation was something that I confronted more or less on my first day working in my new job. (&lt;em&gt;We build &lt;a href=&quot;https://zenml.io&quot;&gt;an open-source framework&lt;/a&gt; that supports someone working to build and deploy machine learning models.&lt;/em&gt;) In order to understand why this is so important, a small detour showing a basic example of a ZenML pipeline is necessary. What follows is an extremely simple example showcasing how pipelines are composed of steps, and how those are in turn run:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zenml.steps&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zenml.pipelines&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;read_integer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;basic_pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_integer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;read_integer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;basic_pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_integer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_integer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Pipelines are constructed out of a series of steps. The steps are defined with an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@step&lt;/code&gt; decorator, and pipeline definitions are composed in a similar way. Finally, at the end we specify which steps correspond to which parts of the pipeline definition and then call the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run()&lt;/code&gt; method to execute our pipeline.&lt;/p&gt;

&lt;p&gt;You’ll also note the presence of some type annotations as part of how we define our step and pipeline. These are required, and while they may seem simplistic and unnecessary at the moment, later on they will make things much clearer.&lt;/p&gt;

&lt;p&gt;Our pipeline isn’t doing much at the moment, you might think. Behind the scenes, however, ZenML is doing a lot of legwork:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;storing the outputs (and inputs, though there aren’t any in this basic example) of all steps&lt;/li&gt;
  &lt;li&gt;caching those output values or objects, such that if the code doesn’t change then we should just retrieve the cached value.&lt;/li&gt;
  &lt;li&gt;validating and checking the types of values that get returned so that we can be sure our code is returning what we hope / think it should be returning.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Moreover, it does all this in a way that all this intermediary state is stored on disk and versioned. If you update your pipeline steps then rerun it, ZenML will save the new outputs such that you can go back and inspect where data came from and so on.&lt;/p&gt;

&lt;p&gt;In order to save all these objects on disk, however, and to bring this story full-circle, ZenML serialises the data when saving the artifacts from pipeline runs, and deserialises that data when those artifacts are needed (by the cache, for example, or when you want to access a step output once your pipeline has completed its run). We call this part of the process ‘materialisation’. (There’s more &lt;a href=&quot;https://docs.zenml.io/developer-guide/advanced-usage/materializer&quot;&gt;in our docs on materialisation here&lt;/a&gt;, and if you’re searching, be sure to search with a ‘z’ and not an ‘s’, coz America.)&lt;/p&gt;

&lt;h2 id=&quot;-a-basic-custom-materializer&quot;&gt;🛠 A basic custom materializer&lt;/h2&gt;

&lt;p&gt;For most kinds of ‘normal’ Python objects, this is no problem at all. But as we saw above, if we’re going to be able to reconstruct and rehydrate an object from a static sequence of bytes, we’re going to need to do a bit more to make this happen. Within ZenML this means that if you have some special kind of object or type, you’ll need to define a ‘custom materialiser’; this is code that defines how ZenML should serialise and deserialise the objects that you want to be stored as state on disk.&lt;/p&gt;

&lt;p&gt;To give you a sense of what this will look like, here’s our code from above but updated a little to fit this new scenario:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zenml.artifacts&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataArtifact&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zenml.io&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fileio&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zenml.materializers.base_materializer&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BaseMaterializer&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zenml.pipelines&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zenml.steps&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MyCustomObject&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;MyCustomMaterializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BaseMaterializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ASSOCIATED_TYPES&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyCustomObject&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ASSOCIATED_ARTIFACT_TYPES&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataArtifact&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;handle_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyCustomObject&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyCustomObject&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Read from artifact store&quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;handle_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fileio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;artifact&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;data.txt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;r&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyCustomObject&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;handle_return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;my_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyCustomObject&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Write to artifact store&quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;handle_return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fileio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;artifact&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;data.txt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;w&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;read_custom_object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyCustomObject&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyCustomObject&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;aria&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pipeline&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;basic_pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_custom_object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;read_custom_object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;basic_pipeline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;read_custom_object&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_custom_object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;with_return_materializers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;MyCustomMaterializer&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You’ll notice a new piece of code which defines the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MyCustomMaterializer&lt;/code&gt; class. This is subclassed off our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BaseMaterializer&lt;/code&gt; class and we just have to define two methods, one that handles how to serialise or save the data to disk, and the other that handles how to deserialise or rehydrate the objects/data from disk. We add a special &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.with_return_materializers&lt;/code&gt; call when we run the pipeline; this lets ZenML that when we encounter a weird type of object, it can go ahead and use our custom defined materialiser to handle it.&lt;/p&gt;

&lt;p&gt;I hope you’ll agree that this stuff isn’t &lt;em&gt;too&lt;/em&gt; hard to grok, and while the precise steps of how you implement all this might take a bit of getting used to, it’s conceptually not too hard once you understand the foundations of what you’re doing. It took me longer than I’m proud to admit to really understand the elegance of this way of doing things, but all these little pieces add up and you can then go off and use them in your real-life projects.&lt;/p&gt;

&lt;h2 id=&quot;️-materialisation-in-practice-icevision-and-custom-objects&quot;&gt;🕵️ Materialisation in practice: IceVision and Custom Objects&lt;/h2&gt;

&lt;p&gt;Case in point: my object detection pipeline. I took a bit of a break over the summer, but now I’m back and working to get my pipeline production-ready. Defining the basic steps of my pipeline were fairly easy; I’ve already described that in &lt;a href=&quot;https://mlops.systems/tools/redactionmodel/computervision/mlops/2022/05/31/redaction-production-introduction.html&quot;&gt;my last blog post&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The moment I started defining my pipeline in code, I immediately hit a whole array of non-standard objects. My data loading steps returned IceVision-specific parsers custom to COCO BBoxes and my training step returned a collection of various custom objects combining code with the trained model parameters. (&lt;em&gt;Note: for some common use cases like training with raw PyTorch or Tensorflow etc, ZenML has defined many standard materialisers already to get you going quickly.&lt;/em&gt;) I realised that I’d have to define custom materialisers to handle these different inputs and outputs.&lt;/p&gt;

&lt;p&gt;Some of this wasn’t trivial to implement. Sometimes you might get lucky and the library you work with has implemented some handy features to help with serialisation and deserialisation. From what I can tell, this seems to be the case when saving models with PyTorch, for example. But for the rest it’s often less clear what need to happen and why code works in the way it does. To save the IceVision &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RecordCollection&lt;/code&gt; object, for example, I had to jump through some hoops, converting several sub levels of custom objects along the way, to make sure that my objects were serialisable.&lt;/p&gt;

&lt;p&gt;Here’s the custom materialiser code responsible for handling those conversions and serialisation for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RecordCollection&lt;/code&gt;. (Think of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RecordCollection&lt;/code&gt; just as a type of stored data, parsed and ready to use for model training.)&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pathlib&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;icevision.all&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;srsly&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zenml.artifacts&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataArtifact&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zenml.io&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fileio&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;zenml.materializers.base_materializer&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BaseMaterializer&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;COCOMaterializerParser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;template_record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;records&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;template_record&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;template_record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;records&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;records&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_map&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ClassMap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;records&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;common&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;classes&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__iter__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;yield&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;records&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__len__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;records&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;record_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Hashable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;common&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;filepath&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;template_record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BaseRecord&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BaseRecord&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;FilepathRecordComponent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;InstancesLabelsRecordComponent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;AreasRecordComponent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;IsCrowdsRecordComponent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;BBoxesRecordComponent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;filepath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pathlib&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;common&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;filepath&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;img_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ImgSize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ImgSize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;common&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;width&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;height&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;common&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;height&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;labels_ids&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Hashable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;detection&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;label_ids&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;areas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;detection&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;areas&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;iscrowds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;bool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;detection&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;iscrowds&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;bboxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;List&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BBox&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bbox&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;detection&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bboxes&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bbox&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;new_bbox&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BBox&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_xyxy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_bbox&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;boxes&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;parse_fields&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BaseRecord&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is_new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;bool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is_new&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_filepath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filepath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_img_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_class_map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_areas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;areas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_iscrowds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iscrowds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_bboxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bboxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;detection&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;labels&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;detection_record_collection_to_json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcoll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RecordCollection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;indexes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcoll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_records&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;records&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rcoll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_records&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;indexes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;classes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rcoll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;class_map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;dict_records&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;records&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dict_records&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;common&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;filepath&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;common&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;filepath&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bboxes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;detection&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bboxes&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;new_bboxes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bbox&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bboxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bbox&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xyxy&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;new_bbox&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;new_bboxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;new_bbox&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;detection&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bboxes&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new_bboxes&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;common&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;classes&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;classes&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;srsly&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json_dumps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dict_records&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;detection_json_str_to_record_collection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;records&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RecordCollection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;srsly&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json_loads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;records&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;template_record&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ObjectDetectionRecord&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;COCOMaterializerParser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;template_record&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;r&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;parsed_records&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_splitter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SingleSplitSplitter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parsed_records&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;COCOBBoxRecordCollectionMaterializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BaseMaterializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ASSOCIATED_TYPES&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RecordCollection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ASSOCIATED_ARTIFACT_TYPES&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataArtifact&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;handle_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RecordCollection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RecordCollection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Read from artifact store&quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;handle_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fileio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;artifact&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DEFAULT_RECORD_COLLECTION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;r&quot;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;detection_json_str_to_record_collection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;handle_return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;record_collection_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RecordCollection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Write to artifact store&quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;handle_return&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;record_collection_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;json_string&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;detection_record_collection_to_json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;record_collection_obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fileio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;artifact&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uri&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DEFAULT_RECORD_COLLECTION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;w&quot;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json_string&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As you can see, there’s a decent amount going on here. In my custom materialiser, I have a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;detection_record_collection_to_json&lt;/code&gt; method that constructs the JSON representation of my custom &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RecordCollection&lt;/code&gt; object. I use &lt;a href=&quot;https://github.com/explosion/srsly&quot;&gt;Explosion’s handy &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;srsly&lt;/code&gt; package&lt;/a&gt; for their forks + bundling together of various Python serialisation libraries. For the rest, that requires a bit more knowledge of how IceVision handles things like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BBox&lt;/code&gt; objects and COCO Records under the hood, but you can get the idea that it’s not completely trivial.&lt;/p&gt;

&lt;h2 id=&quot;-serialisation-is-for-everyone&quot;&gt;🥳 Serialisation is for Everyone!&lt;/h2&gt;

&lt;p&gt;It’s also not completely impossible to implement either, though, lest you feel like I’m leaving you without hope. My aim with this article was to guide you to the point where you feel you can understand why serialisation is important and to know why you might well encounter it during your data science journey. The moment you need to do something just slightly longer-lasting than an ephemeral training run that is tracked nowhere and just lives in a Colab notebook, that’s when you’ll hit serialisation.&lt;/p&gt;

&lt;p&gt;Moreover, I showed how you can incrementally build up your pipelines with a tool like ZenML to handle lots of parts of the complexity that come with your modelling work.&lt;/p&gt;

&lt;p&gt;[&lt;em&gt;Image credit&lt;/em&gt;: Photo by &lt;a href=&quot;https://unsplash.com/@fabioha?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&quot;&gt;fabio&lt;/a&gt; on &lt;a href=&quot;https://unsplash.com/s/photos/data?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&quot;&gt;Unsplash&lt;/a&gt;]&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="redactionmodel" /><category term="computervision" /><category term="mlops" /><category term="python" /><category term="tools" /><category term="zenml" /><summary type="html">(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/serialisation/fabio-oyXis2kALVg-unsplash.jpg" /><media:content medium="image" url="https://mlops.systems/images/serialisation/fabio-oyXis2kALVg-unsplash.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">It takes a tribe: how I’m thinking about putting my object detection model into production</title><link href="https://mlops.systems/tools/redactionmodel/computervision/mlops/2022/05/31/redaction-production-introduction.html" rel="alternate" type="text/html" title="It takes a tribe: how I’m thinking about putting my object detection model into production" /><published>2022-05-31T00:00:00-05:00</published><updated>2022-05-31T00:00:00-05:00</updated><id>https://mlops.systems/tools/redactionmodel/computervision/mlops/2022/05/31/redaction-production-introduction</id><content type="html" xml:base="https://mlops.systems/tools/redactionmodel/computervision/mlops/2022/05/31/redaction-production-introduction.html">&lt;p&gt;&lt;em&gt;(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out &lt;a href=&quot;https://mlops.systems/categories/#redactionmodel&quot;&gt;the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;redactionmodel&lt;/code&gt; taglist&lt;/a&gt;.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So we’ve trained our model… now what? Are we done?&lt;/p&gt;

&lt;p&gt;If we’re just exercising our curiosity or have a tight focus for our work, then we might well be done with our work. Our responsibility within a larger team might only be for this specific step of training the model, for example. For many cases, however, we’re going to want to do something with our model, perhaps making it available to others via some web interface or an online API.&lt;/p&gt;

&lt;p&gt;The next blog posts in this series will focus on the challenges and practicalities of getting a model ‘in production’. I’ll sidestep the nuances of exactly what we mean by ‘in production’ for the moment, but suffice it to say that the end goal is to have a way to not only make our model available to other consumers but also to deal with re-training and/or re-deploying new models to take the place of older or stale models. There is a whole spectrum of variety in this context that goes by the name “MLOps”. This blog post will try to provide a high level overview of some of the basic elements relevant to getting my redaction model into production.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/production-introduction/blemish-pipelines.png&quot; alt=&quot;&quot; title=&quot;Keep reading to learn more about the different parts of my object detection workflows.&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;-tldr-what-will-you-learn&quot;&gt;🚦 TL;DR: What will you learn?&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;🤖 Deploying a model and automating everything on the way requires a decent number of steps and tools to do it properly.&lt;/li&gt;
  &lt;li&gt;👀 I take you through the core steps you need to think about when working with a continuously deployed object detection model.&lt;/li&gt;
  &lt;li&gt;💪 I end by outlining the specific pieces I will need to build as I get my own model out into the world and ‘in production’.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;-what-is-in-production-for-the-redaction-model&quot;&gt;🚀 What is ‘in production’ for the redaction model?&lt;/h2&gt;

&lt;p&gt;“Production” relates heavily to the use case. An image classification model used across the United States to identify dents on rental cars is going to have a very different profile to a privately hosted language model being used as part of an internal company chatbot interface. In the case of our redaction model, there are two main scenarios that I’m interested in supporting:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;an online API which can power other services building on top of the functionality the model enables, albeit document-by-document. A user could upload a specific document and they’d receive specific predictions and results for just that document.&lt;/li&gt;
  &lt;li&gt;an offline-first model which handles large volumes of input data and that does not require internet connectivity. For example, legal teams trying to get a sense of what kinds of documents are redacted as part of a case (and to what extent) could run an extended inference process over an entire set of documents.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These two scenarios have different requirements. Of the two, the online version is perhaps slightly more complex given the more complex serving needs and potential guarantees around speed of execution and so on. The second use offline / on-prem scenario has its own challenges around making sure that inference is fast enough to be able to work over massive document collections, but I’m not sure I’d consider that an MLOps challenge so I will mostly be focusing on the online deployment in this series.&lt;/p&gt;

&lt;h2 id=&quot;️-handling-failure-and-success&quot;&gt;⚖️ Handling failure and success&lt;/h2&gt;

&lt;p&gt;One way to think about what’s required to get a model in production is to think of the answers to the following two questions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;what could go wrong with my deployed online model?&lt;/li&gt;
  &lt;li&gt;what are the possible consequences if everything went really &lt;em&gt;well&lt;/em&gt;?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In terms of the redaction model, there are lots of potential complications:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;our model could be slow and therefore users wouldn’t want to  hang around for inference to take place&lt;/li&gt;
  &lt;li&gt;the data on which our model was originally trained could start to show its age and so maybe our model wouldn’t perform as well on newer documents being released (perhaps with a different style or method of redaction)&lt;/li&gt;
  &lt;li&gt;some software bug or managed infrastructure outage could bring our hosting down and we’d have to redeploy everything&lt;/li&gt;
  &lt;li&gt;there could be a legal or ethical challenge to our deployed model, and we’d perhaps be required to show the exact process used that resulted in a particular model&lt;/li&gt;
  &lt;li&gt;maybe the way we choose to make our model available is expensive and/or unreliable&lt;/li&gt;
  &lt;li&gt;and so on…&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If things went really well, we have a different set of problems: perhaps a million people would be interested in using the service at the same time. This scalability problem could bring down the model inference service completely. Or perhaps the model would be adopted for use in a legal setting and people would start to trust its predictions blindly without taking into account the fact that its performance was starting to decline the further away we got from when it was originally trained. Maybe some legal institution would start using it to make decisions making assumptions about the accuracy of the model’s predictions, or maybe even the various government departments responsible for &lt;em&gt;creating&lt;/em&gt; and &lt;em&gt;applying&lt;/em&gt; the redactions in the first place would use it as a way of more efficiently or voluminously adding redactions, an unintended consequence that could potentially be harmful.&lt;/p&gt;

&lt;p&gt;All the above scenarios and more are some of the reasons why MLOps exists. We care about having repeatable and robust processes for getting our models out in the world because the use cases are themselves often complex. We also care that our models are actually a net positive when released into the world rather than just some process that happens after model training is completed from which we’re completely disconnected.&lt;/p&gt;

&lt;h2 id=&quot;-basic-mlops-building-blocks&quot;&gt;🧱 Basic MLOps building blocks&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/production-introduction/mlops-landscape.png&quot; alt=&quot;&quot; title=&quot;A somewhat notorious image offering an the overview of some of the MLOps tooling available to users.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These are some of the tools available to those trying to put their models into production. This &lt;a href=&quot;https://landscape.lfai.foundation&quot;&gt;‘landscape’&lt;/a&gt; showcases both the explosion of options for various parts of the full lifecycle as well as the way that this space hasn’t yet settled on a set of best-in-class tools. For my redaction project, there are a few basics that will seek to have in place in order to meet the needs of the use case(s):&lt;/p&gt;

&lt;h3 id=&quot;-code-standardisation&quot;&gt;⎂ Code standardisation&lt;/h3&gt;

&lt;p&gt;Perhaps not even worth mentioning, but having some standards around how the code looks and having processes to enforce this is important. Using &lt;a href=&quot;https://pre-commit.com&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pre-commit&lt;/code&gt;&lt;/a&gt; alongside tools like &lt;a href=&quot;https://black.readthedocs.io&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;black&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;https://pycqa.github.io/isort/&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;isort&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;https://github.com/terrencepreilly/darglint&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;darglint&lt;/code&gt;&lt;/a&gt;, &lt;a href=&quot;http://www.pydocstyle.org&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pydocstyle&lt;/code&gt;&lt;/a&gt; and so on will take you a long way in this direction.&lt;/p&gt;

&lt;p&gt;This would be especially important if I were working as part of a team. These tools would ensure some kinds of standards and baseline uniformity within the codebase. In my case, I’m doing everything on my own so it matters less, but these tools all help me collaborate with my future self, several months from now, perhaps when I need to fix some bug or add a new feature. Having code that reads clearly and cleanly goes a long way to getting started on that work.&lt;/p&gt;

&lt;h3 id=&quot;-code-versioning&quot;&gt;📸 Code Versioning&lt;/h3&gt;

&lt;p&gt;Versioning your code with a tool like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; is also another basic requirement of any process involving software. I’d almost say this barely requires mentioning, but I know that the use of regular atomic git commits is by no means standard practice in the world of data science so it bears restating here.&lt;/p&gt;

&lt;p&gt;We version our code because we want to be able to go back to earlier points in our code’s history. If we wanted to see the difference between the code used to train our model today and the code used last week, we’d require a tool like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; to help us with that. (Code versioning tools also help tremendously when collaborating with a larger team or to an open-source community project.)&lt;/p&gt;

&lt;p&gt;There are various options for what tool to use but the vast majority of people use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; for this as of 2022.&lt;/p&gt;

&lt;h3 id=&quot;-data-model-and-artifact-versioning&quot;&gt;🧳 Data, Model and Artifact Versioning&lt;/h3&gt;

&lt;p&gt;Last week I wrote about &lt;a href=&quot;https://dvc.org/&quot;&gt;DVC&lt;/a&gt; and the ways it can be used as a lightweight way to add data versioning to a project. Since data is as important to a model as the code used to train it, we want ways to step backwards and forwards with our data. This will not only enable us to debug and retrain older models but it will help in general with managing our assets such that we don’t just have a sprawling series of folders with names like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;training_data_FINAL_july_2021&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;validation_data_minus_synthetic_FINAL_FINAL_march_2020&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Not only does this make sense from the perspective of productivity, but for sensitive or popular ML applications there are &lt;a href=&quot;https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai&quot;&gt;increasing legal requirements&lt;/a&gt; around this kind of flexibility to introspect how you trained your models.&lt;/p&gt;

&lt;p&gt;DVC is commonly used for this kind of use case, but there are other alternatives such as &lt;a href=&quot;https://www.pachyderm.com&quot;&gt;Pachyderm&lt;/a&gt; or &lt;a href=&quot;https://lakefs.io&quot;&gt;LakeFS&lt;/a&gt; that might be worth considering if you have larger amounts of data.&lt;/p&gt;

&lt;h3 id=&quot;-testing&quot;&gt;🧪 Testing&lt;/h3&gt;

&lt;p&gt;This testing is primarily around ensuring that the code does what you think it’s doing, but we also care about preventing regressions in one part of the codebase when you change something somewhere else.&lt;/p&gt;

&lt;p&gt;There isn’t much in the way of rocket science to testing, but it is a whole world unto its own. For my project, there is a decent amount of code that handles somewhat complicated conversions between different kinds of image formats, multiple competing ideas for how bounding boxes or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;BBox&lt;/code&gt;es should be handled as data structures and so on. Having a way to be sure that my code is actually doing what I intended is a surefire way to letting me sleep better at night if I intend my model to be used by others.&lt;/p&gt;

&lt;h3 id=&quot;-data-validation&quot;&gt;👁 Data Validation&lt;/h3&gt;

&lt;p&gt;Just like our code needs to have some kinds of checks and balances, so does the lifeblood of our project: our data. I recently finished &lt;a href=&quot;https://mlops.systems/categories/#datavalidation&quot;&gt;a three-part series&lt;/a&gt; on data validation in the context of this project, and both &lt;a href=&quot;https://greatexpectations.io&quot;&gt;Great Expectations&lt;/a&gt; and &lt;a href=&quot;https://evidentlyai.com&quot;&gt;Evidently&lt;/a&gt; are excellent options worth considering, depending on your exact requirements.&lt;/p&gt;

&lt;h3 id=&quot;-experiment-tracking&quot;&gt;📝 Experiment Tracking&lt;/h3&gt;

&lt;div class=&quot;jekyll-twitter-plugin&quot;&gt;&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Something that old boring process literature talk about (Toyota, The Goal etc) that applies to software engineering 10000000% is that minimizing the size of the feedback loop is much more important for productivity than minimizing idleness&lt;/p&gt;&amp;mdash; Erik Bernhardsson (@bernhardsson) &lt;a href=&quot;https://twitter.com/bernhardsson/status/1526635195243409408?ref_src=twsrc%5Etfw&quot;&gt;May 17, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/div&gt;

&lt;p&gt;When starting a project from scratch, you want to iterate quickly, trying out lots of training options or combinations of features and/or data. Not only do you want to be able to replicate the precise combination of data and code used to train a particular model, but you also want to be able to compare these various efforts with one another.&lt;/p&gt;

&lt;p&gt;Experiment trackers like &lt;a href=&quot;https://wandb.ai&quot;&gt;Weights &amp;amp; Biases&lt;/a&gt;, &lt;a href=&quot;https://mlflow.org&quot;&gt;MLflow&lt;/a&gt;, &lt;a href=&quot;https://www.tensorflow.org/tensorboard/&quot;&gt;Tensorboard&lt;/a&gt; and &lt;a href=&quot;https://neptune.ai&quot;&gt;Neptune&lt;/a&gt; enable you to compare the results of your models as well as practically any combination of the hyperparameters used to train them. I’ve &lt;a href=&quot;https://mlops.systems/redactionmodel/computervision/tools/2022/03/03/model-improvements.html&quot;&gt;used&lt;/a&gt; &lt;a href=&quot;https://mlops.systems/tools/redactionmodel/computervision/2022/04/06/synthetic-data-results.html&quot;&gt;charts&lt;/a&gt; from my own (Weights &amp;amp; Biases-powered) experiment tracker to showcase the different results obtained as part of my process. Not only is it useful for outward-facing engagement with users, stakeholders or other third-parties, but it can be useful to step back from your experiments and evaluate where your model is performing well and how you might improve it.&lt;/p&gt;

&lt;h3 id=&quot;-monitoring&quot;&gt;📺 Monitoring&lt;/h3&gt;

&lt;p&gt;There’s a lot of potential complexity packed into the simple term ‘monitoring’. (I’d recommend you check out Shreya Shankar’s &lt;a href=&quot;https://www.shreya-shankar.com/tags/machine-learning/&quot;&gt;four-part series on monitoring&lt;/a&gt; if you’re curious to learn more.) For our purposes, this will mainly involve making sure our model doesn’t drift or become stale. We’ll need to make sure that the data used to periodically (re)train or fine-tune our model is somewhat within the original parameters of the original training data. We’ll also want to be monitoring the kinds of predictions that our deployed model is making to make sure that they’re more or less within the reasonable ranges of values that we’d expect. If we start to regularly diverge from these kinds of boundary values it probably should prompt an examination of what’s going on and why we’re overstepping. This is an essential part of what it means to robustly deploy a model in production; you need to know when things are going wrong.&lt;/p&gt;

&lt;h3 id=&quot;-automated-model-deployment&quot;&gt;🏁 Automated Model Deployment&lt;/h3&gt;

&lt;p&gt;Automation is a big part of what people generally consider to be ‘mature’ MLOps practices. It is useful to have an automated way to take your model from when training is complete to having it deployed and user-facing. Perhaps you know you need to retrain or fine-tune your model once a week because the data context is continually changing. Perhaps (or likely!) you have rigorous monitoring processes and in the event of a failure or series of non-sensical / out-of-bounds predictions you want a way to revert the model used in production to something more stable.&lt;/p&gt;

&lt;p&gt;This is where the process of putting a model in production resembles the processes and values of DevOps most closely. For my redaction model, I’ll want to have a way to handle the two cases mentioned above as a starting point along with more complex versions of those cases. I’ll also want to automate the process of converting my IceVision VFNet model into something that can be used in the offline ‘on-prem’ use case I described at the beginning of this post.&lt;/p&gt;

&lt;h3 id=&quot;-dag-cards-model-cards-data-cards-all-the-cards&quot;&gt;🃏 DAG Cards, Model Cards, Data Cards, All the Cards&lt;/h3&gt;

&lt;p&gt;The basic idea is that you write some notes on the context surrounding your data, or your model or the pipelines you’re using as part of your overall workflow. Your processes and artifacts will likely change with the project, and I know from bitter experience that it’s easy to forget the reasoning behind why you chose to do one thing or another. So you write notes to describe what you were thinking when you created or modified this or that asset. You describe the decisions you made and what tradeoffs and downstream effects this might have. Not only is this a good practice that benefits FutureYou™️ and your project, but anything developed in the open will maybe have users or contributors and they’ll also benefit from these notes.&lt;/p&gt;

&lt;p&gt;This is the only part of my ‘requirements’ that is (at least currently) &lt;a href=&quot;https://towardsdatascience.com/dag-card-is-the-new-model-card-70754847a111&quot;&gt;a bit more of a ‘fad’&lt;/a&gt; and I wouldn’t say was commonly found. Even five years from now, I imagine that we’ll have more sophisticated or standardised ways of achieving what cards bring, but for now they resonate strongly with some processes I used when working as a historian, archivist and researcher in &lt;a href=&quot;https://foreignpolicy.com/2009/10/16/see-you-soon-if-were-still-alive/&quot;&gt;my previous life&lt;/a&gt;. Some tools like the Huggingface Model Hub offer this &lt;a href=&quot;https://huggingface.co/docs/hub/model-repos#what-are-model-cards-and-why-are-they-useful&quot;&gt;as a built-in standard&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;-mlops-maturity-models&quot;&gt;👴 MLOps Maturity Models&lt;/h2&gt;

&lt;p&gt;The pieces I described above relate to my particular use case. Different project will require different levels of automation, or even potentially other additional stages or toolkits. There is a vast spectrum of options and decisions to be made and now is probably a good time to mention that various players have tried to define what it means to do the whole ‘putting a model into production and keeping it healthy’ thing in a good way. These “MLOps Maturity Models” are not completely without value, but remember that what works for Google may not be (or &lt;a href=&quot;https://blog.bradfieldcs.com/you-are-not-google-84912cf44afb&quot;&gt;is probably not&lt;/a&gt;) applicable to you as an individual working on a small side-project. I wrote &lt;a href=&quot;https://blog.zenml.io/mlops-maturity-models/&quot;&gt;an overview of the two most commonly cited maturity models&lt;/a&gt; (from Microsoft and Google) over on the ZenML blog and I encourage you to give that a read if you want to learn more.&lt;/p&gt;

&lt;p&gt;But what does this all mean for my project? What specifically will it all look like and how am I implementing it? I’ll get into some of the details in the coming weeks, but for now let me just outline my two main workflows.&lt;/p&gt;

&lt;h2 id=&quot;️-redaction-project-workflow-1-annotation&quot;&gt;✍️ Redaction Project Workflow #1: Annotation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/production-introduction/annotation-pipeline.png&quot; alt=&quot;&quot; title=&quot;The two parts of my annotation workflow.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are two main pieces to this pipeline that happens before we train our model. My model is still thirsty for annotations and data, so from the very beginning I want to integrate the annotation process in as part of how I set up the workflows. This way, I make it as easy as possible to annotate data and use that data for subsequent training or fine-tuning.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ingestion / Import&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here I will check a series of pre-defined URLs to see if there are any new PDF files available for download. If new files are available (and we’ve confirmed that we haven’t already downloaded them, I can download those files. Those PDFs then get converted into image files and the metadata for each image gets saved centrally so we have that to hand when annotating the files. This point is a good one to save a checkpoint version of our data using DVC.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Annotation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We only want to annotate images that haven’t been annotated, so that check is the first to be performed before spinning up Prodigy to randomly select pages from the PDFs (now in the format of image files) to be annotated. 10% of the images that are annotated get saved in a separate ‘test data’ location. This test data is &lt;em&gt;never&lt;/em&gt; used in training and is simply held out for a more realistic final validation of the project. We version the annotations file whenever we are done annotating for the day.&lt;/p&gt;

&lt;h2 id=&quot;-redaction-project-workflow-2-continuous-training-continuous-deployment&quot;&gt;🐙 Redaction Project Workflow #2: Continuous Training, Continuous Deployment&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/production-introduction/continuous-training-pipeline.png&quot; alt=&quot;&quot; title=&quot;The core parts of the continuous training and deployment pipeline.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This longer pipeline contains the core value and most compute-intensive processes like training. We take the data from the raw state as annotations and go all the way to deployment.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Annotation Checker&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We first check to see if there are any new annotations available since we last ran the pipeline. We will probably need some kind of threshold number of annotations which will make it worth our while to trigger the retraining process.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Data Validation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We’ll use Great Expectations to validate the incoming new annotation data.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Synthetic Data Generation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If / as we hit certain thresholds, we might want to generate more synthetic data and add it to the dataset.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Training&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We train or fine-tune the model for a certain number of epochs. We log our experiment metadata with Weights &amp;amp; Biases.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Deployment Trigger / Decision&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At this point we need to decide whether to deploy the model or not, based on some evaluation criteria. Our decision will determine the path of the rest of the workflow.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Deployment&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We take the trained model and make it available for online inference. We save a version of the model using DVC, and we also package it up for use in on-prem / offline settings.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Inference &amp;amp; Monitoring&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This step is crucial. We monitor the performance of our deployed model along with the predictions it is making. We want to be able to catch any cases where we notice the predictions to start to drift, or be aware of sluggish response times from our server and so on.&lt;/p&gt;

&lt;h2 id=&quot;final-thoughts&quot;&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;Nice work on making it all the way to the end! We took a long tour through the various considerations you need to bear in mind when deploying a model, and finished off with a preview of the kinds of things I’ll be building over the coming weeks to actually put my own object detection model in production.&lt;/p&gt;

&lt;p&gt;If you have parts of this that you’d like me to cover in more detail, or questions based on what you read here today, please leave a comment below!&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="tools" /><category term="redactionmodel" /><category term="computervision" /><category term="mlops" /><summary type="html">(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/production-introduction/blemish-pipelines.png" /><media:content medium="image" url="https://mlops.systems/images/production-introduction/blemish-pipelines.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">More Data, More Problems: Using DVC to handle data versioning for a computer vision problem</title><link href="https://mlops.systems/tools/redactionmodel/computervision/mlops/2022/05/24/data-versioning-dvc.html" rel="alternate" type="text/html" title="More Data, More Problems: Using DVC to handle data versioning for a computer vision problem" /><published>2022-05-24T00:00:00-05:00</published><updated>2022-05-24T00:00:00-05:00</updated><id>https://mlops.systems/tools/redactionmodel/computervision/mlops/2022/05/24/data-versioning-dvc</id><content type="html" xml:base="https://mlops.systems/tools/redactionmodel/computervision/mlops/2022/05/24/data-versioning-dvc.html">&lt;p&gt;&lt;em&gt;(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out &lt;a href=&quot;https://mlops.systems/categories/#redactionmodel&quot;&gt;the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;redactionmodel&lt;/code&gt; taglist&lt;/a&gt;.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If you’ve been following along as I train an object detection model to detect redactions, you’ll know that there have been a few iterations in how I go about doing this. For the most part, though, the dataset has remained relatively static. I downloaded a huge tranche of publicly-released government documents right at the beginning and aside from &lt;a href=&quot;https://mlops.systems/tools/redactionmodel/computervision/2022/04/06/synthetic-data-results.html&quot;&gt;my experiments&lt;/a&gt; in &lt;a href=&quot;https://mlops.systems/redactionmodel/computervision/python/tools/2022/02/10/synthetic-image-data.html&quot;&gt;synthetic data creation&lt;/a&gt; I haven’t really been adding to this data.&lt;/p&gt;

&lt;p&gt;When it comes to turning this model into something that can work &lt;em&gt;in production&lt;/em&gt;, it won’t be enough to have a big bucket of image files that I train on. I’ll need to have a bit more control and fine-grained segmentation of the ways this data is being used. In short, if I want to be able to reproduce my workflows then I need some kind of data versioning.&lt;/p&gt;

&lt;h2 id=&quot;-tldr-data-versioning-for-computer-vision&quot;&gt;🚦 TL;DR: Data Versioning for Computer Vision&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;⛽️ We version our data because it is the fuel for our model development and experimentation process.&lt;/li&gt;
  &lt;li&gt;💻 Data versioning tools like DVC allow you to apply the same mental model you have for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; to your data.&lt;/li&gt;
  &lt;li&gt;⋔ The ability to ‘branch’ off your data gives you the flexibility to experiment just as the same is true for branching off your code to try out some new behaviour.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dvc.org/&quot;&gt;DVC&lt;/a&gt; is probably the leading tool that allows you to version your data and flexibly access all the previous ‘commits’ and checkpoints you make along the way.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;-why-do-we-need-data-versioning-isnt-git-enough&quot;&gt;🤔 Why do we need data versioning? Isn’t &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; enough?&lt;/h2&gt;

&lt;p&gt;If the lifeblood of traditional software engineering is code then the equivalent for machine learning is data. We solve the problem of checkpointing what our code looked like at a particular moment with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; and online hubs like Github. Until recently there weren’t many equivalent options for data. We’re trying to solve the problem that often occurs if you’re asked to reproduce the data that was used to train a particular iteration of a model from some point in the past. Without some kind of data version control this is more or less impossible, particularly if your data is constantly changing.&lt;/p&gt;

&lt;p&gt;Even in my case for this redaction project, I wasn’t ingesting new data all the time but I &lt;em&gt;was&lt;/em&gt; removing bad annotations or updating those annotations as I conducted error analysis or &lt;a href=&quot;https://mlops.systems/redactionmodel/computervision/tools/debugging/jupyter/2022/03/12/fiftyone-computervision.html&quot;&gt;used tools like FiftyOne&lt;/a&gt; to understand why my model wasn’t performing as well as I’d have liked.&lt;/p&gt;

&lt;p&gt;Luckily there’s a pretty great tool in this space that seems to be more or less unchallenged for what it does in the data versioning domain: &lt;a href=&quot;https://dvc.org&quot;&gt;Data Version Control&lt;/a&gt; or &lt;a href=&quot;https://dvc.org&quot;&gt;DVC&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;-dvc-use-cases&quot;&gt;👩‍💻 DVC Use Cases&lt;/h2&gt;

&lt;p&gt;DVC does many things, but for our purposes at this moment its core value is that it helps us version our data. It also handles the case where we have large files or a dataset that changes a lot and where we might end up having problems with storing all the versions of this data.&lt;/p&gt;

&lt;p&gt;The core behaviour we want to use with a data versioning tool is to access our data at one particular moment. Just like you incrementally annotate your code updates using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt;, with sometimes atomic progressions as you do your work, so it is with DVC that you can checkpoint your data as you make changes.&lt;/p&gt;

&lt;p&gt;At the beginning this was a slight mental adjustment for me. When working on a project it is now second nature to regularly make &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; commits along the way, but I wasn’t in the habit of making regular data commits as a second step. In the long-run, this requires a bit of a mental shift but this is exactly what will enable the benefits that using a tool like DVC brings.&lt;/p&gt;

&lt;p&gt;In particular, being able to experiment with data in a way that you can always roll-back from feels pretty liberating once you’ve covered your back with DVC. Just as you can use create &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; branches for your code, so you can create branches for your versioned data. &lt;a href=&quot;https://dvc.org/doc/command-reference/checkout&quot;&gt;Checking out&lt;/a&gt; the precise data used for some zany experiment you did is pretty painless. If you realise that the experiment is a dead-end and it’s not helping you move forward, just rewind and reset your data back to a useable state from before you had that crazy idea to create a million synthetic images :)&lt;/p&gt;

&lt;p&gt;One other thing: DVC is built on top of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt; and it follows many of the mental models you might have about how versioning works. In this way, DVC luckily is smart about how it allows you to make incremental changes to your data. When it calculates the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;diff&lt;/code&gt; of your dataset before and after, it really is able to do some atomic updates and logging of what changed rather than just storing all the files multiple times over. This helps prevent you building up a really huge data cache and it helps the whole process be efficient.&lt;/p&gt;

&lt;div class=&quot;Toast&quot;&gt;
   &lt;span class=&quot;Toast-icon&quot;&gt;&lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/span&gt;
   &lt;span class=&quot;Toast-content&quot;&gt;I've mentioned this for other tools like Evidently before so I should also note that the DVC online community (https://dvc.org/community) is a pretty friendly and helpful place to hang out and learn about data versioning or to troubleshoot your problems. Nobody will tell you to RTFM here and their community events are generally beginner-friendly in my experience. This makes a big difference so they should be commended for the efforts they take to foster this kind community atmosphere. ❤️&lt;/span&gt;
&lt;/div&gt;

&lt;h2 id=&quot;-how-to-get-started-with-dvc&quot;&gt;🚀 How to get started with DVC&lt;/h2&gt;

&lt;p&gt;The basics are mostly similar to how you’d use a tool like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You &lt;a href=&quot;https://dvc.org/doc/command-reference/init&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;init&lt;/code&gt;&lt;/a&gt; your repository. This add some DVC superpowers on top of what you already have with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;You &lt;a href=&quot;https://dvc.org/doc/command-reference/add&quot;&gt;specify which files&lt;/a&gt; you want to have DVC manage and track. It would make a lot of sense, for example, to have DVC handle tracking your models, your image files and your data annotations (if those exist as separate files).&lt;/li&gt;
  &lt;li&gt;You can optionally also &lt;a href=&quot;https://dvc.org/doc/command-reference/remote&quot;&gt;specify a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;remote&lt;/code&gt; location&lt;/a&gt; where you want these files to be stored. (DVC supports several types of remote storage: local file system, SSH, Amazon S3, Google Cloud Storage, HTTP, HDFS, among others.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(To get a taste of the full workflow when using DVC for data tracking I’d recommend something like the basic tutorial they have &lt;a href=&quot;https://dvc.org/doc/use-cases/versioning-data-and-model-files/tutorial&quot;&gt;here&lt;/a&gt;. They also &lt;a href=&quot;https://dvc.org/blog/end-to-end-computer-vision-api-part-1-data-versioning-and-ml-pipelines&quot;&gt;recently added a three-part tutorial&lt;/a&gt; specific to computer vision that you might want to check out.)&lt;/p&gt;

&lt;p&gt;If you want to use DVC programmatically using their Python API, you can get some information on this &lt;a href=&quot;https://dvc.org/doc/api-reference&quot;&gt;in their docs here&lt;/a&gt;. Unfortunately, these docs are incomplete and you’ll have to experiment a bit if you want to do anything beyond the simple functionality they themselves list. I’m told it behaves very similarly to how &lt;a href=&quot;https://gitpython.readthedocs.io/en/stable/intro.html&quot;&gt;a tool like GitPython&lt;/a&gt; works, where you can just use the equivalent &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add()&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;checkout()&lt;/code&gt; function call that corresponds to a DVC CLI command, but given the lack of documentation it’s a bit harder to get a full sense of what is possible.&lt;/p&gt;

&lt;div class=&quot;Toast Toast--warning googoo&quot;&gt;
   &lt;span class=&quot;Toast-icon&quot;&gt;&lt;svg class=&quot;octicon octicon-alert&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8.22 1.754a.25.25 0 00-.44 0L1.698 13.132a.25.25 0 00.22.368h12.164a.25.25 0 00.22-.368L8.22 1.754zm-1.763-.707c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0114.082 15H1.918a1.75 1.75 0 01-1.543-2.575L6.457 1.047zM9 11a1 1 0 11-2 0 1 1 0 012 0zm-.25-5.25a.75.75 0 00-1.5 0v2.5a.75.75 0 001.5 0v-2.5z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/span&gt;
   &lt;span class=&quot;Toast-content&quot;&gt;DVC includes a lot of extra functionality around experiment tracking and pipelining of your code. You can safely ignore all that and just use DVC for data versioning. No shame in that :)&lt;/span&gt;
&lt;/div&gt;

&lt;h2 id=&quot;-when-to-use-dvc&quot;&gt;🛠 When to use DVC&lt;/h2&gt;

&lt;p&gt;It probably is a good practice to use something like DVC from the start of most projects. If you know you’re never going to need to update the data you use, or if you will only ever generate one model, then maybe you have no need for data versioning. But realistically, when are you going to do that? Generally speaking you’ll be iterating a lot and you’ll be trying things out, so perhaps just start using DVC at the start of any new project: a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git init&lt;/code&gt; can just as easily be followed by a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dvc init&lt;/code&gt;…&lt;/p&gt;

&lt;p&gt;DVC will thrive in long-lived projects where you go down certain rabbit-holes, trying out different approaches and techniques. If you have a decent amount of data — and you probably do if you’re bringing deep learning to the table — then you can leverage how DVC makes it easy to store your data in the remote infrastructure of your choice with &lt;a href=&quot;https://dvc.org/doc/command-reference/remote&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dvc remote&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;-how-im-using-dvc-in-my-redaction-project&quot;&gt;📄 How I’m using DVC in my redaction project&lt;/h2&gt;

&lt;p&gt;For my purposes, the things I’m tracking with DVC include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the models I train&lt;/li&gt;
  &lt;li&gt;the PDF documents I downloaded from public sources that form the basis of the data in this project&lt;/li&gt;
  &lt;li&gt;the images that I extracted from the PDF documents&lt;/li&gt;
  &lt;li&gt;the annotations I make on the images using &lt;a href=&quot;https://cocodataset.org/&quot;&gt;the standard COCO Dataset format&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This covers the core data that I expect to be working with for this project. I keep all this data synced to a remote Amazon S3 bucket which allows me to easily get set up on a new remote machine if needed.&lt;/p&gt;

&lt;p&gt;I’ll next be writing about how to move towards a ‘production-ready’ system in the coming weeks, but one thing I’ll hope to be adding to the current way I do things is to add some kind of &lt;a href=&quot;https://github.com/PAIR-code/datacardsplaybook&quot;&gt;‘data cards’&lt;/a&gt;. I think a combination of manual comments and annotations alongside some auto-generated data profiles would be a useful thing to get a sense of for every checkpoint we make, particularly as the data grows and is augmented.&lt;/p&gt;

&lt;p&gt;Let me know if you’re using DVC to version your data for computer vision projects! I’m curious if there are any tricks I’m missing out…&lt;/p&gt;

&lt;h2 id=&quot;-appendix-how-to-switch-from-git-lfs-to-dvc&quot;&gt;🏃 Appendix: How to switch from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git-lfs&lt;/code&gt; to DVC&lt;/h2&gt;

&lt;p&gt;When I first started this project, &lt;a href=&quot;https://git-lfs.github.com&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git-lfs&lt;/code&gt;&lt;/a&gt; or Git Large File Storage seemed the best option that didn’t constrain my choices. It allowed me to store any large files I had inside my repository and allowed for some sort of versioning. Over time this ended up being less robust, especially in the context of an ML workflow, so I decided to switch to using DVC backed by an Amazon S3 bucket.&lt;/p&gt;

&lt;p&gt;I didn’t find any useful information on the DVC website or forums on how to make this switch so I’m including my notes on how I switched over myself.&lt;/p&gt;

&lt;div class=&quot;Toast Toast--warning googoo&quot;&gt;
   &lt;span class=&quot;Toast-icon&quot;&gt;&lt;svg class=&quot;octicon octicon-alert octicon octicon-alert&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8.22 1.754a.25.25 0 00-.44 0L1.698 13.132a.25.25 0 00.22.368h12.164a.25.25 0 00.22-.368L8.22 1.754zm-1.763-.707c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0114.082 15H1.918a1.75 1.75 0 01-1.543-2.575L6.457 1.047zM9 11a1 1 0 11-2 0 1 1 0 012 0zm-.25-5.25a.75.75 0 00-1.5 0v2.5a.75.75 0 001.5 0v-2.5z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/span&gt;
   &lt;span class=&quot;Toast-content&quot;&gt;Lots of caution is advised when doing this for your own project or work. I hit some major roadblocks along the way while doing this owing to some quirks of how I'd set 'git-lfs' up in the beginning. Please take all necessary backups and snapshots of your data in case something goes wrong along the way!&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;Some resources I consulted to understand how to do this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/35011366/move-git-lfs-tracked-files-under-regular-git/54119191#54119191&quot;&gt;This Stackoverflow thread&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/git-lfs/git-lfs/issues/3026&quot;&gt;A Github issue&lt;/a&gt; on the same topic&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://gist.github.com/everttrollip/198ed9a09bba45d2663ccac99e662201&quot;&gt;A super useful Gist&lt;/a&gt; I reached via the previous Github issue that ended up guiding me most of the way&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is what I did, step by step:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Commit and push everything to Git / Github / &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git-lfs&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Create a branch, something like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fix/remove-lfs&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Remove the hooks using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git las uninstall&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Go into the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.gitattributes&lt;/code&gt; file and delete whatever tracking you don’t want &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git-lfs&lt;/code&gt; to handle from now on. For me, this involved removing lines referring to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.pth&lt;/code&gt; (model) files and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.jpg&lt;/code&gt; (image) files.&lt;/li&gt;
  &lt;li&gt;Get a list of all the files that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git-lfs&lt;/code&gt; currently is storing using the following command: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git lfs ls-files &amp;gt; files.txt&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Modify the file to remove the beginnings of each line that we don’t need. At the end we want our &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;files.txt&lt;/code&gt; to contain just a series of paths to our files. I did it with a simple Python script:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;filenames.txt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;w&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;files.txt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;r&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;filenames.txt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run git rm --cached&lt;/code&gt; for each file that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git-lfs&lt;/code&gt; is storing. I did this with a simple bash command that uses the file I’d created in the previous step:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;while &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;read &lt;/span&gt;line&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;do &lt;/span&gt;git &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--cached&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$line&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;done&lt;/span&gt; &amp;lt; files.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Initialise the DVC repository with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dvc init&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Add whatever data sources you want tracked (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dvc add FOLDERNAME&lt;/code&gt;)&lt;/li&gt;
  &lt;li&gt;Allow for autostaging with DVC with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dvc config core.autostage true&lt;/code&gt; command&lt;/li&gt;
  &lt;li&gt;Commit everything&lt;/li&gt;
  &lt;li&gt;Check that no &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git-lfs&lt;/code&gt; files are left with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git lfs ls-files&lt;/code&gt; command. Whatever you uncached in previous steps should not show up any more.&lt;/li&gt;
  &lt;li&gt;Remove any lfs with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rm -rf .git/lfs&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Merge your branch into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;(if you’re using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git-lfs&lt;/code&gt; as a team, now is probably the time when other collaborators can uninstall git-lfs as specified above)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;If needed, add your DVC remote storage with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dvc remote add …&lt;/code&gt; (&lt;a href=&quot;https://dvc.org/doc/command-reference/remote&quot;&gt;consult the docs&lt;/a&gt; for exactly how to set this up for your specific needs)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dvc push&lt;/code&gt; to get your files synced with your remote storage&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At this point you should be fully transitioned over. As I mentioned above, there are a ton of weird edge cases and quirks to this process and you probably shouldn’t follow this list blindly. I’m mainly writing this up for my own records as much as anything else, so perhaps it’s helpful for someone else seeking to transition but maybe it should be taken less as a direct list of instructions than an inspiration or general template. (I wish DVC would provide some official-ish guidance on this process through their documentation. I imagine that it’s a fairly common path for someone to outgrow &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;git-lfs&lt;/code&gt; and want to get going with DVC but currently there are no instructions for how to think this through.)&lt;/p&gt;

&lt;p&gt;UPDATE: &lt;em&gt;I originally made reference to ‘continuous training’ in the title of this blogpost but I didn’t actually get into this specific use case in what I covered, so I took that out of the title and we’ll save the specifics for a subsequent post!&lt;/em&gt;&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="tools" /><category term="redactionmodel" /><category term="computervision" /><category term="mlops" /><summary type="html">(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/dvc/dvclogo.png" /><media:content medium="image" url="https://mlops.systems/images/dvc/dvclogo.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Redaction Image Classifier: NLP Edition</title><link href="https://mlops.systems/fastai/nlp/redactionmodel/computervision/huggingface/2022/05/21/nlp-redaction-classifier.html" rel="alternate" type="text/html" title="Redaction Image Classifier: NLP Edition" /><published>2022-05-21T00:00:00-05:00</published><updated>2022-05-21T00:00:00-05:00</updated><id>https://mlops.systems/fastai/nlp/redactionmodel/computervision/huggingface/2022/05/21/nlp-redaction-classifier</id><content type="html" xml:base="https://mlops.systems/fastai/nlp/redactionmodel/computervision/huggingface/2022/05/21/nlp-redaction-classifier.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-05-21-nlp-redaction-classifier.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I've &lt;a href=&quot;https://mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html&quot;&gt;previously
written&lt;/a&gt;
about my use of fastai's &lt;code&gt;vision_learner&lt;/code&gt; to create a classification model that
was pretty good (&amp;gt; 95% accuracy) at detecting whether an image contained
redactions or not.&lt;/p&gt;
&lt;p&gt;This week in the course we switched domains and got to know HuggingFace's
&lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;&lt;code&gt;transformers&lt;/code&gt; library&lt;/a&gt; as a
pathway into NLP (natural language processing) which is all about text inputs. I
struggled quite a bit trying to think of interesting yet self-contained / small
uses of NLP that I could try out this week. A lot of the common uses for simple
NLP modelling seem to be in the area of things like 'sentiment analysis' where I
couldn't really see something I could build. Also there are a lot of NLP uses
cases which feel unethical or creepy (perhaps more so than in the computer
vision, it felt to me).&lt;/p&gt;
&lt;p&gt;I emerged at the end of this thought process with the idea to try to pit image
classification and text classification against one another: could I train an NLP
model that would outperform my image classifier in detecting whether a specific
document or page contains a redaction or not?&lt;/p&gt;
&lt;p&gt;Of course, the first thing I had to do was to OCR all the pages in my image
dataset and convert this all into a text dataset. When it comes to OCR tools,
there are a number of different options available and I'd luckily experimented
around with them. (A pretty useful overview of three leading options can be
found in &lt;a href=&quot;https://francescopochetti.com/easyocr-vs-tesseract-vs-amazon-textract-an-ocr-engine-comparison/&quot;&gt;this
blogpost&lt;/a&gt;
by Francesco Pochetti.) I went with Tesseract as I knew had pretty good
performance and accuracy for English-language documents.&lt;/p&gt;
&lt;p&gt;My process for converting the documents wasn't particularly inspired.
Essentially I just loop over the image files one by one, run the OCR engine over
them to extract the text and then create a new &lt;code&gt;.txt&lt;/code&gt; file with the extracted
text. At the end, I had two folders with my data, one containing texts whose
corresponding images I knew had contained redactions, and one where there were
no redactions.&lt;/p&gt;
&lt;p&gt;I had two hunches that I hoped would help my NLP model.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I hoped that the redactions would maybe create some kind of noise in the
extracted text that the training process could leverage to learn to
distinguish redacted from unredacted.&lt;/li&gt;
&lt;li&gt;I knew that certain kinds of subjects were more likely to warrant redaction
than others, so perhaps even the noise of the OCR trying to deal with a
missing chunk of the image wouldn't be as important as just grasping the
contents of the document.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What follows is my attempt to follow steps initially outlined in Jeremy Howard's
&lt;a href=&quot;https://www.kaggle.com/code/jhoward/getting-started-with-nlp-for-absolute-beginners&quot;&gt;Kaggle
notebook&lt;/a&gt;
that the course reviewed this week in the live lesson. My code doesn't depart
from the original notebook much.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;pip install datasets transformers tokenizers -Uqq

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pathlib&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Path&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I save my &lt;code&gt;.txt&lt;/code&gt; files on the machine and I get a list of all the paths of those files.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;redaction_texts&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;**/*.txt&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;files&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I iterate through all the paths, making of list of all the redacted texts as strings.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;texts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;texts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;ls &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;path&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;ansi-cyan-intense-fg ansi-bold&quot;&gt;redacted&lt;/span&gt;   &lt;span class=&quot;ansi-cyan-intense-fg ansi-bold&quot;&gt;unredacted&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Converting-text-files-into-a-Pandas-DataFrame&quot;&gt;Converting text files into a Pandas DataFrame&lt;a class=&quot;anchor-link&quot; href=&quot;#Converting-text-files-into-a-Pandas-DataFrame&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;I needed a way of obtaining the labels for my dataset. These labels were the
parent label for each path name. The training process below needed the labels to
be floats.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;is_redacted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;Extracts the label for a specific filepath&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;/&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;redacted&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;is_redacted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;0.0&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Converting a Python &lt;code&gt;dict&lt;/code&gt; into a Pandas DataFrame is pretty simple as long as
you provide the data in the right formats. I had to play around with this a
little when I was getting this to work.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;input&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;texts&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;labels&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_redacted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;input&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;labels&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;describe&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;include&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;object&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;
&lt;div&gt;
&lt;style scoped=&quot;&quot;&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: right;&quot;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;input&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;count&lt;/th&gt;
      &lt;td&gt;3886&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;unique&lt;/th&gt;
      &lt;td&gt;3830&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;top&lt;/th&gt;
      &lt;td&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;freq&lt;/th&gt;
      &lt;td&gt;35&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We now have a DataFrame containing 3886 rows of data. You can see here that 35
rows have no visible text. Potentially something went wrong with the OCR
extraction, or the redaction covered the entire image. I didn't really know or
want to fiddle around with that too much, so I left those rows in.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Moving-into-HF-Transformers-Land&quot;&gt;Moving into HF Transformers Land&lt;a class=&quot;anchor-link&quot; href=&quot;#Moving-into-HF-Transformers-Land&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We create a &lt;code&gt;Dataset&lt;/code&gt; object from our DataFrame. It requires that our targets
have the column name &lt;code&gt;labels&lt;/code&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DatasetDict&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ds&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;Dataset({
    features: [&amp;#39;input&amp;#39;, &amp;#39;labels&amp;#39;],
    num_rows: 3886
})&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We're finetuning a pre-trained model here, so I start with the small version of
Deberta which will allow me (I hope!) to iterate quickly and come up with an
initial baseline and sense of whether this is even a viable approach to solving
the problem.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_nm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;microsoft/deberta-v3-small&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Before we finetune our model, we have to do two things to our text data in order
that it works within our gradient descent powered training process:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we have to tokenise our text data&lt;/li&gt;
&lt;li&gt;we have to turn those tokens into numbers so they can be crunched within our
GPU as numbers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tokenisation is the process of splitting our words into shorter stubs of text --
there are varying schools of thought and use cases on the extent to which you
break the words down. We have to use the same tokenisation process that was used
by our pretrained model, so we let &lt;code&gt;transformers&lt;/code&gt; grab the original tokenisers
that was used with &lt;code&gt;deberta-v3-small&lt;/code&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoModelForSequenceClassification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoTokenizer&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tokz&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoTokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_nm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
/opt/conda/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;tok_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;input&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tok_ds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tok_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batched&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We split our data into training and validation subsets as per usual so that we
know how our model is doing while training.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tok_ds&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dds&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;DatasetDict({
    train: Dataset({
        features: [&amp;#39;input&amp;#39;, &amp;#39;labels&amp;#39;, &amp;#39;input_ids&amp;#39;, &amp;#39;token_type_ids&amp;#39;, &amp;#39;attention_mask&amp;#39;],
        num_rows: 2914
    })
    test: Dataset({
        features: [&amp;#39;input&amp;#39;, &amp;#39;labels&amp;#39;, &amp;#39;input_ids&amp;#39;, &amp;#39;token_type_ids&amp;#39;, &amp;#39;attention_mask&amp;#39;],
        num_rows: 972
    })
})&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We define our metric as Pearson's &lt;code&gt;r&lt;/code&gt; AKA &lt;a href=&quot;https://en.wikipedia.org/wiki/Pearson_correlation_coefficient&quot;&gt;the Pearson correlation
coefficient&lt;/a&gt;, a
metric I don't feel an immense instinctual understanding for, but suffice it for
this blogpost to know that a higher value (up to a maximum of 1) is better.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;corr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corrcoef&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;corr_d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eval_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;pearson&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;corr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eval_pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TrainingArguments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Trainer&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here we define our batch size, the number of epochs we want to train for as well
as the learning rate. The defaults in Jeremy's NLP notebook were far higher than
what you see here. His batch size was 128. When I ran the cells that follow, I
hit the infamous &quot;CUDA out of memory&quot; error more or less immediately. I was
running on a machine with a 16GB RAM GPU, but this apparently wasn't enough and
the batch size was &lt;strong&gt;far&lt;/strong&gt; too large. I had to reduce it down to 4, as you can
see, in order to even be able to train the model. There are tradeoffs to this in
terms of how well the model learns, but without spending lots of money on fancy
machines this was the compromise I had to make.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TrainingArguments&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;outputs&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;warmup_ratio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lr_scheduler_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;cosine&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fp16&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;evaluation_strategy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;epoch&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;per_device_train_batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;per_device_eval_batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_train_epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;weight_decay&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;report_to&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;none&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutoModelForSequenceClassification&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model_nm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Trainer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;train&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;eval_dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;test&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tokz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;compute_metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corr_d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: [&amp;#39;lm_predictions.lm_head.dense.weight&amp;#39;, &amp;#39;lm_predictions.lm_head.LayerNorm.weight&amp;#39;, &amp;#39;mask_predictions.dense.bias&amp;#39;, &amp;#39;lm_predictions.lm_head.LayerNorm.bias&amp;#39;, &amp;#39;mask_predictions.classifier.weight&amp;#39;, &amp;#39;lm_predictions.lm_head.bias&amp;#39;, &amp;#39;mask_predictions.LayerNorm.bias&amp;#39;, &amp;#39;lm_predictions.lm_head.dense.bias&amp;#39;, &amp;#39;mask_predictions.dense.weight&amp;#39;, &amp;#39;mask_predictions.classifier.bias&amp;#39;, &amp;#39;mask_predictions.LayerNorm.weight&amp;#39;]
- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: [&amp;#39;classifier.weight&amp;#39;, &amp;#39;classifier.bias&amp;#39;, &amp;#39;pooler.dense.bias&amp;#39;, &amp;#39;pooler.dense.weight&amp;#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Using amp half precision backend
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;The following columns in the training set don&amp;#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.
/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 2914
  Num Epochs = 5
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed &amp;amp; accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 3645
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea &quot;&gt;

    &lt;div&gt;
      
      &lt;progress value=&quot;3645&quot; max=&quot;3645&quot; style=&quot;width:300px; height:20px; vertical-align: middle;&quot;&gt;&lt;/progress&gt;
      [3645/3645 09:15, Epoch 5/5]
    &lt;/div&gt;
    &lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
 &lt;tr style=&quot;text-align: left;&quot;&gt;
      &lt;th&gt;Epoch&lt;/th&gt;
      &lt;th&gt;Training Loss&lt;/th&gt;
      &lt;th&gt;Validation Loss&lt;/th&gt;
      &lt;th&gt;Pearson&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.250100&lt;/td&gt;
      &lt;td&gt;0.168366&lt;/td&gt;
      &lt;td&gt;0.705429&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.171600&lt;/td&gt;
      &lt;td&gt;0.134761&lt;/td&gt;
      &lt;td&gt;0.748499&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.118200&lt;/td&gt;
      &lt;td&gt;0.114869&lt;/td&gt;
      &lt;td&gt;0.784274&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0.089600&lt;/td&gt;
      &lt;td&gt;0.093946&lt;/td&gt;
      &lt;td&gt;0.818484&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.063100&lt;/td&gt;
      &lt;td&gt;0.091717&lt;/td&gt;
      &lt;td&gt;0.822977&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;&lt;p&gt;
&amp;lt;/div&amp;gt;

&amp;lt;/div&amp;gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;Saving model checkpoint to outputs/checkpoint-500
Configuration saved in outputs/checkpoint-500/config.json
Model weights saved in outputs/checkpoint-500/pytorch_model.bin
tokenizer config file saved in outputs/checkpoint-500/tokenizer_config.json
Special tokens file saved in outputs/checkpoint-500/special_tokens_map.json
The following columns in the evaluation set don&amp;#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 972
  Batch size = 8
Saving model checkpoint to outputs/checkpoint-1000
Configuration saved in outputs/checkpoint-1000/config.json
Model weights saved in outputs/checkpoint-1000/pytorch_model.bin
tokenizer config file saved in outputs/checkpoint-1000/tokenizer_config.json
Special tokens file saved in outputs/checkpoint-1000/special_tokens_map.json
The following columns in the evaluation set don&amp;#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 972
  Batch size = 8
Saving model checkpoint to outputs/checkpoint-1500
Configuration saved in outputs/checkpoint-1500/config.json
Model weights saved in outputs/checkpoint-1500/pytorch_model.bin
tokenizer config file saved in outputs/checkpoint-1500/tokenizer_config.json
Special tokens file saved in outputs/checkpoint-1500/special_tokens_map.json
Saving model checkpoint to outputs/checkpoint-2000
Configuration saved in outputs/checkpoint-2000/config.json
Model weights saved in outputs/checkpoint-2000/pytorch_model.bin
tokenizer config file saved in outputs/checkpoint-2000/tokenizer_config.json
Special tokens file saved in outputs/checkpoint-2000/special_tokens_map.json
The following columns in the evaluation set don&amp;#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 972
  Batch size = 8
Saving model checkpoint to outputs/checkpoint-2500
Configuration saved in outputs/checkpoint-2500/config.json
Model weights saved in outputs/checkpoint-2500/pytorch_model.bin
tokenizer config file saved in outputs/checkpoint-2500/tokenizer_config.json
Special tokens file saved in outputs/checkpoint-2500/special_tokens_map.json
The following columns in the evaluation set don&amp;#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 972
  Batch size = 8
Saving model checkpoint to outputs/checkpoint-3000
Configuration saved in outputs/checkpoint-3000/config.json
Model weights saved in outputs/checkpoint-3000/pytorch_model.bin
tokenizer config file saved in outputs/checkpoint-3000/tokenizer_config.json
Special tokens file saved in outputs/checkpoint-3000/special_tokens_map.json
Saving model checkpoint to outputs/checkpoint-3500
Configuration saved in outputs/checkpoint-3500/config.json
Model weights saved in outputs/checkpoint-3500/pytorch_model.bin
tokenizer config file saved in outputs/checkpoint-3500/tokenizer_config.json
Special tokens file saved in outputs/checkpoint-3500/special_tokens_map.json
The following columns in the evaluation set don&amp;#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: input. If input are not expected by `DebertaV2ForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 972
  Batch size = 8


Training completed. Do not forget to share your model on huggingface.co/models =)


&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;

&amp;lt;/div&amp;gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;At the end of all this, we have a Pearson's score of 0.82 on our validation set
which doesn't seem to be as good as our image classifier. I'm not sure how I
would go about comparing these two different metrics. I imagine I'd want to
ensure that both my metrics were identical to make a like-for-like comparison.&lt;/p&gt;
&lt;p&gt;My model is available on the Huggingface Hub
&lt;a href=&quot;https://huggingface.co/strickvl/nlp-redaction-classifier&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;What-did-I-learn?&quot;&gt;What did I learn?&lt;a class=&quot;anchor-link&quot; href=&quot;#What-did-I-learn?&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Training NLP models feels like a bit of a different world from that of
computer vision. There are different constraints in the process that I wasn't
previously aware of and working with the &lt;code&gt;transformers&lt;/code&gt; library exposed me to
a bunch of new errors and hoops I had to jump through.&lt;/li&gt;
&lt;li&gt;It seems that the RAM needed on the GPU is directly correlated with the length
of the text documents. Mine were on the long-ish end of the scale
(particularly when compared with tweets which was what Jeremy was training on
in his notebook). I wonder how people solve this problem, since mine by were
by no means incredibly long.&lt;/li&gt;
&lt;li&gt;NLP models take longer to train than computer vision models; at least, the
transformer-based models that I was working with.&lt;/li&gt;
&lt;li&gt;It's hard to compare two models together that don't share the same metric or
loss function.&lt;/li&gt;
&lt;li&gt;There are MANY fiddly knobs to twist with NLP, particularly around the
pre-processing of text samples, tokenisation strategies and so on. I wonder
how much of those will be abstracted away from the high-level fastai
abstraction when the library integrates with &lt;code&gt;transformers&lt;/code&gt; in the coming
months.&lt;/li&gt;
&lt;li&gt;The end-to-end process is &lt;em&gt;broadly&lt;/em&gt; the same, however, and it was good to have
the foundation that we've been building up over the previous weeks in the
course.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The next model I train hopefully will not be relating to redactions, I promise!&lt;/p&gt;
&lt;p&gt;UPDATE: I read a bit in the new O'Reilly book by the &lt;code&gt;transformers&lt;/code&gt; team,
&lt;a href=&quot;https://transformersbook.com&quot;&gt;&lt;em&gt;Natural Language Processing with Transformers&lt;/em&gt;&lt;/a&gt;,
which seems to address the issue of the same text size:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&quot;Transformer models have a maximum input sequence length that is referred to
as the &lt;em&gt;maximum context size&lt;/em&gt;. For applications using DistilBERT, the maximum
context size is 512 tokens, which amounts to a few paragraphs of text. [...]
Texts that are longer than a model's context size need to be truncated, which
can lead to a loss in performance if the truncated text contains crucial
information.&quot; (pages 28-29 of the paperback edition)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The book suggests plotting out the number of tokens to get a sense of the
distribution of the data by size:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Tokens per document&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;input&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;boxplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;Tokens per document&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;by&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;labels&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;showfliers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;suptitle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAYEAAAEHCAYAAABIsPrhAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAWY0lEQVR4nO3df7DddX3n8ecLAkj9kYBcU0yiYTW10nZAepfFtd1SY5UfrWFm0cVxJLJx4s5ibXfa0bh1inbsDO60oowu3ayxBuqClNUlFaZK+TFddwr1IooKul6RmMRArgjxB1ihvveP88l6uCbcc+89J/cm3+dj5sz5fj+fz/d8P99wOK/7/Xx/paqQJHXTEQvdAUnSwjEEJKnDDAFJ6jBDQJI6zBCQpA4zBCSpwwwBHRRJzkyyc6H7cbB0bXt16DIENGtJftD3+kmSx/rmX7/Q/dPwJbktyZsWuh8aviUL3QEdeqrqGfumk9wPvKmq/m7henRwJVlSVU8sdD+kYXBPQEOT5Jgk70/y7fZ6f5JjDtD2rUnuSbKyLfdnSb6V5MEkf5Hk2NbuzCQ7k/xBkj1Jdie5qO9zzmmf8/0ku5L84QHW98Yk/yfJB5PsTfLVJGv76pcm2dI+f1eS9yQ5ctqylyV5CHjXfj7/2CQfTfJwknuAfzmt/sXtr+lHknwlyaunLfvnSba3vn22lf3MkFKS+5O8ok2/K8lfJ/mrtv1fSvILSd7R/q12JHnlLLbxs+2/w8NJvpnk7Fb3p8CvAx9se3sf3O8XQIckQ0DD9EfAGcCpwCnA6cA7pzdK8sfAG4HfqKqdwKXAL7TlXgisAP64b5GfB5a28g3Ah5Ic1+q2AG+uqmcCvwzc8hT9+1fAN4ATgEuATyQ5vtV9FHiirf8lwCuBN01b9j5gOfCn+/nsS4AXtNergPV923sU8DfAZ4DnAL8LfCzJi1qTPwN+FfjXwPHA24CfPMV29Psd4CrgOOAu4NP0/r9eAfwJ8N/62g6yjV+j9+/zX4AtSVJVfwT8b+AtVfWMqnrLgH3ToaCqfPma8wu4H3hFm/4GcE5f3auA+9v0mcAu4H3AZ4GlrTzAD4EX9C33UuCbfcs9Bizpq98DnNGmvwW8GXjWDP18I/BtIH1l/wi8gd4P+z8Bx/bVvQ64tW/Zb83w+fcBZ/XNbwR2tulfBx4Ajuirv5reHsURbftO2c9nnrnvMw7w7/0u4Ka+ut8BfgAc2eafCRSwbMBtnOyr+7m27M+3+dvoDfst+HfO13BfHhPQMD0X2N43v72V7bOM3o/jv6uqva1sjN4Pzp1J9rULcGTfcg/Vk8fgHwX2HZf4t/T2Ni5Ncjewqar+4QD921XtF21a/54PHAXs7uvDEcCOvrb90/vz3Glttk+vq6qfTKtfQe+v7qfRC9C5eLBv+jHgO1X1z33z0Pu3ei4zb+MD+yaq6tHW7hnosOZwkIbp2/R+UPd5Xivb52Hgt4G/TPKyVvYdej9Wv1RVy9prafUdfH4qVfW5qlpHb5jlfwHXPkXzFen7Bezr3w56fyWf0NeHZ1XVL/Wvaoau7AZWTfvsfb4NrEpyxLT6XfS2/0f0hpGm+yG9gASgjd+PzdCPAxlkG5+Ktxs+TBkCGqargXcmGUtyAr1x/b/qb1BVtwGvpzcef3r76/i/A5cleQ5AkhVJXjXTypIcneT1SZZW1ePA93jqsfTnAG9NclSS1wAvBm6sqt30xuv/PMmzkhyR5AVJfmMW234t8I4kxyVZSW/cf5876O29vK2t+0x6QzfXtO3/CPC+JM9NcmSSl7YD6v8XeFqSc9txhXcC+z3QPpMhbOODwL+Yy7q1uBkCGqb3ABPA3cCXgM+3siepqpuAfw/8TZLTgLcDk8DtSb4H/B3wounLHcAbgPvbcv+BXsAcyB3AGnp/ff8pcH5VPdTqLgSOBu6ht8dyHXDigH0AeDe9IZ5v0vuxvWpfRVX9mN6P/tlt3f8VuLCqvtqa/CG9f6/PAd8F3kvv+MFe4D8CH6a31/BDYD4XoM1nGz8AnN/OHLp8Hn3QIpMnD5FKh6ckb6R3YPPXFrov0mLinoAkdZghIEkd5nCQJHWYewKS1GGGgCR12KK4YviEE06o1atXL3Q3JOmwdeedd36nqn7mYsNFEQKrV69mYmJiobshSYetJNv3V+5wkCR12EAhkOQ/tXugfznJ1UmeluSkJHckmUzy8SRHt7bHtPnJVr96pFsgSZqzGUMgyQrgrcB4Vf0yvbs7XkDv0vbLquqF9C5B39AW2QA83Mova+0kSYvQoMNBS4Bjkyyhd1fD3cDL6d17BGArcF6bXtfmafVrp925UZK0SMwYAlW1i96Tj75F78d/L3An8EjfPd530rs3Ou19R1v2idb+2dM/N8nGJBNJJqampua7HZKkORhkOOg4en/dn0TvwRRPB86a74qranNVjVfV+NjYXG+RLkmaj0GGg15B71F/U+2e7Z8AXgYsa8NDACvp3eqW9r4KoNUvBR5CkrToDBIC3wLOSPJzbWx/Lb37kd8KnN/arAeub9Pb+OlDts8HbilvUCRJi9KMF4tV1R1JrqP3gJAngLuAzcANwDVJ3tPKtrRFtgBXJZmk94CMC0bRcf3UXI+7m82SFsVdRMfHx8srhodv9aYbuP/Scxe6G5IWgSR3VtX49HKvGJakDjMEJKnDDAFJ6jBDQJI6zBCQpA4zBCSpwwwBSeowQ0CSOswQkKQOMwQkqcMMAUnqMENAkjrMEJCkDjMEJKnDDAFJ6jBDQJI6bJAHzb8oyRf6Xt9L8vtJjk9yU5Kvt/fjWvskuTzJZJK7k5w2+s2QJM3FjCFQVV+rqlOr6lTgV4FHgU8Cm4Cbq2oNcHObBzgbWNNeG4ErRtBvSdIQzHY4aC3wjaraDqwDtrbyrcB5bXodcGX13A4sS3LiMDorSRqu2YbABcDVbXp5Ve1u0w8Ay9v0CmBH3zI7W9mTJNmYZCLJxNTU1Cy7IUkahoFDIMnRwKuBv55eV72n1c/qifVVtbmqxqtqfGxsbDaLSpKGZDZ7AmcDn6+qB9v8g/uGedr7nla+C1jVt9zKViZJWmRmEwKv46dDQQDbgPVtej1wfV/5he0soTOAvX3DRpKkRWTJII2SPB34LeDNfcWXAtcm2QBsB17bym8EzgEm6Z1JdNHQeitJGqqBQqCqfgg8e1rZQ/TOFpretoCLh9I7SdJIecWwJHWYISBJHWYISFKHGQKS1GGGgCR1mCEgSR1mCEhShxkCktRhhoAkdZghIEkdZghIUocZApLUYYaAJHWYISBJHWYISFKHGQKS1GEDhUCSZUmuS/LVJPcmeWmS45PclOTr7f241jZJLk8ymeTuJKeNdhMkSXM16J7AB4C/rapfBE4B7gU2ATdX1Rrg5jYPvQfSr2mvjcAVQ+2xJGloZgyBJEuBfwNsAaiqH1fVI8A6YGtrthU4r02vA66sntuBZUlOHHK/JUlDMMiewEnAFPCXSe5K8uH24PnlVbW7tXkAWN6mVwA7+pbf2cqeJMnGJBNJJqampua+BZKkORskBJYApwFXVNVLgB/y06Ef4P8/XL5ms+Kq2lxV41U1PjY2NptFJUlDMkgI7AR2VtUdbf46eqHw4L5hnva+p9XvAlb1Lb+ylUmSFpkZQ6CqHgB2JHlRK1oL3ANsA9a3svXA9W16G3BhO0voDGBv37CRJGkRWTJgu98FPpbkaOA+4CJ6AXJtkg3AduC1re2NwDnAJPBoaytJWoQGCoGq+gIwvp+qtftpW8DF8+uWJOlg8IphSeowQ0CSOswQkKQOMwQkqcMGPTtIkoYmyZyW6513omFyT0DSQVdV+309/+2fOmCdATAahoAkdZghIEkdZghIUocZApLUYYaAJHWYISBJHWYISFKHGQKS1GGGgCR1mCEgSR02UAgkuT/Jl5J8IclEKzs+yU1Jvt7ej2vlSXJ5kskkdyc5bZQbIEmau9nsCfxmVZ1aVfueMLYJuLmq1gA3t3mAs4E17bURuGJYnZUkDdd8hoPWAVvb9FbgvL7yK6vndmBZkhPnsR5J0ogMGgIFfCbJnUk2trLlVbW7TT8ALG/TK4AdfcvubGVPkmRjkokkE1NTU3PouiRpvgZ9nsCvVdWuJM8Bbkry1f7Kqqoks7rPa1VtBjYDjI+Pe49YSVoAA+0JVNWu9r4H+CRwOvDgvmGe9r6nNd8FrOpbfGUrkyQtMjOGQJKnJ3nmvmnglcCXgW3A+tZsPXB9m94GXNjOEjoD2Ns3bCRJWkQGGQ5aDnyyPQ5uCfA/qupvk3wOuDbJBmA78NrW/kbgHGASeBS4aOi9liQNxYwhUFX3Aafsp/whYO1+ygu4eCi9kySNlFcMS1KHGQKS1GGGgCR1mCEgSR1mCEhShxkCktRhhoAkdZghIEkdZghIUocZApLUYYaAJHWYISBJHWYISFKHGQKS1GGGgCR1mCEgSR02cAgkOTLJXUk+1eZPSnJHkskkH09ydCs/ps1PtvrVI+q7JGmeZrMn8HvAvX3z7wUuq6oXAg8DG1r5BuDhVn5ZaydJWoQGCoEkK4FzgQ+3+QAvB65rTbYC57XpdW2eVr+2tZckLTKD7gm8H3gb8JM2/2zgkap6os3vBFa06RXADoBWv7e1f5IkG5NMJJmYmpqaW+8lSfMyYwgk+W1gT1XdOcwVV9XmqhqvqvGxsbFhfrQkaUBLBmjzMuDVSc4BngY8C/gAsCzJkvbX/kpgV2u/C1gF7EyyBFgKPDT0nkuS5m3GPYGqekdVrayq1cAFwC1V9XrgVuD81mw9cH2b3tbmafW3VFUNtdeSpKEYZE/gQN4OXJPkPcBdwJZWvgW4Kskk8F16waEhOOXdn2HvY4/PapnVm26YVfulxx7FFy955ayWkXTomlUIVNVtwG1t+j7g9P20+RHwmiH0TdPsfexx7r/03JGuY7ahIenQ5hXDktRhhoAkdZghIEkdZghIUocZApLUYYaAJHWYISBJHWYISFKHGQKS1GGGgCR1mCEgSR1mCEhShxkCktRhhoAkdZghIEkdZghIUocN8qD5pyX5xyRfTPKVJO9u5ScluSPJZJKPJzm6lR/T5idb/eoRb4MkaY4G2RP4J+DlVXUKcCpwVpIzgPcCl1XVC4GHgQ2t/Qbg4VZ+WWsnSVqEBnnQfFXVD9rsUe1VwMuB61r5VuC8Nr2uzdPq1ybJsDosSRqegY4JJDkyyReAPcBNwDeAR6rqidZkJ7CiTa8AdgC0+r3As/fzmRuTTCSZmJqamtdGSJLmZqAQqKp/rqpTgZX0Hi7/i/NdcVVtrqrxqhofGxub78dJkuZgVmcHVdUjwK3AS4FlSZa0qpXArja9C1gF0OqXAg8No7OSpOFaMlODJGPA41X1SJJjgd+id7D3VuB84BpgPXB9W2Rbm/+HVn9LVdUI+t45z3zxJn5l66YRrwPg3JGuQ9LiMWMIACcCW5McSW/P4dqq+lSSe4BrkrwHuAvY0tpvAa5KMgl8F7hgBP3upO/feyn3XzraH+jVm24Y6edLWlxmDIGquht4yX7K76N3fGB6+Y+A1wyld5KkkfKKYUnqMENAkjrMEJCkDjMEJKnDBjk7SJJm7ZR3f4a9jz0+6+Vme4ba0mOP4ouXvHLW61GPISBpJPY+9vjIT2kGT2ueL4eDJKnDDAFJ6jBDQJI6zBCQpA4zBCSpwwwBSeowQ0CSOswQkKQOMwQkqcNmDIEkq5LcmuSeJF9J8nut/PgkNyX5ens/rpUnyeVJJpPcneS0UW+EJGluBtkTeAL4g6o6GTgDuDjJycAm4OaqWgPc3OYBzgbWtNdG4Iqh91qSNBQzhkBV7a6qz7fp7wP3AiuAdcDW1mwrcF6bXgdcWT2303sg/YnD7rgkaf5mdUwgyWp6j5q8A1heVbtb1QPA8ja9AtjRt9jOViZJWmQGDoEkzwD+J/D7VfW9/rqqKqBms+IkG5NMJJmYmpqazaKSpCEZKASSHEUvAD5WVZ9oxQ/uG+Zp73ta+S5gVd/iK1vZk1TV5qoar6rxsbGxufZfkjQPg5wdFGALcG9Vva+vahuwvk2vB67vK7+wnSV0BrC3b9hIkrSIDPJQmZcBbwC+lOQLrew/A5cC1ybZAGwHXtvqbgTOASaBR4GLhtlhSdLwzBgCVfVZIAeoXruf9gVcPM9+SZIOAq8YlqQOMwQkqcMMAUnqMENAkjrMEJCkDjMEJKnDDAFJ6jBDQJI6zBCQpA4zBCSpwwa5d5AkzdozX7yJX9m6aeaG814PwLkjX8/hyhCQNBLfv/dS7r909D/OqzfdMPJ1HM4cDpKkDjMEJKnDDAFJ6jBDQJI6bJDHS34kyZ4kX+4rOz7JTUm+3t6Pa+VJcnmSySR3JzltlJ2XJM3PIHsCHwXOmla2Cbi5qtYAN7d5gLOBNe21EbhiON2UJI3CII+X/Pskq6cVrwPObNNbgduAt7fyK9sjJm9PsizJiT5ofnhGfTrc0mOPGunnS1pc5nqdwPK+H/YHgOVtegWwo6/dzlZmCAzBbM+5Xr3phoNynrakQ9e8Dwy3v/prtssl2ZhkIsnE1NTUfLshSZqDuYbAg0lOBGjve1r5LmBVX7uVrexnVNXmqhqvqvGxsbE5dkOSNB9zDYFtwPo2vR64vq/8wnaW0BnAXo8HSNLiNeMxgSRX0zsIfEKSncAlwKXAtUk2ANuB17bmNwLnAJPAo8BFI+izJGlIBjk76HUHqFq7n7YFXDzfTkmSDg6vGJakDjMEJKnDDAFJ6jBDQJI6zBCQpA4zBCSpw3zGsKSRORjP//Wmh/NjCEgaibncvNCbHh58DgdJUocZApLUYYaAJHWYISBJHWYISFKHGQKS1GGGgCR1mCEgSR02khBIclaSryWZTLJpFOuQJM3f0EMgyZHAh4CzgZOB1yU5edjrkSTN3yj2BE4HJqvqvqr6MXANsG4E65EkzdMoQmAFsKNvfmcrkyQtMgt2A7kkG4GNAM973vMWqhuHhSQHrnvvgZerqhH0RpqZ39nFYxR7AruAVX3zK1vZk1TV5qoar6rxsbGxEXSjO6pqTi9pofidXTxGEQKfA9YkOSnJ0cAFwLYRrEeSNE9DHw6qqieSvAX4NHAk8JGq+sqw1yNJmr+RHBOoqhuBG0fx2ZKk4fGKYUnqMENAkjrMEJCkDjMEJKnDDAFJ6rAshgswkkwB2xe6H4ehE4DvLHQnpFnwOzs6z6+qn7kyd1GEgEYjyURVjS90P6RB+Z09+BwOkqQOMwQkqcMMgcPb5oXugDRLfmcPMo8JSFKHuScgSR1mCBwGkpyV5GtJJpNs2k/9MUk+3urvSLJ6AbopAZDkI0n2JPnyAeqT5PL2fb07yWkHu49dYggc4pIcCXwIOBs4GXhdkpOnNdsAPFxVLwQuA57i2U3SyH0UOOsp6s8G1rTXRuCKg9CnzjIEDn2nA5NVdV9V/Ri4Blg3rc06YGubvg5Ym6d6vp80QlX198B3n6LJOuDK6rkdWJbkxIPTu+4xBA59K4AdffM7W9l+21TVE8Be4NkHpXfS7A3yndaQGAKS1GGGwKFvF7Cqb35lK9tvmyRLgKXAQweld9LsDfKd1pAYAoe+zwFrkpyU5GjgAmDbtDbbgPVt+nzglvICES1e24AL21lCZwB7q2r3QnfqcDWSZwzr4KmqJ5K8Bfg0cCTwkar6SpI/ASaqahuwBbgqySS9A3IXLFyP1XVJrgbOBE5IshO4BDgKoKr+gt7zyc8BJoFHgYsWpqfd4BXDktRhDgdJUocZApLUYYaAJHWYISBJHWYISFKHGQKS1GGGgCR1mCEgSR32/wC8TSTIB9GhrQAAAABJRU5ErkJggg==&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here we can see that we have a fairly wide distribution, with quite a few texts
going all the way up to 800 tokens in length, so that is probably responsible
for the large amounts of RAM needed, but perhaps the truncation of texts is also
harming our performance.&lt;/p&gt;
&lt;p&gt;When I visit &lt;a href=&quot;https://huggingface.co/microsoft/deberta-v3-small&quot;&gt;the &lt;code&gt;deberta-v3-small&lt;/code&gt; model
card&lt;/a&gt; on Huggingface, I also
see reference to a maximum sequence length of 256 which would indeed harm my
model and its ability to learn, I reckon.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&amp;lt;/div&amp;gt;
 

&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="fastai" /><category term="nlp" /><category term="redactionmodel" /><category term="computervision" /><category term="huggingface" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/nlp-redaction-classifier/filenames.png" /><media:content medium="image" url="https://mlops.systems/images/nlp-redaction-classifier/filenames.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A neural network for Fashion MNIST data</title><link href="https://mlops.systems/fastai/computervision/pytorch/2022/05/15/fashion-mnist-neural-network.html" rel="alternate" type="text/html" title="A neural network for Fashion MNIST data" /><published>2022-05-15T00:00:00-05:00</published><updated>2022-05-15T00:00:00-05:00</updated><id>https://mlops.systems/fastai/computervision/pytorch/2022/05/15/fashion-mnist-neural-network</id><content type="html" xml:base="https://mlops.systems/fastai/computervision/pytorch/2022/05/15/fashion-mnist-neural-network.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-05-15-fashion-mnist-neural-network.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;pip install -Uqq fastbook nbdev torch
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;fastbook&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fastbook&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setup_book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;fastai.vision.all&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;fastbook&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.transforms&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FashionMNIST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FashionMNIST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;training_dresses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;training_pullovers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_dresses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_pullovers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;training_dresses_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_dresses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;training_pullovers_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_pullovers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_dresses_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_dresses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_pullovers_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_pullovers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_dresses_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_pullovers_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_dresses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_pullovers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;valid_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_dresses_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_pullovers_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;valid_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_dresses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_pullovers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train_dset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;valid_dset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valid_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train_dl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;valid_dl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid_dset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dls&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoaders&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valid_dl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;initialise_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_grad_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fashion_mnist_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;batch_accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;correct&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_batch&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;correct&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In &lt;a href=&quot;https://mlops.systems/fastai/computervision/pytorch/2022/05/14/SGD-fashion-mnist.html&quot;&gt;the previous
post&lt;/a&gt;
we used stochastic gradient descent to train a model to fit a linear function to
our Fashion MNIST data, specifically the difference between a pullover and a
dress.&lt;/p&gt;
&lt;p&gt;In this final stage, we will take the next step to creating a neural network in
code that will be used to detect that same difference between a pullover and a
dress. The key difference here is that we will need to 'add non-linearity' to our
function. I have no mathematics background so I have very little intuitive (or
learned!) understanding of specifically what that means, but my current mental
model as learned during the course is that linear functions just aren't flexible
enough to learn more complex patterns. In the end, what we want is a function
that will fit to the patterns in our training data (as mapped to a
multidimensional space). Simple linear functions aren't going to cut it.&lt;/p&gt;
&lt;p&gt;What this looks like in code is this:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialise_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bias1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialise_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;weights2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialise_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bias2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialise_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;simple_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@weights1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@weights2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;result&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;You can see the three layers of our simple network pretty clearly in the code
above. The middle layer is what otherwise is known as a ReLU or rectified linear
unit. It basically means that negative values passing through that function
become zero and all positive values are unchanged. When you plot the function it
looks like this:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAX4AAAD7CAYAAABt0P8jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAlIklEQVR4nO3deXxU5dn/8c8lW1hlC7ixCyoga9zXqi0uVWlRK4vLoxVF0bo8Vlux4lK3trZqebD8qkUFcUFwrUut8rhrQ9gMQlBkEQTCngQIkFy/P2byvMYxISdkZk4m832/XvNi5j73OXOdw8yVM/ecuW5zd0REJHPsE3YAIiKSWkr8IiIZRolfRCTDKPGLiGQYJX4RkQzTMOwAgmjfvr137do17DBERNLK7Nmz17t7dnx7WiT+rl27kpubG3YYIiJpxcyWV9auoR4RkQyjxC8ikmGU+EVEMowSv4hIhlHiFxHJMNUmfjNrYmaPm9lyMysys7lmdsYe+t9gZmvMbKuZPWFmTWKWdTWz98xsm5ktMrPTErUjIiISTJAz/obASuAkYF9gHPC8mXWN72hmQ4BbgVOBLkB34M6YLtOAOUA74DZgupn94BpTERFJnmoTv7uXuPt4d1/m7uXu/hrwDTC4ku6XAI+7e767bwLuBi4FMLNewCDgDnff7u4vAguAYQnaFxGReuPrwmL++NZidpeVJ3zbNR7jN7OOQC8gv5LFfYB5MY/nAR3NrF102VJ3L4pb3qeK5xltZrlmlltYWFjTMEVE0ta2nbsZM2U2z3y+gg0lOxO+/RolfjNrBEwFnnT3RZV0aQFsiXlccb9lJcsqlres7LncfZK757h7Tna2RoNEJDO4O7fN/IIl64p5+MIBdGyVlfDnCJz4zWwf4GlgJzC2im7FQKuYxxX3iypZVrG8CBERAWDqZyuYOWcV15/aixN6JuekN1DiNzMDHgc6AsPcfVcVXfOB/jGP+wNr3X1DdFl3M2sZt7yyISMRkYwz/9vN3PXqQk7qlc21pxyctOcJesY/ETgMONvdt++h31PA5WbW28xaE7kCaDKAuxcAc4E7zCzLzH4G9ANe3LvQRUTqj00lOxkzJY/slk34yy8GsM8+lrTnCnIdfxfgSmAAsMbMiqO3kWbWOXq/M4C7vwk8CLwHrACWA3fEbO5CIAfYBNwPnOfu+uZWRDJaeblzw/NzWVe0gwkjB9GmeeOkPl+1ZZndfTmwpz89LeL6PwQ8VMW2lgEnBw9PRKT+m/DeV8xaXMjd5/ZhQKfWSX8+lWwQEQnRh0vW89A7BQwdcACjju6SkudU4hcRCcl3W7Zz3bNz6NmhBff+/HAi19EknxK/iEgIdu4u55qpeZTuKmPiqME0a5y6CRHTYupFEZH65r43viRvxWYmjBhEj+wW1a+QQDrjFxFJsdfmr+YfHy3jsuO6cVa//VP+/Er8IiIp9NW6Ym6ZPp/BXdrwmzMPDSUGJX4RkRQpKY0UX8tq1IAJIwbRqEE4KVhj/CIiKeDu/HbmAr4qLGbK5Uex376JL74WlM74RURSYMqny3l57mpuPK0Xxx3cPtRYlPhFRJJs7srN3PXaQn50SDbX/Ch5xdeCUuIXEUmiTSU7uWZqHh1aZvHnJBdfC0pj/CIiSVJe7lz/3FwKi0qZPuYYWjdLbvG1oHTGLyKSJI+++xX/W1DIHef0pt9BrcMO5/8o8YuIJMH7BYX85d8F/HzggYw4snPY4XyPEr+ISIKt3rydXz07h14dWvL7n6Wu+FpQQadeHGtmuWZWamaT99DvsZiJWoqj/Ytils8ysx0xyxcnYB9EROqMnbvLuXpqHrvKnImjBtG0cYOwQ/qBoF/urgbuAYYATavq5O5XAVdVPI7+kSiP6zbW3f9eszBFRNLDvf/8krkrNzNx5CC6p7j4WlCBEr+7zwAwsxzgoCDrmFlzYBjw072OTkQkjbwybzWTP17GL4/vxhmHp774WlDJHOMfBhQC78e132dm683sIzM7uaqVzWx0dHgpt7BQ0/KKSN22ZG0Rt744n5wubbjljHCKrwWVzMR/CfCUu3tM2y1Ad+BAYBLwqpn1qGxld5/k7jnunpOdnZ3EMEVEaqekdDdjpubRrHEDJowMr/haUEmJzsw6E5lU/anYdnf/zN2L3L3U3Z8EPgLOTEYMIiKp4O7cOmMBSwuLeWT4QDq2Cq/4WlDJ+rN0EfCRuy+tpp8Ddes6JxGRGnjqk+W8Om81N/3kEI7tEW7xtaCCXs7Z0MyygAZAAzPLMrM9fTF8MTA5bhutzWxIxbpmNhI4EXhzL2MXEQlV3opN3PP6Qk49tANjTqp01LpOCnrGPw7YDtwKjIreH2dmnaPX4//fz9LM7BgiV/68ELeNRkQuCS0E1gPXAkPdvaB2uyAiknobiku5Zmoe++2bxUMX1I3ia0EFvZxzPDC+isXfu1DV3T8BmleyjULgiJqFJyJS95RFi69tKNnJjDHHsm+zRmGHVCN1+6tnEZE66OF/L+GDJeu585w+9D1w37DDqTElfhGRGpi1eB2PvruEYYMO4sIjOoUdzl5R4hcRCWjV5u1c/9xcDunYknuG9q1zxdeCUuIXEQmgdHcZV0/No6zMmThqcJ0svhaUZuASEQng969/ybyVm3ls1CC6tf/B9StpRWf8IiLVeHnuKp76ZDlXnNCN0/vW3eJrQSnxi4jsQcHaIm59cQFHdG3Dr0+v28XXglLiFxGpQnHpbq6aMpvmTRry1xF1v/haUPVjL0REEszdueXF+SxbX8IjwwekRfG1oJT4RUQqMfnjZbw+/ztuHnJo2hRfC0qJX0Qkzuzlm/j9619y2mEdueqk7mGHk3BK/CIiMTYUlzL2mTwOaN2UP13QP21/pLUnuo5fRCSqrNz51bMxxdeaplfxtaB0xi8iEvXwOwV8+NV67j43PYuvBaXELyICvLdoHY+8+xXnDz6IXxzRufoV0ljQGbjGmlmumZWa2eQ99LvUzMqik7NU3E6OWd7VzN4zs21mtsjMTqv1HoiI1NLKjdu4/rm5HLZ/K+4e2jfscJIu6Bj/aiKzZw0BmlbT9xN3P76KZdOAT4hMsH4mMN3MekYnaRERSbnS3WVc80we5eXOxJGDyGqUvsXXggp0xu/uM9z9JWDD3j6RmfUCBgF3uPt2d38RWAAM29ttiojU1l2vLmT+t1v44wX96ZrmxdeCSsYY/0AzW29mBWZ2e8yk7H2Ape5eFNN3XrT9B8xsdHR4KbewUB8IRCTxZuR9y9TPVnDlid0Z0me/sMNJmUQn/veBvkAHImfyw4Gbo8taAFvi+m8BWla2IXef5O457p6TnZ2d4DBFJNMtWrOV385cwJHd2nLzkEPCDielEpr43X2pu3/j7uXuvgC4CzgvurgYaBW3SiugCBGRFCrasYsxU/JomdWIv44YSMN6UnwtqGTvrQMVP3vLB7qbWewZfv9ou4hISrg7v54+nxUbt/HX4QPp0LL+FF8LKujlnA3NLAtoADQws6yYsfvYfmeYWcfo/UOB24GXAdy9AJgL3BFd/2dAP+DFhOyJiEgAj3/4DW98sYZfDzmEo7q3CzucUAQ94x8HbAduBUZF748zs87Ra/Urfu1wKjDfzEqAfwIzgHtjtnMhkANsAu4HztOlnCKSKrnLNnL/G4v4Se+OjD6x/hVfC8rcPewYqpWTk+O5ublhhyEiaWx9cSlnPfIBWY0a8MrY4+ttHZ5YZjbb3XPi21WkTUTqvbJy57ppc9i8bRczrz4yI5L+nijxi0i999C/FvPx1xt48Lx+9D4g/uLCzJNZ1zCJSMb595drmfDe1/wipxMX5HQKO5w6QYlfROqtlRu3ccNzc+m9fyvuPLfSIgEZSYlfROqlHbvKuHpqHg5MHJUZxdeC0hi/iNRLd766kAWrtvD/Ls6hS7vMKL4WlM74RaTeeXH2t0z7fAVjTu7Bj3t3DDucOkeJX0TqlUVrtnLbSws4pns7bvpxr7DDqZOU+EWk3tgaLb7WKqsRjwzPvOJrQWmMX0TqBXfn1y9Eiq9Nu+Josls2CTukOkt/DkWkXvj7B9/wZv4abj39UI7s1jbscOo0JX4RSXuff7OR+99cxJA+HfnlCd3CDqfOU+IXkbS2rmgHY5/Jo1Obpvzh/P6YWfUrZTiN8YtI2tpdVs510+awdccunrzsSFplZXbxtaCCTsQyNjrxeamZTd5Dv0vMbLaZbTWzb83swdgJW8xslpntiNbwLzazxQnYBxHJUH/6VwGfLt3IPUMP57D9VXwtqKBDPauBe4AnqunXDLgeaA8cRWRilv+O6zPW3VtEb5k1w7GIJMy/Fq5l4qyvGX5kJ84bfFDY4aSVQEM97j4DwMxygCqPsLtPjHm4ysymAj+qVYQiInFWbNjGjc/Ppe+BrbjjbBVfq6lkf7l7Ij+cTP0+M1tvZh+Z2clVrWhmo6PDS7mFhZqdUUQiduwqY8zU2RgwceRgFV/bC0lL/GZ2GZH5df8Y03wL0B04EJgEvGpmPSpb390nuXuOu+dkZ2cnK0wRSTPjX8knf/VW/nLhADq1bRZ2OGkpKYnfzIYC9wFnuPv6inZ3/8zdi9y91N2fBD4CzkxGDCJS/zyfu5Jn/7OSa37Ug1MOVfG1vZXwyznN7HTg/wFnufuCaro7oItuRaRa+au3cPtLX3Bsj3bc+GNdF1IbQS/nbGhmWUADoIGZZcVephnT7xRgKjDM3T+PW9bazIZUrGtmI4l8B/Bm7XdDROqzLdt3cfXUPFo3ixRfa7CPzhdrI+hQzzhgO3ArMCp6f5yZdY5ej9852u92YF/gnzHX6r8RXdaIyCWhhcB64FpgqLsXJGhfRKQecndufmEeqzZtZ8KIQbRvoeJrtRX0cs7xwPgqFreI6VflpZvuXggcUYPYRESY9P5S3l64lnFnHUZOVxVfSwTV6hGROuuzpRt48K3FnHn4flx+vIqvJYoSv4jUSeu27mDstDl0aduMB4b1U/G1BFKRNhGpc3aXlXPttDkU79jNlMuPoqWKryWUEr+I1Dl/eHsxn32zkT//oj+H7Ncy7HDqHQ31iEid8nb+Gv72v0sZeVRnfjZQxdeSQYlfROqM5RtKuOmFefQ7aF9+d3bvsMOpt5T4RaRO2LGrjDFT8tjHjAkjBtGkoYqvJYvG+EWkTvjdy1+w8Lut/OPSI1R8Lcl0xi8ioXvuPyt4Pvdbrj3lYH50aIeww6n3lPhFJFRfrNrC7S/nc/zB7bn+tF5hh5MRlPhFJDQVxdfaNmvMwxcOUPG1FNEYv4iEorzcuen5eazevJ3nrjyadiq+ljI64xeRUPzt/aW88+VafnvmYQzuouJrqaTELyIp98nXG/jDW4s4q9/+/NdxXcMOJ+Mo8YtISq3buoNrp82ha/vmKr4WkqAzcI01s1wzKzWzydX0vcHM1pjZVjN7wsyaxCzrambvmdk2M1tkZqfVMn4RSSO7ysoZ+8wcSkp389iowbRooq8ZwxD0jH81kdmznthTJzMbQmSWrlOBLkB34M6YLtOAOUA74DZgupll1zBmEUlTf3hrMZ8v28j9ww6nV0cVXwtLoMTv7jPc/SVgQzVdLwEed/d8d98E3A1cCmBmvYBBwB3uvt3dXwQWAMP2MnYRSSNvfrGGSe8v5aKju3DugAPDDiejJXqMvw8wL+bxPKCjmbWLLlvq7kVxy/tUtiEzGx0dXsotLCxMcJgikkrL1pdw8wvz6N+pNeN+eljY4WS8RCf+FsCWmMcV91tWsqxieaWf99x9krvnuHtOdrZGg0TS1fadZVw1ZTYNGhgTRgxU8bU6INHfrBQDrWIeV9wvqmRZxfIiRKRecnduf/kLFq8t4h+XHsFBbVR8rS5I9Bl/PtA/5nF/YK27b4gu625mLeOW5yc4BhGpI577z0qmz/6Wa0/pycmHqPhaXRH0cs6GZpYFNAAamFmWmVX2aeEp4HIz621mrYFxwGQAdy8A5gJ3RNf/GdAPeLHWeyEidc4Xq7bwu1fyOaFne351as+ww5EYQc/4xwHbiVyqOSp6f5yZdTazYjPrDODubwIPAu8BK4DlwB0x27kQyAE2AfcD57m7vrkVqWe2bNvFVVNm0655Yx6+cKCKr9Ux5u5hx1CtnJwcz83NDTsMEQmgvNy54qlc3l9SyPNXHsPAzm3CDiljmdlsd8+Jb1fJBhFJqMfe/5p/L1rHuLN6K+nXUUr8IpIwH3+9nj++tZiz+x/Axcd0CTscqYISv4gkxJotO7hu2hy6tW/O/T8/XMXX6jBVSBKRWosUX8tj284ypl1xNM1VfK1O0/+OiNTaA28sInf5Jh4ZPpCeKr5W52moR0Rq5Y0F3/H3D7/hkmO6cE7/A8IORwJQ4heRvba0sJibp89nQKfW3HZW77DDkYCU+EVkr2zfWcbVU/No1MCYMHIQjRsqnaQLjfGLSI25O7e9tIDFa4t48r+O5MDWTcMOSWpAf6JFpMamfb6SGXmr+NWpPTmxl8qmpxslfhGpkfnfbmb8K/mc2Cub605R8bV0pMQvIoFt3raTMVPyaN+iMX/5xQD2UfG1tKQxfhEJpLzcueG5uawr2sELVx1L2+aNww5J9pLO+EUkkP+Z9RXvLS7k9p/2ZkCn1mGHI7WgxC8i1froq/U89K8Czul/ABcdreJr6S7oDFxtzWymmZWY2XIzG1FFvzeiE7NU3Haa2YKY5cvMbHvM8rcTtSMikhwVxde6Z7fgPhVfqxeCjvFPAHYCHYEBwOtmNs/dvzdfrrufEfvYzGYB78Zt62x3f2evohWRlNpVVs41z+SxY1cZj40arOJr9US1Z/xm1hwYBtzu7sXu/iHwCnBRNet1BU4gMg+viKSh+/65iNnLN/HAef04uEOLsMORBAky1NML2B2dLL3CPKBPNetdDHzg7svi2qeaWaGZvW1m/ata2cxGm1mumeUWFmpaXpFUe33+dzzx0TdcemxXftpPxdfqkyCJvwWwNa5tC1Bd7dWLgclxbSOBrkAXIhOyv2VmrStb2d0nuXuOu+dkZ+uXgSKp9HVhMb+ePo9BnVvz2zMPCzscSbAgib8YaBXX1gooqmoFMzse2A+YHtvu7h+5+3Z33+bu9wGbiQwHiUgdsW3nbsZMmU2TRg1UfK2eCvI/WgA0NLPY32b3B/Kr6A9wCTDD3Yur2bYDukRApI5wd26b+QVL1hXz8IUD2H9fFV+rj6pN/O5eAswA7jKz5mZ2HHAu8HRl/c2sKXABccM8ZtbZzI4zs8ZmlmVmNwPtgY9quQ8ikiBTP1vBzDmruOG0XpzQU0Os9VXQz3BXA02BdcA0YIy755vZCWYWf1Y/lMgQzntx7S2BicAmYBVwOnCGu2/Yu9BFJJHmrdzMXa8u5ORDshn7o4PDDkeSyNw97BiqlZOT47m5uWGHIVJvbSrZyU8f/RCA1649njaqw1MvmNlsd8+Jb9evMUQyXHm5c8PzcyksKuWFq45R0s8A+rpeJMP99b2vmLW4kNvP7k1/FV/LCEr8IhnsgyWF/PmdAoYOOIBRR3UOOxxJESV+kQy1evN2fvXsXHp2aMG9Kr6WUZT4RTLQzt3ljH0mj9JdZUwcNZhmjfV1XybR/7ZIBrr3n1+St2IzE0YMoke2iq9lGp3xi2SYV+etZvLHy7jsuG6c1W//sMORECjxi2SQr9YVc+uL8xncpQ2/OfPQsMORkCjxi2SIktJI8bWsRg2YMGIQjRro7Z+pNMYvkgHcnd/OXMDXhcU8fflR7LdvVtghSYj0J18kA0z5dDkvz13NjT/uxXEHtw87HAmZEr9IPTdnxSbuem0hpxzagatPVvE1UeIXqdc2luzkmql5dGyVxUMX9GefffQjLdEYv0i9VVbuXP/cXNYX72T6mGNo3UzF1yQi0Bm/mbU1s5lmVmJmy81sRBX9xpvZLjMrjrl1j1k+wMxmm9m26L8DErQfIhLn0XeX8H5BIXec05t+B7UOOxypQ4IO9UwAdgIdiUyYPtHM+lTR9zl3bxFzWwpgZo2Bl4EpQBvgSeDlaLuIJND/FhTy8L+X8POBBzLiSBVfk++rNvGbWXNgGHC7uxe7+4fAK8BFNXyuk4kMLf3F3Uvd/REi8+2eUsPtiMgerNq8neufnUOvDi35/c9UfE1+KMgZfy9gt7sXxLTNA6o64z/bzDaaWb6ZjYlp7wPM9+9P+TW/qu2Y2WgzyzWz3MLCwgBhikjp7jKunprHrjJn4qhBNG3cIOyQpA4KkvhbAFvj2rYQmUM33vPAYUA2cAXwOzMbHrOdLQG3g7tPcvccd8/JztakzyJB/P71L5m3cjN/PL8f3VV8TaoQJPEXA63i2loBRfEd3X2hu6929zJ3/xh4GDivptsRkZp7ee4qnvpkOb88vhun91XxNalakMRfADQ0s54xbf2B/ADrOpFxfKL9+9n3Bxz7BdyOiOzBkrVF/GbGAo7o2oZbzlDxNdmzahO/u5cAM4C7zKy5mR0HnAs8Hd/XzM41szYWcSRwHZEreQBmAWXAdWbWxMzGRtvfTcB+iGSs4tLdXDVlNs0aN+DR4Sq+JtUL+gq5GmgKrAOmAWPcPd/MTjCz4ph+FwJfERm+eQp4wN2fBHD3ncBQ4GJgM3AZMDTaLiJ7wd259cX5fLO+hEeGD1TxNQkk0C933X0jkaQd3/4BkS9tKx4Pj+8T138OMLhmIYpIVZ78eBmvzf+Om4ccwrE9VHxNgtFnQpE0NXv5Ju55/UtOPbQDY07qEXY4kkaU+EXS0IbiUsY+k8f+rbN46IIBKr4mNaIibSJppqL42oaSncwYcyz7NmsUdkiSZnTGL5JmHv73Ej5Ysp47z+lD3wP3DTscSUNK/CJpZNbidTz67hKGDTqIC4/oFHY4kqaU+EXSxLebtnH9c3M5pGNL7hnaV8XXZK8p8YukgYria2VlzsRRg1V8TWpFX+6KpIG7X1vI/G+38NiowXRr3zzscCTN6YxfpI57ac4qpny6gtEnduf0vvuFHY7UA0r8InVYQbT42pFd23LzkEPCDkfqCSV+kTqqovha8yYN+euIgSq+JgmjV5JIHeTu3DJ9PsvWl/Do8IF0aKXia5I4SvwiddA/PlrG6wu+4+Yhh3JMj3ZhhyP1jBK/SB2Tu2wj9/7zS047rCNXntg97HCkHlLiF6lD1heXcs0zeRzQuil/uqC/iq9JUgRK/GbW1sxmmlmJmS03sxFV9LvZzL4wsyIz+8bMbo5bvszMtptZcfT2diJ2QqQ+KCt3fvXsHDZt28X/jBzEvk1VfE2SI+gPuCYAO4GOwADgdTOb5+7x8+UakRm25gM9gLfNbKW7PxvT52x3f6d2YYvUP3/+VwEffbWBB4YdruJrklTVnvGbWXNgGHC7uxe7+4fAK8BF8X3d/UF3z3P33e6+mMh8u8clOmiR+ubdRWv563tfcf7gg/jFEZ3DDkfquSBDPb2A3e5eENM2D+izp5UsUkHqBCD+U8FUMys0s7fNrP8e1h9tZrlmlltYWBggTJH0tHLjNm54bh6H7d+Ku4f2DTscyQBBEn8LYGtc2xagZTXrjY9u/x8xbSOBrkAX4D3gLTNrXdnK7j7J3XPcPSc7OztAmCLpZ8euSPG1cnceGzWIrEYqvibJFyTxFwOt4tpaAUVVrWBmY4mM9Z/l7qUV7e7+kbtvd/dt7n4fsJnIpwKRjHTXawtZsGoLfzq/P13aqfiapEaQxF8ANDSznjFt/fnhEA4AZnYZcCtwqrt/W822ncgXwiIZZ0betzzz2QquOqkHP+mj4muSOtUmfncvAWYAd5lZczM7DjgXeDq+r5mNBO4FfuzuS+OWdTaz48yssZllRS/1bA98lIgdEUkni9Zs5bczF3B097b89096hR2OZJigP+C6GmgKrAOmAWPcPd/MTjCz4ph+9wDtgP/EXKv/WHRZS2AisAlYBZwOnOHuGxKxIyLpomjHLsZMyaNVViMeGT6Qhiq+JikW6Dp+d98IDK2k/QMiX/5WPO62h23kA/1qHqJI/eHu/Hr6fFZs3Ma0K46mQ0sVX5PU06mGSAo9/uE3vPHFGm45/RCO7NY27HAkQynxi6TIf5Zt5L43FjGkT0euOEHF1yQ8SvwiKVBYVMo1U/Po1KYpfzi/P5HfN4qEQ5OtiyTZ7rJyrps2hy3bdzH5v46kVZaKr0m4lPhFkuyhfxXwydIN/OG8fvQ+IP63kCKpp6EekSR6Z+Fa/mfW11x4RCfOz+kUdjgigBK/SNKs2LCNG5+fS58DWjH+nD3WNBRJKSV+kSTYsauMq5+ZDcDEkYNVfE3qFI3xiyTBna/m88Wqrfz94hw6t2sWdjgi36MzfpEEmz77W6Z9vpKrT+7Bab07hh2OyA8o8Ysk0JffbeW2mQs4pns7bvyxiq9J3aTEL5IgW3fsYsyU2ezbVMXXpG7TGL9IArg7v35hPis3befZ0UeT3bJJ2CGJVEmnJCIJ8PcPvuHN/DX85oxDOaKriq9J3abEL1JLny3dwP1vLuKMvvtx+fFVViYXqTMCJX4za2tmM82sxMyWm9mIKvqZmT1gZhuitwcsphqVmQ0ws9lmti3674AE7YdIKD5duoFrnplD57bNePC8fiq+Jmkh6Bn/BGAn0BEYCUw0s8p+ijiayIQt/YlMunI2cCWAmTUGXgamAG2AJ4GXo+0iaaVoxy5um7mACyd9SrPGDfjbRYNpqeJrkiaq/XLXzJoDw4C+7l4MfGhmrwAXEZlUPdYlwJ8qJlk3sz8BVwCPASdHn+8v7u7AI2b238ApwJuJ2Z3v++WT/2H5hm3J2LRkuPXFpWzZvotfHt+Nm35yCE0b65e5kj6CXNXTC9jt7gUxbfOAkyrp2ye6LLZfn5hl86NJv8L8aPsPEr+ZjSbyCYLOnTsHCPOHOrdtTuOG+hpDEq/PAa245NiuDOzcJuxQRGosSOJvAWyNa9tCZPL0yvpuievXIjrOH79sT9vB3ScBkwBycnK8sj7V+d3ZvfdmNRGRei3I6XAxEF9EvBVQFKBvK6A4epZfk+2IiEiSBEn8BUBDM+sZ09YfyK+kb350WWX98oF+9v3LHvpVsR0REUmSahO/u5cAM4C7zKy5mR0HnAs8XUn3p4AbzexAMzsAuAmYHF02CygDrjOzJmY2Ntr+bu12QUREaiLoN59XA02BdcA0YIy755vZCWZWHNPvb8CrwALgC+D1aBvuvpPIpZ4XA5uBy4Ch0XYREUkR+/5FNnVTTk6O5+bmhh2GiEhaMbPZ7p4T365rHUVEMowSv4hIhlHiFxHJMGkxxm9mhcDyvVy9PbA+geEkiuKqGcVVM4qrZuprXF3cPTu+MS0Sf22YWW5lX26ETXHVjOKqGcVVM5kWl4Z6REQyjBK/iEiGyYTEPynsAKqguGpGcdWM4qqZjIqr3o/xi4jI92XCGb+IiMRQ4hcRyTBK/CIiGaZeJf5ouefHzWy5mRWZ2VwzO6OadW4wszVmttXMnjCzJkmKbayZ5ZpZqZlNrqbvpWZWZmbFMbeTw44r2j9Vx6utmc00s5Lo/+eIPfQdb2a74o5X91TGYREPmNmG6O2BuLknEq4GsSXt+FTyXDV5nafktVSTuFL53os+X41yVqKOWb1K/ESmklxJZD7gfYFxwPNm1rWyzmY2hMiE8acCXYDuwJ1Jim01cA/wRMD+n7h7i5jbrLDjSvHxmgDsBDoCI4GJZtZnD/2fizteS1Mcx2giZcf7E5lg6GzgygTFUNvYIHnHJ16g11OKX0uB44pK1XsPapCzEnrM3L1e34hM6D6simXPAPfGPD4VWJPkeO4BJlfT51LgwxQfpyBxpeR4Ac2JJLReMW1PA/dX0X88MCXMOICPgdExjy8HPk3i/1dNYkvK8anN6ymM917AuFL+3qskhkpzViKPWX074/8eM+sI9KLq6R37APNiHs8DOppZu2THFsBAM1tvZgVmdruZNQw7IFJ3vHoBu929IO659nTGf7aZbTSzfDMbE0IclR2bPcWbytggOcenNvTeq0Q1OSthx6zeJn4zawRMBZ5090VVdGsBbIl5XHG/ZTJjC+B9oC/QARgGDAduDjWiiFQdrxbA1ri2LXt4nueBw4Bs4Argd2Y2PMVxVHZsWiRxnL8msSXr+NSG3ntxAuSshB2ztEr8ZjbLzLyK24cx/fYh8rF3JzC2yg1CMdAq5nHF/aJkxBWUuy9192/cvdzdFwB3AefVdDuJjovUHa/456l4rkqfx90Xuvtqdy9z94+Bh9mL41WJmsRR2bEp9uhn8iQIHFsSj09tJOS1lGiJeu/VVMCclbBjllaJ391Pdner4nY8RK6uAB4n8oXXMHfftYdN5hP5Mq5Cf2Ctu29IdFy15ECNzxyTEFeqjlcB0NDMesY9V1VDdj94CvbieFWiJnFUdmyCxpvs2OIl6vjURkJeSymQ9GNVg5yVsGOWVok/oIlEPtae7e7bq+n7FHC5mfU2s9ZEvlGfnIygzKyhmWUBDYAGZpZV1dihmZ0RHevDzA4FbgdeDjsuUnS83L0EmAHcZWbNzew44FwiZ0SV7cO5ZtbGIo4EriMBx6uGcTwF3GhmB5rZAcBNJOm1VNPYknV8KlOD11PK3ns1iSuV770YQXNW4o5ZmN9eJ/pG5BInB3YQ+VhUcRsZXd45+rhzzDo3AmuJjJf+A2iSpNjGR2OLvY2vLC7gj9GYSoClRD5uNgo7rhQfr7bAS9FjsAIYEbPsBCLDKBWPpwEborEuAq5LdhyVxGDAg8DG6O1BorWwkvh6Dxpb0o5P0NdTmK+lmsSVyvde9PmqzFnJPGYq0iYikmHq41CPiIjsgRK/iEiGUeIXEckwSvwiIhlGiV9EJMMo8YuIZBglfhGRDKPELyKSYf4/iyWZgwCdKHcAAAAASUVORK5CYII=&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;When we put a non-linear function in between two linear functions, then this
network is able to encode and express more complicated patterns. This is
basically all we're doing with deep learning: we stack these layers on to make
the functions more and more capable of modelling and representing complex things.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We can express the above simple network in PyTorch-specific code (functionally
it's the same):&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;simple_net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;At this point, training a model is similar to what we did last time round:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Learner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;simple_net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;opt_func&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fashion_mnist_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea &quot;&gt;

&lt;style&gt;
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
&lt;/style&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea &quot;&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: left;&quot;&gt;
      &lt;th&gt;epoch&lt;/th&gt;
      &lt;th&gt;train_loss&lt;/th&gt;
      &lt;th&gt;valid_loss&lt;/th&gt;
      &lt;th&gt;batch_accuracy&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.166980&lt;/td&gt;
      &lt;td&gt;0.079354&lt;/td&gt;
      &lt;td&gt;0.959000&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.087369&lt;/td&gt;
      &lt;td&gt;0.057471&lt;/td&gt;
      &lt;td&gt;0.961000&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.059999&lt;/td&gt;
      &lt;td&gt;0.050108&lt;/td&gt;
      &lt;td&gt;0.963500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.048050&lt;/td&gt;
      &lt;td&gt;0.046509&lt;/td&gt;
      &lt;td&gt;0.963500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0.041847&lt;/td&gt;
      &lt;td&gt;0.044165&lt;/td&gt;
      &lt;td&gt;0.964000&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.038310&lt;/td&gt;
      &lt;td&gt;0.042841&lt;/td&gt;
      &lt;td&gt;0.963500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;0.036514&lt;/td&gt;
      &lt;td&gt;0.041464&lt;/td&gt;
      &lt;td&gt;0.964500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0.034653&lt;/td&gt;
      &lt;td&gt;0.040640&lt;/td&gt;
      &lt;td&gt;0.964500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;0.033204&lt;/td&gt;
      &lt;td&gt;0.039827&lt;/td&gt;
      &lt;td&gt;0.965000&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0.032370&lt;/td&gt;
      &lt;td&gt;0.039344&lt;/td&gt;
      &lt;td&gt;0.965000&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;0.032060&lt;/td&gt;
      &lt;td&gt;0.038778&lt;/td&gt;
      &lt;td&gt;0.965000&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;0.031814&lt;/td&gt;
      &lt;td&gt;0.038854&lt;/td&gt;
      &lt;td&gt;0.965500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;0.031212&lt;/td&gt;
      &lt;td&gt;0.037872&lt;/td&gt;
      &lt;td&gt;0.965000&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;0.030837&lt;/td&gt;
      &lt;td&gt;0.037836&lt;/td&gt;
      &lt;td&gt;0.964500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;0.030193&lt;/td&gt;
      &lt;td&gt;0.037372&lt;/td&gt;
      &lt;td&gt;0.965000&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;15&lt;/td&gt;
      &lt;td&gt;0.030213&lt;/td&gt;
      &lt;td&gt;0.037250&lt;/td&gt;
      &lt;td&gt;0.965000&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;16&lt;/td&gt;
      &lt;td&gt;0.030058&lt;/td&gt;
      &lt;td&gt;0.036885&lt;/td&gt;
      &lt;td&gt;0.965000&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;17&lt;/td&gt;
      &lt;td&gt;0.029849&lt;/td&gt;
      &lt;td&gt;0.036862&lt;/td&gt;
      &lt;td&gt;0.965500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;0.029551&lt;/td&gt;
      &lt;td&gt;0.036518&lt;/td&gt;
      &lt;td&gt;0.965500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;0.029381&lt;/td&gt;
      &lt;td&gt;0.036236&lt;/td&gt;
      &lt;td&gt;0.966000&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;20&lt;/td&gt;
      &lt;td&gt;0.029358&lt;/td&gt;
      &lt;td&gt;0.035996&lt;/td&gt;
      &lt;td&gt;0.966500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;21&lt;/td&gt;
      &lt;td&gt;0.028987&lt;/td&gt;
      &lt;td&gt;0.036024&lt;/td&gt;
      &lt;td&gt;0.966000&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;22&lt;/td&gt;
      &lt;td&gt;0.028436&lt;/td&gt;
      &lt;td&gt;0.035731&lt;/td&gt;
      &lt;td&gt;0.966500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;23&lt;/td&gt;
      &lt;td&gt;0.028494&lt;/td&gt;
      &lt;td&gt;0.035813&lt;/td&gt;
      &lt;td&gt;0.967000&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;24&lt;/td&gt;
      &lt;td&gt;0.028174&lt;/td&gt;
      &lt;td&gt;0.035535&lt;/td&gt;
      &lt;td&gt;0.966000&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;25&lt;/td&gt;
      &lt;td&gt;0.028089&lt;/td&gt;
      &lt;td&gt;0.035361&lt;/td&gt;
      &lt;td&gt;0.966500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;26&lt;/td&gt;
      &lt;td&gt;0.027961&lt;/td&gt;
      &lt;td&gt;0.035035&lt;/td&gt;
      &lt;td&gt;0.966500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;27&lt;/td&gt;
      &lt;td&gt;0.027761&lt;/td&gt;
      &lt;td&gt;0.035248&lt;/td&gt;
      &lt;td&gt;0.966000&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;28&lt;/td&gt;
      &lt;td&gt;0.028016&lt;/td&gt;
      &lt;td&gt;0.035006&lt;/td&gt;
      &lt;td&gt;0.966500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;29&lt;/td&gt;
      &lt;td&gt;0.027430&lt;/td&gt;
      &lt;td&gt;0.034841&lt;/td&gt;
      &lt;td&gt;0.967500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;L&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recorder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;itemgot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;[&amp;lt;matplotlib.lines.Line2D at 0x150bb3490&amp;gt;]&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recorder&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;0.9674999713897705&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We don't actually emerge at the end of this with a vastly superior score to what
we had at the end of the last notebook, but this basis (the simple neural
network) has far more open vistas within which we can work and build upon.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Finally, to round out my understanding, I put together a little diagram showing
the various pieces that go into the &lt;code&gt;Learner&lt;/code&gt; class when we instantiate it,
adding some of the other concepts etc below it as I felt was appropriate. This
isn't a complete picture by any means, but I find it helpful to visualise how
things are layered and pieced together:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/nb_images/ml-training-big-picture/learner-abstractions.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="fastai" /><category term="computervision" /><category term="pytorch" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/ml-training-big-picture/neuralnetwork.png" /><media:content medium="image" url="https://mlops.systems/images/ml-training-big-picture/neuralnetwork.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Using the seven-step SGD process for Fashion MNIST</title><link href="https://mlops.systems/fastai/computervision/pytorch/2022/05/14/SGD-fashion-mnist.html" rel="alternate" type="text/html" title="Using the seven-step SGD process for Fashion MNIST" /><published>2022-05-14T00:00:00-05:00</published><updated>2022-05-14T00:00:00-05:00</updated><id>https://mlops.systems/fastai/computervision/pytorch/2022/05/14/SGD-fashion-mnist</id><content type="html" xml:base="https://mlops.systems/fastai/computervision/pytorch/2022/05/14/SGD-fashion-mnist.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-05-14-SGD-fashion-mnist.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;pip install -Uqq fastbook nbdev torch
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;fastbook&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fastbook&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setup_book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;fastai.vision.all&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;fastbook&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.transforms&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In &lt;a href=&quot;https://mlops.systems/fastai/computervision/pytorch/2022/05/13/sgd-whole-game.html&quot;&gt;the previous
post&lt;/a&gt;
I used the seven-step process to fit to an unknown function. The process as a
whole is fairly simple to get your head around, but there are a good few details
to keep track of along the way. This will continue to be the case as we get into
this walkthrough of how to do the same for the Fashion MNIST pullover vs dress data.&lt;/p&gt;
&lt;h1 id=&quot;Getting-our-data-into-the-right-format&quot;&gt;Getting our data into the right format&lt;a class=&quot;anchor-link&quot; href=&quot;#Getting-our-data-into-the-right-format&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;The first thing we need to handle is making sure our data is in the right
format, shape and so on. We begin by downloading our data and splitting the data
into training and test sets.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FashionMNIST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FashionMNIST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;training_dresses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;training_pullovers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_dresses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_pullovers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;training_dresses_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_dresses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;training_pullovers_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_pullovers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_dresses_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_dresses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_pullovers_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_pullovers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;training_dresses_tensor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_dresses_tensor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(torch.Size([6000, 28, 28]), torch.Size([1000, 28, 28]))&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_dresses_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_pullovers_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_dresses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_pullovers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;valid_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_dresses_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_pullovers_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;valid_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_dresses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_pullovers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(torch.Size([12000, 784]), torch.Size([12000, 1]))&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We transform our images tensors from matrices into vectors with all the values
one after another. We create a &lt;code&gt;train_y&lt;/code&gt; vector with our labels which we can use
to check how well we did with our predictions.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We create datasets out of our tensors. This means that we can feed our data into
our training functions in the way that is most convenient (i.e. an image is
paired with the correct label).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;valid_dset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valid_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Initialising-our-weights-and-bias&quot;&gt;Initialising our weights and bias&lt;a class=&quot;anchor-link&quot; href=&quot;#Initialising-our-weights-and-bias&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;As in the previous times where we've done this, we initialise our parameters or
weights with random values. This means that for every pixel represented in the
images, we'll start off with purely random values. We initialise our bias to a
random number as well.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;initialise_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_grad_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialise_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialise_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor([2.8681], grad_fn=&amp;lt;AddBackward0&amp;gt;)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Matrix-multiplication-to-calculate-our-predictions&quot;&gt;Matrix multiplication to calculate our predictions&lt;a class=&quot;anchor-link&quot; href=&quot;#Matrix-multiplication-to-calculate-our-predictions&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;We'll need to make many calculations like the one we just made, and luckily the
technique of matrix multiplication helps us with exactly the scenario we have:
we want to multiply the values of our image (laid out in a single vector) with
the weights and to add the bias.&lt;/p&gt;
&lt;p&gt;In Python, matrix multiplication is carried out with a simple &lt;code&gt;@&lt;/code&gt; operator, so
we can bring all of this together as a function:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;linear1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;@weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor([[  2.8681],
        [ -7.6810],
        [-17.5719],
        ...,
        [ -3.8665],
        [  2.0646],
        [ -2.5148]], grad_fn=&amp;lt;AddBackward0&amp;gt;)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We can check our accuracy for these predictions:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corrects&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_y&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;corrects&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;0.35324999690055847&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Our accuracy is pretty poor! A lot worse than even 50/50 luck which is what
you'd expect to get on average from a random set of initial weights. Apparently
we had a bad draw of luck this time round!&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;A-loss-function-to-evaluate-model-performance&quot;&gt;A loss function to evaluate model performance&lt;a class=&quot;anchor-link&quot; href=&quot;#A-loss-function-to-evaluate-model-performance&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;We now need a loss function which will tell us how well we are doing in our
predictions, and that can be used as part of the gradient calculations to let us
know (as we iterate) how to update our weights.&lt;/p&gt;
&lt;p&gt;The problem, especially in the data set we're working with, is that we have a
binary probability: either it's a dress or a pullover. Zero or one. Unlike in a
regression problem, or something similar, we don't have any smooth selection of
contiguous values that get predicted. We have zero or one.&lt;/p&gt;
&lt;p&gt;At this point we learn about the sigmoid function which is a way to reframe this
problem in a way that we can use to our advantage. The sigmoid function when
plotted looks like this:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plot_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Sigmoid&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This function, as you can see, takes any input value and squashes it down such
that the output value is between 0 and 1. It also has a smooth curve, all headed
in the same direction, between those values. This is ideal for our situation.
The first thing we must do as part of our loss function, therefore, is to apply
the sigmoid function to the inputs.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fashion_mnist_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This &lt;code&gt;torch.where(...)&lt;/code&gt; function is a handy way of iterating through all our
data, checking whether our target is 1 or not, then outputting the distance from
the correct prediction and calculating the mean of these predictions across the
entire dataset.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;DataLoaders-and-Datasets&quot;&gt;DataLoaders and Datasets&lt;a class=&quot;anchor-link&quot; href=&quot;#DataLoaders-and-Datasets&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;We've already created datasets for our training and validation data. The process
of iterating through our data, however, requires some thought as to how we'll do
it. Our options:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;we could iterate through the entire dataset, making the relevant loss and
gradient calculations and adjusting the weights but this might make the
process quite long, even though we'd benefit from the increased accuracy this
would bring since we'd be seeing the entire dataset each iteration.&lt;/li&gt;
&lt;li&gt;we could do our calculations after just seeing a single image, but then our
model would be over-influenced and perturbed by the fluctuations from image to
image. This also wouldn't be what we want.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In practice, we'll need to choose something in between. This is where
mini-batches or just 'batches' come in. These will be need to be large enough
(and randomly populated!) that our model can meaningfully learn from them, but
not so large that our process takes too long.&lt;/p&gt;
&lt;p&gt;Luckily, we have the abstraction of the &lt;code&gt;DataLoader&lt;/code&gt; which will create all our
randomly assigned batches for us.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;valid_dl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid_dset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Training-our-model&quot;&gt;Training our model&lt;a class=&quot;anchor-link&quot; href=&quot;#Training-our-model&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Now we can bring the whole process together and train our model:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialise_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;initialise_params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;calculate_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fashion_mnist_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train_epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# iterate over the training data, batch by batch&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_dl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# calculate the gradients&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;calculate_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# set the gradients to zero&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;param&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;batch_accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;correct&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_batch&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;correct&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;validate_epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;accs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valid_dl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;validate_epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;0.2173&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We start there, but now we can train and watch our accuracy improving:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;validate_epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;0.5001 0.5016 0.7994 0.9357 0.9481 0.9523 0.9537 0.9555 0.9572 0.9582 0.9578 0.9604 0.9608 0.9609 0.962 0.9611 0.9622 0.9626 0.9631 0.9625 0.963 0.963 0.9633 0.9638 0.964 0.9631 0.9638 0.9638 0.9643 0.9645 &lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We had 91% accuracy on our validation dataset &lt;a href=&quot;https://mlops.systems/fastai/computervision/pytorch/2022/05/11/fashion-mnist-pixel-similarity.html#Using-broadcasting-to-check-our-predictions-on-our-validation-data&quot;&gt;last time we tried
this&lt;/a&gt;
with pixel similarity.&lt;/p&gt;
&lt;p&gt;After 30 epochs of training with our new process we've achieved 96%, but we
could still do better! We'll tackle that in the next post.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Optimising-with-an-Optimiser&quot;&gt;Optimising with an Optimiser&lt;a class=&quot;anchor-link&quot; href=&quot;#Optimising-with-an-Optimiser&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Everything that we've been doing so far is so common that there is pre-built
functionality to handle all of the pieces.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;our &lt;code&gt;linear1&lt;/code&gt; function (which calculated predictions based on our weights and
biases) can be replaced with PyTorch's &lt;code&gt;nn.Linear&lt;/code&gt; module. Actually,
&lt;code&gt;nn.Linear&lt;/code&gt; does the same thing as our &lt;code&gt;initialise_params&lt;/code&gt; and our &lt;code&gt;linear1&lt;/code&gt;
function combined.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Our PyTorch module carries an internal representation of our weights and our biases:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linear_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(torch.Size([1, 784]), torch.Size([1]))&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;an optimiser bundles the step functionality and the &lt;code&gt;zero_grad_&lt;/code&gt;
functionality. In the book we see how to create our own very basic optimiser,
but &lt;code&gt;fastai&lt;/code&gt; provides the basic &lt;code&gt;SGD&lt;/code&gt; class which we can use that handles
these same behaviours.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We'll need to amend our training function a little to take this into account:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train_epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_batch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_dl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;calculate_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;train_epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;validate_epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot; &amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linear_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;0.96 0.9642 0.966 0.9659 0.9663 0.9672 0.9677 0.9668 0.9678 0.9684 0.9681 0.9674 0.9681 0.9671 0.9678 0.9677 0.9684 0.968 0.9687 0.9677 0.9681 0.968 0.9693 0.9684 0.968 0.9686 0.9688 0.9693 0.9698 0.9697 &lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Some-extra-fastai-abstractions&quot;&gt;Some extra &lt;code&gt;fastai&lt;/code&gt; abstractions&lt;a class=&quot;anchor-link&quot; href=&quot;#Some-extra-fastai-abstractions&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;code&gt;fastai&lt;/code&gt; handles so much of this for us all, such that the &lt;code&gt;Learner&lt;/code&gt; is actually
the thing we can use to get all of the above logic built in.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;Learner&lt;/code&gt; takes all of the pieces that we've spent the last few blogs
creating:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;code&gt;DataLoaders&lt;/code&gt; (iterators providing the data in batches, in the right
format with paired &lt;code&gt;x&lt;/code&gt; and &lt;code&gt;y&lt;/code&gt; values)&lt;/li&gt;
&lt;li&gt;the model itself (our function that we're trying to optimise)&lt;/li&gt;
&lt;li&gt;the optimisation function (which receives our weights and bias parameters as
well as the learning rate)&lt;/li&gt;
&lt;li&gt;the loss function&lt;/li&gt;
&lt;li&gt;any optional metrics we want printed&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dls&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoaders&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valid_dl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Learner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;opt_func&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_func&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fashion_mnist_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea &quot;&gt;

&lt;style&gt;
    /* Turns off some styling */
    progress {
        /* gets rid of default border in Firefox and Opera. */
        border: none;
        /* Needs to be in here for Safari polyfill so background images work as expected. */
        background-size: auto;
    }
    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
        background: #F44336;
    }
&lt;/style&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea &quot;&gt;
&lt;table border=&quot;1&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: left;&quot;&gt;
      &lt;th&gt;epoch&lt;/th&gt;
      &lt;th&gt;train_loss&lt;/th&gt;
      &lt;th&gt;valid_loss&lt;/th&gt;
      &lt;th&gt;batch_accuracy&lt;/th&gt;
      &lt;th&gt;time&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.063289&lt;/td&gt;
      &lt;td&gt;0.045487&lt;/td&gt;
      &lt;td&gt;0.963500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.044268&lt;/td&gt;
      &lt;td&gt;0.042236&lt;/td&gt;
      &lt;td&gt;0.964500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.037112&lt;/td&gt;
      &lt;td&gt;0.040228&lt;/td&gt;
      &lt;td&gt;0.965500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.034010&lt;/td&gt;
      &lt;td&gt;0.038743&lt;/td&gt;
      &lt;td&gt;0.967000&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0.032013&lt;/td&gt;
      &lt;td&gt;0.038781&lt;/td&gt;
      &lt;td&gt;0.966000&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.030633&lt;/td&gt;
      &lt;td&gt;0.037635&lt;/td&gt;
      &lt;td&gt;0.966500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;0.030458&lt;/td&gt;
      &lt;td&gt;0.037530&lt;/td&gt;
      &lt;td&gt;0.967500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0.029747&lt;/td&gt;
      &lt;td&gt;0.036593&lt;/td&gt;
      &lt;td&gt;0.967000&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;0.029511&lt;/td&gt;
      &lt;td&gt;0.036479&lt;/td&gt;
      &lt;td&gt;0.967500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0.029305&lt;/td&gt;
      &lt;td&gt;0.035645&lt;/td&gt;
      &lt;td&gt;0.967500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;0.028643&lt;/td&gt;
      &lt;td&gt;0.035400&lt;/td&gt;
      &lt;td&gt;0.966500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;0.028562&lt;/td&gt;
      &lt;td&gt;0.035477&lt;/td&gt;
      &lt;td&gt;0.966500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;0.028788&lt;/td&gt;
      &lt;td&gt;0.035191&lt;/td&gt;
      &lt;td&gt;0.968000&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;13&lt;/td&gt;
      &lt;td&gt;0.028261&lt;/td&gt;
      &lt;td&gt;0.034843&lt;/td&gt;
      &lt;td&gt;0.968000&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;14&lt;/td&gt;
      &lt;td&gt;0.028172&lt;/td&gt;
      &lt;td&gt;0.034883&lt;/td&gt;
      &lt;td&gt;0.968500&lt;/td&gt;
      &lt;td&gt;00:00&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;So there we have it. We learned how to create a linear learner. Obviously 96.8%
accuracy is pretty good, but it could be better. Next time we're going to add
the final touches to this process by creating a neural network, adding layers of
nonlinearity to ensure our function can fit the complex patterns in our data.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="fastai" /><category term="computervision" /><category term="pytorch" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/fashion-mnist/fashion-mnist-small.png" /><media:content medium="image" url="https://mlops.systems/images/fashion-mnist/fashion-mnist-small.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Stochastic Gradient Descent: a mini-example of the whole game</title><link href="https://mlops.systems/fastai/computervision/pytorch/2022/05/13/sgd-whole-game.html" rel="alternate" type="text/html" title="Stochastic Gradient Descent: a mini-example of the whole game" /><published>2022-05-13T00:00:00-05:00</published><updated>2022-05-13T00:00:00-05:00</updated><id>https://mlops.systems/fastai/computervision/pytorch/2022/05/13/sgd-whole-game</id><content type="html" xml:base="https://mlops.systems/fastai/computervision/pytorch/2022/05/13/sgd-whole-game.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-05-13-sgd-whole-game.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;pip install -Uqq fastbook nbdev

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;fastai.vision.all&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;An-End-To-End-Example-of-SGD&quot;&gt;An End-To-End Example of SGD&lt;a class=&quot;anchor-link&quot; href=&quot;#An-End-To-End-Example-of-SGD&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;In &lt;a href=&quot;https://mlops.systems/fastai/computervision/pytorch/2022/05/12/seven-steps-gradient-calculations.html&quot;&gt;the previous post&lt;/a&gt; we learned about the high-level seven-step process that we
can use to update the weights of our function or model. We can now replicate
that process and try it out on a very simple function to show how it works from
start to finish. We will iterate a few times so that the gradual improvement is
visible and clear.&lt;/p&gt;
&lt;p&gt;Note this example is pretty closely taken from chapter 4 of the fastai fastbook,
available for free
&lt;a href=&quot;https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Let's assume that we're plotting out the speed a rollercoaster is running at as
it climbs up to one of its peaks, slowing as it reaches the top but then
accelerating as it passes over the peak.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# setup twenty values to represent time passing&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,
        14., 15., 16., 17., 18., 19.])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;speed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.75&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;9.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;speed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The curve is overall distinguishable, but the noise makes it non-trivial to just
make a prediction.&lt;/p&gt;
&lt;p&gt;If we think of this function as a quadratic equation of the form &lt;code&gt;a * (time **
2) + (b * time) + c&lt;/code&gt;, we can boil our problem down to the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for any value &lt;code&gt;time&lt;/code&gt;, we want to be able to find the &lt;code&gt;speed&lt;/code&gt; value&lt;/li&gt;
&lt;li&gt;given that we've stated that the equation is a quadratic equation of the form
stated above, we basically just have to find the three values &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt; and
&lt;code&gt;c&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we were doing this manually, we could manipulate each value and find the
perfect combination such that we had the best possible prediction. (If we throw
in loss calculation as part of this process, then we could say that we'd know
that we have the right values for &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; when we have the lowest loss.)&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# for continuous values, the mean squared error is a good choice&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;mse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predictions&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Now we can go through the seven-step process applying what we already know we
need to do.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;1.-Initialise-the-parameters&quot;&gt;1. Initialise the parameters&lt;a class=&quot;anchor-link&quot; href=&quot;#1.-Initialise-the-parameters&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We begin with random values. We also make sure to set up our Tensor so that
we're able to calculate the gradients.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_grad_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;2.-Calculate-the-predictions&quot;&gt;2. Calculate the predictions&lt;a class=&quot;anchor-link&quot; href=&quot;#2.-Calculate-the-predictions&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We make the calculations by passing our parameter values into our function &lt;code&gt;f&lt;/code&gt;.
We can visualise what our predictions would look like with those parameters.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;show_preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;speed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scatter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to_np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;red&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;show_preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;You can see that there's a decent amount of difference between the curve
denoting our predictions for the params (in red) and the actual function (in blue).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.-Calculate-the-loss&quot;&gt;3. Calculate the loss&lt;a class=&quot;anchor-link&quot; href=&quot;#3.-Calculate-the-loss&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We use the mean squared error as a way of calculating our loss.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;speed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor(200.6502, grad_fn=&amp;lt;SqrtBackward0&amp;gt;)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This number is a measure of how far off our predictions are from the actual
values. We want to improve this loss and drive it down even further, and for
that we'll need the gradients.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;4.-Calculate-the-gradient&quot;&gt;4. Calculate the gradient&lt;a class=&quot;anchor-link&quot; href=&quot;#4.-Calculate-the-gradient&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;As described &lt;a href=&quot;https://mlops.systems/fastai/computervision/pytorch/2022/05/12/seven-steps-gradient-calculations.html&quot;&gt;in the previous
post&lt;/a&gt;,
we use PyTorch's inbuilt ability to calculate gradients.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor([166.3746,  10.6914,   0.6876])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We can update our parameters based on the learning rate. For our purposes here
we can choose a really small learning rate: 0.00001 or 1e-5. This is what the
values of our parameters would look like after that operation:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.00001&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor([ 1.2895e-05,  5.8427e-06, -2.3174e-06], grad_fn=&amp;lt;MulBackward0&amp;gt;)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor([ 1.2895,  0.5843, -0.2317], requires_grad=True)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;5.-Step-our-weights&quot;&gt;5. Step our weights&lt;a class=&quot;anchor-link&quot; href=&quot;#5.-Step-our-weights&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We can step our parameters using the formula &lt;a href=&quot;https://mlops.systems/fastai/computervision/pytorch/2022/05/12/seven-steps-gradient-calculations.html&quot;&gt;previously
described&lt;/a&gt;:
multiply the learning rate by the gradients.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.00001&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor([166.3746,  10.6914,   0.6876])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We can visualise whether this has improved our function's curve or not:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;speed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor(200.3723, grad_fn=&amp;lt;SqrtBackward0&amp;gt;)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show_preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Our loss has gone from 268.4112 to 268.1312. An improvement, but it feels like a
small improvement!&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;6.-Repeat-and-iterate&quot;&gt;6. Repeat and iterate&lt;a class=&quot;anchor-link&quot; href=&quot;#6.-Repeat-and-iterate&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;To save ourselves some time, we can create a function that will help us in
iterating through and repeating the above steps:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;repeat_sgd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;speed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kc&quot;&gt;None&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;repeat_sgd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;200.3722686767578
199.81639099121094
199.53848266601562
199.2605743408203
198.98269653320312
198.70480346679688
198.4269561767578
198.14910888671875
197.87124633789062
197.59344482421875
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We see that our loss is going down, so everything is improving as we'd hope. The
progress seems slow, but it's progress nonetheless. I imagine we could
increasing the learning rate to make the loss go down faster.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;7.-Stop-when-we've-iterated-enough&quot;&gt;7. Stop when we've iterated enough&lt;a class=&quot;anchor-link&quot; href=&quot;#7.-Stop-when-we've-iterated-enough&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We've only iterated a few times here, but really what we'd want to do is keep
going until we reached our stopping point (either we've taken too long or our
model is 'good enough' for our needs).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Takeaways&quot;&gt;Takeaways&lt;a class=&quot;anchor-link&quot; href=&quot;#Takeaways&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;This remains a fairly simple example. We are optimising our three parameter
values &lt;code&gt;a&lt;/code&gt;, &lt;code&gt;b&lt;/code&gt; and &lt;code&gt;c&lt;/code&gt; and the calculations are pretty easy to visualise. We've
seen how our loss function increases as we repeat the steps of gradient and loss
calculation, 'stepping' and so on.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="fastai" /><category term="computervision" /><category term="pytorch" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/ml-training-big-picture/sgd-mini.png" /><media:content medium="image" url="https://mlops.systems/images/ml-training-big-picture/sgd-mini.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Some foundations for machine learning with PyTorch</title><link href="https://mlops.systems/fastai/computervision/pytorch/2022/05/12/seven-steps-gradient-calculations.html" rel="alternate" type="text/html" title="Some foundations for machine learning with PyTorch" /><published>2022-05-12T00:00:00-05:00</published><updated>2022-05-12T00:00:00-05:00</updated><id>https://mlops.systems/fastai/computervision/pytorch/2022/05/12/seven-steps-gradient-calculations</id><content type="html" xml:base="https://mlops.systems/fastai/computervision/pytorch/2022/05/12/seven-steps-gradient-calculations.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-05-12-seven-steps-gradient-calculations.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;pip install -Uqq fastbook nbdev torch
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;fastbook&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fastbook&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setup_book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;fastai.vision.all&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;fastbook&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.transforms&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In the &lt;a href=&quot;https://mlops.systems/fastai/computervision/pytorch/2022/05/12/fashion-mnist-pixel-similarity.html&quot;&gt;previous post&lt;/a&gt; I showed a naive approach to calculating the similarity or difference between images, and how that could be used to create a function that did pretty well at estimating whether any particular image was a pullover or a dress.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/fastai/fastbook/blob/master/04_mnist_basics.ipynb&quot;&gt;Chapter 4 of the fastbook&lt;/a&gt; then takes us on a journey showing a smarter approach where the computer can make even better estimations and predictions. The broad strokes of this approach are simple to grasp, but of course the individual details are where the nuances of machine learning are to be found.&lt;/p&gt;
&lt;h1 id=&quot;Seven-steps-for-a-machine-to-learn&quot;&gt;Seven steps for a machine to learn&lt;a class=&quot;anchor-link&quot; href=&quot;#Seven-steps-for-a-machine-to-learn&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;/images/copied_from_nb/nb_images/ml-training-big-picture/seven-steps.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Initialise a set of weights&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We use random values start with. There could potentially be more elaborate /
fancy ways to calculate some starting values that are closer to our end goal,
but in practice it is unnecessary because we have a process that we are going to
use to update our values.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Use the weights to make a prediction&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We check whether the sets of weights we have currently set as part of our
function are the right ones. We check the prediction (i.e. what came out the
other end of our model / function) and get a sense of how well our model did.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Loss: see how well we did with our predictions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We calculate the loss for our data. How well did we do at predicting what we
were trying to predict? We use a number that will be small if our function is
doing well. (Note: this is just a convention and otherwise there's no special
reason for this.)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. Calculate the gradients across all the weights&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are calculating the gradient for lots of numbers, not just a single value.
For every stage, we get back the gradient for all of the numbers. This is done
sequentially, where we calculate the gradient for one weight / number, keeping
all the other numbers constant, and then we repeat this for all the other
weights.&lt;/p&gt;
&lt;p&gt;Gradient calculation relates to calculating derivatives and with this we are
stepping firmly into the space of calculus. I don't fully understand how all
this works, but intuitively, the important thing to know is this: we are
calculating the change of the value, not the value itself. I.e. we want to know
how things will change (by how much, and in what direction) if we shift this
value slightly.&lt;/p&gt;
&lt;p&gt;Note that the process of calculating the gradients is a performance
optimisation. We could just as well have done this with a (slower) manual
process where we adjust a little bit each time. With the gradient calculations
we can take bigger steps in the direction we want, with more precision guiding
our guesses about the direction and distance we want to go.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5. 'Step': Update the weights&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is where we increase or decrease our own weights by a small amount and see
whether the loss goes up or down. As hinted in step 4, we use calculus to figure
out:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;which direction to go in&lt;/li&gt;
&lt;li&gt;how much we should increase or decrease our own weights&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;6. Repeat starting at step 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is an iterative process.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;7. Iterate until we decide to stop&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are various criteria determining when we should stop. Some possible
stopping points might include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;when the model is 'good enough' for our use case&lt;/li&gt;
&lt;li&gt;when we have run out of time (or money!)&lt;/li&gt;
&lt;li&gt;when the accuracy starts getting worse (i.e. the model isn't performing as well)&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Calculating-gradients-with-PyTorch&quot;&gt;Calculating gradients with PyTorch&lt;a class=&quot;anchor-link&quot; href=&quot;#Calculating-gradients-with-PyTorch&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;For those of us who don't bring a strong mathematics foundation into this space,
the mention of calculus, derivatives and gradients isn't especially reassuring,
but rest assured that PyTorch can help us out during this process.&lt;/p&gt;
&lt;p&gt;There are three main parts to using PyTorch to calculate gradients (i.e. step
four of the seven steps listed above.)&lt;/p&gt;
&lt;h2 id=&quot;1.-Setup:-add-.requires_grad_()-to-a-tensor&quot;&gt;1. Setup: add &lt;code&gt;.requires_grad_()&lt;/code&gt; to a tensor&lt;a class=&quot;anchor-link&quot; href=&quot;#1.-Setup:-add-.requires_grad_()-to-a-tensor&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;For any Tensor where we know we're going to want to calculate the gradients of
values, we call &lt;code&gt;.require_grad()&lt;/code&gt; on that Tensor.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_grad_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;y_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;y_tensor&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor(9., grad_fn=&amp;lt;PowBackward0&amp;gt;)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Here we can see that 3 squared is indeed 9, and we can see the &lt;code&gt;grad_fn&lt;/code&gt; as part
of the Tensor.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;2.-Use-.backward()-to-calculate-the-gradient&quot;&gt;2. Use &lt;code&gt;.backward()&lt;/code&gt; to calculate the gradient&lt;a class=&quot;anchor-link&quot; href=&quot;#2.-Use-.backward()-to-calculate-the-gradient&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;This actually refers to backpropagation, something which is explained much later
in the book. This step is also known as the 'backward pass'. Note, that this is
again another piece of jargon that we just have to learn. In reality this method
might as well have been called &lt;code&gt;.calculate_gradients()&lt;/code&gt;.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_tensor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;3.-Access-the-gradient-via-the-.grad-attribute&quot;&gt;3. Access the gradient via the &lt;code&gt;.grad&lt;/code&gt; attribute&lt;a class=&quot;anchor-link&quot; href=&quot;#3.-Access-the-gradient-via-the-.grad-attribute&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;We view the gradient by checking this &lt;code&gt;.grad&lt;/code&gt; attribute.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_tensor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor(6.)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I can't explain why this is the case, since I've never learned how to calculate
gradients or derivatives (or anything about calculus, for that matter!) but in
any case it's not really important.&lt;/p&gt;
&lt;p&gt;Note that we can do this whole process over Tensors that are more complex than
illustrated in the above simple example:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;complex_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;12.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_grad_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;complex_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;complex_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;complex_y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;complex_x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor([ 6., 10., 24.])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Something else I discovered while doing this was that gradients can only be
calculated on floating point values, so this is why when we create &lt;code&gt;x_tensor&lt;/code&gt;
and &lt;code&gt;complex_x&lt;/code&gt; we create them with floating point values (&lt;code&gt;3.&lt;/code&gt; etc) instead of
just integers. In reality, I think there will be some kind of normalisation of
our values as part of the process, so they would probably &lt;em&gt;already&lt;/em&gt; be floats,
but it's worth noting.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;'Stepping':-using-learning-rates-to-figure-out-how-much-to-step&quot;&gt;'Stepping': using learning rates to figure out how much to step&lt;a class=&quot;anchor-link&quot; href=&quot;#'Stepping':-using-learning-rates-to-figure-out-how-much-to-step&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Now that we know how to calculate gradients, the key question for the fifth step
in our seven-step process is the following:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&quot;How much should we shift our values based on what we get back from our
gradient calculations?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We call this amount the 'learning rate', and it is usually a value between 0.001
and 0.1. Very small, in other words :)
We adjust our weights / parameters by this basic equation:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weights&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This process is called &quot;stepping the parameters&quot; and it uses an optimisation
step.&lt;/p&gt;
&lt;p&gt;The learning rate shouldn't be too high, else our loss can get higher / worse or
otherwise we can just bounce around within the boundaries of our function
without ever reaching the optimum (== lowest) loss.&lt;/p&gt;
&lt;p&gt;At the same time, we shouldn't use a very tiny learning rate (i.e. even tinier
than the 0.001-0.1 mentioned above) since then the process will take a really
long time and while we might reach the optimum loss at some point, it might not
be the fastest way to get there.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Takeaways-from-the-seven-step-process&quot;&gt;Takeaways from the seven-step process&lt;a class=&quot;anchor-link&quot; href=&quot;#Takeaways-from-the-seven-step-process&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Most of this makes a lot of intuitive sense to me. The parts that don't are what
is going on with the gradient calculations and finding of derivatives and so on.
For now, it appears that I can get away without understanding precisely how that
works. It is enough to appreciate that we have a way to make these calculations,
and those calculations are optimised for us&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="fastai" /><category term="computervision" /><category term="pytorch" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/ml-training-big-picture/seven-steps.png" /><media:content medium="image" url="https://mlops.systems/images/ml-training-big-picture/seven-steps.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset</title><link href="https://mlops.systems/fastai/computervision/pytorch/2022/05/11/fashion-mnist-pixel-similarity.html" rel="alternate" type="text/html" title="A dress is not a pullover: learning about PyTorch Tensors and pixel similarity using the Fashion MNIST dataset" /><published>2022-05-11T00:00:00-05:00</published><updated>2022-05-11T00:00:00-05:00</updated><id>https://mlops.systems/fastai/computervision/pytorch/2022/05/11/fashion-mnist-pixel-similarity</id><content type="html" xml:base="https://mlops.systems/fastai/computervision/pytorch/2022/05/11/fashion-mnist-pixel-similarity.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-05-11-fashion-mnist-pixel-similarity.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;pip install -Uqq fastbook nbdev torch
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;fastbook&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fastbook&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setup_book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;fastai.vision.all&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;fastbook&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dataset&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.transforms&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;image&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Greys&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Get-the-data&quot;&gt;Get the data&lt;a class=&quot;anchor-link&quot; href=&quot;#Get-the-data&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;First thing to do: we'll download the MNIST dataset for us to use as our example.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;untar_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;URLs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MNIST_SAMPLE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BASE_PATH&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(#3) [Path(&amp;#39;valid&amp;#39;),Path(&amp;#39;labels.csv&amp;#39;),Path(&amp;#39;train&amp;#39;)]&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;train&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(#2) [Path(&amp;#39;train/7&amp;#39;),Path(&amp;#39;train/3&amp;#39;)]&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;3 and 7 are the labels or targets of our dataset. We are trying to predict whether, given a particular image, we are looking at a 3 or a 7.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;threes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;train&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;3&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sevens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&amp;#39;train&amp;#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;7&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;threes&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(#6131) [Path(&amp;#39;train/3/10.png&amp;#39;),Path(&amp;#39;train/3/10000.png&amp;#39;),Path(&amp;#39;train/3/10011.png&amp;#39;),Path(&amp;#39;train/3/10031.png&amp;#39;),Path(&amp;#39;train/3/10034.png&amp;#39;),Path(&amp;#39;train/3/10042.png&amp;#39;),Path(&amp;#39;train/3/10052.png&amp;#39;),Path(&amp;#39;train/3/1007.png&amp;#39;),Path(&amp;#39;train/3/10074.png&amp;#39;),Path(&amp;#39;train/3/10091.png&amp;#39;)...]&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im3_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;im3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im3_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;im3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea output_execute_result&quot;&gt;
&lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA9ElEQVR4nM3Or0sDcRjH8c/pgrfBVBjCgibThiKIyTWbWF1bORhGwxARxH/AbtW0JoIGwzXRYhJhtuFY2q1ocLgbe3sGReTuuWbwkx6+r+/zQ/pncX6q+YOldSe6nG3dn8U/rTQ70L8FCGJUewvxl7NTmezNb8xIkvKugr1HSeMP6SrWOVkoTEuSyh0Gm2n3hQyObMnXnxkempRrvgD+gokzwxFAr7U7YXHZ8x4A/Dl7rbu6D2yl3etcw/F3nZgfRVI7rXM7hMUUqzzBec427x26rkmlkzEEa4nnRqnSOH2F0UUx0ePzlbuqMXAHgN6GY9if5xP8dmtHFfwjuQAAAABJRU5ErkJggg==&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Now we're getting a slice of the image to see how it is represented underneath as numbers.&lt;/p&gt;
&lt;p&gt;The first index value is the rows we want, and then we get the columns.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;array([[  0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,  29],
       [  0,   0,   0,  48, 166, 224],
       [  0,  93, 244, 249, 253, 187],
       [  0, 107, 253, 253, 230,  48],
       [  0,   3,  20,  20,  15,   0]], dtype=uint8)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
       [  0,   0,   0,   0,   0,  29, 150, 195, 254, 255, 254],
       [  0,   0,   0,  48, 166, 224, 253, 253, 234, 196, 253],
       [  0,  93, 244, 249, 253, 187,  46,  10,   8,   4,  10],
       [  0, 107, 253, 253, 230,  48,   0,   0,   0,   0,   0],
       [  0,   3,  20,  20,  15,   0,   0,   0,   0,   0,  43]], dtype=uint8)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We can also have the same as a tensor. This is a PyTorch object which will allow us to get the benefits of everything PyTorch has to offer (plus speed optimisation if we use a GPU).&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],
        [  0,   0,   0,   0,   0,  29, 150, 195, 254, 255, 254],
        [  0,   0,   0,  48, 166, 224, 253, 253, 234, 196, 253],
        [  0,  93, 244, 249, 253, 187,  46,  10,   8,   4,  10],
        [  0, 107, 253, 253, 230,  48,   0,   0,   0,   0,   0],
        [  0,   3,  20,  20,  15,   0,   0,   0,   0,   0,  43]], dtype=torch.uint8)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;It basically looks the same except for the name of the object at this moment.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;If we plot out the values of a different slice we can visualise how the numbers are used to output the actual image.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im3_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im3_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;22&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_properties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;font-size&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;6pt&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;background_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;Greys&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;


&lt;div class=&quot;output_html rendered_html output_subarea output_execute_result&quot;&gt;
&lt;style type=&quot;text/css&quot;&gt;
#T_09c33_row0_col0, #T_09c33_row0_col1, #T_09c33_row0_col2, #T_09c33_row0_col3, #T_09c33_row0_col4, #T_09c33_row0_col5, #T_09c33_row0_col6, #T_09c33_row0_col7, #T_09c33_row0_col8, #T_09c33_row0_col9, #T_09c33_row0_col10, #T_09c33_row0_col11, #T_09c33_row0_col12, #T_09c33_row0_col13, #T_09c33_row0_col14, #T_09c33_row0_col15, #T_09c33_row0_col16, #T_09c33_row0_col17, #T_09c33_row1_col0, #T_09c33_row1_col1, #T_09c33_row1_col2, #T_09c33_row1_col3, #T_09c33_row1_col4, #T_09c33_row1_col15, #T_09c33_row1_col16, #T_09c33_row1_col17, #T_09c33_row2_col0, #T_09c33_row2_col1, #T_09c33_row2_col2, #T_09c33_row2_col15, #T_09c33_row2_col16, #T_09c33_row2_col17, #T_09c33_row3_col0, #T_09c33_row3_col15, #T_09c33_row3_col16, #T_09c33_row3_col17, #T_09c33_row4_col0, #T_09c33_row4_col6, #T_09c33_row4_col7, #T_09c33_row4_col8, #T_09c33_row4_col9, #T_09c33_row4_col10, #T_09c33_row4_col15, #T_09c33_row4_col16, #T_09c33_row4_col17, #T_09c33_row5_col0, #T_09c33_row5_col5, #T_09c33_row5_col6, #T_09c33_row5_col7, #T_09c33_row5_col8, #T_09c33_row5_col9, #T_09c33_row5_col15, #T_09c33_row5_col16, #T_09c33_row5_col17, #T_09c33_row6_col0, #T_09c33_row6_col1, #T_09c33_row6_col2, #T_09c33_row6_col3, #T_09c33_row6_col4, #T_09c33_row6_col5, #T_09c33_row6_col6, #T_09c33_row6_col7, #T_09c33_row6_col8, #T_09c33_row6_col9, #T_09c33_row6_col14, #T_09c33_row6_col15, #T_09c33_row6_col16, #T_09c33_row6_col17, #T_09c33_row7_col0, #T_09c33_row7_col1, #T_09c33_row7_col2, #T_09c33_row7_col3, #T_09c33_row7_col4, #T_09c33_row7_col5, #T_09c33_row7_col6, #T_09c33_row7_col13, #T_09c33_row7_col14, #T_09c33_row7_col15, #T_09c33_row7_col16, #T_09c33_row7_col17, #T_09c33_row8_col0, #T_09c33_row8_col1, #T_09c33_row8_col2, #T_09c33_row8_col3, #T_09c33_row8_col4, #T_09c33_row8_col13, #T_09c33_row8_col14, #T_09c33_row8_col15, #T_09c33_row8_col16, #T_09c33_row8_col17, #T_09c33_row9_col0, #T_09c33_row9_col1, #T_09c33_row9_col2, #T_09c33_row9_col3, #T_09c33_row9_col4, #T_09c33_row9_col16, #T_09c33_row9_col17, #T_09c33_row10_col0, #T_09c33_row10_col1, #T_09c33_row10_col2, #T_09c33_row10_col3, #T_09c33_row10_col4, #T_09c33_row10_col5, #T_09c33_row10_col6, #T_09c33_row10_col17 {
  font-size: 6pt;
  background-color: #ffffff;
  color: #000000;
}
#T_09c33_row1_col5 {
  font-size: 6pt;
  background-color: #efefef;
  color: #000000;
}
#T_09c33_row1_col6, #T_09c33_row1_col13 {
  font-size: 6pt;
  background-color: #7c7c7c;
  color: #f1f1f1;
}
#T_09c33_row1_col7 {
  font-size: 6pt;
  background-color: #4a4a4a;
  color: #f1f1f1;
}
#T_09c33_row1_col8, #T_09c33_row1_col9, #T_09c33_row1_col10, #T_09c33_row2_col5, #T_09c33_row2_col6, #T_09c33_row2_col7, #T_09c33_row2_col11, #T_09c33_row2_col12, #T_09c33_row2_col13, #T_09c33_row3_col4, #T_09c33_row3_col12, #T_09c33_row3_col13, #T_09c33_row4_col1, #T_09c33_row4_col2, #T_09c33_row4_col3, #T_09c33_row4_col12, #T_09c33_row4_col13, #T_09c33_row5_col12, #T_09c33_row6_col11, #T_09c33_row9_col11, #T_09c33_row10_col11, #T_09c33_row10_col12, #T_09c33_row10_col13, #T_09c33_row10_col14, #T_09c33_row10_col15, #T_09c33_row10_col16 {
  font-size: 6pt;
  background-color: #000000;
  color: #f1f1f1;
}
#T_09c33_row1_col11 {
  font-size: 6pt;
  background-color: #606060;
  color: #f1f1f1;
}
#T_09c33_row1_col12 {
  font-size: 6pt;
  background-color: #4d4d4d;
  color: #f1f1f1;
}
#T_09c33_row1_col14 {
  font-size: 6pt;
  background-color: #bbbbbb;
  color: #000000;
}
#T_09c33_row2_col3 {
  font-size: 6pt;
  background-color: #e4e4e4;
  color: #000000;
}
#T_09c33_row2_col4, #T_09c33_row8_col6 {
  font-size: 6pt;
  background-color: #6b6b6b;
  color: #f1f1f1;
}
#T_09c33_row2_col8, #T_09c33_row2_col14, #T_09c33_row3_col14 {
  font-size: 6pt;
  background-color: #171717;
  color: #f1f1f1;
}
#T_09c33_row2_col9, #T_09c33_row3_col11 {
  font-size: 6pt;
  background-color: #4b4b4b;
  color: #f1f1f1;
}
#T_09c33_row2_col10, #T_09c33_row7_col10, #T_09c33_row8_col8, #T_09c33_row8_col10, #T_09c33_row9_col8, #T_09c33_row9_col10 {
  font-size: 6pt;
  background-color: #010101;
  color: #f1f1f1;
}
#T_09c33_row3_col1 {
  font-size: 6pt;
  background-color: #272727;
  color: #f1f1f1;
}
#T_09c33_row3_col2 {
  font-size: 6pt;
  background-color: #0a0a0a;
  color: #f1f1f1;
}
#T_09c33_row3_col3 {
  font-size: 6pt;
  background-color: #050505;
  color: #f1f1f1;
}
#T_09c33_row3_col5 {
  font-size: 6pt;
  background-color: #333333;
  color: #f1f1f1;
}
#T_09c33_row3_col6 {
  font-size: 6pt;
  background-color: #e6e6e6;
  color: #000000;
}
#T_09c33_row3_col7, #T_09c33_row3_col10 {
  font-size: 6pt;
  background-color: #fafafa;
  color: #000000;
}
#T_09c33_row3_col8 {
  font-size: 6pt;
  background-color: #fbfbfb;
  color: #000000;
}
#T_09c33_row3_col9 {
  font-size: 6pt;
  background-color: #fdfdfd;
  color: #000000;
}
#T_09c33_row4_col4 {
  font-size: 6pt;
  background-color: #1b1b1b;
  color: #f1f1f1;
}
#T_09c33_row4_col5 {
  font-size: 6pt;
  background-color: #e0e0e0;
  color: #000000;
}
#T_09c33_row4_col11 {
  font-size: 6pt;
  background-color: #4e4e4e;
  color: #f1f1f1;
}
#T_09c33_row4_col14 {
  font-size: 6pt;
  background-color: #767676;
  color: #f1f1f1;
}
#T_09c33_row5_col1 {
  font-size: 6pt;
  background-color: #fcfcfc;
  color: #000000;
}
#T_09c33_row5_col2, #T_09c33_row5_col3 {
  font-size: 6pt;
  background-color: #f6f6f6;
  color: #000000;
}
#T_09c33_row5_col4, #T_09c33_row7_col7 {
  font-size: 6pt;
  background-color: #f8f8f8;
  color: #000000;
}
#T_09c33_row5_col10, #T_09c33_row10_col7 {
  font-size: 6pt;
  background-color: #e8e8e8;
  color: #000000;
}
#T_09c33_row5_col11 {
  font-size: 6pt;
  background-color: #222222;
  color: #f1f1f1;
}
#T_09c33_row5_col13, #T_09c33_row6_col12 {
  font-size: 6pt;
  background-color: #090909;
  color: #f1f1f1;
}
#T_09c33_row5_col14 {
  font-size: 6pt;
  background-color: #d0d0d0;
  color: #000000;
}
#T_09c33_row6_col10, #T_09c33_row7_col11, #T_09c33_row9_col6 {
  font-size: 6pt;
  background-color: #060606;
  color: #f1f1f1;
}
#T_09c33_row6_col13 {
  font-size: 6pt;
  background-color: #979797;
  color: #f1f1f1;
}
#T_09c33_row7_col8 {
  font-size: 6pt;
  background-color: #b6b6b6;
  color: #000000;
}
#T_09c33_row7_col9 {
  font-size: 6pt;
  background-color: #252525;
  color: #f1f1f1;
}
#T_09c33_row7_col12 {
  font-size: 6pt;
  background-color: #999999;
  color: #f1f1f1;
}
#T_09c33_row8_col5 {
  font-size: 6pt;
  background-color: #f9f9f9;
  color: #000000;
}
#T_09c33_row8_col7 {
  font-size: 6pt;
  background-color: #101010;
  color: #f1f1f1;
}
#T_09c33_row8_col9, #T_09c33_row9_col9 {
  font-size: 6pt;
  background-color: #020202;
  color: #f1f1f1;
}
#T_09c33_row8_col11 {
  font-size: 6pt;
  background-color: #545454;
  color: #f1f1f1;
}
#T_09c33_row8_col12 {
  font-size: 6pt;
  background-color: #f1f1f1;
  color: #000000;
}
#T_09c33_row9_col5 {
  font-size: 6pt;
  background-color: #f7f7f7;
  color: #000000;
}
#T_09c33_row9_col7 {
  font-size: 6pt;
  background-color: #030303;
  color: #f1f1f1;
}
#T_09c33_row9_col12 {
  font-size: 6pt;
  background-color: #181818;
  color: #f1f1f1;
}
#T_09c33_row9_col13 {
  font-size: 6pt;
  background-color: #303030;
  color: #f1f1f1;
}
#T_09c33_row9_col14 {
  font-size: 6pt;
  background-color: #a9a9a9;
  color: #f1f1f1;
}
#T_09c33_row9_col15 {
  font-size: 6pt;
  background-color: #fefefe;
  color: #000000;
}
#T_09c33_row10_col8, #T_09c33_row10_col9 {
  font-size: 6pt;
  background-color: #bababa;
  color: #000000;
}
#T_09c33_row10_col10 {
  font-size: 6pt;
  background-color: #393939;
  color: #f1f1f1;
}
&lt;/style&gt;
&lt;table id=&quot;T_09c33&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th class=&quot;blank level0&quot;&gt;&amp;nbsp;&lt;/th&gt;
      &lt;th id=&quot;T_09c33_level0_col0&quot; class=&quot;col_heading level0 col0&quot;&gt;0&lt;/th&gt;
      &lt;th id=&quot;T_09c33_level0_col1&quot; class=&quot;col_heading level0 col1&quot;&gt;1&lt;/th&gt;
      &lt;th id=&quot;T_09c33_level0_col2&quot; class=&quot;col_heading level0 col2&quot;&gt;2&lt;/th&gt;
      &lt;th id=&quot;T_09c33_level0_col3&quot; class=&quot;col_heading level0 col3&quot;&gt;3&lt;/th&gt;
      &lt;th id=&quot;T_09c33_level0_col4&quot; class=&quot;col_heading level0 col4&quot;&gt;4&lt;/th&gt;
      &lt;th id=&quot;T_09c33_level0_col5&quot; class=&quot;col_heading level0 col5&quot;&gt;5&lt;/th&gt;
      &lt;th id=&quot;T_09c33_level0_col6&quot; class=&quot;col_heading level0 col6&quot;&gt;6&lt;/th&gt;
      &lt;th id=&quot;T_09c33_level0_col7&quot; class=&quot;col_heading level0 col7&quot;&gt;7&lt;/th&gt;
      &lt;th id=&quot;T_09c33_level0_col8&quot; class=&quot;col_heading level0 col8&quot;&gt;8&lt;/th&gt;
      &lt;th id=&quot;T_09c33_level0_col9&quot; class=&quot;col_heading level0 col9&quot;&gt;9&lt;/th&gt;
      &lt;th id=&quot;T_09c33_level0_col10&quot; class=&quot;col_heading level0 col10&quot;&gt;10&lt;/th&gt;
      &lt;th id=&quot;T_09c33_level0_col11&quot; class=&quot;col_heading level0 col11&quot;&gt;11&lt;/th&gt;
      &lt;th id=&quot;T_09c33_level0_col12&quot; class=&quot;col_heading level0 col12&quot;&gt;12&lt;/th&gt;
      &lt;th id=&quot;T_09c33_level0_col13&quot; class=&quot;col_heading level0 col13&quot;&gt;13&lt;/th&gt;
      &lt;th id=&quot;T_09c33_level0_col14&quot; class=&quot;col_heading level0 col14&quot;&gt;14&lt;/th&gt;
      &lt;th id=&quot;T_09c33_level0_col15&quot; class=&quot;col_heading level0 col15&quot;&gt;15&lt;/th&gt;
      &lt;th id=&quot;T_09c33_level0_col16&quot; class=&quot;col_heading level0 col16&quot;&gt;16&lt;/th&gt;
      &lt;th id=&quot;T_09c33_level0_col17&quot; class=&quot;col_heading level0 col17&quot;&gt;17&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th id=&quot;T_09c33_level0_row0&quot; class=&quot;row_heading level0 row0&quot;&gt;0&lt;/th&gt;
      &lt;td id=&quot;T_09c33_row0_col0&quot; class=&quot;data row0 col0&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row0_col1&quot; class=&quot;data row0 col1&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row0_col2&quot; class=&quot;data row0 col2&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row0_col3&quot; class=&quot;data row0 col3&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row0_col4&quot; class=&quot;data row0 col4&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row0_col5&quot; class=&quot;data row0 col5&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row0_col6&quot; class=&quot;data row0 col6&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row0_col7&quot; class=&quot;data row0 col7&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row0_col8&quot; class=&quot;data row0 col8&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row0_col9&quot; class=&quot;data row0 col9&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row0_col10&quot; class=&quot;data row0 col10&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row0_col11&quot; class=&quot;data row0 col11&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row0_col12&quot; class=&quot;data row0 col12&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row0_col13&quot; class=&quot;data row0 col13&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row0_col14&quot; class=&quot;data row0 col14&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row0_col15&quot; class=&quot;data row0 col15&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row0_col16&quot; class=&quot;data row0 col16&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row0_col17&quot; class=&quot;data row0 col17&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th id=&quot;T_09c33_level0_row1&quot; class=&quot;row_heading level0 row1&quot;&gt;1&lt;/th&gt;
      &lt;td id=&quot;T_09c33_row1_col0&quot; class=&quot;data row1 col0&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row1_col1&quot; class=&quot;data row1 col1&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row1_col2&quot; class=&quot;data row1 col2&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row1_col3&quot; class=&quot;data row1 col3&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row1_col4&quot; class=&quot;data row1 col4&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row1_col5&quot; class=&quot;data row1 col5&quot;&gt;29&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row1_col6&quot; class=&quot;data row1 col6&quot;&gt;150&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row1_col7&quot; class=&quot;data row1 col7&quot;&gt;195&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row1_col8&quot; class=&quot;data row1 col8&quot;&gt;254&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row1_col9&quot; class=&quot;data row1 col9&quot;&gt;255&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row1_col10&quot; class=&quot;data row1 col10&quot;&gt;254&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row1_col11&quot; class=&quot;data row1 col11&quot;&gt;176&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row1_col12&quot; class=&quot;data row1 col12&quot;&gt;193&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row1_col13&quot; class=&quot;data row1 col13&quot;&gt;150&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row1_col14&quot; class=&quot;data row1 col14&quot;&gt;96&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row1_col15&quot; class=&quot;data row1 col15&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row1_col16&quot; class=&quot;data row1 col16&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row1_col17&quot; class=&quot;data row1 col17&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th id=&quot;T_09c33_level0_row2&quot; class=&quot;row_heading level0 row2&quot;&gt;2&lt;/th&gt;
      &lt;td id=&quot;T_09c33_row2_col0&quot; class=&quot;data row2 col0&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row2_col1&quot; class=&quot;data row2 col1&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row2_col2&quot; class=&quot;data row2 col2&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row2_col3&quot; class=&quot;data row2 col3&quot;&gt;48&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row2_col4&quot; class=&quot;data row2 col4&quot;&gt;166&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row2_col5&quot; class=&quot;data row2 col5&quot;&gt;224&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row2_col6&quot; class=&quot;data row2 col6&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row2_col7&quot; class=&quot;data row2 col7&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row2_col8&quot; class=&quot;data row2 col8&quot;&gt;234&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row2_col9&quot; class=&quot;data row2 col9&quot;&gt;196&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row2_col10&quot; class=&quot;data row2 col10&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row2_col11&quot; class=&quot;data row2 col11&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row2_col12&quot; class=&quot;data row2 col12&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row2_col13&quot; class=&quot;data row2 col13&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row2_col14&quot; class=&quot;data row2 col14&quot;&gt;233&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row2_col15&quot; class=&quot;data row2 col15&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row2_col16&quot; class=&quot;data row2 col16&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row2_col17&quot; class=&quot;data row2 col17&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th id=&quot;T_09c33_level0_row3&quot; class=&quot;row_heading level0 row3&quot;&gt;3&lt;/th&gt;
      &lt;td id=&quot;T_09c33_row3_col0&quot; class=&quot;data row3 col0&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row3_col1&quot; class=&quot;data row3 col1&quot;&gt;93&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row3_col2&quot; class=&quot;data row3 col2&quot;&gt;244&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row3_col3&quot; class=&quot;data row3 col3&quot;&gt;249&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row3_col4&quot; class=&quot;data row3 col4&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row3_col5&quot; class=&quot;data row3 col5&quot;&gt;187&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row3_col6&quot; class=&quot;data row3 col6&quot;&gt;46&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row3_col7&quot; class=&quot;data row3 col7&quot;&gt;10&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row3_col8&quot; class=&quot;data row3 col8&quot;&gt;8&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row3_col9&quot; class=&quot;data row3 col9&quot;&gt;4&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row3_col10&quot; class=&quot;data row3 col10&quot;&gt;10&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row3_col11&quot; class=&quot;data row3 col11&quot;&gt;194&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row3_col12&quot; class=&quot;data row3 col12&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row3_col13&quot; class=&quot;data row3 col13&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row3_col14&quot; class=&quot;data row3 col14&quot;&gt;233&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row3_col15&quot; class=&quot;data row3 col15&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row3_col16&quot; class=&quot;data row3 col16&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row3_col17&quot; class=&quot;data row3 col17&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th id=&quot;T_09c33_level0_row4&quot; class=&quot;row_heading level0 row4&quot;&gt;4&lt;/th&gt;
      &lt;td id=&quot;T_09c33_row4_col0&quot; class=&quot;data row4 col0&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row4_col1&quot; class=&quot;data row4 col1&quot;&gt;107&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row4_col2&quot; class=&quot;data row4 col2&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row4_col3&quot; class=&quot;data row4 col3&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row4_col4&quot; class=&quot;data row4 col4&quot;&gt;230&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row4_col5&quot; class=&quot;data row4 col5&quot;&gt;48&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row4_col6&quot; class=&quot;data row4 col6&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row4_col7&quot; class=&quot;data row4 col7&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row4_col8&quot; class=&quot;data row4 col8&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row4_col9&quot; class=&quot;data row4 col9&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row4_col10&quot; class=&quot;data row4 col10&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row4_col11&quot; class=&quot;data row4 col11&quot;&gt;192&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row4_col12&quot; class=&quot;data row4 col12&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row4_col13&quot; class=&quot;data row4 col13&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row4_col14&quot; class=&quot;data row4 col14&quot;&gt;156&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row4_col15&quot; class=&quot;data row4 col15&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row4_col16&quot; class=&quot;data row4 col16&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row4_col17&quot; class=&quot;data row4 col17&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th id=&quot;T_09c33_level0_row5&quot; class=&quot;row_heading level0 row5&quot;&gt;5&lt;/th&gt;
      &lt;td id=&quot;T_09c33_row5_col0&quot; class=&quot;data row5 col0&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row5_col1&quot; class=&quot;data row5 col1&quot;&gt;3&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row5_col2&quot; class=&quot;data row5 col2&quot;&gt;20&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row5_col3&quot; class=&quot;data row5 col3&quot;&gt;20&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row5_col4&quot; class=&quot;data row5 col4&quot;&gt;15&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row5_col5&quot; class=&quot;data row5 col5&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row5_col6&quot; class=&quot;data row5 col6&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row5_col7&quot; class=&quot;data row5 col7&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row5_col8&quot; class=&quot;data row5 col8&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row5_col9&quot; class=&quot;data row5 col9&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row5_col10&quot; class=&quot;data row5 col10&quot;&gt;43&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row5_col11&quot; class=&quot;data row5 col11&quot;&gt;224&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row5_col12&quot; class=&quot;data row5 col12&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row5_col13&quot; class=&quot;data row5 col13&quot;&gt;245&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row5_col14&quot; class=&quot;data row5 col14&quot;&gt;74&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row5_col15&quot; class=&quot;data row5 col15&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row5_col16&quot; class=&quot;data row5 col16&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row5_col17&quot; class=&quot;data row5 col17&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th id=&quot;T_09c33_level0_row6&quot; class=&quot;row_heading level0 row6&quot;&gt;6&lt;/th&gt;
      &lt;td id=&quot;T_09c33_row6_col0&quot; class=&quot;data row6 col0&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row6_col1&quot; class=&quot;data row6 col1&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row6_col2&quot; class=&quot;data row6 col2&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row6_col3&quot; class=&quot;data row6 col3&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row6_col4&quot; class=&quot;data row6 col4&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row6_col5&quot; class=&quot;data row6 col5&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row6_col6&quot; class=&quot;data row6 col6&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row6_col7&quot; class=&quot;data row6 col7&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row6_col8&quot; class=&quot;data row6 col8&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row6_col9&quot; class=&quot;data row6 col9&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row6_col10&quot; class=&quot;data row6 col10&quot;&gt;249&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row6_col11&quot; class=&quot;data row6 col11&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row6_col12&quot; class=&quot;data row6 col12&quot;&gt;245&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row6_col13&quot; class=&quot;data row6 col13&quot;&gt;126&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row6_col14&quot; class=&quot;data row6 col14&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row6_col15&quot; class=&quot;data row6 col15&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row6_col16&quot; class=&quot;data row6 col16&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row6_col17&quot; class=&quot;data row6 col17&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th id=&quot;T_09c33_level0_row7&quot; class=&quot;row_heading level0 row7&quot;&gt;7&lt;/th&gt;
      &lt;td id=&quot;T_09c33_row7_col0&quot; class=&quot;data row7 col0&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row7_col1&quot; class=&quot;data row7 col1&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row7_col2&quot; class=&quot;data row7 col2&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row7_col3&quot; class=&quot;data row7 col3&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row7_col4&quot; class=&quot;data row7 col4&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row7_col5&quot; class=&quot;data row7 col5&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row7_col6&quot; class=&quot;data row7 col6&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row7_col7&quot; class=&quot;data row7 col7&quot;&gt;14&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row7_col8&quot; class=&quot;data row7 col8&quot;&gt;101&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row7_col9&quot; class=&quot;data row7 col9&quot;&gt;223&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row7_col10&quot; class=&quot;data row7 col10&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row7_col11&quot; class=&quot;data row7 col11&quot;&gt;248&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row7_col12&quot; class=&quot;data row7 col12&quot;&gt;124&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row7_col13&quot; class=&quot;data row7 col13&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row7_col14&quot; class=&quot;data row7 col14&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row7_col15&quot; class=&quot;data row7 col15&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row7_col16&quot; class=&quot;data row7 col16&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row7_col17&quot; class=&quot;data row7 col17&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th id=&quot;T_09c33_level0_row8&quot; class=&quot;row_heading level0 row8&quot;&gt;8&lt;/th&gt;
      &lt;td id=&quot;T_09c33_row8_col0&quot; class=&quot;data row8 col0&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row8_col1&quot; class=&quot;data row8 col1&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row8_col2&quot; class=&quot;data row8 col2&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row8_col3&quot; class=&quot;data row8 col3&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row8_col4&quot; class=&quot;data row8 col4&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row8_col5&quot; class=&quot;data row8 col5&quot;&gt;11&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row8_col6&quot; class=&quot;data row8 col6&quot;&gt;166&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row8_col7&quot; class=&quot;data row8 col7&quot;&gt;239&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row8_col8&quot; class=&quot;data row8 col8&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row8_col9&quot; class=&quot;data row8 col9&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row8_col10&quot; class=&quot;data row8 col10&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row8_col11&quot; class=&quot;data row8 col11&quot;&gt;187&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row8_col12&quot; class=&quot;data row8 col12&quot;&gt;30&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row8_col13&quot; class=&quot;data row8 col13&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row8_col14&quot; class=&quot;data row8 col14&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row8_col15&quot; class=&quot;data row8 col15&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row8_col16&quot; class=&quot;data row8 col16&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row8_col17&quot; class=&quot;data row8 col17&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th id=&quot;T_09c33_level0_row9&quot; class=&quot;row_heading level0 row9&quot;&gt;9&lt;/th&gt;
      &lt;td id=&quot;T_09c33_row9_col0&quot; class=&quot;data row9 col0&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row9_col1&quot; class=&quot;data row9 col1&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row9_col2&quot; class=&quot;data row9 col2&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row9_col3&quot; class=&quot;data row9 col3&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row9_col4&quot; class=&quot;data row9 col4&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row9_col5&quot; class=&quot;data row9 col5&quot;&gt;16&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row9_col6&quot; class=&quot;data row9 col6&quot;&gt;248&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row9_col7&quot; class=&quot;data row9 col7&quot;&gt;250&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row9_col8&quot; class=&quot;data row9 col8&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row9_col9&quot; class=&quot;data row9 col9&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row9_col10&quot; class=&quot;data row9 col10&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row9_col11&quot; class=&quot;data row9 col11&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row9_col12&quot; class=&quot;data row9 col12&quot;&gt;232&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row9_col13&quot; class=&quot;data row9 col13&quot;&gt;213&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row9_col14&quot; class=&quot;data row9 col14&quot;&gt;111&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row9_col15&quot; class=&quot;data row9 col15&quot;&gt;2&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row9_col16&quot; class=&quot;data row9 col16&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row9_col17&quot; class=&quot;data row9 col17&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th id=&quot;T_09c33_level0_row10&quot; class=&quot;row_heading level0 row10&quot;&gt;10&lt;/th&gt;
      &lt;td id=&quot;T_09c33_row10_col0&quot; class=&quot;data row10 col0&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row10_col1&quot; class=&quot;data row10 col1&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row10_col2&quot; class=&quot;data row10 col2&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row10_col3&quot; class=&quot;data row10 col3&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row10_col4&quot; class=&quot;data row10 col4&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row10_col5&quot; class=&quot;data row10 col5&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row10_col6&quot; class=&quot;data row10 col6&quot;&gt;0&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row10_col7&quot; class=&quot;data row10 col7&quot;&gt;43&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row10_col8&quot; class=&quot;data row10 col8&quot;&gt;98&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row10_col9&quot; class=&quot;data row10 col9&quot;&gt;98&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row10_col10&quot; class=&quot;data row10 col10&quot;&gt;208&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row10_col11&quot; class=&quot;data row10 col11&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row10_col12&quot; class=&quot;data row10 col12&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row10_col13&quot; class=&quot;data row10 col13&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row10_col14&quot; class=&quot;data row10 col14&quot;&gt;253&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row10_col15&quot; class=&quot;data row10 col15&quot;&gt;187&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row10_col16&quot; class=&quot;data row10 col16&quot;&gt;22&lt;/td&gt;
      &lt;td id=&quot;T_09c33_row10_col17&quot; class=&quot;data row10 col17&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Note that the rest of this is 28x28 pixels for the whole image, and each pixel can contain values from 0 to 255 to represent all the shades from white to black.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;APPROACH-1:-PIXEL-SIMILARITY&quot;&gt;APPROACH 1: PIXEL SIMILARITY&lt;a class=&quot;anchor-link&quot; href=&quot;#APPROACH-1:-PIXEL-SIMILARITY&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ol&gt;
&lt;li&gt;Find the average pixel value for each pixel of the 3s and 7s&lt;/li&gt;
&lt;li&gt;Use this 'ideal' 3 and 7 to compare a single image that we want to know whether it's a 3 or a 7&lt;/li&gt;
&lt;/ol&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;three_tensors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;threes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;seven_tensors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sevens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;three_tensors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seven_tensors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(6131, 6265)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We can view these images with the fastai &lt;code&gt;show_image&lt;/code&gt; function. This takes a tensor and makes it viewable in a Jupyter Notebook:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;three_tensors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;three_tensors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor([[  0,   0,   0,   0,   0,   0,   0,  42, 118, 219, 166],
        [  0,   0,   0,   0,   0,   0, 103, 242, 254, 254, 254],
        [  0,   0,   0,   0,   0,   0,  18, 232, 254, 254, 254],
        [  0,   0,   0,   0,   0,   0,   0, 104, 244, 254, 224],
        [  0,   0,   0,   0,   0,   0,   0,   0, 207, 254, 210],
        [  0,   0,   0,   0,   0,   0,   0,   0,  84, 206, 254],
        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 209],
        [  0,   0,   0,   0,   0,   0,   0,   0,  91, 137, 253],
        [  0,   0,   0,   0,   0,   0,  40, 214, 250, 254, 254],
        [  0,   0,   0,   0,   0,   0,  81, 247, 254, 254, 254],
        [  0,   0,   0,   0,   0,   0,   0, 110, 246, 254, 254]], dtype=torch.uint8)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;It's fairly common to have to convert a collection (i.e. in our case, a list) of tensors into a single tensor object with an extra dimension, so that's why we have the &lt;code&gt;torch.stack&lt;/code&gt; method which takes a collection and returns a rank-3 tensor.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash flash-success&quot;&gt;
    &lt;svg class=&quot;octicon octicon-checklist&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M2.5 1.75a.25.25 0 01.25-.25h8.5a.25.25 0 01.25.25v7.736a.75.75 0 101.5 0V1.75A1.75 1.75 0 0011.25 0h-8.5A1.75 1.75 0 001 1.75v11.5c0 .966.784 1.75 1.75 1.75h3.17a.75.75 0 000-1.5H2.75a.25.25 0 01-.25-.25V1.75zM4.75 4a.75.75 0 000 1.5h4.5a.75.75 0 000-1.5h-4.5zM4 7.75A.75.75 0 014.75 7h2a.75.75 0 010 1.5h-2A.75.75 0 014 7.75zm11.774 3.537a.75.75 0 00-1.048-1.074L10.7 14.145 9.281 12.72a.75.75 0 00-1.062 1.058l1.943 1.95a.75.75 0 001.055.008l4.557-4.45z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Tip: &lt;/strong&gt;You&amp;#8217;ll see here that we also convert our values into floats and normalise them (i.e. divide by 255) since this will help us at a later stage.
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stacked_threes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;three_tensors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;stacked_sevens&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seven_tensors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We can see that the first axis of this tensor is our 6131 images, and each image has 28x28 pixels. The &lt;code&gt;shape&lt;/code&gt; tells you the length of each axis.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stacked_threes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;torch.Size([6131, 28, 28])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Note: the &lt;em&gt;length&lt;/em&gt; of a tensor's shape is its &lt;em&gt;rank&lt;/em&gt;. We have a rank-3 tensor:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stacked_threes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;3&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stacked_threes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndim&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;3&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Rank = the number of axes in a tensor.&lt;/li&gt;
&lt;li&gt;Shape = the size of each axis of a tensor.&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Getting-the-ideal-3&quot;&gt;Getting the ideal 3&lt;a class=&quot;anchor-link&quot; href=&quot;#Getting-the-ideal-3&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;We take our stack of threes (i.e. a rank-3 tensor) and we calculate the mean across the first dimension (i.e. the 6131 individual threes).&lt;/p&gt;
&lt;p&gt;By the end, you can see that &lt;code&gt;mean3&lt;/code&gt; is actually a 28x28 tensor (i.e. a single image) that represents the ideal version of our threes data.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stacked_threes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;torch.Size([28, 28])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean7&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stacked_sevens&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;show_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Comparing-our-ideal-numbers-with-an-individual-sample&quot;&gt;Comparing our ideal numbers with an individual sample&lt;a class=&quot;anchor-link&quot; href=&quot;#Comparing-our-ideal-numbers-with-an-individual-sample&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stacked_threes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;35&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sample_7&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stacked_sevens&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;35&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;show_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There are two main ways we can measure the distance (i.e. similarity) in this context:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the &lt;strong&gt;Mean Absolute Difference&lt;/strong&gt; aka &lt;em&gt;L1 Norm&lt;/em&gt; -- the mean of the absolute value of differences (our absolute value replaces negative values with positive values)&lt;/li&gt;
&lt;li&gt;the &lt;strong&gt;Root Mean Squared Error&lt;/strong&gt; (RMSE) aka &lt;em&gt;L2 Norm&lt;/em&gt; -- we square all the differences (making everything positive), get the mean of those values, and then square root everything (which undoes all the squaring).&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_l1_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_l2_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l1_norm_distance_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_l1_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;l2_norm_distance_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_l2_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;l1_norm_distance_3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_norm_distance_3&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(tensor(0.1401), tensor(0.2545))&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l1_norm_distance_7&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_l1_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;l2_norm_distance_7&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_l2_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;l1_norm_distance_7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_norm_distance_7&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(tensor(0.1670), tensor(0.3121))&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The differences from our &lt;code&gt;sample_3&lt;/code&gt; to the &lt;code&gt;mean3&lt;/code&gt; is less than the differences between our &lt;code&gt;sample_3&lt;/code&gt; and the &lt;code&gt;mean7&lt;/code&gt;. This totally makes sense and is what we were expecting!&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1_norm_distance_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1_norm_distance_7&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_norm_distance_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_norm_distance_7&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;The-PyTorch-built-in-ways-of-calculating-loss&quot;&gt;The PyTorch built-in ways of calculating loss&lt;a class=&quot;anchor-link&quot; href=&quot;#The-PyTorch-built-in-ways-of-calculating-loss&quot;&gt; &lt;/a&gt;&lt;/h3&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;PyTorch exposes a variety of loss functions at &lt;code&gt;torch.nn.functional&lt;/code&gt; which it recommends importing as &lt;code&gt;F&lt;/code&gt;. These are available by default under that name in fastai.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# !pip install -Uqq rich&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# from rich import inspect as rinspect&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# rinspect(F, methods=True)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pytorch_l1_norm_distance_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l1_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pytorch_l1_norm_distance_7&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l1_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pytorch_l1_norm_distance_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pytorch_l1_norm_distance_7&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pytorch_l1_norm_distance_3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pytorch_l1_norm_distance_7&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(tensor(0.1401), tensor(0.1670))&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;For the L2 norm, the PyTorch function only calculates the mean squared error loss, so we have to add in the square root at the end ourselves:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pytorch_l2_norm_distance_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l1_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pytorch_l2_norm_distance_7&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l1_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pytorch_l2_norm_distance_3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pytorch_l2_norm_distance_7&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pytorch_l2_norm_distance_3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pytorch_l2_norm_distance_7&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(tensor(0.3743), tensor(0.4087))&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;When to choose one vs the other?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MSE penalises bigger mistakes more than the L1 norm&lt;/li&gt;
&lt;li&gt;MSE is more lenient with small mistakes than the L1 norm&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Fashion-MNIST&quot;&gt;Fashion MNIST&lt;a class=&quot;anchor-link&quot; href=&quot;#Fashion-MNIST&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Load-the-Fashion-MNIST-dataset-with-PyTorch&quot;&gt;Load the Fashion MNIST dataset with PyTorch&lt;a class=&quot;anchor-link&quot; href=&quot;#Load-the-Fashion-MNIST-dataset-with-PyTorch&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The &lt;a href=&quot;https://github.com/zalandoresearch/fashion-mnist&quot;&gt;Fashion MNIST dataset&lt;/a&gt; was created as a more advanced / difficult (and perhaps interesting) alternative to MNIST for computer vision tasks. It was first released in 2017 and I thought it might be interesting to try the same technique as we tried there on this new data set.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FashionMNIST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FashionMNIST&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;data&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels_map&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;T-Shirt&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Trouser&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Pullover&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Dress&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Coat&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Sandal&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Shirt&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Sneaker&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Bag&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Ankle Boot&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rows&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rows&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sample_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_subplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels_map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;off&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;squeeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;gray&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I found it a bit hard to extract the data for the images, stored as it was together with the labels. Here you can see me getting the image and the label.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;0&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;There are 60,000 items in this training dataset, and 10 categories, so it makes sense that the set of images for the two categories I extracted would be 6000 each.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_dresses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;training_pullovers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_data&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_dresses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_pullovers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(6000, 6000)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_dresses_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_dresses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;training_pullovers_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_pullovers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training_dresses_tensor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;torch.Size([6000, 28, 28])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Calculate-the-mean-image-for-a-dress-and-a-pullover&quot;&gt;Calculate the mean image for a dress and a pullover&lt;a class=&quot;anchor-link&quot; href=&quot;#Calculate-the-mean-image-for-a-dress-and-a-pullover&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_training_dress&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_dresses_tensor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;mean_training_pullover&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training_pullovers_tensor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_training_dress&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;torch.Size([28, 28])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_training_dress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAIEklEQVR4nO1bW2/aTBA9ttc3AqRBpVWl/IP+/7/Sxz5VqoraBFJ899rfA5rteLKGAIuC9DFSFC72ePfsmesuXt/3uMk/8d97ANcmN0CE3AARcgNEyA0QIerA90eFIB6x+r5H3/do2xZVVSHLMvz+/RtlWSLLMoRhiDAMkSQJ0jSF7/vwPA91XaOqKhRFgaqqEAQBfN/HcrnEdDrFbDZDFEUIggCe58HzPPNM/voNYr34ECAnSd/36LoObduiaRpkWYbtdouXlxcURYH1em0AmUwmSJIEnuchCAJUVYW6rpFlGcqyRBRFUEohjuPdgNVuyGEYGrCOBGKvOAOEGNH3PbTWqKoKz8/P+PPnD759+4bVaoUfP34YYAisKIrMiiulUFUVmqYxbEvTFHEc49OnT/jw4QO+fv2K5XKJx8dHTKfTAVuAo1nySpwAIk2F2JHnOf7+/Yunpyes12tsNhtkWYb1em1MI4oixHEMpdQAEGA3Oa016ro2DHl6ekKSJKiqCkmSQCk1YEnf92eB4pwhxI6Xlxf8/PkTv379wnq9xna7RVmWaJoGbdsCgDEHz/OMv/F9H2EYGr0EblmWCMMQq9UKXdfh/v4eXdfh48ePA1/y7gwZc6TkTKuqgtYaWmszOQKE7um6zqws6QB2k+u6znxP99d1jbqu0TSN+Y4z4xyWODMZGrDWGmVZoigKbLdbVFVlPm+aBlVVIc9zM2BiCdfVdR0AwPf9gV6ttQGkKArkeY66rqGUQhAEZ5sLcIEowxlCfxIwmjAANE0zmITWesAWrTWUUgOWkJ62bc31rorUswCxOVNyghQ+27Y1n5PDtJkMAPO/73sTUoMgQBAEAyC6rkNZlkYfB+VqnCoJ9wmS7twfABgwhYT7E5oY18efwVlzFQzhwgHg5jIGBn2utUYQBAB2PoMDAgxZQzroGZcwGye1DF85SV3P80ye4Pu7x3EwZEInV590k/mQDvncsTEdK05Nhk8G2KXZlKKHYYgoiuB53qvV5XkHgIFPSZIEABAEwSCj5c9zyZCL1DIADDuCIDCAUFYp2aG1fuUIOWN834dSClEUIQxD+L5vWCdBuBqnyqkP7AAJwxBxHGM6naIsS5NqU+Thk7EBwrPPJEkwnU6RJAmiKBqYjkun6rQfIidI7Ijj2NCdEqhj9HFwiW22KvcqTGaMsjSJruswm82QZZkp4val6gAGq0/AJkmC2WyGyWRi9ND9Vx12SSiqhGGIvu9Neg3sogwA857rsIVbup9aBdQLGfMVV1HLyNfEEIo0cRybYk1rbQ2fdB//nKKWUgp3d3eYTCZGr7z2XU3mUOz3PA9KKRNBKLrQBHllKnXIbJZSfTIfcqo2HVcVZbjQ4MksKP+g2kOGTAkmTYoqZKpvyEFT6D00pmPBcR5lOHW5nXddZ3oYx1CblwJUGdt6qe9qMjaay9c8ZadEjDrph3TziRJD6rpGWZa7QSs1yq5z5aJ5CFGa2gHUMLLlD7Ka5a+phZhl2cCfcB8idZwqzvIQWdRRuk39kaIosNlsUBTFq9BKQh0y4F+0ofvzPMdmsxk0lC4Rds9iiG01aOX4pLMsG/RXeR3CV5VXuPI7Ylhd1wPQD038WMY4Lf+NUlbua62x3W6R5znKshwAwu+XPkG+L8ty0KPlJikz3nPE+d4uDZQiQdu2ZiK8vzrmN2wNIQDG7KgtycO63NI8R5yW/7zkp1XUWpttSapygf1U7rrOrD43maIoDMvoOW/ZkznGpzgHhNhB9QpFCLJ97kg5O+SAOSgATHO6aRqzs0csPORLjmGPM0C4M6VBUsgsimKQkHF28D0Ymb3KDaqmaUwI5wnaoYz1GHHqQ7jn5/sqlK4D+03lUMJHGast0kg/cqqDvZhTBTAAhO/FDAYwsrqSTZTtEkt438UlQ5z7EC58C5MijPweeL19aVttMh8yHVsx6UKcQCtP8UiGyO1LAK+izRjFORikhwCxmcq54rxBJPdpee5Bsg8MGV3kM2THXj7zXHHuVPl/udVoc3q2DFWGZn6t3IdxzRBnJmNLjmzptJwg/2+7zsYuAoybjCtgLsoQYL9vGJNDIZMzaAyMQ9nrmDgDRCZltijB38vXtppG/uff0zP581yw5GxAbOYiy3uSU9giv7c1ocbYcYqcBIgEwJa2yzScZMyn2Kpf2waUZMi+bPUUuUgtIxtEUvYxxTahsbAt2enCqV40U93XtJFMADBI6PjrsebRWLQ5R5z4EJu8BQz5/q17tLZw7EqcMET6Cxk5pPOz0Z+LLVchPbZrbOM5VS728xC+ii5s2wbGJRhyNiBjqTcf8Fu6WtIvWAdrOSQzFolOlYt03TkwsmcxBgrPYawDPXBAxhVbnB+6IyD4ylF3XJ5GBP75n7HchthFG1+kgx/RlFnuOSZ6sUN3+0Q6TCr5bcncpXzFmJwMiFxZuTIyJ6DOGe3RSscor6fuPf9BAW1S7auSry5TpYkopZCmKebzOebzOfI8x2KxsJ4N4wwhXaQnSRKEYYjFYoHFYoHZbIb5fG7OmdnS9lMrXQDwDtDxTVyVWScdyM3zHFmWYbVa4fn5Gd+/f8d2uzW/qKJuPD+nShOjM2l3d3dI0xRfvnzBw8MDHh8fMZ/P8fDwYE4lyvMibwTicj9C5ObjeZ45NRTHMfq+x/39PQDg8+fPSNMUQRCYcx/UZiQ9EpD5fI40TbFcLjGfz81JRPqdjWxKn2syhxjyv5PbD5mF3AARcgNEyA0QITdAhNwAEfIfcL30e08CFSQAAAAASUVORK5CYII=&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_training_pullover&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEQAAABECAYAAAA4E5OyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAKjUlEQVR4nO1bSW/bzBIskuIuS7J1sQ8OAgO55Zj//z/yIcjBQBJEC7RR3Jd3yKtJa0zJpGT7e3hwAwK1zlJTXd09HBlN0+Dd/pr5bw/gf83eAdHsHRDN3gHR7B0QzQbPfN4rBDFiNU2Duq5RliXKskSSJNjtdpjNZoiiCLPZDLZtw3EcOI4D13UBAIZhIM9zFEWBKIqQpinCMITnebi/v8f19TVGoxEcx4Ft27AsC4ZhwDAM9fse1vrl5wDpZG1A1HWNPM+RpimiKMJms8F8Psd6vcZisYBlWTBNU4HCyRRFgTzPkSQJ8jzHcDiE7/sIw1D14/s+fN+HZVkYDAYwTROm+YfsPUF5YhcD0jSNAqIoCmRZhsVige12i8fHR8xmM3z//h2bzQa/fv3CarXCbDZDmqZIkgSmaapJGYaBLMuQ57la/el0islkgg8fPmA0GuHjx4+YTqf49OkTxuMx7u7uEAQBPM9T7VwCzEWASDDoGlEUYblcYr1e4+fPn1gsFgogXne7HdI0RRzHanUJCF1mMBjAsiy4rgvTNLFcLlFVFYIgQFEUCMMQeZ4jCAJUVQXLsv5MSIDypoCQvlVVIcsy7HY7fPv2DcvlEv/88w/2+z3W67W6pmkKy7IQhiEGgwGyLEOapqjrGjJb5nPXdWHbNobDodKYJEmwWCyw3+8RRRGCIMDj4yNubm7w+fNnjMdjTCYTBeY5LLmIIXVdo65rZFmGOI6xXC4xn8+x3W6RJInSgaIoUJYlqqoCAFiWBdu21e/5Ps00TQUImdM0DcqyRFEUsCwLSZKgrmus12s0TYPtdqsAvERTzgKErlJVFfI8x3w+x2w2w9evXxFFEVarFbIsQ5IkCiwygkCapgnP81R7NMMw1OqapommaZDnuQLEcRw0TaP0BgC22y1GoxH2+z2CIDgA5U0A4SSoHXEcY7/fq0eSJCpaZFl2wA4AT3y8ruuDz2Q4leADf6IQhbiua6RpiqZpEEURdrsdiqJAVVUKdIL3qoDIqBLHMebzOX78+KH8e7PZHLhKkiSKBVx5Pmd7BEayg4DQteq6VmBnWQbbtlEUBVzXxe/fv1HXNR4eHpQYs70+1hsQSW8yhKxI01S5htQNCcaxa9M0CgR51fVFvq7rGrZtA4CKWnmeqzzoHLuIIUy8FosFlsslVquVYojuBgAOVh94Knikt1xV/oZ9VlWltAuAYshqtYJpmtjtdgiCQDGqr7hepCFVVaEsS6RpqtjBxIqDkAPSV183OQFe+R7pL8M9AQGg9KooChRF8SScd7WLokxRFEiSBJvNBlEUYbvdKoGlHwNQeUFXhsjPKIycXFEUajEkW6Iogm3b2O12GI1GCpBXFVV2IJWfuYEUUfrwYDA4Opg+g5TiS11hJOFYOA5GtTdniKxdKKpJkqicQ88WTyk+NYJt87UETT6nmNNdTNNEHMdwXRf7/V6BIhfvTTWE0YSrVpYlAKj3pN9LV5BgtKXv+nPDMNTKV1WlALEsS42DQLWVBF1AuYghzEaZkXJAeZ4rlyHFZWglOLrAMjLJMM2++F0CTPGm4CZJgjAMEccx4jg+cKk+1rsslDSUm0BkCUOjnq/w+zobnutL/638jP2xz1MM6Wovkocw5Em34WrzedM0sCxLhUvddY4lUpIhMixLkEzTVCGfrNVF9800RGcFV4U1iWVZTyakD1DPRoF2gGTixuJNMoRRjgw5hyWdAdFDLt2FyRB1Q/o/N3f0STLBIqh6TdMGDK+O48A0Tdi2jTzP1UIwIZNs1TWpC1MuchmZc0i2sBplnSEnJLNO+V5XY9sM68DfaEZQpJ69magyD9EHAEC5iu/7av9CrpJsB0Cr5uguw+9LsAm4XBw9OdP7enFA2AFFVe4/SEBs24bruhgM2knYFnF0ENpA4U6767pwXVeFZkYZeX1VDZGD4uRluk5gAKjVYy0Tx/GTNphkSXc5VdvQLMuC4zjwPO9ApGX2yrHo4b+L9a5leOUA9LqCgscVJEhtLiPFVKbveiovr0zkuAEkSwOZi8h6pg8oZzGEFNU3kAkIV9DzPLUfKk0XWF1U20CTv5GVtBRWWWhKhrwqIBwcRVUXVK4e9YN7nzTui0iXaQOF1sYQwzDU3T65gUQQZOb8Ki6jRwapIToghmHA9311G4ErqLuF7jJdtvxkH6yTuMUgmSsfctyvVtwREOmrqkERAZhN6qukR5Yuu+NSD5qmUdFL7rlIHXl1UdULKymqcoWZoTKj1MVSB6HttbRjIDF9Z84jx8UkTQekC0t65SHSZSQt5YQty4LnebBtW3X+XK0i3bAt99D7ZqbLBE0vD+RDsqqLXZSYyYSM6DPK6LtlOgB8/1Tmqtci8jcM72SKHJccmw7Gc8Cclbq3rQLwhyEyQ9ULQr0dOUE9mkgA5HOyksLKwzOMWm2A9LHeLiOf6z5KEWXhRfC6Dkpny7Hql8cf5IEZ+fkx9+tiL8IQZoykMV3mOTAkSzoN9r85S1mWSq8kQ9imXIhTLG3to9NI8JTKbR0QFK6ajD4y+TqmJ2396O3rC8DkT7JEsvdVXYbWpt4cHA/Rcbuwbc/jVEF3zCQDyBDZnwRbZ0kf6wRIm+DJTlVjIi+QGqJP6hKjhrA/ht62SvmYUJ+yZwHp4ioADny6TUP6HJ08NgF5W4I1jW3bCILgicuccpdTAJ19Ok3vlIBIn9bB0G9892GMdAmZFbNP/Qa6Pr5XTcyODVjugzAPYVYpb3S3VbY6WHzO9F8ek2ASSEYGQXDUbfra2ZvMunGAcvIADiak37Rua0Ne+fzYd9mnPJxHa9tP7QLQRacQ5T0SUpcawhXjKnLgHOypQq7tTAlfy7AuK2vHcdRB4EvsRY52S1+WLKF76BmlrID1duRVP2KlvyfbZn+XRrKLAJGDAqA2hnzfR5qm8DwPVVVhNBophshQ3JbYsV2+li4H/L3F4XkeXNeF53nIsuxgQ+pVTzK33VXTI4Q8Vs2Jy4fneUp0TwEi+9PPpUlAZIbKPiRDTunOxYCcjNmCITy2HQQBwjBEWZa4vr5W904kk9ryhGOHY3RRpnuOx2NcXV1hvV6rM+9RFLX+ZaSPG/VymbaOOFG5akynfd9H0/w5yyGz11OqrwOjP7gIFFI+5DiORasXAYQCKDuREUVSl/9rmUwmcBwHVVUhTVOlIWRIF5fRgZeuNBgMcHt7i9FohM1mo/4dQYbQbeXfTrraWaLKVdJdRoITBIH695PcGnhuW68NEDkp9sV/SchSQaYAHFvfqHOWy7AzKWYEwvM8VVuMx2N1jlXuwHfZn9Bdhe/JKEMt2e/3B2yQWe2xdl4UEFKS/zwYj8ewbRtXV1cYDocH/24qy1IJK92vbR9V7+fYZNgus9PhcIgwDBGGIa6vr5Wwe553sPPf1ToBIv2XdLUsC1++fMFut8P9/T1s28Z0OsV0OsXd3d2TbUQCIIF4jiESlDaAqG23t7cIwxCr1UqB8vDwoNxKD8kXA8LByJ11imhd17i5ucFgMMDV1ZUqtE5FlS61RVuE0CfUNA0cx4Hv+5hMJgCA8XiMIAgOyoc+DDHOqQj/n+39j8yavQOi2Tsgmr0Dotk7IJq9A6LZfwBq8F801cGSJQAAAABJRU5ErkJggg==&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I chose the two because I wondered whether there might be a decent amount of crossover between the two items. You can see even in the 'mean' / average versions of the items that they look fairly similar.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Assemble-the-validation-data&quot;&gt;Assemble the validation data&lt;a class=&quot;anchor-link&quot; href=&quot;#Assemble-the-validation-data&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_dresses&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_pullovers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_data&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_dresses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_pullovers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(1000, 1000)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_dresses_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_dresses&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_pullovers_tensor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_pullovers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_dresses_tensor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;torch.Size([1000, 28, 28])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I also extract a single dress and a single pullover to check the loss in the next section.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_test_dress&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_dresses_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sample_test_pullover&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_pullovers_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_test_dress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_test_pullover&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_png output_subarea &quot;&gt;
&lt;img src=&quot;data:image/png;base64,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&quot; /&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Calculate-the-loss-comparing-a-random-pullover-with-validation-data&quot;&gt;Calculate the loss comparing a random pullover with validation data&lt;a class=&quot;anchor-link&quot; href=&quot;#Calculate-the-loss-comparing-a-random-pullover-with-validation-data&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_l1_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_l2_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l1_norm_distance_dress&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_l1_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_test_dress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_training_dress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;l2_norm_distance_dress&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_l2_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_test_dress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_training_dress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;l1_norm_distance_dress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_norm_distance_dress&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(tensor(0.1134), tensor(0.1766))&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;l1_norm_distance_pullover&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_l1_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_test_pullover&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_training_pullover&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;l2_norm_distance_pullover&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_l2_norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_test_pullover&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_training_pullover&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;l1_norm_distance_pullover&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_norm_distance_pullover&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(tensor(0.1713), tensor(0.2220))&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;The differences from our &lt;code&gt;sample_test_dress&lt;/code&gt; to the &lt;code&gt;mean_training_dress&lt;/code&gt; is less than the differences between our &lt;code&gt;sample_test_dress&lt;/code&gt; and the &lt;code&gt;mean_training_pullover&lt;/code&gt;. This totally makes sense and is what we were expecting!&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1_norm_distance_dress&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l1_norm_distance_pullover&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_norm_distance_dress&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2_norm_distance_pullover&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Using-broadcasting-to-check-our-predictions-on-our-validation-data&quot;&gt;Using broadcasting to check our predictions on our validation data&lt;a class=&quot;anchor-link&quot; href=&quot;#Using-broadcasting-to-check-our-predictions-on-our-validation-data&quot;&gt; &lt;/a&gt;&lt;/h2&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This function returns the L1 norm loss when calculated between two tensors. Because of the final tuple passed into &lt;code&gt;.mean()&lt;/code&gt; (i.e. &lt;code&gt;(-1, -2)&lt;/code&gt;), we can actually use this function to calculate the distance between a single image as compared to a full rank-3 tensor.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fashion_mnist_distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensor2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fashion_mnist_distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_test_dress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_training_dress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor(0.1134)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fashion_mnist_distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_test_dress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_training_pullover&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor(0.2864)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Again, our dress is 'closer' to the &lt;code&gt;mean_training_dress&lt;/code&gt; than it is to the &lt;code&gt;mean_training_pullover&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We can now create a function that predicts (by way of calculating and comparing these two losses) whether an item is a dress or not.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;is_dress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fashion_mnist_distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_training_dress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fashion_mnist_distance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_training_pullover&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_dress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_test_dress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor(True)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_dress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample_test_pullover&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor(False)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;...as expected...&lt;/p&gt;
&lt;p&gt;Finally, we can get an overall sense of how well our prediction function is on average. We get it to calculate predictions for the entire test dataset and average them out.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dress_accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is_dress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_dresses_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pullover_accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is_dress&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_pullovers_tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;combined_accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dress_accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pullover_accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;combined_accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dress_accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pullover_accuracy&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;(tensor(0.9175), tensor(0.9730), tensor(0.8620))&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Overall, I think 91% accuracy using this fairly simple mechanism isn't too bad at all! That said, in the fastai course we're here to learn about deep learning, so in the next post I will dive more into the beginnings of a more advanced approach to making the same calculation.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="fastai" /><category term="computervision" /><category term="pytorch" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/fashion-mnist/fashion-mnist-small.png" /><media:content medium="image" url="https://mlops.systems/images/fashion-mnist/fashion-mnist-small.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A painless way to create an MVP demo using computer vision models</title><link href="https://mlops.systems/fastai/computervision/redactionmodel/tools/2022/05/07/redaction-mvp-huggingface.html" rel="alternate" type="text/html" title="A painless way to create an MVP demo using computer vision models" /><published>2022-05-07T00:00:00-05:00</published><updated>2022-05-07T00:00:00-05:00</updated><id>https://mlops.systems/fastai/computervision/redactionmodel/tools/2022/05/07/redaction-mvp-huggingface</id><content type="html" xml:base="https://mlops.systems/fastai/computervision/redactionmodel/tools/2022/05/07/redaction-mvp-huggingface.html">&lt;h1 id=&quot;-motivation&quot;&gt;🚦 Motivation&lt;/h1&gt;

&lt;p&gt;After the second class of the fastai course, we’re encouraged to create mini-projects that result in models we can deploy online. Deployment is a huge field with its own complexities, of course, but having an option to get something out in the world that’s visible and usable is extremely useful.&lt;/p&gt;

&lt;p&gt;In this post, I will walk you through how I built a super quick MVP of my redacted document detector project. I used:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/fastai/fastai&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fastai&lt;/code&gt;&lt;/a&gt; to classify and extract redacted pages extracted from PDFs&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://airctic.com/&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;icevision&lt;/code&gt;&lt;/a&gt; (&lt;a href=&quot;https://twitter.com/ai_fast_track&quot;&gt;@ai_fast_track&lt;/a&gt;) to detect the redacted areas&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://huggingface.co/spaces&quot;&gt;HuggingFace Spaces&lt;/a&gt; (with &lt;a href=&quot;https://gradio.app&quot;&gt;Gradio&lt;/a&gt; and &lt;a href=&quot;https://streamlit.io&quot;&gt;Streamlit&lt;/a&gt;) to deploy my MVP&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The post shows how I went about thinking through the task, showcasing some examples of small prototypes I built along the way, including the final stage where I built:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;an app including everything that would be needed by a final ‘deployed’ use case of my model&lt;/li&gt;
  &lt;li&gt;two models working in tandem in the same app (one classification, one object detection)&lt;/li&gt;
  &lt;li&gt;optional PDF generation of items detected by the model (!)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I also explore why you might want to have a minimal deployed version of your application in the first place!&lt;/p&gt;

&lt;h1 id=&quot;-step-by-step-iteration-by-iteration&quot;&gt;🐾 Step by step, iteration by iteration&lt;/h1&gt;

&lt;p&gt;This week I chose to use my previous work on redacted images to leverage a dataset &lt;a href=&quot;https://mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html&quot;&gt;I’d previously collected&lt;/a&gt;. I wanted to showcase something useful and interesting and I ended up slightly blocked as to what I was going to build. After discussing it with &lt;a href=&quot;https://www.meetup.com/delft-fast-ai-study-group/&quot;&gt;the study group&lt;/a&gt; briefly, I was reminded not to try to bite off too much: start small with the smallest possible next version of what you want, and then continue from there.&lt;/p&gt;

&lt;p&gt;Since I already had a large dataset of redacted and unredacted images (extracted from PDF documents available online), I used this to train a classification model that could tell whether a page contained redactions or not.&lt;/p&gt;

&lt;p&gt;With that model exported, it was then easy to get a simple &lt;a href=&quot;https://gradio.app&quot;&gt;Gradio&lt;/a&gt; app demo up and running, particularly with the suggestions in Tanishq Abraham’s &lt;a href=&quot;https://tmabraham.github.io/blog/gradio_hf_spaces_tutorial&quot;&gt;really useful tutorial blogpost&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It’s an easy step to go from having a Gradio app deployed to then hosting that same demo as a Huggingface Space, so I then did that. You can &lt;a href=&quot;https://huggingface.co/spaces/strickvl/fastai_redaction_classifier&quot;&gt;access the demo here&lt;/a&gt; at &lt;a href=&quot;https://huggingface.co/spaces/strickvl/fastai_redaction_classifier&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;strickvl/fastai_redaction_classifier&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/redaction-mvp-huggingface/classification-demo.png&quot; alt=&quot;&quot; title=&quot;A Gradio app hosted on Huggingface Spaces: an image classifier that detects whether an image input contains a redaction or not.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At this first stage I had the exported model itself uploaded inside the Spaces repository, but &lt;a href=&quot;https://huggingface.co/blog/fastai&quot;&gt;this useful blog&lt;/a&gt; by Omar Espejel showed how I could just upload my model directly to the Huggingface model hub. Instead of calling &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;learn.export('model.pkl')&lt;/code&gt; and uploading the model file itself, I could just run the following code after authentication:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;huggingface_hub&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;push_to_hub_fastai&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;repo_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;MY_USERNAME/MY_LEARNER_NAME&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;push_to_hub_fastai&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learner&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;repo_id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;repo_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;My model &lt;a href=&quot;https://huggingface.co/strickvl/redaction-classifier-fastai&quot;&gt;lives here&lt;/a&gt; on the Huggingface model hub and can be directly saved or just used via the hosted Inference API.&lt;/p&gt;

&lt;h1 id=&quot;️-using-the-inference-api-for-more-flexibility&quot;&gt;⚡️ Using the inference API for more flexibility&lt;/h1&gt;

&lt;p&gt;Buoyed on by Tanishq’s blog and the workflow we’d seen in the lecture that week, I thought it might be worth running my inference requests through the HTTP API instead of letting Huggingface handle all that.&lt;/p&gt;

&lt;p&gt;Thanks to a really simple and comprehensible &lt;a href=&quot;https://github.com/nuvic/predict_image&quot;&gt;example&lt;/a&gt; made by &lt;a href=&quot;https://github.com/nuvic&quot;&gt;@Nuvic&lt;/a&gt; I was quickly able to get something up and running. The forked source code is available &lt;a href=&quot;https://github.com/strickvl/predict_redaction_classification&quot;&gt;here&lt;/a&gt; and the main website where you can try out the tool is here: &lt;a href=&quot;https://strickvl.github.io/predict_redaction_classification/&quot;&gt;https://strickvl.github.io/predict_redaction_classification/&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;If you &lt;a href=&quot;https://duckduckgo.com/?q=redacted+document&amp;amp;t=osx&amp;amp;iax=images&amp;amp;ia=images&quot;&gt;search for ‘redacted document’ images&lt;/a&gt; and save one of them do your local computer you can use those to try it out. It uses simple Javascript code to pass the image you upload into the inference API using a simple HTTP request. It parses the results and displays them as shown here:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/redaction-mvp-huggingface/gh-pages-demo.png&quot; alt=&quot;&quot; title=&quot;A demo using Github Pages to host a simple app showcasing the model inference.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;While the demo gives a sense of the model’s capabilities, in reality you would probably not find it very helpful to use a web app that required you to feed a document’s pages to it one by one. I started to think about a more complex application where you could upload a PDF and it would split the PDF for you and do all the inference behind the scenes.&lt;/p&gt;

&lt;h1 id=&quot;-building-an-mvp-of-a-redaction-detection-application&quot;&gt;🚀 Building an MVP of a redaction detection application&lt;/h1&gt;

&lt;p&gt;I spent a brief half-hour considering deploying a simple &lt;a href=&quot;https://flask.palletsprojects.com/en/2.1.x/&quot;&gt;Flask&lt;/a&gt; web app hosted somewhere for free before realising I didn’t even need to go that far to create a proof of concept that would have the required functionality. I returned back to Huggingface Spaces hoping that I’d be able to build everything out.&lt;/p&gt;

&lt;p&gt;You can access the demo / MVP app that I created here:
&lt;a href=&quot;https://huggingface.co/spaces/strickvl/redaction-detector&quot;&gt;https://huggingface.co/spaces/strickvl/redaction-detector&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/redaction-mvp-huggingface/demo-screenshot.png&quot; alt=&quot;&quot; title=&quot;An MVP app for detection, extraction and analysis of PDF documents that contain redactions.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This MVP app runs two models to mimic the experience of what a final deployed version of the project might look like.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The first model (a classification model trained with fastai, available on the Huggingface Hub &lt;a href=&quot;https://huggingface.co/strickvl/redaction-classifier-fastai&quot;&gt;here&lt;/a&gt; and testable as a standalone demo &lt;a href=&quot;https://huggingface.co/spaces/strickvl/fastai_redaction_classifier&quot;&gt;here&lt;/a&gt;), classifies and determines which pages of the PDF are redacted. I’ve written about how I trained this model &lt;a href=&quot;https://mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The second model (an object detection model trained using &lt;a href=&quot;https://airctic.com/&quot;&gt;IceVision&lt;/a&gt; (itself built partly on top of fastai)) detects which parts of the image are redacted. This is a model I’ve been working on for a while and I described my process in &lt;a href=&quot;https://mlops.systems/categories/#redactionmodel&quot;&gt;a series of blog posts&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This MVP app does several things:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;it extracts any pages it considers to contain redactions and displays that subset as an &lt;a href=&quot;https://gradio.app/docs/#o_carousel&quot;&gt;image carousel&lt;/a&gt;. It also displays some text alerting you to which specific pages were redacted.&lt;/li&gt;
  &lt;li&gt;if you click the “Analyse and extract redacted images” checkbox, it will:&lt;/li&gt;
  &lt;li&gt;pass the pages it considered redacted through the object detection model&lt;/li&gt;
  &lt;li&gt;calculate what proportion of the total area of the image was redacted as well as what proportion of the actual content (i.e. excluding margins etc where there is no content)&lt;/li&gt;
  &lt;li&gt;create a PDF that you can download that contains only the redacted images, with an overlay of the redactions that it was able to identify along with the confidence score for each item.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;converting-a-gradio-app-over-to-streamlit&quot;&gt;Converting a Gradio app over to Streamlit&lt;/h1&gt;

&lt;p&gt;I was curious about the differences between the main two options enabled by Huggingface Spaces, so I then worked a little on converting my Gradio app to a Streamlit app. The process of conversion was fairly easy for the most part; the only difference is the style of programming expected by Streamlit:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Streamlit is a less declarative style of creating your app. It runs your code from top to bottom, rendering whatever elements you specify.&lt;/li&gt;
  &lt;li&gt;This seems to result in more verbose code (e.g. compare &lt;a href=&quot;https://huggingface.co/spaces/strickvl/redaction-detector/blob/main/app.py&quot;&gt;this&lt;/a&gt; with &lt;a href=&quot;https://huggingface.co/spaces/strickvl/redaction-detector-streamlit/blob/main/streamlit_app.py&quot;&gt;this&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are two easy ways to deploy a Streamlit app: either host it natively on Streamlit itself, or host it on Huggingface Spaces. The advantage of hosting natively on Streamlit is that you essentially have what looks and feels like a custom website that is 100% your application. In the end, I didn’t go down this route for two reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Hosting via Huggingface Spaces keeps the connection between your demo app and your username. You can click through to view all of my demos and applications &lt;a href=&quot;https://huggingface.co/strickvl&quot;&gt;here&lt;/a&gt;, for example. On Streamlit there is currently no concept of a user’s portfolio. If you’re trying to showcase your work, Huggingface Spaces is the clear winner in this regard.&lt;/li&gt;
  &lt;li&gt;Hosting on Streamlit seems to have restrictive memory constraints. I frequently ran into restrictions on the machine that was running my application and would quite often be encouraged to reboot my app, clearing its cache, and instructed to refer to &lt;a href=&quot;https://blog.streamlit.io/common-app-problems-resource-limits/&quot;&gt;docs on how to make my application more efficient&lt;/a&gt;. The docs were useful, but I ran into issues using the Streamlit cache (the main solution offered) because of the models I was using. Luckily, Huggingface Spaces’ backend instances seem far more generous in terms of resources. For small / trivial apps not doing much you’ll be fine with Streamlit, but for anything more involved there’s more of a decision to be made.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I didn’t convert all the various parts of my Gradio app over to work on Streamlit — in particular extraction of images and display as a carousel was non-trivial — but you can get a sense of the flexibility with this image:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/redaction-mvp-huggingface/demo-streamlit.png&quot; alt=&quot;&quot; title=&quot;A partly converted version of my demo app using Streamlit.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(Alternatively, you can try it out over on Huggingface Spaces &lt;a href=&quot;https://huggingface.co/spaces/strickvl/redaction-detector-streamlit&quot;&gt;here&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;You can see that I was able to insert a chart to display the proportion calculations. This is much more pleasant than the pure text version. Streamlit’s &lt;a href=&quot;https://docs.streamlit.io&quot;&gt;documentation&lt;/a&gt; is pretty great and their basic &lt;a href=&quot;https://docs.streamlit.io/library/get-started&quot;&gt;‘Get started’ tutorial&lt;/a&gt; should indeed be your first port of call.&lt;/p&gt;

&lt;h1 id=&quot;-lessons-learned&quot;&gt;🤔 Lessons learned&lt;/h1&gt;

&lt;div class=&quot;Toast&quot;&gt;
   &lt;span class=&quot;Toast-icon&quot;&gt;&lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/span&gt;
   &lt;span class=&quot;Toast-content&quot;&gt;&lt;p&gt;In this post you learned:&lt;/p&gt;&lt;p&gt;1️⃣ to start with simple prototypes&lt;/p&gt; &lt;p&gt;2️⃣ how to easily deploy fastai models on Huggingface Spaces and the Hub and&lt;/p&gt; &lt;p&gt;3️⃣ that you can create functional MVP demos of real products and applications&lt;/p&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;I was — and continue to be — surprised that the free Huggingface Spaces environment has no problem running all this fairly compute-intensive inference on their backend. (That said, if you try to upload a document containing dozens or hundreds of pages and you’ll quickly hit up against the edge of what they allow.)&lt;/p&gt;

&lt;p&gt;I became very familiar with the &lt;a href=&quot;https://gradio.app/docs/&quot;&gt;Gradio interface docs&lt;/a&gt; while creating this app and was impressed by how customisable the final application could be. You don’t have as much freedom as a web application written from scratch, but you still &lt;em&gt;do&lt;/em&gt; have a lot of freedom.&lt;/p&gt;

&lt;h2 id=&quot;-when-to-use-gradio&quot;&gt;📐 When to use Gradio&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;if you have a simple use case that you want to highlight&lt;/li&gt;
  &lt;li&gt;if your inputs and outputs are clearly defined&lt;/li&gt;
  &lt;li&gt;if you have a single model to showcase&lt;/li&gt;
  &lt;li&gt;if you want to get something quickly deployed&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;-when-to-use-streamlit&quot;&gt;🌊 When to use Streamlit&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;if your use case is more interactive or less simple than just basic input-then-output&lt;/li&gt;
  &lt;li&gt;if you want more control on how your demo application is displayed&lt;/li&gt;
  &lt;li&gt;if you enjoy a more imperative style of programming&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Given how much inference is going on behind the scenes, I’m surprised that these applications run as fast as it does. For a document with 4 or 5 redacted pages, it takes around 10 seconds to do all the steps described above. 10 seconds is still far too long for a scenario where you wanted to run inference over millions of pages, but in that scenario you wouldn’t be manually uploading them on a web app either.&lt;/p&gt;

&lt;p&gt;It’s extremely gratifying to have these kinds of tools available to use for free, and really exciting that you get to build out prototypes of this kind after just two weeks of study on the fastai course.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="fastai" /><category term="computervision" /><category term="redactionmodel" /><category term="tools" /><summary type="html">🚦 Motivation</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/redaction-mvp-huggingface/demo-screenshot.png" /><media:content medium="image" url="https://mlops.systems/images/redaction-mvp-huggingface/demo-screenshot.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">How my pet cat taught me a lesson about validation data for image classification</title><link href="https://mlops.systems/fastai/computervision/2022/05/02/pet-cat-image-classifier-fastai.html" rel="alternate" type="text/html" title="How my pet cat taught me a lesson about validation data for image classification" /><published>2022-05-02T00:00:00-05:00</published><updated>2022-05-02T00:00:00-05:00</updated><id>https://mlops.systems/fastai/computervision/2022/05/02/pet-cat-image-classifier-fastai</id><content type="html" xml:base="https://mlops.systems/fastai/computervision/2022/05/02/pet-cat-image-classifier-fastai.html">&lt;p&gt;I’m participating in &lt;a href=&quot;https://itee.uq.edu.au/event/2022/practical-deep-learning-coders-uq-fastai&quot;&gt;the latest iteration&lt;/a&gt; of &lt;a href=&quot;https://www.fast.ai&quot;&gt;the fastai course&lt;/a&gt; as taught by Jeremy Howard. This past week we got a very high-level overview of some of the ways deep learning is proving very powerful in solving problems as well as how we can use its techniques to fairly quickly get great results on image classification problems.&lt;/p&gt;

&lt;p&gt;I’ve done the earlier parts of the course before, so some of these demonstrations were less mind-blowing than the first time I saw them. For this iteration of the course, Jeremy showcased &lt;a href=&quot;https://www.kaggle.com/code/jhoward/is-it-a-bird-creating-a-model-from-your-own-data&quot;&gt;a Kaggle notebook&lt;/a&gt; which trains a model to distinguish whether an image is of a bird or not.&lt;/p&gt;

&lt;p&gt;Last time I did the course, I &lt;a href=&quot;https://mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html#fn:3&quot;&gt;trained an image classifier model&lt;/a&gt; to distinguish whether an image was redacted or not to around 95% accuracy. (This actually was the genesis of &lt;a href=&quot;https://mlops.systems/categories/#redactionmodel&quot;&gt;my larger redaction object detection project&lt;/a&gt; that I’ve been blogging about for the past few months.)&lt;/p&gt;

&lt;h1 id=&quot;the-key-ingredients-what-goes-into-a-model&quot;&gt;The key ingredients: what goes into a model?&lt;/h1&gt;

&lt;p&gt;The course teaches things top-down, so we start off with both the practical experience of training state-of-the-art models as well as the overall context to what goes into these high-level functions. These pieces include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;your input data — this style of programming differs from traditional software engineering where your functions take data in order to ‘learn’ how to make their predictions&lt;/li&gt;
  &lt;li&gt;the ‘weights’ — when we’re using pre-trained models, you can think of these as an initial set of variables that are already pretty useful in that configuration and can do a lot of things.&lt;/li&gt;
  &lt;li&gt;your model — this is what you’re training and, once trained, you can think of it as a function in and of itself that takes in inputs and outputs predictions.&lt;/li&gt;
  &lt;li&gt;the predictions — these are the guesses that your model makes, based on whatever you pass in as inputs. So if you pass in an image of a cat to a model (see below), the prediction could be whether that cat is one particular kind or another.&lt;/li&gt;
  &lt;li&gt;your ‘loss’ — this is a measure of checking how well your model is doing as it trains.&lt;/li&gt;
  &lt;li&gt;a means of updating your weights — depending on how well (or badly) the training goes, you’ll want a way of updating the weights so that each time it gets a bit better at optimising for whatever you’ve set up your model to do. In lesson one we learn about &lt;em&gt;stochastic gradient descent&lt;/em&gt;, a way of optimising and updating these weights automatically.&lt;/li&gt;
  &lt;li&gt;your labels — these are the ground truth assertions that get used to determine how well the model is doing as it trains.&lt;/li&gt;
  &lt;li&gt;transformations &amp;amp; augmentations — more on this will come in lesson two, but these allow you to squeeze more value out of your data. This is especially valuable when you’re fine-tuning a model and don’t have massive amounts of data to use for training.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Represented in code, the classic fastai example where you train a model to distinguish between cats and dogs is as follows:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;fastai.vision.all&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;untar_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;URLs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PETS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'images'&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;is_cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isupper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dls&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ImageDataLoaders&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_name_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_image_files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valid_pct&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;label_func&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item_tfms&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Resize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;224&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vision_learner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet34&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fine_tune&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This small code snippet contains all the various parts just mentioned. The high-level API and abstractions that fastai provides allows you to work with these concepts in a way that is fast and flexible, though if you need to dive into the details you can do so as well.&lt;/p&gt;

&lt;h1 id=&quot;image-classification-isnt-just-about-images&quot;&gt;Image classification isn’t just about images&lt;/h1&gt;

&lt;p&gt;One of the parts of the first chapter I enjoy the most is the examples of projects where image classification was applied to problems or scenarios where it doesn’t first appear that the problem has anything to do with computer vision.&lt;/p&gt;

&lt;p&gt;We see &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/8328749&quot;&gt;malware converted into images&lt;/a&gt; and distinguished using classification. We see &lt;a href=&quot;https://medium.com/@etown/great-results-on-audio-classification-with-fastai-library-ccaf906c5f52&quot;&gt;sounds in an urban environment&lt;/a&gt; converted into images and classified with fastai. In &lt;a href=&quot;https://www.meetup.com/delft-fast-ai-study-group/&quot;&gt;the study group&lt;/a&gt; I host for some student on the course, one of our members presented &lt;a href=&quot;https://kurianbenoy.com/ml-blog/fastai/fastbook/2022/05/01/AudioCNNDemo.html&quot;&gt;an initial proof of concept&lt;/a&gt; of using images of music to distinguish genre:&lt;/p&gt;

&lt;div class=&quot;jekyll-twitter-plugin&quot;&gt;&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Saturday evening side-project: Trained a baseline ML model to classify audio files to identify their music genre using &lt;a href=&quot;https://twitter.com/fastdotai?ref_src=twsrc%5Etfw&quot;&gt;@fastdotai&lt;/a&gt; based on a kaggle dataset.&lt;br /&gt;&lt;br /&gt;Acheived only 50% accuracy, probably because problem is hard. Next job is to check what &lt;a href=&quot;https://twitter.com/DienhoaT?ref_src=twsrc%5Etfw&quot;&gt;@DienhoaT&lt;/a&gt; has done to win a GPU. &lt;a href=&quot;https://t.co/EahvgtYBDL&quot;&gt;pic.twitter.com/EahvgtYBDL&lt;/a&gt;&lt;/p&gt;&amp;mdash; Kurian Benoy (@kurianbenoy2) &lt;a href=&quot;https://twitter.com/kurianbenoy2/status/1520470393760272384?ref_src=twsrc%5Etfw&quot;&gt;April 30, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/div&gt;

&lt;p&gt;I like the creativity needed to think of how to turn problems and data into a form such that they can become computer vision problems.&lt;/p&gt;

&lt;h1 id=&quot;my-own-efforts-classifying-my-cat&quot;&gt;My own efforts: classifying my cat&lt;/h1&gt;

&lt;p&gt;True story: a few years ago my cat escaped from the vet and a reward was mentioned for anyone who found our cute ginger cat.  Throughout the course of the day, the vets were perplexed to see people coming in with random ginger cats that they’d found in the neighborhood, but none of them were ours! With this iteration of the course, therefore, I was curious to try out this simple but slightly silly example and see how well a deep learning model could do at recognising distinguishing Mr Blupus — don’t ask! — from other random photos of ginger cats.&lt;/p&gt;

&lt;p&gt;Training the model was pretty easy. Like any cat owner, I have thousands of photos of our cat so an initial dataset to use was quick to assemble. I downloaded a few hundred random ginger cat photos via DuckDuckGo using some code Jeremy had used in &lt;a href=&quot;https://www.kaggle.com/code/jhoward/is-it-a-bird-creating-a-model-from-your-own-data&quot;&gt;his bird vs forest Kaggle notebook&lt;/a&gt;. A few minutes and ten epochs later, I had achieved 96.5% accuracy on my validation data after fine-tuning &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;resnet50&lt;/code&gt;!&lt;/p&gt;

&lt;div class=&quot;jekyll-twitter-plugin&quot;&gt;&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Little Saturday afternoon side-project: training an image classification model using &lt;a href=&quot;https://twitter.com/fastdotai?ref_src=twsrc%5Etfw&quot;&gt;@fastdotai&lt;/a&gt; to distinguish my cat (&amp;#39;Mr Blupus&amp;#39; -- don&amp;#39;t ask!) from other random ginger cat photos.&lt;br /&gt;&lt;br /&gt;Achieved 96.5% accuracy on the validation set within a few minutes! &lt;a href=&quot;https://t.co/by5ZlM0Kkp&quot;&gt;pic.twitter.com/by5ZlM0Kkp&lt;/a&gt;&lt;/p&gt;&amp;mdash; Alex Strick van Linschoten (@strickvl) &lt;a href=&quot;https://twitter.com/strickvl/status/1520405802091175936?ref_src=twsrc%5Etfw&quot;&gt;April 30, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/div&gt;

&lt;p&gt;After the initial excitement died down, I realised that the result was probably an illusion. Our cat is an indoor cat and we have a relatively small house. Couple that with the fact that the backdrops to the photos of Mr Blupus are relatively distinctive (particular kinds of sheets or carpets) and it seems pretty clear that the model wasn’t learning how to identify our cat, but rather it was learning how to distinguish photos of our house or our carpets.&lt;/p&gt;

&lt;p&gt;☹️&lt;/p&gt;

&lt;p&gt;Luckily, chapter one gets into exactly this problem, showing an example of how exactly this validation issue can give you a false sense of confidence in your model. When I evaluated my model on the validation data it wasn’t a fair test, since in all likeliness may model had already seen a similar backdrop to whatever was found inside the validation set.&lt;/p&gt;

&lt;p&gt;I discussed this when I presented this to those at the study group / meetup yesterday and we agreed that it’d be best if I held out some settings or locations from the training entirely. I took 30 minutes to do that in the evening and had a third ‘test’ dataset which consisted of 118 images of our cat in certain locations that the model wasn’t trained on and thus couldn’t use to cheat. I added a few more photos to the training data so that there were enough examples from which to learn.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blupus_detection/training-data.png&quot; alt=&quot;&quot; title=&quot;Input data for the new model.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I was supposedly getting 98% accuracy now, but I knew that number to be false. I then needed to figure out how to get the accuracy for my held-out test set. With a lot of help from &lt;a href=&quot;https://twitter.com/Fra_Pochetti&quot;&gt;Francesco&lt;/a&gt; and &lt;a href=&quot;https://benjaminwarner.dev/2021/10/01/inference-with-fastai#batch-prediction&quot;&gt;a really useful blogpost&lt;/a&gt; on doing batch inference with fastai, I first got the predictions for my test data:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;test_files&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fn&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/path/to/test_set_blupus_photos&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'**/*'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_dl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_dl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_dl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I then created a tensor with the ground truth predictions for my test set and compared them with what my model had predicted:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;gts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;118&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this point, getting the final accuracy was as simple as getting the proportion of correct guesses:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This gave me an accuracy on my held-out test set of 93.2% which was surprisingly good.&lt;/p&gt;

&lt;p&gt;I half wonder whether there is still some cheating going on somehow, some quality of the photos or the iPhone camera I used to take them that is being used to distinguish the photos of my cat vs other ginger cats.&lt;/p&gt;

&lt;p&gt;Nevertheless, this was a useful lesson for me to learn. I realised while working with the tensors in the final step above that I’m not at all comfortable manipulating data with PyTorch so luckily that’ll get covered in future lessons.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;UPDATE:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Following some discussion in the fastai forums, it was suggested that I take a look at &lt;a href=&quot;http://gradcam.cloudcv.org&quot;&gt;Grad-CAM&lt;/a&gt; in chapter 18. This is a technique to visualise the activations which allows you to see which parts of the image it is paying the most attention to (sort of). I ran the code using a sample Blupus image and this was the result. I don’t understand how most (any?) of this works, but it was really cool to have a working result of sorts nonetheless!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/blupus_detection/gradcam.png&quot; alt=&quot;&quot; title=&quot;Mr Blupus activating the model.&quot; /&gt;&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="fastai" /><category term="computervision" /><summary type="html">I’m participating in the latest iteration of the fastai course as taught by Jeremy Howard. This past week we got a very high-level overview of some of the ways deep learning is proving very powerful in solving problems as well as how we can use its techniques to fairly quickly get great results on image classification problems.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/blupus_detection/blupus-training.png" /><media:content medium="image" url="https://mlops.systems/images/blupus_detection/blupus-training.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">How to trust the data you feed your model: alternative data validation solutions in a computer vision context (part 3)</title><link href="https://mlops.systems/tools/redactionmodel/computervision/datavalidation/2022/04/28/data-validation-great-expectations-part-3.html" rel="alternate" type="text/html" title="How to trust the data you feed your model: alternative data validation solutions in a computer vision context (part 3)" /><published>2022-04-28T00:00:00-05:00</published><updated>2022-04-28T00:00:00-05:00</updated><id>https://mlops.systems/tools/redactionmodel/computervision/datavalidation/2022/04/28/data-validation-great-expectations-part-3</id><content type="html" xml:base="https://mlops.systems/tools/redactionmodel/computervision/datavalidation/2022/04/28/data-validation-great-expectations-part-3.html">&lt;p&gt;&lt;em&gt;(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out &lt;a href=&quot;https://mlops.systems/categories/#redactionmodel&quot;&gt;the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;redactionmodel&lt;/code&gt; taglist&lt;/a&gt;.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://mlops.systems/tools/redactionmodel/computervision/datavalidation/2022/04/19/data-validation-great-expectations-part-1.html&quot;&gt;previous&lt;/a&gt; &lt;a href=&quot;https://mlops.systems/tools/redactionmodel/computervision/datavalidation/2022/04/26/data-validation-great-expectations-part-2.html&quot;&gt;two&lt;/a&gt; posts in this series have made the case for why you might want to consider adding a Great Expectations step or stage to your computer vision project, particularly once it becomes something you’re going to want to iterate on a few times.&lt;/p&gt;

&lt;p&gt;This post begins by showcasing how you can use Evidently’s open-source library to calculate and visualise comparisons between your data. I list some of the lighter alternatives to Great Expectations and Evidently, concluding with some thoughts on when you might use it as part of your computer vision pipeline.&lt;/p&gt;

&lt;h2 id=&quot;tldr-alternatives-for-data-validation-using-python&quot;&gt;TL;DR: Alternatives for data validation using Python&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;🛠 Data validation tools come in many flavours, from full-featured libraries like Great Expectations down to the humble &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;assert&lt;/code&gt; statement in Python.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;⚠️ The tool you choose should be appropriate to your particular use case and situation. You might not need or want to add a large dependency or take on extra code / project complexity, in which case there are alternative options available to you.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;⏰ You’ll also want to think about &lt;em&gt;when&lt;/em&gt; you’re doing your validation. Two key moments stand out for machine learning projects: when you’re ingesting data prior to training or fine-tuning a model, and at the moment where you’re doing inference on a trained model.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;📃 For my project, I’m using a variety of tools as part of my process because I’ve found it gives me confidence in the predictions my model is making and it gives me freedom to experiment and iterate, without needing to also worry that I’m silently breaking something with downstream effects on my model performance.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;alternatives-using-evidently-for-drift-detection&quot;&gt;Alternatives: Using Evidently for drift detection&lt;/h2&gt;

&lt;p&gt;I’ve &lt;a href=&quot;https://blog.zenml.io/zenml-loves-evidently/&quot;&gt;previously written&lt;/a&gt; about why &lt;a href=&quot;https://evidentlyai.com&quot;&gt;Evidently&lt;/a&gt; is a great tool to use for drift detection and data monitoring over &lt;a href=&quot;https://blog.zenml.io/zenml-loves-evidently/&quot;&gt;on the ZenML blog&lt;/a&gt;. At its core, Evidently takes two chunks of data and compares them. The statistical comparisons going on under the hood are quite sophisticated, but as an interface to be used it is extremely trivial to get going.&lt;/p&gt;

&lt;p&gt;In the case of my redaction project data, I did the work of transforming my annotation and image metadata into Pandas DataFrames for Great Expectations already, so using it with Evidently at this point is trivial:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;evidently.dashboard&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dashboard&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;evidently.dashboard.tabs&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataDriftTab&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;evidently.pipeline.column_mapping&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ColumnMapping&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;real_annotations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main_annotations_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'area'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'category_name'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'top_left_x'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'top_left_y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'width'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'height'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'orientation'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;easy_synth_annotations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;easy_synth_annotations_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'area'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'category_name'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'top_left_x'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'top_left_y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'width'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'height'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'orientation'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;hard_synth_annotations&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hard_synth_annotations_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'area'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'category_name'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'top_left_x'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'top_left_y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'width'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'height'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'orientation'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;column_mapping&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ColumnMapping&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;numerical_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;area&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;width&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;height&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'top_left_x'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'top_left_y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;categorical_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;category_name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'orientation'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;drift_report&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dashboard&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tabs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataDriftTab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;drift_report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;calculate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;real_annotations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hard_synth_annotations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;column_mapping&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;column_mapping&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;drift_report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;reports/my_report.html&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this code, I’m comparing between the real (i.e. manually annotated) annotations and the ‘hard’ synthetic annotations that I created (and &lt;a href=&quot;https://mlops.systems/tools/redactionmodel/computervision/2022/04/06/synthetic-data-results.html&quot;&gt;blogged about recently&lt;/a&gt;). I choose the columns I care about, tell Evidently which columns are numerical vs categorical features and save the report. (I can also display the report directly within a Jupyter notebook.) When I open the report, I see this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/great_expectations/evidently-drift-comparison.png&quot; alt=&quot;&quot; title=&quot;The drift detection report produced by Evidently.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can unfold the graphs to dive into the details for specific features, as in the following example where I take a look at the orientation of my annotations and see the difference between my manual annotations and the synthetically generated ‘hard’ batch:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/great_expectations/comparing-orientation.png&quot; alt=&quot;&quot; title=&quot;Comparing landscape vs portrait orientation of my redaction annotations using Evidently.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It doesn’t surprise me too much that we have this disparity, since the only annotations that are portrait in the synthetically-generated set are those for the content box around the whole page. All the rest are landscape, and that’s by design. (Note: you can make the comparisons using different statistical tests depending on your use case. I’m told that the next Evidently release will increase the number of available options for this.)&lt;/p&gt;

&lt;p&gt;I can repeat the same test for the image DataFrame. I’ve included some metadata for each image such as how many annotations are associated with the image, or how many redaction vs content annotations are associated and so on. The code is basically the same, except now taking into account the different columns and their types:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# comparing between real images and hard_synth images
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;column_mapping&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ColumnMapping&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;numerical_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;area&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;width&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;height&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'annotation_count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'content_annotation_count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'redaction_annotation_count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'area'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'file_size_bytes'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;categorical_features&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'orientation'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'format'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'mode'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;drift_report&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dashboard&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tabs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataDriftTab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;drift_report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;calculate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;main_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hard_synth_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;column_mapping&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;column_mapping&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;drift_report&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;reports/my_report-real-vs-hard-images.html&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And we get this report:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/great_expectations/images-drift.png&quot; alt=&quot;&quot; title=&quot;Viewing a drift report comparing the core image data with my synthetically-created images.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can immediately see how certain things like the number of annotations and the number of redactions in an image was a bit different when comparing the two. We also seem to have a far more even distribution of file sizes in the synthetically generated images and that makes sense since that was essentially randomly determined.&lt;/p&gt;

&lt;p&gt;Note that all the data that goes into making these reports can be accessed programatically as a Python object or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;JSON&lt;/code&gt; through &lt;a href=&quot;https://docs.evidentlyai.com/features/profiling&quot;&gt;Evidently’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Profile&lt;/code&gt; feature&lt;/a&gt;, which is probably what you’re going to want when assessing for drift as part of a continuous training / continuous deployment cycle.&lt;/p&gt;

&lt;p&gt;If you change just a few things once more, you get a really useful &lt;a href=&quot;https://docs.evidentlyai.com/reports/data-quality&quot;&gt;data quality report&lt;/a&gt; showing distributions, correlations, and various other features of your data at a single glance:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# profiling data quality
&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;evidently.dashboard.tabs&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataQualityTab&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;quality_dashboard&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dashboard&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tabs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataQualityTab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;quality_dashboard&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;calculate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;main_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hard_synth_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;column_mapping&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images_column_mapping&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;quality_dashboard&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;reports/quality-report.html&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can get an idea of the report that it produces in the following screen recording from my browser:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/great_expectations/evidently-data-quality.gif&quot; alt=&quot;&quot; title=&quot;The Evidently data quality report when comparing my real image data with what was synthetically created.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As a place to get started with understanding a dataset, this is a pretty nice visualisation and report to have in your toolkit, but even after immersion in your data it can be useful to take a step back with something like this data quality overview. For instance, it reveals quite clearly how the average number of annotations in my manually annotated dataset is quite a bit lower than that of my synthetically generated examples. Of course, that was by intention, but it is nice to see that confirmed in the data.&lt;/p&gt;

&lt;p&gt;Once you have your model ready, there are other reports that Evidently offers which perhaps I’ll return to in a subsequent blogpost but for now I hope this has given you a flavour of the tool and how easy it is to get going with it.&lt;/p&gt;

&lt;p&gt;(As a side-note, &lt;a href=&quot;https://evidentlyai.com/community&quot;&gt;Evidently’s community&lt;/a&gt; is friendly, welcoming and filled with interesting people thinking about these issues. I find it a welcome breath of fresh air when compared with some other tools’ forums or chat platforms, so it also has that going for it!)&lt;/p&gt;

&lt;h2 id=&quot;alternatives-some-other-options&quot;&gt;Alternatives: some other options&lt;/h2&gt;

&lt;p&gt;With Evidently, we drifted a little into the ‘visualise your data’ territory which wasn’t really the point of this post, but you can see how they combined clear visualisation with the statistical validation working underneath to calculate whether data was drifting. The following are some other tools I’ve come across that might help you in validating data in a computer vision context. I haven’t found a use for them in my project, but it’s possible that they might gel with what you’re doing:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/tensorflow/data-validation&quot;&gt;TensorFlow Data Validation (TFDV)&lt;/a&gt; — This is a part of TensorFlow and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tfx&lt;/code&gt; which uses schemas to validate your data. If you’re using TensorFlow, you might have heard of this and might even be using it already, but I don’t get the sense that this is often much recommended. I include it as it is a prominent option available to you.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://deepchecks.com&quot;&gt;Deepchecks&lt;/a&gt; — Deepchecks is adjacent to what Great Expectations offers, albeit with an emphasis on the kinds of tests you might want to do for ML model training code. It has some features and documented use cases for computer vision (object detection and classification) but I haven’t used it myself. Feels like a tool worth keeping your eye on, however. (Works on Pandas dataframes and numpy arrays.)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pandera.readthedocs.io/en/latest/index.html&quot;&gt;pandera&lt;/a&gt; — This is a statistical tool for validating data inside dataframes, and it overlaps quite a bit in its functionality with Great Expectations, particularly with the &lt;a href=&quot;https://pandera.readthedocs.io/en/latest/hypothesis.html#hypothesis&quot;&gt;hypothesis testing&lt;/a&gt; functionality. Worth checking out.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.python-cerberus.org/en/stable/index.html&quot;&gt;Cerberus&lt;/a&gt; — Offers a lightweight schema-based validation functionality for Python objects.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://python-jsonschema.readthedocs.io/en/latest/&quot;&gt;jsonschema&lt;/a&gt; — similar in approach to Cerberus, above, this is a lightweight way to test your JSON files based on how they conform to a defined schema. Useful in the case of annotations files, perhaps, if you really want something minimal.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/keleshev/schema&quot;&gt;schema&lt;/a&gt; — More of the same: a Pythonic way to validate JSON or YAML files based on schema.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;assert&lt;/code&gt; — We shouldn’t forget &lt;a href=&quot;https://docs.python.org/3/reference/simple_stmts.html#the-assert-statement&quot;&gt;the humble assert statement&lt;/a&gt;, which I have sprinkled in various places within my code where it makes sense to make sure that data flowing through conforms to whatever implicit or explicit contracts exist.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I mention these various options not to suggest that you should use them all, but rather to state that you have options ranging the whole spectrum of complexity and dependency.&lt;/p&gt;

&lt;h2 id=&quot;when-to-do-data-validation-in-your-project&quot;&gt;When to do data validation in your project&lt;/h2&gt;

&lt;p&gt;Regularly! I’ve written previously about how you can think about data validation as testing for your data. Just like many (most?) engineering teams run their tests every time you add a new commit to the codebase, it’s worth thinking of these kinds of tests as something that get run at any point where the underlying data gets updated.&lt;/p&gt;

&lt;p&gt;There are three points where it might make sense to do some data validation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;at the point of data ingestion&lt;/li&gt;
  &lt;li&gt;at the point just prior to training a model, i.e. after your data has been split into training and validation sets&lt;/li&gt;
  &lt;li&gt;at the point of inference (i.e. using the data being passed into the trained model)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/great_expectations/validation-when.png&quot; alt=&quot;&quot; title=&quot;The times it might make sense to do some validation.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The first (at data ingestion) is essential, especially if you have any kind of continuous training or continuous deployment loop going on. You don’t want to be training on data that clearly is unsuitable for training, or where the distribution has shifted so much that it’s going to cause hidden problems down the line.&lt;/p&gt;

&lt;p&gt;The second (at training-validation split time) may or may not be important depending on your use case. For my redaction project I don’t think there is a great deal of benefit from this and so haven’t incorporated it as such.&lt;/p&gt;

&lt;p&gt;The third (at inference time) is quite important to have, even though the behaviour when an anomaly is detected might be different from if you were to detect issues earlier on in the process. You might choose to just log the result of your validation check internally, or you could potentially also feed the result back to a user in the terms of some sort of warning (i.e. if the image that they were uploading was a very different kind of image from the data that had been used to train the model).&lt;/p&gt;

&lt;h2 id=&quot;what-im-using-for-my-redaction-project&quot;&gt;What I’m using for my redaction project&lt;/h2&gt;

&lt;p&gt;I don’t have any general advice as to which tool you should use as part of your computer vision model training pipeline. It’s likely to be heavily context-dependent and will differ based on the particular use case or problem you’re trying to solve. For my own project, however, I can be more specific.&lt;/p&gt;

&lt;p&gt;I’m using plain &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;assert&lt;/code&gt; statements liberally scattered through my code, in part leftover from when writing the code but also as a failsafe should strange or anomalous data make its way into my functions. I’m not sure if this is a best practice or not — I could imagine someone telling me that it’s not advised — but for now it’s helpful, especially as I continue to change things in the innards of various parts of the process.&lt;/p&gt;

&lt;p&gt;I’m using &lt;a href=&quot;https://greatexpectations.io/&quot;&gt;Great Expectations&lt;/a&gt; as a general-purpose validation tool to lay out my ‘expectations’ of my data in a assertive and declarative way, and even though it took a little longer to wrap my head round how it worked, I’m glad I made the effort as it seems really helpful.&lt;/p&gt;

&lt;p&gt;I’m using &lt;a href=&quot;https://evidentlyai.com/&quot;&gt;Evidently&lt;/a&gt; to do similar things as Great Expectations, but I find they have different strengths and benefits even as they serve the same purpose. Evidently is a bit more of a lighter piece in the process, I feel, and as such it’s a bit more flexible and you can iterate faster with it. I am not quite at the point where I’m serving my model to accept inference requests from outside, but Evidently will be part of that process when I do, for sure.&lt;/p&gt;

&lt;p&gt;Finally, &lt;a href=&quot;https://voxel51.com/docs/fiftyone/&quot;&gt;FiftyOne&lt;/a&gt; is also somehow part of the validation process. (I’ve &lt;a href=&quot;https://mlops.systems/redactionmodel/computervision/tools/debugging/jupyter/2022/03/12/fiftyone-computervision.html&quot;&gt;written about that previously&lt;/a&gt;.) Having visual tools that allow you to quickly test out a hypothesis or debug something unexpected in your training results is an essential part of the work of developing computer vision models.&lt;/p&gt;

&lt;p&gt;This brings my short series on data validation for computer vision to a close. I’m fully conscious that I might have missed some obvious opportunities, tricks or workflows that may be widely used in this field, so I welcome any comments and feedback that you might have.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="tools" /><category term="redactionmodel" /><category term="computervision" /><category term="datavalidation" /><summary type="html">(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/great_expectations/evidently_ai_logo_fi.png" /><media:content medium="image" url="https://mlops.systems/images/great_expectations/evidently_ai_logo_fi.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">How to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 2)</title><link href="https://mlops.systems/tools/redactionmodel/computervision/datavalidation/2022/04/26/data-validation-great-expectations-part-2.html" rel="alternate" type="text/html" title="How to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 2)" /><published>2022-04-26T00:00:00-05:00</published><updated>2022-04-26T00:00:00-05:00</updated><id>https://mlops.systems/tools/redactionmodel/computervision/datavalidation/2022/04/26/data-validation-great-expectations-part-2</id><content type="html" xml:base="https://mlops.systems/tools/redactionmodel/computervision/datavalidation/2022/04/26/data-validation-great-expectations-part-2.html">&lt;p&gt;&lt;em&gt;(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out &lt;a href=&quot;https://mlops.systems/categories/#redactionmodel&quot;&gt;the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;redactionmodel&lt;/code&gt; taglist&lt;/a&gt;.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In &lt;a href=&quot;https://mlops.systems/tools/redactionmodel/computervision/datavalidation/2022/04/19/data-validation-great-expectations-part-1.html&quot;&gt;the first part&lt;/a&gt; of this series, I made the case for why you might want to include some kind of data validation if you’re working on training a model in general, and if your working on object detection in specific. There are many things that can go wrong with your data inputs and you ought to have some kind of safeguards in place to prevent some tricky failures and bugs.&lt;/p&gt;

&lt;h2 id=&quot;tldr-for-data-validation-with-great-expectations&quot;&gt;TL;DR for data validation with Great Expectations&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;👀 Data validation helps give you confidence in the raw ingredients that feed into your models, especially in scenarios where you retrain or fine-tune regularly.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;✅ For object detection problems, there are many ways your data can fail in some silent way. You should want to be aware of when your training data isn’t meeting your assumptions of what it should look like.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;🛠 Great Expectations is a general purpose data validation tool that goes a long way to restoring trust in your data, and their automatic profiling feature is really useful when getting started.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;💪 In this second post on data validation for the computer vision context, I show how you can use the automatic profiling feature of Great Expectations to get you started with increasing your confidence in your object detection annotations. I will show you a concrete example where I created some validation rules for my manually-annotated dataset. I then applied those rules to my synthetic dataset in order to validate it.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;initial-notebook-based-setup&quot;&gt;Initial notebook-based setup&lt;/h2&gt;

&lt;p&gt;In &lt;a href=&quot;https://mlops.systems/tools/redactionmodel/computervision/datavalidation/2022/04/19/data-validation-great-expectations-part-1.html&quot;&gt;the last post&lt;/a&gt; I showed how you can easily use the Great Expectations library directly on a Pandas &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DataFrame&lt;/code&gt;, manually specifying values you expect to be the case for your data. For example, perhaps your data should always have certain columns, or the values of a certain column should always be a certain type or mostly range between certain values. You can &lt;a href=&quot;https://docs.greatexpectations.io/docs/guides/expectations/how_to_create_and_edit_expectations_based_on_domain_knowledge_without_inspecting_data_directly&quot;&gt;define all these fairly easily&lt;/a&gt;, leveraging your domain knowledge of the data.&lt;/p&gt;

&lt;p&gt;If you know you’re going to want to use Great Expectations as a more fully-fledged part of your pipeline or workflow, you’ll probably want to go through the more extensive setup stages and create a dedicated ‘context’ which can be longer-lasting than just length of your script runtime. Think of the ‘context’ as somewhere all your expectations and configuration of how to access your data is stored.&lt;/p&gt;

&lt;p&gt;Full instructions on how to set all this up can be found &lt;a href=&quot;https://docs.greatexpectations.io/docs/guides/setup/setup_overview/&quot;&gt;in the docs&lt;/a&gt;, but for the most part it’s a matter of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip&lt;/code&gt; installing Great Expectations, running &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;great_expectations init&lt;/code&gt; , and then &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;great_expectations datasource new&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;That final command will take you through an interactive setup that has you fill in and amend Jupyter notebooks. (I’m not fully sold on the prominence of this workflow that has you spinning up a Jupyter runtime, dynamically editing notebooks and so on, but I found doing it for my project wasn’t as inconvenient as I’d expected. Plus, there are non-interactive and pure Pythonic ways to get everything configured if you need or prefer that.)&lt;/p&gt;

&lt;p&gt;Once you have your context created and your data sources connected, you can move on to the main course: using &lt;a href=&quot;https://docs.greatexpectations.io/docs/terms/profiler&quot;&gt;the Profiler&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;using-the-great-expectations-profiler&quot;&gt;Using the Great Expectations Profiler&lt;/h2&gt;

&lt;p&gt;Setting up your validations (i.e. your ‘expectations’) for your data can be done in a number of different ways. We saw &lt;a href=&quot;https://mlops.systems/tools/redactionmodel/computervision/datavalidation/2022/04/19/data-validation-great-expectations-part-1.html&quot;&gt;last time&lt;/a&gt; how you can define these manually, but in this post I want to show how you can follow another recommended workflow by allowing the profiler to review your data and to make an initial set of assumptions about the boundaries and patterns embedded in those values.&lt;/p&gt;

&lt;p&gt;Note, &lt;a href=&quot;https://docs.greatexpectations.io/docs/guides/expectations/how_to_create_and_edit_expectations_with_a_profiler/&quot;&gt;as the docs mention&lt;/a&gt;, the expectations that are automatically generated from your dataset are “deliberately over-fitted on your data”. This means that if your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DataFrame&lt;/code&gt; has 10,321 rows, one of the expectations generated will be that datasets due for validation with this suite of expectations will also have exactly 10,321 rows:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“The intention is for this Expectation Suite to be edited and updated to better suit your specific use case - it is not specifically intended to be used as is.” (&lt;a href=&quot;https://docs.greatexpectations.io/docs/guides/expectations/how_to_create_and_edit_expectations_with_a_profiler/&quot;&gt;source&lt;/a&gt;)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You’ll want and have to do a decent amount of manual checking through, amending and updating any expectations that get created during this process. That said, I am finding that it makes a lot of sense to start with some kind of initial baseline of assumptions that can be corrected versus starting from complete zero and building things up purely based on your domain knowledge of the data.&lt;/p&gt;

&lt;p&gt;Needless to say, this whole process assumes you have a decent grasp on the domain context and have explored your data already. You probably wouldn’t go to the trouble of setting up Great Expectations if you were doing something that required only a quick solution, but it bears repeating that the expectations you define are only as good as your understanding of the limits and underlying realities of your data. This is probably why something like Great Expectations lends itself quite well to a data-centric approach.&lt;/p&gt;

&lt;p&gt;Getting the profiler to work requires a few interlocking abstractions to be created or instantiated:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;expectation_suite_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;redaction_annotations_suite&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;main_batch_request&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RuntimeBatchRequest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;datasource_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;redaction_data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data_connector_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;default_runtime_data_connector_name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;data_asset_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;main_annotations_df&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# This can be anything that identifies this data_asset for you
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;runtime_parameters&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;batch_data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main_annotations_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# df is your dataframe
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;batch_identifiers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;default_identifier_name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;default_identifier&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_expectation_suite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;expectation_suite_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expectation_suite_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;overwrite_existing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# toggle this as appropriate
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;validator&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_validator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;batch_request&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;main_batch_request&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;expectation_suite_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expectation_suite_name&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;profiler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;UserConfigurableProfiler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;profile_dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;validator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;suite&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;profiler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;build_suite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save_expectation_suite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;suite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# use this to save your suite in the context for reuse
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The above code perhaps seems like a lot, but really all you’re doing is getting your data, making the relevant connections between Great Expectations and your context, and then running the profiler so it can work its magic.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/great_expectations/profiler.jpg&quot; alt=&quot;&quot; title=&quot;You'll see output from the final `build_suite()` call that looks something like this.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can’t yet see the specific values that were imputed from your data, but even this high-level output shows you some of the expectations that it’s thinking would be useful to create.&lt;/p&gt;

&lt;p&gt;At this stage, you’ll want to take some time to review the specific expectations. You’ll want to:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ensure that they make sense for your dataset&lt;/li&gt;
  &lt;li&gt;remove any of the really rigid expectations (e.g. that any dataset must have exactly the same number of rows)&lt;/li&gt;
  &lt;li&gt;use the inputed expectations as a springboard for any other ideas that might come to mind&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that this is an essential step to complete before moving forward. You could use the unedited auto-generated expectations suite as your data validation, but it would almost certainly have little use or value for you. &lt;em&gt;The auto-generated suite is a starting place that you need to amend and tailor to your specific situation.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In my case, I was able to amend some of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;min&lt;/code&gt; / &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;max&lt;/code&gt; values to more suitable defaults. (You amend these expectations in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.json&lt;/code&gt; file that was created inside the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;expectations&lt;/code&gt; subfolder within your context.) I also included some other domain-driven expectations that the profiler couldn’t have known to include. For example, I know from having immersed myself in this data for several months now that most annotations should have a ‘horizontal’ or ‘square’ orientation. Great Expectations doesn’t create this expectation automatically, so I add it to the list of basic assumptions already generated.&lt;/p&gt;

&lt;h2 id=&quot;viewing-data-docs-reports-on-validated-data&quot;&gt;Viewing Data Docs reports on validated data&lt;/h2&gt;

&lt;p&gt;Once you have a suite of expectations set up to your liking, you can run a checkpoint against your original data just to make sure you haven’t introduced or amended something that doesn’t match up with the original data. You should get no errors at this point.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;checkpoint_config&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;my_checkpoint&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;config_version&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;class_name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;SimpleCheckpoint&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;validations&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;&quot;batch_request&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;&quot;datasource_name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;redaction_data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;&quot;data_connector_name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;default_runtime_data_connector_name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;&quot;data_asset_name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;main_annotations_df&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;&quot;expectation_suite_name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;expectation_suite_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_checkpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;checkpoint_config&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run_checkpoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;checkpoint_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;my_checkpoint&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;batch_request&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;runtime_parameters&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;batch_data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main_annotations_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;batch_identifiers&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;&quot;default_identifier_name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;default_identifier&quot;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;context&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;build_data_docs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# builds data docs to inspect the results
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What you really want, however, is to run your expectations suite against &lt;em&gt;new&lt;/em&gt; data. That’s the real value of what Great Expectations brings, i.e. to check that incoming data due to be added to your larger base dataset conforms to the broad realities of that base dataset.&lt;/p&gt;

&lt;p&gt;In my case, the first thing I was interested to check was whether &lt;a href=&quot;https://mlops.systems/redactionmodel/computervision/python/tools/2022/02/10/synthetic-image-data.html&quot;&gt;the synthetic images&lt;/a&gt; &lt;a href=&quot;https://mlops.systems/tools/redactionmodel/computervision/2022/04/06/synthetic-data-results.html&quot;&gt;I created&lt;/a&gt; would match the expectations suite I’d created based off my core hand-annotated data. (Quick context if you haven’t been following the project so far: I have a core dataset which is manually annotated for the objects inside images. I also created two sets of synthetic data to supplement the manual annotations, which &lt;a href=&quot;https://mlops.systems/tools/redactionmodel/computervision/2022/04/06/synthetic-data-results.html&quot;&gt;boosted my model performance&lt;/a&gt; considerably.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/great_expectations/ge_ui_validation.png&quot; alt=&quot;&quot; title=&quot;The web validation UI looks like this once you generate and open it.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The web UI is where you can go to get a visual overview of where your data is passing and failing to meet your (great) expectations. You will want (and I will need) to configure your expectations suite to meet the core assumptions you make about your data derived from your particular domain.&lt;/p&gt;

&lt;p&gt;For my case, some expectations I will add that are specific to my use case:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;redaction annotations should mostly be of horizontal orientation&lt;/li&gt;
  &lt;li&gt;content annotations should mostly be of portrait orientation&lt;/li&gt;
  &lt;li&gt;most images should have only one content annotation&lt;/li&gt;
  &lt;li&gt;annotations shouldn’t be larger than the associated image, or positioned outside the boundaries of that image. (Because of how you define them, in reality this is several expectations, but conceptually it’s just one or two).&lt;/li&gt;
  &lt;li&gt;the area taken up by most annotations should be less than half that taken up by the total image&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;…and so on. I hope it’s clear now how Great Expectations can be a tremendous asset that can give you confidence in your data.&lt;/p&gt;

&lt;p&gt;In the next and final post of the series, I will explore some other tools that you can consider when performing these kinds of validation. I will also offer my take on when each tool would be appropriate, as well as where they would be appropriate to use within the machine learning workflow and lifecycle.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="tools" /><category term="redactionmodel" /><category term="computervision" /><category term="datavalidation" /><summary type="html">(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/great_expectations/g_e_logo.jpeg" /><media:content medium="image" url="https://mlops.systems/images/great_expectations/g_e_logo.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">How to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 1)</title><link href="https://mlops.systems/tools/redactionmodel/computervision/datavalidation/2022/04/19/data-validation-great-expectations-part-1.html" rel="alternate" type="text/html" title="How to trust the data you feed your model: data validation with Great Expectations in a computer vision context (part 1)" /><published>2022-04-19T00:00:00-05:00</published><updated>2022-04-19T00:00:00-05:00</updated><id>https://mlops.systems/tools/redactionmodel/computervision/datavalidation/2022/04/19/data-validation-great-expectations-part-1</id><content type="html" xml:base="https://mlops.systems/tools/redactionmodel/computervision/datavalidation/2022/04/19/data-validation-great-expectations-part-1.html">&lt;p&gt;&lt;em&gt;(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out &lt;a href=&quot;https://mlops.systems/categories/#redactionmodel&quot;&gt;the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;redactionmodel&lt;/code&gt; taglist&lt;/a&gt;.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data validation is a process of verifying that data is accurate and consistent. It plays a crucial role in end-to-end machine learning pipelines.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;There is a lack of validation tools in Computer Vision (CV) due the complexity of the data used by the domain.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;In this series of articles, I will show you how to leverage the &lt;a href=&quot;https://greatexpectations.io&quot;&gt;Great Expectations&lt;/a&gt; open-source library to validate object detection data. This will help you to feed your model with data less prone to break your model performance.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When something goes wrong with a newly trained or newly deployed version of your model, where do you look first? Where does your gut tell you the bug or issue is likely to be found? For me, knee deep into my redaction model project, I immediately think of my data. For sure, I could probably have more in the way of testing to be sure my code is working how I expect it to work, but issues with the data are far more likely to be silent killers. Issues with data are unlikely to raise a loud exception and suddenly bring my training to a stop. Instead, my training will continue, but I’ll either get really unsatisfactory results or I’ll get results that are underperforming the real potential of the data I’m feeding into my model. That’s the scary part: I most likely won’t even know that my data is broken or faulty.&lt;/p&gt;

&lt;iframe src=&quot;https://giphy.com/embed/YTJXDIivNMPuNSMgc0&quot; width=&quot;442&quot; height=&quot;480&quot; frameborder=&quot;0&quot; class=&quot;giphy-embed&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;For software engineers, testing your code is a tried-and-tested way to find some confidence in what you’re trying to accomplish. (I am exploring some of these best practices in &lt;a href=&quot;https://mlops.systems/categories/#robustpython&quot;&gt;my review series&lt;/a&gt; about Patrick Viafore’s excellent &lt;a href=&quot;https://www.amazon.com/Robust-Python-Patrick-Viafore-ebook-dp-B09982C9FX/dp/B09982C9FX/ref=mt_other?qid=&amp;amp;me=&amp;amp;tag=soumet-20&amp;amp;_encoding=UTF8&quot;&gt;&lt;em&gt;Robust Python&lt;/em&gt; &lt;/a&gt;book, which covers testing along with typing and various other patterns.) For critical systems, testing is one of the things that allows you to sleep soundly. For those living in the world of machine learning or data science, data validation is like writing tests for your data. You can be confident that your data looks and has the shape of what you feel it should when you address the data quality issue head-on.&lt;/p&gt;

&lt;p&gt;If you think of your model training workflow as a pipeline, there are certain places where it makes sense to do some kind of data validation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;at the very beginning, when you’re seeing your data for the first time: a lot of exploration and basic analysis really helps at this point. It will help you build up intuition for the general patterns and boundaries of the data you’re going to use to train your model.&lt;/li&gt;
  &lt;li&gt;any time you do some kind of conversion: perhaps you have to — as I do with my project — convert from one image annotation format into another and you’re juggling x and y coordinates constantly, or maybe you’re using different image formats at different points?&lt;/li&gt;
  &lt;li&gt;prior to training your model: ‘garbage in, garbage out’ as the saying goes… You probably want to make sure that you only have high quality data passing through into your model training pipeline step.&lt;/li&gt;
  &lt;li&gt;as part of a continuous training loop: perhaps you’ve trained and deployed a model, but now a few months have passed, you have more data and you want to retrain your model. Are you confident that the new data retains the same characteristics and qualities of your original data?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As you can see, there are many different approaches that you might take. To discuss where you might want to validate your data is to discuss where your processes might be flawed in some way. For most projects of any size or significance, you probably will find that taking the care with your data inputs will pay dividends.&lt;/p&gt;

&lt;h1 id=&quot;data-validation-and-computer-vision&quot;&gt;Data validation and computer vision&lt;/h1&gt;

&lt;p&gt;It often seems like computer vision exists in a world unto its own, particularly when it comes to the data used to train models. These idiosyncrasies amount to a strong case for some kind of data validation:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;image data isn’t always easily introspectable, especially on the aggregate level (i.e. what is the ‘average’ of a series of images, or how to think of the standard deviation of your images?)&lt;/li&gt;
  &lt;li&gt;for something like object detection, the annotations are stored in a separate location from the images to which they correspond, leaving the door open for a creeping data drift between the original image locations and what is listed in the annotations.&lt;/li&gt;
  &lt;li&gt;For massive data sets, the original data will likely not be stored in the environment where you’re doing some fine-tuning with new data.&lt;/li&gt;
  &lt;li&gt;Different model architectures require different kinds of pre-processing for your data and sometimes annotations need converting into slightly different formats (perhaps for your evaluation metric)&lt;/li&gt;
  &lt;li&gt;The pure images (or image-adjacent objects like medical scans) contain a lot of sub-surface metadata that isn’t easily accessed and isn’t automatically used as criteria for comparison or error detection.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In short, there are lots of ways that training a computer vision model can go wrong, and implementing even basic safeguards against this can give you confidence in the data you’re using. Unfortunately, the landscape of tooling for data validation in the computer vision space feels like it’s lagging behind what exists for tabular data, for example, but that’s almost certainly because it’s just a harder problem. The big data validation libraries don’t really cater towards computer vision as a core domain, and (as you’ll see below) you’ll probably have to crowbar your data into the formats they expect.&lt;/p&gt;

&lt;h1 id=&quot;big-picture-what-might-this-look-like-for-my-project&quot;&gt;Big picture: what might this look like for my project?&lt;/h1&gt;

&lt;p&gt;As I outlined above, there are lots of different places where you might want to use some kind of data validation strategy. At the level of code, you might want to make your input and output validation a bit more solid by using type annotations and a type checker like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mypy&lt;/code&gt;. You can add tests to ensure that edge cases are being handled, and that your assumptions about your code behaviour are proven. You also have your tests to ensure that changes in one function or area of your codebase don’t break something somewhere else.&lt;/p&gt;

&lt;p&gt;At the level of your data, you can of course use simple &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;assert&lt;/code&gt; statements within the functional meat of your codebase. For example, at the point where you’re ingesting data pre-training you could assert that each image is of a certain format and size, and perhaps even that annotations associated with that image ‘make sense’ as per the context of whatever problem you’re solving. You can handle some of these assertions and checks with simple conditionals, perhaps, earlier on in the process when you are ingesting or pre-processing your data.&lt;/p&gt;

&lt;p&gt;A significant benefit of having these simple assertions inside your core functions is that you are handling the ways things can go wrong at the same time as you’re writing the functionality itself. A disadvantage is that your code can easily become cluttered with all this non-core behaviour. It feels a little like the validation can become an afterthought in this scenario. For this reason, it seems to make sense to me that you’d want to have one or more dedicated checkpoints where your data undergoes some kind of validation process. In the context of a pipeline, this means you probably will want one or more steps where this happens.&lt;/p&gt;

&lt;h1 id=&quot;tradeoffs&quot;&gt;Tradeoffs&lt;/h1&gt;

&lt;p&gt;For tiny throwaway projects, or for proof-of-concept experimentation, it might not make sense to start off by working up a massive data validation suite. A really rigorous validation process early on might slow you down more than is useful. Instead, simple &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;assert&lt;/code&gt; statements coupled with type annotations on your functions might be the way to go for safeguards and will-this-be-readable-in-the-future sanity checks.&lt;/p&gt;

&lt;p&gt;Ideally, you’ll want to create some kind of end-to-end pipeline or workflow at the beginning of your process, since this will allow you to iterate faster in a manner that’s meaningful for whatever you’re trying to solve. With a basic pipeline in place, data validation can be added as a stage of its own without too much disruption once you have an initial working prototype. As with most things in life, investing for the longer term is going to take a bit more upfront effort but that shouldn’t be too much an issue as long as your project has that kind of a horizon to it.&lt;/p&gt;

&lt;h1 id=&quot;what-kind-of-validation-does-great-expectations-offer&quot;&gt;What kind of validation does Great Expectations offer?&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://greatexpectations.io&quot;&gt;Great Expectations&lt;/a&gt; is an open-source data validation tool. It is somewhat agnostic as to what specific use case you have, but I don’t think it’d be wrong to say that it isn’t primarily developed for those working on computer vision problems; tabular data seems to be a much cosier fit.&lt;/p&gt;

&lt;p&gt;I stated above that Great Expectations could be used as if you were adding tests for your data. At a very high level, you can think of it as a fancier way of adding assertions about your data. The ‘expectations’ in the title are like those assertion statements, only in this case there are dozens of different pre-made ‘expectations’ you can choose from. For example, you could assert that you expect that the values of a particular column of a Pandas DataFrame be between 0 and 100, and that if they exceeded those boundaries then it would be only a very small proportion that did so.&lt;/p&gt;

&lt;p&gt;Your expectations make up a ‘suite’, and you run your suite of expectations against a batch or data asset. There are another 10 or 20 concepts or terms that I’d need to define and connect together in a mental map before we covered everything about how Great Expectations works. Unfortunately, this is one of the things I found most confusing about getting to know the library through its documentation. From the outside, it appears that they had one set of terminology, but now it’s partially changed to a different set of terms or abstractions. Presumably for reasons of backwards compatibility, some of the old abstractions remain in the documentation and explanations, which makes it not always clear to understand how the various pieces fit together.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/great_expectations/great_expecations_abstractions.png&quot; alt=&quot;&quot; title=&quot;My initial attempt at understanding how all the abstractions of Great Expectations related to one another…&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can read &lt;a href=&quot;https://docs.greatexpectations.io/docs/glossary&quot;&gt;the glossary&lt;/a&gt; over at their documentation site if you want to learn more, but for now everything I explained above should suffice.&lt;/p&gt;

&lt;p&gt;There seem to be two main ways of setting up and using Great Expectations. One is heavily interactive and driven by executing cells in a series of notebooks. The other is as you’d expect — code-based using a Python library, backed by some external configuration files and templates. I didn’t find the notebook-based configuration and setup very compelling, but it is the one emphasised in the documentation and in online materials, so I will give it due attention in the next part of this blog series. For now, it might suffice to show a very simple version of how the code-based use works:&lt;/p&gt;

&lt;h1 id=&quot;a-simple-example-of-using-great-expectations-for-data-validation&quot;&gt;A simple example of using Great Expectations for data validation&lt;/h1&gt;

&lt;p&gt;The first thing I did was to convert my annotations data into a Pandas DataFrame. You can use Pandas, SQL and Apache Spark as sources for your data to be validated through Great Expectations, and luckily my COCO annotations file was just a JSON file so it was easily converted. While doing the conversion, I made sure to add some extra metadata along the way: a column noting whether an image or a redaction was horizontal or vertical in its orientation, for example, or splitting the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bbox&lt;/code&gt; array into its four constituent parts.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;great_expectations&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ge&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;annotations_df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;annotations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;feature_columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'area'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'iscrowd'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'image_id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'category_id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'synthetically_generated'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'category_name'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feature_columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;annotations_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expect_column_to_exist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
&lt;span class=&quot;n&quot;&gt;annotations_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expect_column_values_to_be_in_set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;category_name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;content&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;redaction&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Great Expectations wraps the Pandas library, so importing the data was easy. Then adding the expectations (methods beginning with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;expect…&lt;/code&gt;) was trivial. Below you can see the result from the second of the expectations. All of the column values were in that set, so the test passed.&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;success&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;result&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;element_count&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6984&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;missing_count&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;missing_percent&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;unexpected_count&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;unexpected_percent&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;unexpected_percent_total&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;unexpected_percent_nonmissing&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;partial_unexpected_list&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;meta&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;exception_info&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;raised_exception&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;exception_traceback&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;exception_message&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the second part of this series, I’ll explore how the interactive way of using Great Expectations works, and I’ll also show the web results interface for your expectations suite. It’s much fancier than the dictionary / object that was output above, and what’s even better is that you can have Great Expectations make some of its own guesses about what the right expectations for your particular dataset might be.&lt;/p&gt;

&lt;p&gt;I hope for now that I’ve made the case for why data validation is probably worth doing, and started you thinking about how that might apply to a computer vision use case.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="tools" /><category term="redactionmodel" /><category term="computervision" /><category term="datavalidation" /><summary type="html">(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/great_expectations/g_e_logo.jpeg" /><media:content medium="image" url="https://mlops.systems/images/great_expectations/g_e_logo.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">‘I guess this is what data-centric AI is!’: Performance boosts after training with synthetic data</title><link href="https://mlops.systems/tools/redactionmodel/computervision/2022/04/06/synthetic-data-results.html" rel="alternate" type="text/html" title="‘I guess this is what data-centric AI is!’: Performance boosts after training with synthetic data" /><published>2022-04-06T00:00:00-05:00</published><updated>2022-04-06T00:00:00-05:00</updated><id>https://mlops.systems/tools/redactionmodel/computervision/2022/04/06/synthetic-data-results</id><content type="html" xml:base="https://mlops.systems/tools/redactionmodel/computervision/2022/04/06/synthetic-data-results.html">&lt;p&gt;&lt;em&gt;(This is part of a series of blog posts documenting my work to train a model
that detects redactions in documents. To read other posts, check out
&lt;a href=&quot;https://mlops.systems/categories/#redactionmodel&quot;&gt;the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;redactionmodel&lt;/code&gt; taglist&lt;/a&gt;.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A clean and focused dataset is probably at the top of the list of things that
would be nice to have when starting to tackle a machine learning problem. For
object detection, there are some
&lt;a href=&quot;https://huggingface.co/datasets?task_categories=task_categories:object-detection&amp;amp;sort=downloads&quot;&gt;useful starting points&lt;/a&gt;,
but for many use cases you’re probably going to have to start from scratch. This
is what I’ve been doing
&lt;a href=&quot;https://mlops.systems/categories/#redactionmodel&quot;&gt;for the past few months&lt;/a&gt;:
working to bootstrap my way into a dataset that allows me to get decent
performance training a model that can recognise redactions made on documents.&lt;/p&gt;

&lt;p&gt;As part of that journey so far, some of the big things that I’ve taken time to
do include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://mlops.systems/redactionmodel/computervision/datalabelling/2021/11/29/prodigy-object-detection-training.html&quot;&gt;manually annotating&lt;/a&gt;
1000+ images&lt;/li&gt;
  &lt;li&gt;using a model-in-the-loop to help bootstrap that annotation process by
&lt;a href=&quot;https://mlops.systems/python/fastai/tools/redactionmodel/2022/01/16/midway-report-redaction-project.html&quot;&gt;pre-filling annotation suggestions&lt;/a&gt;
on an image that I could then correct&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mlops.systems/redactionmodel/computervision/python/tools/2022/02/10/synthetic-image-data.html&quot;&gt;creating synthetic images&lt;/a&gt;
to increase the size of my dataset used in training&lt;/li&gt;
  &lt;li&gt;spending time
&lt;a href=&quot;https://mlops.systems/redactionmodel/computervision/tools/debugging/jupyter/2022/03/12/fiftyone-computervision.html&quot;&gt;looking at what the model found difficult&lt;/a&gt;,
or what it got wrong&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;At the end of my synthetic data creation blogpost, I mentioned that the next
step would be to test the effect of adding in the new synthetic examples. Well…
the results are in!&lt;/p&gt;

&lt;h2 id=&quot;a-failed-attempt-to-train-with-synthetic-data&quot;&gt;A failed attempt to train with synthetic data&lt;/h2&gt;

&lt;p&gt;I wasn’t sure exactly how much synthetic data would be appropriate or performant
to use, so created a loose experiment where I started with 20% of the total
images and increasing up until I reached 50%. (I figured that more than 50%
synthetic data probably wasn’t a great idea and would probably not help my model
perform out in the real world.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/synthetic-data-results/synthetic-early-results.png&quot; alt=&quot;&quot; title=&quot;Results from the first phase using synthetic data&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see above: my initial experiment did not show great results. In fact,
in several places, if I added synthetic data my model actually performed
&lt;em&gt;worse&lt;/em&gt;. This was a strong repudiation of my intuition of what would happen.
After all, the whole point of adding the synthetic data was to get the model
more of a chance to learn / train and thus improve its ability to recognise
redaction object in documents.&lt;/p&gt;

&lt;p&gt;I dug into the data that I’d generated and the data I’d been using to train, and
discovered a nasty bug which was tanking the performance. A week of debugging
mislabelled bounding boxes in evenings after work and I was back with results that
finally made sense.&lt;/p&gt;

&lt;h2 id=&quot;performance-boosts-after-adding-synthetic-data&quot;&gt;Performance boosts after adding synthetic data&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/synthetic-data-results/synthetic-mid-results.png&quot; alt=&quot;&quot; title=&quot;Results from the first phase using synthetic data&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this chart, at the bottom you can see how training the model without the
synthetic data (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;no-synthetic-batch16&lt;/code&gt;) performed. Ok, not great. Then the next
best performing (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;combined-75real-25synthetic-randomsplit&lt;/code&gt;)was when 25% of the
total number of images was synthetic, and the rest were real manually annotated
images. At the top, with around an 81% COCO score, was the model where I used
50% synthetic and 50% real images. This seemed to fit what my intuition said
would happen.&lt;/p&gt;

&lt;p&gt;More synthetic data helped. I guessed that if I had millions of labelled images
then the synthetic data would perhaps have been less useful, but starting from
scratch it was really supporting the process.&lt;/p&gt;

&lt;p&gt;I was curious what would happen when I returned to &lt;a href=&quot;https://voxel51.com&quot;&gt;FiftyOne&lt;/a&gt; to carry out some
error analysis on the new model’s performance. Even before I had reached those
results, I had a hunch that the synthetic images I’d created were perhaps too
generic. I think they probably were helping boost some baseline performance of
my model, but I knew they weren’t helping with the hard parts of detecting
redactions.&lt;/p&gt;

&lt;h2 id=&quot;hard-examples-creating-targeted-synthetic-data&quot;&gt;‘Hard examples’: creating targeted synthetic data&lt;/h2&gt;

&lt;p&gt;As a reminder, this is the kind of image that is ‘hard’ for my model (or even a
human) to be able to identify all the redactions:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/synthetic-data-results/hard-detection.png&quot; alt=&quot;&quot; title=&quot;White
boxes on white backgrounds are hard to identify as redactions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://voxel51.com&quot;&gt;FiftyOne&lt;/a&gt; visualisations of what was and wasn’t working validated my hunch:
yes, synthetic data helped somewhat, but the model’s low performance seemed much
more vulnerable to misrecognition of the hard examples. Even with a 50/50 split
between synthetic data and real manually annotated data, the hard examples were
still hard! (And the converse was also true: the model was &lt;em&gt;already&lt;/em&gt; pretty good
at identifying ‘easy’ redactions (e.g. of the black box type).&lt;/p&gt;

&lt;p&gt;If we look back at the example of a ‘hard’ redaction above, two things stood
out:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;They’re hard, even for a human! This was borne out in the way I needed to
take special care not to forget or mislabel when I was adding manual
annotations.&lt;/li&gt;
  &lt;li&gt;There are &lt;em&gt;lots&lt;/em&gt; of redactions on a single page/image.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The second point was probably important, not only in the sense that there were
more chances of getting something wrong on a single page, but also in the sense
that the redactions were (relatively) small. The detection of small objects is
almost its own field in the world of computer vision and I don’t know too much
about it, but I do know it’s somewhat an unsolved problem. That said, finding a
way to boost the performance of the models on these ‘hard’ examples (there were
a few other types of hard image) seemed like it might tackle a significant
shortcoming of my model.&lt;/p&gt;

&lt;p&gt;I decided to try creating a separate batch of synthetic image data, this time
fully tailored to tackling some of the hardness mentioned above: it would have
many small redactions on a single page, they would all be white boxes and there
might also be things like tables with white box-like shapes coexisting next to
redactions.&lt;/p&gt;

&lt;p&gt;Luckily, the work I’d done previously on creating synthetic data helped me get
started quickly. I returned to &lt;a href=&quot;https://borbpdf.com&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;borb&lt;/code&gt;&lt;/a&gt;, an open-source
tool for quickly creating PDF documents that allows for a pretty flexible
prototyping of layouts with all sorts of bells and whistles added. These were
some of the documents I generated:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/synthetic-data-results/hard-synthetic.gif&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The hard images were hard, and I had created some synthetic chimeras that (I
believed) approximated some of the features of the original hard images. I did
not want to overbalance my training data, however, and took care not to create
too many of this type of image.&lt;/p&gt;

&lt;p&gt;My script — as with
&lt;a href=&quot;https://mlops.systems/redactionmodel/computervision/python/tools/2022/02/10/synthetic-image-data.html&quot;&gt;the previous synthetic data&lt;/a&gt;
— also required me to create the annotation files at the same time as creating
the document. With &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;borb&lt;/code&gt; it was &lt;em&gt;relatively&lt;/em&gt; trivial to get the bounding box
data for objects created, and there was even in-built functionality to create
and apply redactions onto a document. (I’m moving fairly quickly over the
mechanics of how this all worked, but it’s not too far distant from how I
described it in my previous post so I’d refer you there
&lt;a href=&quot;https://mlops.systems/redactionmodel/computervision/python/tools/2022/02/10/synthetic-image-data.html&quot;&gt;for more details&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Once the images were created and added to my datasets, it was time to retrain
the model and see what benefit it brought.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/synthetic-data-results/hard-synthetic-performance-boost.png&quot; alt=&quot;&quot; title=&quot;~3.5% boost when adding hard synthetic images into training data!&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see, the model jumped up from around 80.5 to 84% when I aded the hard
synthetic examples in. That’s a pretty nice jump as far as I’m concerned,
especially given that I only added in 300 images to the training data. I still
had a little over a thousand of the original basic synthetic images that I was
using, but this result showed me that tackling the badly performing parts of the
model head-on seemed to have a positive outcome.&lt;/p&gt;

&lt;p&gt;At this point, I did some more experiments around the edges, applying other
things I knew would probably boost the performance even more, notably first
checking what would happen if I increased the image size from 512 to 640. I got
up to an 86% COCO score with that improvement alone.&lt;/p&gt;

&lt;p&gt;In a final twist, I second-guessed myself and wondered whether the original
synthetic data was even helping at all… I removed the thousand or so ‘basic’
synthetic images from the data and retrained the model. To my surprise, I
achieved more or less the same COCO score as I had &lt;em&gt;with&lt;/em&gt; the basic synthetic
images. I’m taking this as a strong suggestion that my basic synthetic images
aren’t actually helping as much as I’d thought, and that probably a smaller
number of them as a % of the total would be beneficial.&lt;/p&gt;

&lt;h2 id=&quot;reflections-on-experimenting-with-synthetic-data&quot;&gt;Reflections on experimenting with synthetic data&lt;/h2&gt;

&lt;p&gt;So, what can I conclude from this whole excursion into the world of synthetic
image creation as a way of boosting model performance?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;adding synthetic data really can help!&lt;/li&gt;
  &lt;li&gt;the world of synthetic data creation is a &lt;em&gt;huge&lt;/em&gt; rabbit hole and potentially you
can get lost trying to create the perfect synthetic versions of your original
data. (I mean this both in the sense of ‘there’s lots to learn’ as well as
‘you can spend or lose a &lt;em&gt;ton&lt;/em&gt; of time here’.)&lt;/li&gt;
  &lt;li&gt;Targeted synthetic data designed to clear up issues where the model has been
identified as underperforming is probably best. (Conversely, and I’ll be
careful how much I generalise here, middle-of-the-road synthetic data that
doesn’t resemble the original dataset may not be worth your time.)&lt;/li&gt;
  &lt;li&gt;Knowing your original data and domain really well helps. A lot. My intuition
about what things the model would stumble on was fuelled by this knowledge of
the documents and the domain, as well as by the experience of having done
manual annotations for many hours.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are probably many (many) more things I can do to continually tinker away
at this model to improve it:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;continue down the path of more error analysis, which would fuel more targeted
addition of annotations, and so on.&lt;/li&gt;
  &lt;li&gt;create better versions of synthetic data with more variation to encompass the
various kinds of documents out in the real world.&lt;/li&gt;
  &lt;li&gt;more self-training with the model in the loop to fuel my manual annotation
process.&lt;/li&gt;
  &lt;li&gt;further increases to the image size (perhaps in conjunction with progressive
resizing).&lt;/li&gt;
  &lt;li&gt;increasing the backbone from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;resnet50&lt;/code&gt; to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;resnet101&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In general, improving the quality of the data used to train my model seems to
have been (by far) the best way to improve my model performance. Hyper-parameter
tuning of the sort that is often referenced in courses or in blog posts does not
seem to have had much of a benefit.&lt;/p&gt;

&lt;p&gt;It is probably (mostly) good enough for my use case and for where I want to be
heading with this project. There are other things that need addressing around
the edges, notably parts of the project that could be made more robust and
‘production-ready’. More about that in due course, but for now please do comment
below if you have suggestions for things that I haven’t thought of that might
improve my model performance!&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="tools" /><category term="redactionmodel" /><category term="computervision" /><summary type="html">(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/synthetic-data-results/hard-synthetic-performance-boost.png" /><media:content medium="image" url="https://mlops.systems/images/synthetic-data-results/hard-synthetic-performance-boost.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Some characteristics of best-in-class ML portfolio projects</title><link href="https://mlops.systems/computervision/skillbuilding/2022/04/04/ml-portfolio-best-practices.html" rel="alternate" type="text/html" title="Some characteristics of best-in-class ML portfolio projects" /><published>2022-04-04T00:00:00-05:00</published><updated>2022-04-04T00:00:00-05:00</updated><id>https://mlops.systems/computervision/skillbuilding/2022/04/04/ml-portfolio-best-practices</id><content type="html" xml:base="https://mlops.systems/computervision/skillbuilding/2022/04/04/ml-portfolio-best-practices.html">&lt;p&gt;&lt;a href=&quot;https://ekko-realtime.com&quot;&gt;Ekko&lt;/a&gt; was the last time I worked on a big project that would be presented publicly. An open-source framework that provided realtime infrastructure and
in-transit message processing for web applications was a group project that I worked on together with three other colleagues, and we took the time to really make the &lt;em&gt;how&lt;/em&gt; and the &lt;em&gt;why&lt;/em&gt; really explicit. We made animations, diagrams, charts, and I learned a lot about what’s hard when explaining technical projects, even when the audience is expected to be (mostly) technically literate.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ml-portfolio-best-practices/ekko.png&quot; alt=&quot;&quot; title=&quot;A screenshot from the Ekko website&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I’ve been working on &lt;a href=&quot;https://mlops.systems/categories/#redactionmodel&quot;&gt;my redaction project&lt;/a&gt; since December and slowly but surely I’m tying the ends together and getting ready for it to come to a close. As part of the final touches, I want to offer something equivalent to how we presented &lt;a href=&quot;https://ekko-realtime.com&quot;&gt;Ekko&lt;/a&gt;. From reading around and exposure to various projects over the years, it seems to me that machine learning projects sometimes have different emphases and conventions. This blog post is my attempt to list some of the characteristics of really great ML portfolio projects, with an emphasis on how the project is presented.&lt;/p&gt;

&lt;h2 id=&quot;some-top-projects&quot;&gt;Some top projects&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://explosion.ai/blog/healthsea&quot;&gt;Healthsea&lt;/a&gt; by Edward Schmuhl (&lt;a href=&quot;https://twitter.com/aestheticedwar1&quot;&gt;@aestheticedwar1&lt;/a&gt;) is my current favourite project writeup, blending amazing visuals, full explanation and a clear overview&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.ahmedbesbes.com/blog/end-to-end-machine-learning&quot;&gt;This project&lt;/a&gt; (by &lt;a href=&quot;https://twitter.com/ahmed_besbes_&quot;&gt;@ahmed_besbes_&lt;/a&gt;) was recommended to me and although it’s more of a step-through of how the project works and was created, it also is clearly presented and very visual.&lt;/li&gt;
  &lt;li&gt;For computer vision projects, &lt;a href=&quot;https://huggingface.co/spaces&quot;&gt;Hugging Face Spaces&lt;/a&gt; is a great place to find interesting Gradio demos, though after a while they blend into each other a little. HF Spaces also doesn’t seem like it gets used for full project explanation that often.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;characteristics-of-top-projects&quot;&gt;Characteristics of top projects&lt;/h2&gt;

&lt;p&gt;Some things I think make a great portfolio project stand out:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;visual design — looks count for a lot, for better or for worse.&lt;/li&gt;
  &lt;li&gt;interactivity — if there is some kind of a demo or application that I can play around with in order to relate to concepts being written about, that’d be great.&lt;/li&gt;
  &lt;li&gt;visual explanations alongside pure text — a diagram or animation can really help bring explanations to life.&lt;/li&gt;
  &lt;li&gt;a clear overview — the structure of the writeup should be clear and readers should be able to get a high-level overview first without necessarily needing to read through every last detail.&lt;/li&gt;
  &lt;li&gt;explain what problem you’re solving — spend (probably) more time than you think is necessary to explain what problem you’re solving and set up the context for the work you did.&lt;/li&gt;
  &lt;li&gt;code snippets are ok, but don’t just dump your source code.&lt;/li&gt;
  &lt;li&gt;present your dead ends — don’t just present the happy path; feel free to present things that didn’t work out as well. Readers will want to know that you encountered difficulties and there are benefits from seeing how you made decisions along the way.&lt;/li&gt;
  &lt;li&gt;present further work and next steps — offer hints at what other work could be done on the project, even if you’re done with it for now.&lt;/li&gt;
  &lt;li&gt;don’t lose track of the use case — show that you thought about the specific problem you were solving and not just as a technical problem in a void. (Real-world use cases have constraints, and your solution should live within a universe where those constraints directed you).&lt;/li&gt;
  &lt;li&gt;Feel free to link out — you can easily link to other places where you’ve gone into the details about a particular problem you encountered. No need to cram every single last detail into the project portfolio.&lt;/li&gt;
  &lt;li&gt;Don’t forget the purpose of the portfolio — it doesn’t need to be an exhaustive catalogue of every last detail; it just needs to offer a compelling overview that is understandable as an independent entity.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are other aspects which are more table stakes for anything you write online — no typos, clear writing and so on.&lt;/p&gt;

&lt;p&gt;I took the time to step back from the project to write this down as I move into a phase where I’ll increasingly focus on the full writeup and I wanted to have a list to remind me of the things I valued in these kinds of projects.&lt;/p&gt;

&lt;p&gt;If you have good examples of ML portfolio projects (or really great blog write-ups with interactivity and so on), please let me know in the comments!&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="computervision" /><category term="skillbuilding" /><summary type="html">Ekko was the last time I worked on a big project that would be presented publicly. An open-source framework that provided realtime infrastructure and in-transit message processing for web applications was a group project that I worked on together with three other colleagues, and we took the time to really make the how and the why really explicit. We made animations, diagrams, charts, and I learned a lot about what’s hard when explaining technical projects, even when the audience is expected to be (mostly) technically literate.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/ml-portfolio-best-practices/healthsea.png" /><media:content medium="image" url="https://mlops.systems/images/ml-portfolio-best-practices/healthsea.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Building my own image to use IceVision with Paperspace</title><link href="https://mlops.systems/tools/docker/computervision/2022/03/25/paperspace-docker-icevision.html" rel="alternate" type="text/html" title="Building my own image to use IceVision with Paperspace" /><published>2022-03-25T00:00:00-05:00</published><updated>2022-03-25T00:00:00-05:00</updated><id>https://mlops.systems/tools/docker/computervision/2022/03/25/paperspace-docker-icevision</id><content type="html" xml:base="https://mlops.systems/tools/docker/computervision/2022/03/25/paperspace-docker-icevision.html">&lt;p&gt;I’ve been using &lt;a href=&quot;https://www.paperspace.com/&quot;&gt;Paperspace&lt;/a&gt; right to fuel my ML/Deep Learning experimentation since more or less the beginning. It was one of the recommended platforms that offered GPUs for the &lt;a href=&quot;https://www.fast.ai/&quot;&gt;fastai&lt;/a&gt; course and when I started working on &lt;a href=&quot;https://mlops.systems/categories/#redactionmodel&quot;&gt;my redaction project&lt;/a&gt; I chose to keep going since I had little reason to change.&lt;/p&gt;

&lt;p&gt;Fast-forward a few months, and I’ve had a few issues along the way. Paperspace works by provisioning a Docker image, connecting it to a fixed filesystem / storage backend and then serving this up to you in a web interface as a Jupyter notebook. I found that sometimes there were issues with dependencies breaking, or special &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install&lt;/code&gt; magic I had to include in my notebook so that things would work again.&lt;/p&gt;

&lt;p&gt;Included in this is the reality that a full install of IceVision — an amazing library for computer vision that handles a lot of the pain around integrating various libraries and use cases — simply takes a while as it has to download and setup some pretty hefty dependencies. I had found that going from zero to working on the day’s specific issue took around 20 minutes when you factored in all the setup, updates from the Github repo, syncing data and so on.&lt;/p&gt;

&lt;p&gt;Inspired by &lt;a href=&quot;https://mlops.systems/tools/dockerinamonthoflunches/books-i-read/2022/03/21/docker-in-a-month.html&quot;&gt;my reading and study of Docker&lt;/a&gt; — and with &lt;a href=&quot;https://github.com/joshua-paperspace/python-runtime/blob/main/Dockerfile&quot;&gt;a tip from a Paperspace engineer&lt;/a&gt; about how I could get started — I set out to build a custom image that handled most of the setup upfront and automatically updated with the latest changes and data.&lt;/p&gt;

&lt;p&gt;Amazingly, it worked more or less immediately! I created &lt;a href=&quot;https://gist.github.com/strickvl/956f233ab53b3b56d463aebb95d7104c&quot;&gt;a new Dockerfile&lt;/a&gt; based of the original suggestion and the core additions were the following:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;RUN wget https://raw.githubusercontent.com/airctic/icevision/master/icevision_install.sh &amp;amp;&amp;amp; bash icevision_install.sh cuda11 &amp;amp;&amp;amp; rm icevision_install.sh

RUN pip install torchtext==0.11.0 --upgrade
RUN pip install opencv-python ipywidgets icevision-dashboards
RUN apt update &amp;amp;&amp;amp; apt install -y libsm6 libxext6
RUN apt-get install -y libxrender-dev

CMD make lfs &amp;amp;&amp;amp; git lfs pull
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In order to set this up with Paperspace, you first have to go to your notebooks inside a project and click to create a new Paperspace notebook.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/paperspace-docker-icevision/paperspace1.png&quot; alt=&quot;&quot; title=&quot;Creating a Paperspace notebook&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once there, you can ignore the suggestion to “select a runtime”, but rather select your machine from the available GPUs. I usually choose the RTX5000 and set it up for an auto-shutdown after 6 hours.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/paperspace-docker-icevision/paperspace2.png&quot; alt=&quot;&quot; title=&quot;Selecting a GPU within Paperspace&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Then you want to click the ‘Advanced Options’ toggle so you can add in all the details of the image being used.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/paperspace-docker-icevision/paperspace3.png&quot; alt=&quot;&quot; title=&quot;Setting up the custom image in Paperspace&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is what worked for me. In order to use &lt;a href=&quot;https://jupyter.org/&quot;&gt;JupyterLab&lt;/a&gt;, the container command should be:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jupyter lab --allow-root --ip=0.0.0.0 --no-browser --ServerApp.trust_xheaders=True --ServerApp.disable_check_xsrf=False --ServerApp.allow_remote_access=True --ServerApp.allow_origin='*' --ServerApp.allow_credentials=True
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I enter my private GitHub repo (along with my username and a custom token generated to allow Paperspace to download the repo) in the ‘Workspace’ section.&lt;/p&gt;

&lt;p&gt;Then when I click ‘Start Notebook’, it works! No more hanging around for IceVision to install. My Docker image already has this!&lt;/p&gt;

&lt;p&gt;I realise that I’m probably a little late to the party in terms of using Docker and seeing how it can bring some real improvements in terms of reproducibility of environments as well as these little quality-of-life perks like not hanging around to install everything each time you want to use it. This was a really useful experience for me to learn from, and I’ll certainly be using this going forward in other projects I work on.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="tools" /><category term="docker" /><category term="computervision" /><summary type="html">I’ve been using Paperspace right to fuel my ML/Deep Learning experimentation since more or less the beginning. It was one of the recommended platforms that offered GPUs for the fastai course and when I started working on my redaction project I chose to keep going since I had little reason to change.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/paperspace-docker-icevision/docker-image-cover.png" /><media:content medium="image" url="https://mlops.systems/images/paperspace-docker-icevision/docker-image-cover.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Starting Docker In A Month Of Lunches</title><link href="https://mlops.systems/tools/dockerinamonthoflunches/books-i-read/2022/03/21/docker-in-a-month.html" rel="alternate" type="text/html" title="Starting Docker In A Month Of Lunches" /><published>2022-03-21T00:00:00-05:00</published><updated>2022-03-21T00:00:00-05:00</updated><id>https://mlops.systems/tools/dockerinamonthoflunches/books-i-read/2022/03/21/docker-in-a-month</id><content type="html" xml:base="https://mlops.systems/tools/dockerinamonthoflunches/books-i-read/2022/03/21/docker-in-a-month.html">&lt;p&gt;As far as software engineers go, I’m still barely a spring chicken, six months into my job with &lt;a href=&quot;https://zenml.io/&quot;&gt;ZenML&lt;/a&gt;. Working at a startup is fairly fast-paced and the ability to get going quickly with any number of libraries and tools is a requirement of the work. The number of things I could study, learn or practice is vastly larger than the amount of time I have. Accordingly, it helps to try to pick things that will be long-lasting, teach a cross-cutting skill or that are somehow fundamental.&lt;/p&gt;

&lt;p&gt;Two closely-connected technologies that I’ve realised I can no longer avoid are &lt;a href=&quot;https://www.docker.com/&quot;&gt;Docker&lt;/a&gt; and &lt;a href=&quot;https://kubernetes.io/&quot;&gt;Kubernetes&lt;/a&gt;. I have some high-level knowledge of both, having worked with Docker images on Ekko and having encountered Kubernetes in recent months, but it’s become clear in the last few weeks that they aren’t going away. More than that, it seems that not having a better (practical) grasp of some of the ins and outs of both is holding me back from grasping more complex decisions that are being made at work.&lt;/p&gt;

&lt;p&gt;[&lt;em&gt;Side-note: I’m very curious about &lt;a href=&quot;https://podman.io/&quot;&gt;Podman&lt;/a&gt; as a Docker-adjacent alternative, but I need to understand Docker better first before I can make comparisons. I’d also note that I’m pretty sure that there are lots of cases where Kubernetes is overkill, and where it doesn’t make much sense to add all that complexity, particularly for smaller teams and projects. It’s nevertheless a feature of life in the MLOps space, it seems, so I must understand it.&lt;/em&gt;]&lt;/p&gt;

&lt;p&gt;I’ve had my eye on two Manning books by &lt;a href=&quot;https://blog.sixeyed.com/&quot;&gt;Elton Stoneman&lt;/a&gt; for a while, and now seems the perfect time to dive in. &lt;a href=&quot;https://www.amazon.com/Learn-Docker-Month-Lunches-Stoneman-ebook/dp/B097824MVJ/ref=tmm_kin_swatch_0?qid=&amp;amp;sr=&amp;amp;tag=soumet-20&amp;amp;_encoding=UTF8&quot;&gt;Learn Docker in a Month of Lunches&lt;/a&gt; and &lt;a href=&quot;https://www.amazon.com/gp/product/B0978175TP/ref=dbs_a_def_rwt_bibl_vppi_i1?tag=soumet-20&quot;&gt;Learn Kubernetes in a Month of Lunches&lt;/a&gt; are very practical introductions to their subjects, come with good reviews and feedback and were published relatively recently. I’m especially happy that both books are extremely hands-on and even though I won’t in any way be an expert in either technology by the end, I’ll at least have some experience of having encountered the core use cases of both and maybe have a strong idea of what I do and don’t know.&lt;/p&gt;

&lt;p&gt;I’m not sure whether I’ll complete each one in exactly a month, but I’ll try to fast-track my reading. The chapters are written in such a way as to be digestible (including exercises) in around an hour. Stoneman says in the introduction to the Kubernetes book that it’s best to start with the Docker one, which I suppose makes sense given that one builds on the other.&lt;/p&gt;

&lt;p&gt;Just like &lt;a href=&quot;https://mlops.systems/categories/#robustpython&quot;&gt;my posts as I read through Robust Python&lt;/a&gt; (which I haven’t stopped doing), I’ll write up various things that I learn along the way, mainly as notes for myself but perhaps it will have value beyond this limited goal. So far I’ve read through the first three chapters of the Docker book, so what follows are some notes on the key points from that.&lt;/p&gt;

&lt;h1 id=&quot;core-docker-commands&quot;&gt;Core Docker Commands&lt;/h1&gt;

&lt;p&gt;The book has you running a lot of examples. Two commands mentioned (specific to this book) to help clean up the images and containers that you were using:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# clean up containers and application packages&lt;/span&gt;
docker container &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;docker container &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-aq&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# to reclaim disk space&lt;/span&gt;
docker image &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;docker image &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;reference&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'dial/*'&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-q&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Some core commands for interacting with a container:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# run a container&lt;/span&gt;
docker container run CONTAINERNAME

&lt;span class=&quot;c&quot;&gt;# run an interactive container with a connected terminal session&lt;/span&gt;
docker container run &lt;span class=&quot;nt&quot;&gt;--interactive&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--tty&lt;/span&gt; CONTAINERNAME

&lt;span class=&quot;c&quot;&gt;# list running containers&lt;/span&gt;
docker container &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# list all containers with any status&lt;/span&gt;
docker container &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--all&lt;/span&gt; 

&lt;span class=&quot;c&quot;&gt;# list processes running in a container&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# CONTAINERNAME could be part of the container ID as well&lt;/span&gt;
docker container top CONTAINERNAME

&lt;span class=&quot;c&quot;&gt;# show logs for a container&lt;/span&gt;
docker container logs CONTAINERNAME

&lt;span class=&quot;c&quot;&gt;# view all details about a container&lt;/span&gt;
docker container inspect CONTAINERNAME

&lt;span class=&quot;c&quot;&gt;# get stats on a particular container&lt;/span&gt;
docker container stats CONTAINERNAME

&lt;span class=&quot;c&quot;&gt;# special flags&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# --detach starts the container in the background&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# --publish publishes a port from the container to the computer&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# delete containers&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# the force flag shuts it down if still running&lt;/span&gt;
docker container &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--force&lt;/span&gt; CONTAINERNAME

&lt;span class=&quot;c&quot;&gt;# the nuclear option&lt;/span&gt;
docker container &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--force&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;docker container &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--all&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--quiet&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;building-your-own-images-with-dockerfiles&quot;&gt;Building your own images with Dockerfiles&lt;/h1&gt;

&lt;p&gt;Some commands which are useful for making your own images:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# gets the image from DockerHub registry&lt;/span&gt;
docker image pull IMAGENAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Key mental models for Docker images:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;images are made up of ‘layers’&lt;/li&gt;
  &lt;li&gt;Docker images are stored as lots of small files, brought together as one image when you build with a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;each layer of an image corresponds to a line in your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;layers are successively built on top of each other&lt;/li&gt;
  &lt;li&gt;the order of the layers determines the cache invalidation. If something changes in a lower layer, then all subsequent layers are regenerated. It’s for this reason that it pays to be careful about the order in which you write the commands that make up your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It seems to be considered a good practice (at least where I am right now in the book) to pass in environment variables from the outside into your Docker image. This way you can keep configuration separate from how you actually run it. So you’d have a command something like this:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker container run &lt;span class=&quot;nt&quot;&gt;--env&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;EPOCHS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;30 SOMEUSER/CONTAINERNAME
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;which would pass the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EPOCHS&lt;/code&gt; environment variable into the runtime of the Docker image if it had been set up with something like this as a line inside the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ENV EPOCHS=&quot;1&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that only the environment variables that you specifically select to be passed into the container get passed in.&lt;/p&gt;

&lt;h2 id=&quot;dockerfile-layout&quot;&gt;Dockerfile layout&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt;s seem to have some commonalities in terms of the structure:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;you start with a base image on which you’re building. These seem usually or often to be a base image containing a runtime for whatever language your application uses&lt;/li&gt;
  &lt;li&gt;Then there’s often environment variables afterwards&lt;/li&gt;
  &lt;li&gt;Then you can specify a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WORKDIR&lt;/code&gt; which is the working directory for the application&lt;/li&gt;
  &lt;li&gt;Then you can &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;COPY&lt;/code&gt; files from your local filesystem into that working directory&lt;/li&gt;
  &lt;li&gt;Then at the end you specify which &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CMD&lt;/code&gt; needs to be run in order to execute the application.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once you’re done with writing your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt;, use the following command to build your image:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker image build &lt;span class=&quot;nt&quot;&gt;--tag&lt;/span&gt; SOMENAME &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that final &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.&lt;/code&gt; trailing that command. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.&lt;/code&gt; states that the current directory is the ‘context’ and thus is used for when you’re copying in files using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;COPY&lt;/code&gt; command.&lt;/p&gt;

&lt;p&gt;You can view the layers of your Docker image with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker image history IMAGENAME&lt;/code&gt; command (which will output them to the terminal).&lt;/p&gt;

&lt;p&gt;To see how much disk space your containers and images are taking up, type &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker system df&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;When you rebuild an image, you can specify this with a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:v2&lt;/code&gt; after the name, as in this command:&lt;/p&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker image build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; web-app:v2 &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When it comes to optimising your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Dockerfile&lt;/code&gt;, bear the following in mind:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Any Dockerfile you write should be optimised so that the instructions are ordered by how frequently they change — with instructions that are unlikely to change at the start of the Dockerfile, and instructions most likely to change at the end. The goal is for most builds to only need to execute the last instruction, using the cache for everything else. That saves time, disk space, and network bandwidth when you start sharing your images.” (pp. 42-43)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Some command tips and tricks:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;combine multiple commands onto the same line&lt;/li&gt;
  &lt;li&gt;put the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CMD&lt;/code&gt; instruction early on as it’s unlikely to change&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="tools" /><category term="dockerinamonthoflunches" /><category term="books-i-read" /><summary type="html">As far as software engineers go, I’m still barely a spring chicken, six months into my job with ZenML. Working at a startup is fairly fast-paced and the ability to get going quickly with any number of libraries and tools is a requirement of the work. The number of things I could study, learn or practice is vastly larger than the amount of time I have. Accordingly, it helps to try to pick things that will be long-lasting, teach a cross-cutting skill or that are somehow fundamental.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/diamol/diamol-cover.jpg" /><media:content medium="image" url="https://mlops.systems/images/diamol/diamol-cover.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Figuring out why my object detection model is underperforming with FiftyOne, a great tool you probably haven’t heard of</title><link href="https://mlops.systems/redactionmodel/computervision/tools/debugging/jupyter/2022/03/12/fiftyone-computervision.html" rel="alternate" type="text/html" title="Figuring out why my object detection model is underperforming with FiftyOne, a great tool you probably haven’t heard of" /><published>2022-03-12T00:00:00-06:00</published><updated>2022-03-12T00:00:00-06:00</updated><id>https://mlops.systems/redactionmodel/computervision/tools/debugging/jupyter/2022/03/12/fiftyone-computervision</id><content type="html" xml:base="https://mlops.systems/redactionmodel/computervision/tools/debugging/jupyter/2022/03/12/fiftyone-computervision.html">&lt;p&gt;&lt;em&gt;(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out &lt;a href=&quot;https://mlops.systems/categories/#redactionmodel&quot;&gt;the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;redactionmodel&lt;/code&gt; taglist&lt;/a&gt;.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So you’ve trained a computer vision model, but you think it could do better. What do you do next? This is a common scenario, especially for computer vision problems where fine-tuning someone else’s pre-trained model is a pretty normal initial step that gets taken. You emerge with a decent score on whatever metric you care about, but it also isn’t great.&lt;/p&gt;

&lt;p&gt;One part of the solution is certainly ‘more data’. This approach was recently highlighted by Boris Dayma on Twitter:&lt;/p&gt;

&lt;div class=&quot;jekyll-twitter-plugin&quot;&gt;&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Easy recipe to get quickly a cool classification model on your own dataset 🤓&lt;br /&gt;✅ spend 1-2h to sort part of your data&lt;br /&gt;✅ split in train/val as 90/10&lt;br /&gt;✅ fine-tune a model (HuggingFace makes it easy)&lt;br /&gt;✅ use that model to sort faster more data&lt;br /&gt;✅ train again &amp;amp; repeat until happy!&lt;/p&gt;&amp;mdash; Boris Dayma 🖍️ (@borisdayma) &lt;a href=&quot;https://twitter.com/borisdayma/status/1502317249423679495?ref_src=twsrc%5Etfw&quot;&gt;March 11, 2022&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/div&gt;

&lt;p&gt;In my case, I currently have a little over 1200 images that have been annotated, but of those some 600 of them don’t contain any redactions at all (i.e. they just have content boxes). I did mention that I was using a similar approach early on, where I’d use the model to help pre-annotate images, but I haven’t been using that recently.&lt;/p&gt;

&lt;p&gt;I’m realising that more important than pure volume of data is to annotate types of images that are the hardest for the model to learn. So what I really want to know at this point is where I should place my focus when it comes to supplementing the training data. My images aren’t currently divided into separate classes, but I have a proxy (the filename) which will be really helpful once I’ve identified which types I need to supplement.&lt;/p&gt;

&lt;p&gt;When seeking to improve computer vision models with error analysis, some kind of visual inspection is essential. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fastai&lt;/code&gt; had &lt;a href=&quot;https://docs.fast.ai/interpret.html&quot;&gt;a number of utility methods&lt;/a&gt; that helped in the interpretation of where a model was underperforming, but for object detection I think you do need something that was built to purpose, where you can really dive into the specific ways each object was or wasn’t detected.&lt;/p&gt;

&lt;p&gt;Enter &lt;a href=&quot;https://voxel51.com/docs/fiftyone/&quot;&gt;FiftyOne&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://voxel51.com/docs/fiftyone/&quot;&gt;FiftyOne&lt;/a&gt; is an open-source tool built specifically to support the curation and creation of datasets for computer vision models. It is almost two years old in its open-source incarnation, and (or but?) it feels very solid and robust in its implementation. &lt;a href=&quot;https://voxel51.com&quot;&gt;Voxel51&lt;/a&gt;, the company behind it, has taken great pains to write excellent documentation and guides, and they have &lt;a href=&quot;https://join.slack.com/t/fiftyone-users/shared_invite/zt-gtpmm76o-9AjvzNPBOzevBySKzt02gg&quot;&gt;a supportive community&lt;/a&gt; behind the scenes, too.&lt;/p&gt;

&lt;h1 id=&quot;fiftyone-basics&quot;&gt;FiftyOne Basics&lt;/h1&gt;

&lt;p&gt;FiftyOne is a Python library that offers a visual interface to your data. For my redaction model, the base interface looks something like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/fiftyone-computervision/fiftyone-overview.png&quot; alt=&quot;&quot; title=&quot;The basic view of the FiftyOne app&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You need to convert your dataset such that FiftyOne can interpret the structure of where images are stored as well as the annotations themselves, but many commonly-used formats are supported. In my case, COCO annotations are supported out of the box, so it was trivial to import the data to generate the above visualisation.&lt;/p&gt;

&lt;p&gt;You can use the FiftyOne application inside a Jupyter Notebook, or you can have it open in a separate tab. A separate tab is my preference as it allows for a larger interface. (There is also a completely separate Desktop app interface you can use, but I think not all functionality works there so you might want to stick to a separate tab).&lt;/p&gt;

&lt;p&gt;Luckily for me, my computer vision framework of choice is IceVision, and &lt;a href=&quot;https://airctic.com/0.12.0/using_fiftyone_in_icevision/&quot;&gt;they recently integrated&lt;/a&gt; with FiftyOne which makes creating datasets a breeze.&lt;/p&gt;

&lt;p&gt;So how did FiftyOne help me understand how my model was performing? (Note: the sections that follow were significantly helped by following &lt;a href=&quot;https://voxel51.com/docs/fiftyone/tutorials/evaluate_detections.html&quot;&gt;this&lt;/a&gt;, &lt;a href=&quot;https://voxel51.com/docs/fiftyone/tutorials/uniqueness.html&quot;&gt;this&lt;/a&gt; and &lt;a href=&quot;https://voxel51.com/docs/fiftyone/tutorials/detection_mistakes.html&quot;&gt;this&lt;/a&gt; part of the &lt;a href=&quot;https://voxel51.com/docs/fiftyone/&quot;&gt;FiftyOne docs&lt;/a&gt;.)&lt;/p&gt;

&lt;h1 id=&quot;comparing-ground-truth-with-predictions&quot;&gt;Comparing ground truth with predictions&lt;/h1&gt;

&lt;p&gt;The first thing I did was visualise the ground truth annotations alongside the predictions of my model. (This is the model mentioned in &lt;a href=&quot;https://mlops.systems/redactionmodel/computervision/tools/2022/03/03/model-improvements.html&quot;&gt;my last blogpost&lt;/a&gt;, which had a COCO score of almost 80%.)&lt;/p&gt;

&lt;p&gt;This requires performing inference on a slice of our images. Unfortunately, I had to do that inference on my local (CPU) machine because FiftyOne doesn’t work on Paperspace cloud machines on account of port forwarding choices that Paperspace make. This makes for a slightly slower iteration cycle, but once the inference is done you don’t have to do it again.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/fiftyone-computervision/groundtruth-predictions.gif&quot; alt=&quot;&quot; title=&quot;Comparing ground truth with predictions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can see here that it’s possible to selectively turn off and on the various overlaid annotations. If you want to compare how redactions are detected (and not see the content box), then this is an easy way to toggle between.&lt;/p&gt;

&lt;h2 id=&quot;viewing-only-high-confidence-predictions&quot;&gt;Viewing only high-confidence predictions&lt;/h2&gt;

&lt;p&gt;Not all predictions are created equal, too, so it would be useful to view only those predictions where the confidence was higher than 75%. FiftyOne makes this kind of conditional view easy. You can do it in code, as in the following snippet, or you can do it via the GUI inside the app.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;fiftyone&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ViewField&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Only contains detections with confidence &amp;gt;= 0.75
# `dataset` is the FiftyOne core object that was created before
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;high_conf_view&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filter_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;prediction&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;confidence&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.75&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/fiftyone-computervision/highconfidence.png&quot; alt=&quot;&quot; title=&quot;Viewing high-confidence predictions&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;patches-detailed-views-for-detected-objects&quot;&gt;‘Patches’: detailed views for detected objects&lt;/h2&gt;

&lt;p&gt;For a more fine-grained understanding on the ways our model is predicting redactions, we can create what are called ‘patches’ to view and scroll through prediction-by-prediction.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/fiftyone-computervision/redaction-patch.png&quot; alt=&quot;&quot; title=&quot;Patch view of some predicted redactions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is an excellent way to view things through the eyes of your model. These are all the objects it considers to be redactions. We’ll get to finding the ones where it doesn’t do as well in a bit, but this view allows us to immerse ourselves in the reality of how our model is predicting redaction boxes. We can see that certain types of boxes are well-represented in our dataset: coloured or shaded rectangles in particular.&lt;/p&gt;

&lt;h1 id=&quot;understanding-how-our-model-performs-for-separate-classes&quot;&gt;Understanding how our model performs for separate classes&lt;/h1&gt;

&lt;p&gt;We only have two classes in our training data: redaction and content, so doing a class analysis doesn’t help us too much for this problem, but using the mean average precision (MAP) calculation we can see the difference between how well our model does on redactions vs content:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/fiftyone-computervision/class-comparison.png&quot; alt=&quot;&quot; title=&quot;A classification report for our dataset&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can also easily plot an interactive chart that quite clearly displays these differences:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/fiftyone-computervision/plotting-curves.png&quot; alt=&quot;&quot; title=&quot;Plotting the precision vs recall curves for our two classes&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;viewing-the-false-positives-and-false-negatives&quot;&gt;Viewing the false positives and false negatives&lt;/h1&gt;

&lt;p&gt;The previous calculations also added some metadata to each image, denoting whether it was considered a true positive, false positive or false negative. It’s really useful to be able to easily switch between these views, and identifying the images with the largest numbers of false positives and false negatives will help appreciate what our model struggles with.&lt;/p&gt;

&lt;p&gt;This view is sorted by total number of false positives in an image. False positives are where the model confidently has predicted something to be a redaction box, for example, that is not actually a redaction box.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/fiftyone-computervision/false-positives.png&quot; alt=&quot;&quot; title=&quot;Predicted false positives&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this image you can see that the model predicts a redaction box with 82% confidence that is clearly not a redaction. Note, too, how the smaller redactions to the right and the large partial redaction to the left were not detected.&lt;/p&gt;

&lt;p&gt;False negatives are where there were some redactions to be predicted, but our model never made those predictions (or was very unconfident in doing so).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/fiftyone-computervision/false-negatives.png&quot; alt=&quot;&quot; title=&quot;False negatives &amp;amp; missing predictions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this image excerpt, you can see that some predictions were made, but many were also missed. This image shows the ground truth reality of what should have been predicted:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/fiftyone-computervision/false-negatives-overlaid.png&quot; alt=&quot;&quot; title=&quot;Overlaying the ground truth, showing many false negatives&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Scrolling through the examples with high numbers of false positives and false negatives gives me a really useful indication of which kinds of redactions with which I need to annotate and supplement my training data. I already had a sense of this from my own intuition, but it’s excellent to see this confirmed in the data.&lt;/p&gt;

&lt;h1 id=&quot;finding-detection-mistakes-with-fiftyone-brains-mistakenness-calculation&quot;&gt;Finding detection mistakes with FiftyOne Brain’s &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mistakenness&lt;/code&gt; calculation&lt;/h1&gt;

&lt;p&gt;FiftyOne is not only the visual interface, but it also has something called the FiftyOne &lt;a href=&quot;https://voxel51.com/docs/fiftyone/user_guide/brain.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;brain&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;Toast&quot;&gt;
   &lt;span class=&quot;Toast-icon&quot;&gt;&lt;svg class=&quot;octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/span&gt;
   &lt;span class=&quot;Toast-content&quot;&gt;It's worth being aware of the distinction between the two: FiftyOne itself is open-source and free to use. The brain is closed-source and free for non-commercial uses.&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;The brain allows you to perform various calculations on your dataset to determine (among other things):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;visual similarity&lt;/li&gt;
  &lt;li&gt;uniqueness&lt;/li&gt;
  &lt;li&gt;mistakenness&lt;/li&gt;
  &lt;li&gt;hardness&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(You can also &lt;a href=&quot;https://voxel51.com/docs/fiftyone/user_guide/brain.html#brain-embeddings-visualization&quot;&gt;visualise embeddings&lt;/a&gt; to cluster image or annotation types, but I haven’t used that feature yet so can’t comment as its effectiveness.)&lt;/p&gt;

&lt;p&gt;For my dataset, visualising similarity and uniqueness revealed what I already knew: that lots of the images were similar. Knowing the context of the documents well means I’m familiar with how a lot of the documents look the same. Not much of a revelation there.&lt;/p&gt;

&lt;p&gt;The mistakenness calculation is useful, however. It compares between the ground truth and the predictions to get a sense of which images it believes contains annotations that might be wrong. I can filter these such that we only show images where it is more than 80% confident mistakes have been made. Instantly it reveals a few examples where there have been annotation mistakes. To take one example, here you can see the ground truth annotations:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/fiftyone-computervision/groundtruth-mistake.png&quot; alt=&quot;&quot; title=&quot;Ground truth mistakes&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And here you can see what was predicted:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/fiftyone-computervision/mistake-what-was-predicted.png&quot; alt=&quot;&quot; title=&quot;What was predicted, revealing mistakes in the ground truth annotations&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this example, it was even clear from the beginning that redactions had been missed, and that the single annotation that had been made (a content box) was incorrect.&lt;/p&gt;

&lt;h1 id=&quot;finding-missing-annotations&quot;&gt;Finding missing annotations&lt;/h1&gt;

&lt;p&gt;We can also view images that the FiftyOne brain tagged as containing missing annotations:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;possible_missing&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/images/fiftyone-computervision/missing-annotations.png&quot; alt=&quot;&quot; title=&quot;Suggestions for images containing annotations that were missing&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Unfortunately the &lt;a href=&quot;https://voxel51.com/docs/fiftyone/user_guide/brain.html#brain-sample-hardness&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;compute_hardness&lt;/code&gt;&lt;/a&gt; method only works for classification models currently, but regardless I think we have a lot to work with already.&lt;/p&gt;

&lt;h1 id=&quot;conclusions-and-next-steps&quot;&gt;Conclusions and Next Steps&lt;/h1&gt;

&lt;p&gt;I hope this practical introduction to FiftyOne has given you a high-level overview of the ways the tool can be useful in evaluating your computer vision models.&lt;/p&gt;

&lt;p&gt;For my redaction project, I’m taking some clear action steps I need to work on as a result of some of this analysis.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I need do annotate more of the kinds of images it struggles with. Specifically, this means images containing redactions that are just white boxes, with a bonus for those white redaction boxes being superimposed on top of a page filed with white boxes (i.e. some sort of table or form).&lt;/li&gt;
  &lt;li&gt;I need to remove some of the bad/false ground truth annotations that the FiftyOne brain helpfully identified.&lt;/li&gt;
  &lt;li&gt;I will probably want to repeat this process together in a model that was trained together with the synthetic data to see what differences can be observed.&lt;/li&gt;
  &lt;li&gt;As a general point, I probably want to incorporate visual inspection of the data at various points in the training pipeline, not just after the model has been trained.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you know any other tools that help with this kind of visual analysis of model performance and how to improve in a data-driven approach, please do &lt;a href=&quot;https://twitter.com/strickvl&quot;&gt;let me know&lt;/a&gt;!&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="redactionmodel" /><category term="computervision" /><category term="tools" /><category term="debugging" /><category term="jupyter" /><summary type="html">(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/fiftyone-computervision/fiftyone-overview.png" /><media:content medium="image" url="https://mlops.systems/images/fiftyone-computervision/fiftyone-overview.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Incremental Improvements to my Redaction Detection Model</title><link href="https://mlops.systems/redactionmodel/computervision/tools/2022/03/03/model-improvements.html" rel="alternate" type="text/html" title="Incremental Improvements to my Redaction Detection Model" /><published>2022-03-03T00:00:00-06:00</published><updated>2022-03-03T00:00:00-06:00</updated><id>https://mlops.systems/redactionmodel/computervision/tools/2022/03/03/model-improvements</id><content type="html" xml:base="https://mlops.systems/redactionmodel/computervision/tools/2022/03/03/model-improvements.html">&lt;p&gt;&lt;em&gt;(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out &lt;a href=&quot;https://mlops.systems/categories/#redactionmodel&quot;&gt;the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;redactionmodel&lt;/code&gt; taglist&lt;/a&gt;.)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Last time I wrote about my work on this project, I’d just finished &lt;a href=&quot;https://mlops.systems/redactionmodel/computervision/python/tools/2022/02/10/synthetic-image-data.html&quot;&gt;creating synthetic image data&lt;/a&gt; to supplement my manual annotations. Before integrating those into the model training, I wanted to make changes to a few hyper parameters to ensure that I’m getting as much out of the current configuration as possible.&lt;/p&gt;

&lt;p&gt;I focused on three ways of improving the model’s performance, each of which ended up having an effect albeit in different ways.&lt;/p&gt;

&lt;h1 id=&quot;increasing-the-image-size&quot;&gt;Increasing the image size&lt;/h1&gt;

&lt;p&gt;When I started the project, I set the size of the images that would be passed into the training as the training dataset to 384x384 pixels. (It is a convention relating to how some of the older pre-trained models (like EfficientDet) were such that the image size must be divisible by 128.) This turned out to be too small.&lt;/p&gt;

&lt;p&gt;The next steps up were 512 and 640. The GPU / hardware on which I was training my model seemed to have no problem with either of these image sizes and the performance increased as I worked with either 512 or 640 as the base image sizes.&lt;/p&gt;

&lt;h1 id=&quot;increasing-the-batch-size&quot;&gt;Increasing the batch size&lt;/h1&gt;

&lt;p&gt;Another important lever at my disposal was either to increase the batch size (the number of images that are used in each epoch) or to decrease the learning rate. (&lt;a href=&quot;https://twitter.com/borisdayma/status/1488297953429266433&quot;&gt;A useful Twitter thread by Boris Dayma&lt;/a&gt; explains some of the tradeoffs for one versus the other, along with some references to things to read).&lt;/p&gt;

&lt;p&gt;I had started off with a batch size of 8, but increasing to 16 and then 32 had a big effect on my model’s performance:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/model-improvements/batch-size-experiments.png&quot; alt=&quot;&quot; title=&quot;COCOMetric and validation loss charts for the different batch size increases&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Batch sizes of both 16 and 32 eventually converged on more or less the same COCOMetric score of around 74%. The validation loss rate showed pretty clearly that the highest (32) batch size overfit far faster than for 16. It seems that 16 is the best choice for now.&lt;/p&gt;

&lt;h1 id=&quot;backbone-model-size&quot;&gt;Backbone model size&lt;/h1&gt;

&lt;p&gt;The way I’ve set things up to train this object detection model requires two main choices in terms of architecture: a particular pre-trained model and a backbone. VFNet (&lt;a href=&quot;https://mlops.systems/redactionmodel/computervision/2021/11/30/vfnet-basics.html&quot;&gt;as mentioned previously&lt;/a&gt;) outperformed basically everything else I’ve tried and I think it seems to be a clear best choice for the model. In terms of the backbone, I’d been using resnet50 until now, but following some of the above experiments, it made sense to try increasing the backbone size as well. (An obvious disadvantage to this approach was slower training times and a larger final model size.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/model-improvements/backbone-size-experiments.png&quot; alt=&quot;&quot; title=&quot;COCOMetric and validation loss charts for the different backbone size increases&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In this image you can see the stages of improvements we made throughout this whole process. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vfnet-pre-synthetic-base&lt;/code&gt; was the lowest performer at the beginning, then doubling the batch size gave another boost of almost 8% to our model performance. Then the final increase to the backbone size added another 4% increase bringing us to a score of around 78% for the COCOMetric.&lt;/p&gt;

&lt;p&gt;It remains to be seen how much of these changes will make sense when I introduce the synthetic data, or if there are more effective boosters to the model performance in the form of adding annotations to areas where the model struggles the most.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="redactionmodel" /><category term="computervision" /><category term="tools" /><summary type="html">(This is part of a series of blog posts documenting my work to train a model that detects redactions in documents. To read other posts, check out the redactionmodel taglist.)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/model-improvements/coco_scores.png" /><media:content medium="image" url="https://mlops.systems/images/model-improvements/coco_scores.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Three Python Helpers for Parsing Inputs</title><link href="https://mlops.systems/python/tools/2022/02/27/python-parsers.html" rel="alternate" type="text/html" title="Three Python Helpers for Parsing Inputs" /><published>2022-02-27T00:00:00-06:00</published><updated>2022-02-27T00:00:00-06:00</updated><id>https://mlops.systems/python/tools/2022/02/27/python-parsers</id><content type="html" xml:base="https://mlops.systems/python/tools/2022/02/27/python-parsers.html">&lt;p&gt;I continue to slowly work my way through the &lt;a href=&quot;https://calmcode.io&quot;&gt;calmcode&lt;/a&gt; back catalogue. This week I learned about three tiny utility packages that make certain parsing tasks less painful.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/r1chardj0n3s/parse&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;parse&lt;/code&gt;&lt;/a&gt; (introduced &lt;a href=&quot;https://calmcode.io/parse/introduction.html&quot;&gt;here&lt;/a&gt;) is a way of turning simple text patterns into restructured data. Take the following example as an illustration:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;parse&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parse&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;https://github.com/strickvl/some-repo/&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;parse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;https://github.com/{owner}/{repo}/&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;named&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# returns {'owner': 'strickvl', 'repo': 'some-repo'}
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As &lt;a href=&quot;https://calmcode.io/parse/introduction.html&quot;&gt;Vincent explains&lt;/a&gt;, it’s sort of the inverse or opposite operation to what happens with an f-string.&lt;/p&gt;

&lt;p&gt;For URLs of various kinds that you want to decompose easily, &lt;a href=&quot;https://yarl.readthedocs.io/en/latest/&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;yarl&lt;/code&gt;&lt;/a&gt; (introduced &lt;a href=&quot;https://calmcode.io/shorts/yarl.py.html&quot;&gt;here&lt;/a&gt;) is a great way to approach that in Python.&lt;/p&gt;

&lt;p&gt;For dates stored in some kind of a string format, you might want to try &lt;a href=&quot;https://datefinder.readthedocs.io&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;datefinder&lt;/code&gt;&lt;/a&gt; (introduced &lt;a href=&quot;https://calmcode.io/shorts/datefinder.py.html&quot;&gt;here&lt;/a&gt;), an elegant if not always perfect way for converting date strings into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;datetime.datetime&lt;/code&gt; objects.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="python" /><category term="tools" /><summary type="html">I continue to slowly work my way through the calmcode back catalogue. This week I learned about three tiny utility packages that make certain parsing tasks less painful.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/python-parsers/parse.png" /><media:content medium="image" url="https://mlops.systems/images/python-parsers/parse.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">It’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model</title><link href="https://mlops.systems/redactionmodel/computervision/python/tools/2022/02/10/synthetic-image-data.html" rel="alternate" type="text/html" title="It’s raining bboxes: how I wrote a Python script to create 2097 synthetic images to help improve my machine learning model" /><published>2022-02-10T00:00:00-06:00</published><updated>2022-02-10T00:00:00-06:00</updated><id>https://mlops.systems/redactionmodel/computervision/python/tools/2022/02/10/synthetic-image-data</id><content type="html" xml:base="https://mlops.systems/redactionmodel/computervision/python/tools/2022/02/10/synthetic-image-data.html">&lt;p&gt;&lt;img src=&quot;/images/synthetic-image-data/synthetic-images-cover.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This blog outlines my process (and a few false starts) for generating a series
of synthetic images (and corresponding annotations) to supplement training data
used in a machine learning project. This problem is one for which there aren’t
many (any?) pre-existing data sets that I can repurpose so I’ve been trying to
find ways to bootstrap and improve the performance of the model I’m training.&lt;/p&gt;

&lt;p&gt;Before I dive into the details, I wanted to include a little context on
&lt;a href=&quot;https://mlops.systems/categories/#redactionmodel&quot;&gt;the wider project&lt;/a&gt; and what
I’m seeking to accomplish. It is a relatively common practice for documents
released as part of &lt;a href=&quot;https://www.foia.gov&quot;&gt;FOIA&lt;/a&gt; requests to contain redactions.
With so many documents being released — and perhaps in specific cases where
legal teams are dealing with huge numbers of those redacted documents — it can
be useful to identify which documents are redacted and/or to get a sense of just
how much has been redacted. If you have 10 or 20 documents you can fairly easily
get that overview, but if you have 10,000 or a million documents? That’s where
my project comes in: I want to train a model to make it easy to detect
redactions in a document and to generate statistics on what proportion of a
document or documents have been redacted.&lt;/p&gt;

&lt;p&gt;You can
&lt;a href=&quot;https://mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html&quot;&gt;read more about the problem domain&lt;/a&gt;,
about
&lt;a href=&quot;https://mlops.systems/redactionmodel/computervision/datalabelling/2021/11/29/prodigy-object-detection-training.html&quot;&gt;my initial forays&lt;/a&gt;
into annotating a dataset for this problem, as well as view
&lt;a href=&quot;https://mlops.systems/redactionmodel/2021/12/15/redaction-taxonomy.html&quot;&gt;some examples&lt;/a&gt;
of these redactions (and perhaps why they’re not as easy to identify as you
might think). You can even try out a demo showing some of what the model can
identify &lt;a href=&quot;https://huggingface.co/spaces/strickvl/redaction-model-demo&quot;&gt;here&lt;/a&gt;.
Note that this isn’t the latest version of the model so it’s not the absolute
best performance.&lt;/p&gt;

&lt;h2 id=&quot;whats-the-deal-with-synthetic-images&quot;&gt;What’s the deal with synthetic images?&lt;/h2&gt;

&lt;p&gt;It’s a truism that in computer vision projects you probably need or want a lot
of data to get good results. For the Facebooks and Bytedances of the world this
perhaps isn’t an issue: they have access to a ton of data, for better or for
worse. But for me, I don’t have teams of data annotators or millions of users
generating all this data. This is probably the norm for small- to medium-sized
computer vision problems being solved out in the world, especially with more
non-traditional entrants into the field who are just trying to &lt;em&gt;do&lt;/em&gt; things with
the skills instead of generating research and so on.&lt;/p&gt;

&lt;p&gt;Instead of using huge amounts of data, we need to be smarter about how we work,
levelling ourselves up with whatever tricks of the trade we can muster. The
&lt;a href=&quot;https://course.fast.ai/&quot;&gt;fastai course&lt;/a&gt; contains a great number of these best
practices, perhaps unsurprisingly since it is in some way targeted at
individuals seeking to solve their domain-specific problems. One of the key
insights I took away from earlier parts of
&lt;a href=&quot;https://github.com/fastai/fastbook&quot;&gt;the fastai book&lt;/a&gt; was the benefits of using
pre-trained models. With a wealth of these models
&lt;a href=&quot;https://github.com/balavenkatesh3322/CV-pretrained-model&quot;&gt;available&lt;/a&gt; and
&lt;a href=&quot;https://github.com/shubham-shahh/Open-Source-Models&quot;&gt;accessible&lt;/a&gt;, you don’t
need to start your work from scratch. Instead, fine-tune your model and benefit
from the expertise and hard work of others.&lt;/p&gt;

&lt;p&gt;You do need &lt;em&gt;some&lt;/em&gt; data to get started with fine-tuning a pre-trained model,
however. That’s why I took a bit of time to
&lt;a href=&quot;https://mlops.systems/redactionmodel/computervision/datalabelling/2021/11/29/prodigy-object-detection-training.html&quot;&gt;make some initial annotations&lt;/a&gt;.
I currently have annotated 2097 images, labelling where I have found redactions
on the images as well as a box to show which parts of the image contain text or
content. That approach has done pretty well so far, with in the low to mid
seventies in terms of a % &lt;a href=&quot;https://cocodataset.org/#detection-eval&quot;&gt;COCO score&lt;/a&gt;.
(This is a commonly-used metric to assess the performance for object detection
problems.) I want to go further, though, which is where synthetic images come
in.&lt;/p&gt;

&lt;p&gt;The big bottleneck in the annotation process is, of course, me. Depending on how
many redactions any particular image contains, it could take me 5-10 minutes for
a single image’s worth of annotations. This does not scale. Part of the speedup
for this process is to use self-training, but I’ll write about that separately.
Another option that has is often used is to generate images which approximate
(to a greater or lesser degree) the actual real images. The useful thing about
generating the images yourself is that you know where you placed the redactions,
so you have the annotations at the same time.&lt;/p&gt;

&lt;p&gt;My overall goal here was to boost my model’s performance. I didn’t know how how
well these synthetic images would contribute, or even if they’d contribute to
any boost at all. I was also quite conscious of the fact that you could probably
spend a year generating pixel-perfect synthetic redacted documents. I didn’t
want to waste too much time doing that, so at various points I had to make
decisions as to whether a particular stage was good enough.&lt;/p&gt;

&lt;h2 id=&quot;phase-1-get-a-baseline--naive-trial&quot;&gt;Phase 1: Get a Baseline / Naive Trial&lt;/h2&gt;

&lt;p&gt;When I started this, I didn’t know how hard or easy it was going to be, so I set
myself a low bar. I knew it was theoretically possible to create images with
Python, but I’d never done it before so didn’t have a sense of the range of
possibilities.&lt;/p&gt;

&lt;p&gt;In situations like this, I find Jupyter notebooks really reveal their strengths.
Experimentation is easy and the pace of iteration can be really high. A few
minutes of searching around and it seemed like
&lt;a href=&quot;https://pillow.readthedocs.io/en/stable/&quot;&gt;Pillow&lt;/a&gt; (aka ‘PIL’) was probably the
best option to go with. I noted that you could edit, resize, copy and paste
images. For my basic version of a synthetic image generator, that’s most of what
I needed to do:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Take an image that we know contains no redactions.&lt;/li&gt;
  &lt;li&gt;Get a separate image file that is of a redaction box / squiggle or shape.&lt;/li&gt;
  &lt;li&gt;Randomly resize the redaction shape.&lt;/li&gt;
  &lt;li&gt;Paste the redaction shape at a random location on top of the base unredacted
image.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And voila! Finding unredacted images was easy since
&lt;a href=&quot;https://mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html&quot;&gt;I had previously used &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fastai&lt;/code&gt;&lt;/a&gt;
to build a model that could detect to ~95% accuracy whether an image contained a
redaction or not. For the redactions, it took me about an hour with
&lt;a href=&quot;https://www.pixelmator.com/pro/&quot;&gt;Pixelmator Pro&lt;/a&gt; and its ‘quick selection’ tool
to extract 100 examples of various kinds of redaction that I knew were commonly
found in the data set. You can see some of this variety in the illustration that
follows, though note that each individual redaction snippet was its own separate
image for the purposes of my synthetic generation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/synthetic-image-data/redaction-snippets.jpg&quot; alt=&quot;&quot; title=&quot;These are the kinds of redactions
(superimposed for the purposes of showing the variety) I selected.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I found that it was pretty trivial to generate images of the kind I proposed
above. The placement of the redactions didn’t always make sense, and sometimes
the random resize that the redaction underwent meant that it was either far too
small or far too large. I also hadn’t included any steps to capture the
annotation in this prototype, but I knew it was possible so continued onwards.&lt;/p&gt;

&lt;h2 id=&quot;detour-get-stuck-pretty-quickly-experience-bbox-sprawl&quot;&gt;Detour: Get Stuck Pretty Quickly, Experience bbox Sprawl&lt;/h2&gt;

&lt;p&gt;Buoyed by my success in the prototype stage, I immediately added a bunch of
improvements and features to what I wanted to achieve. I knew I wanted to make
sure that the redaction stayed within the boundaries of the original base image.
I also wanted to ensure that it stayed within the boundaries of the content of
the base image — i.e. redactions generally tend to be made on top of content
which tends not to be right on the outer margins.&lt;/p&gt;

&lt;p&gt;I rushed into things too fast without thinking the problem through and quite
quickly got into deep waters as all the various pieces started to overlap. I was
somehow still in notebook mode, passing various objects through various other
custom libraries, not sure what I was passing where. In short: it was a mess.&lt;/p&gt;

&lt;p&gt;One thing that tripped me up really fast was bboxes. (A bbox, in case this means
nothing to you, is a data structure or type that allows you to represent where a
box is positioned if you were to paste it on top of a base image (for example).
It seems that there are different conventions about how to represent this
concept of the location of a box on top of some other larger space. Some people
represented it with pairs of coordinates, such that for each of the four corners
of the box you’d have an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[x, y]&lt;/code&gt; pair to represent each point. Others took this
bbox type to contain references to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xmin&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ymin&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;xmax&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ymax&lt;/code&gt; values
of the box. In this way you could reconstruct the various corners since you had
two opposite corners specified. Another option was that used by COCO, which was
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[xmin, ymin, width, height]&lt;/code&gt;. And yet another option was to represent a
bounding box by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[x_center, y_center, width, height]&lt;/code&gt;. (This is
&lt;a href=&quot;https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/&quot;&gt;a useful article&lt;/a&gt;
that details some of these representation differences.)&lt;/p&gt;

&lt;p&gt;I’m sure there are people who are really good at keeping multiple types of x and
y coordinates, each with slightly different nuances, in their heads. I am not
such a person and after an hour or two of struggling in these deep waters I
realised I needed to regroup.&lt;/p&gt;

&lt;p&gt;My notebook experiments had been good for uncovering the range of possibility,
but now that I had a better sense of the edges of the problem — and the twists
and turns of dealing with bounding boxes — I had to take a more systematic
approach. I spent some time with pen and paper thinking through the flow that
this synthetic generation process would have to include. I thought through what
the various independent parts of this could be, and how data would flow through
this set of steps.&lt;/p&gt;

&lt;h2 id=&quot;phase-2-generate-my-own-base-images&quot;&gt;Phase 2: Generate My Own Base Images&lt;/h2&gt;

&lt;p&gt;The first part of this process was to generate my own base images. In general,
the types of base unredacted images in the core data set were relatively
unremarkable. These were mostly letters, reports or some kind of form / table. I
figured I could approximate this pretty quickly. By chance, that very weekend I
happened to listen to
&lt;a href=&quot;https://realpython.com/podcasts/rpp/84/&quot;&gt;an episode of the Real Python podcast&lt;/a&gt;
which interviewed the creator of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;borb&lt;/code&gt;, a Python package for creating and
manipulating PDFs. I knew I wanted images in the end, but I had
&lt;a href=&quot;https://mlops.systems/python/jupyter/fastai/tools/2022/01/06/nbdev-early-impressions.html&quot;&gt;already created a tool to extract images from PDFs&lt;/a&gt;
and I figured &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;borb&lt;/code&gt; would probably save me time, even if it meant I had to do
some converting back and forth between images and PDF files.&lt;/p&gt;

&lt;p&gt;The great thing about &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;borb&lt;/code&gt; is that it offers an easy abstraction with which to
reason about creating PDF documents. Have some text and want it to be displayed
on a page? Done. Want that text to be displayed in three columns? Done. Want do
insert some images and have the text flow round it? Done. Have styling
requirements? Done. And on and on. I figured that this was just the level of
abstraction I needed — rather than staying in the world of pixel primitives like
lines and boxes.&lt;/p&gt;

&lt;p&gt;Once I got going it was easy to generate base images with multi-column text and
some random coloured shapes thrown in here and there. (I used
&lt;a href=&quot;https://lorem-text.readthedocs.io/en/latest/&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lorem-text&lt;/code&gt;&lt;/a&gt; to generate random
Latin texts.) After I created the PDF I then had to convert it into an image
format for use elsewhere in the generator pipeline but I think that speed hit
was a price worth paying.&lt;/p&gt;

&lt;h2 id=&quot;phase-3-generate-my-own-redactions&quot;&gt;Phase 3: Generate My Own Redactions&lt;/h2&gt;

&lt;p&gt;The redactions weren’t quite as easy as the base images. The easiest version of
a redaction box was literally that: a black box that sits on top of the base
image. That much was easy to create.
&lt;a href=&quot;https://pillow.readthedocs.io/en/stable/&quot;&gt;Pillow&lt;/a&gt; had some useful interfaces
that I could use to quickly create randomly sized boxes. I could even add text
to them in the upper left corner as I’d noticed that many of the real redactions
did that.&lt;/p&gt;

&lt;p&gt;It was less clear to me how I’d go about generating the other kinds of
redactions, particularly ones that resembled a handwritten mark in thick black
marker over the top of a document. In the end, I decided not to go any further
with anything that wasn’t a box, but I did make the redaction boxes more varied.
I set it such that the box would be filled with a random colour. If the colour
was dark enough, I made sure that the text was in a light (contrasting) colour.
And ensure that there wasn’t always a text on the box.&lt;/p&gt;

&lt;p&gt;Not perfect, but still it gave me a way to move forward.&lt;/p&gt;

&lt;h2 id=&quot;the-big-picture-bringing-it-all-together&quot;&gt;The Big Picture: Bringing It All Together&lt;/h2&gt;

&lt;p&gt;With these pieces complete, I had the basics of the next version of my synthetic
image generation. You can see the flow and progression of my script in the
following diagram:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/synthetic-image-data/flow-v2.png&quot; alt=&quot;&quot; title=&quot;Synthetic image generation v2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You’ll note that there were a number of other steps that supported the image
creation. I did again descend into bbox hell when calculating exactly where to
paste the redaction image, but with a much more modularised approach to my code
I didn’t get lost.
&lt;a href=&quot;https://mlops.systems/robustpython/python/books-i-read/2022/01/03/robust-python-3.html&quot;&gt;Type hints&lt;/a&gt;
also kept me honest about what variables I was passing in and out of the
functions I’d created.&lt;/p&gt;

&lt;p&gt;I ended up using the initial model I’d trained so far in the step that figured
out where the content of the image was. You’ll recall that this was one of the
annotations I’d already been generating when I annotated my data, and since it’s
a fairly simple computer vision task I was already seeing excellent performance
from that specific class in object detection.
&lt;a href=&quot;https://airctic.com/0.11.0/&quot;&gt;IceVision&lt;/a&gt;, a library that I’m using for the
computer vision and deep learning parts of this project, allowed me to fairly
easily make this inference on the images and extract the bbox coordinates for
the content box.&lt;/p&gt;

&lt;p&gt;I made sure to include a lot of random variation in the first two steps where
the base and redaction images were created. I didn’t remove the original naive
approach completely. Instead, I made it 50% likely that we’d generate an image
versus just picking one of the unredacted images from our store. Then I gave the
same chance for the redaction as to whether we’d use an actual redaction snippet
or one of the computer-generated boxes. There was lots of resizing and colouring
and various other randomisation that was also included.&lt;/p&gt;

&lt;h2 id=&quot;phase-5-make-the-images-look-old-and-worn&quot;&gt;Phase 5: Make The Images Look Old and Worn&lt;/h2&gt;

&lt;p&gt;Only one step remained. I realised that when I generated the images completely
from scratch, not using any of the real base images or redaction snippets, that
they looked very new and unrealistic. A significant proportion of the documents
in the collection looked like they’d been photocopied a thousand times and in
general had seen better days. Sometimes the quality was such to make them
unreadable. I realised if I was going to get good results with the overall goal
(i.e. improve my model’s performance) I’d have to make the synthetic creations
look old somehow.&lt;/p&gt;

&lt;p&gt;After some exploration I settled on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;augraphy&lt;/code&gt; as how I’d process the newly
generated images to look old and worn. Luckily for me, this package seems to
have been created explicitly to support machine learning workflows for synthetic
data creation, and it seemed to be (somewhat) actively maintained. There was a
default set of so-called ‘augmentations’ that Augraphy suggested I apply to my
image. Unfortunately it was simply too aggressive. I guess for some workflows it
would have been great, but the page ended up looking somewhat unintelligible by
the end. Compare these two examples:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/synthetic-image-data/augraphy-transforms.jpg&quot; alt=&quot;&quot; title=&quot;Comparing levels of Augraphy
image augmentation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Not only did the default Augraphy transforms often make the redaction
indistinguishable, it shifted parts of the image around on the page for these
crinkle and scrunch effects, which would have rendered my annotations
inaccurate.&lt;/p&gt;

&lt;p&gt;That said, as you can see from the left image, it was pretty easy to switch out
the default for a set of random transforms to be applied that wasn’t quite so
aggressive. I’m thankful that tools like this exist out in the open-source space
and that allow me to get on with the work of solving the actual problem I’m
interested in working on.&lt;/p&gt;

&lt;h2 id=&quot;final-results-2097-synthetic-images&quot;&gt;Final Results: 2097 Synthetic Images&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/synthetic-image-data/some-synthetic-images.gif&quot; alt=&quot;&quot; title=&quot;Some of the generated images&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This gif gives you a brief sense of some of the images I generated as a result
of the process I’ve detailed above. They’re not perfect, and as I write I
currently don’t know how well they will perform when training my model.&lt;/p&gt;

&lt;p&gt;I have 2097 real annotated images, so I’m going to combine them with a maximum
of an equal number of synthetic images. I’ll try out different proportions of
real to synthetic, but that’s also a topic for another blogpost to follow. Stay
tuned!&lt;/p&gt;

&lt;p&gt;It took about three and a half hours to create these 2000+ images on my laptop.
There are LOTS of places where I could have made speed improvements, notably all
the conversion between PDF and image objects, the inference for the content box
and also the fact that the pipeline wasn’t performed in parallel on all my CPU
cores. I spent about 30 minutes exploring &lt;a href=&quot;https://www.ray.io&quot;&gt;Ray&lt;/a&gt; as a means
to getting this process to be executed in parallel but it ended up being not as
simple as I’d initially thought so I’ve left that to one side for now. In any
case, I won’t be creating so many synthetic images at once so often, so it
wasn’t a real blocking point for my work.&lt;/p&gt;

&lt;p&gt;Note, too, that the annotations get created as part of the same script. I append
them to a synthetic annotations file at the same time as the synthetic images is
generated, and the file is subject to being combined with the real annotations
at a later stage.&lt;/p&gt;

&lt;p&gt;There are obviously lots of ways this synthetic data creation process could be
optimised, but I was recently reminded that it’s also important not to lose
momentum and not to let the perfect be the enemy of the good.&lt;/p&gt;

&lt;p&gt;The next step is to carry out an experiment to see the effect of adding in the
synthetic annotations on model performance. There are a bunch of really tricky
aspects to this (most notably finding ways to make sure not to allow my training
data to leak into the validation data) but I’ll save all that for my next
blogpost.&lt;/p&gt;

&lt;p&gt;(If you got all the way to the end, well done!)&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="redactionmodel" /><category term="computervision" /><category term="python" /><category term="tools" /><summary type="html"></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/synthetic-image-data/synthetic-images-post.jpg" /><media:content medium="image" url="https://mlops.systems/images/synthetic-image-data/synthetic-images-post.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">What are invariants and how can they help make your Python classes more robust?</title><link href="https://mlops.systems/robustpython/python/books-i-read/2022/02/08/robust-python-10.html" rel="alternate" type="text/html" title="What are invariants and how can they help make your Python classes more robust?" /><published>2022-02-08T00:00:00-06:00</published><updated>2022-02-08T00:00:00-06:00</updated><id>https://mlops.systems/robustpython/python/books-i-read/2022/02/08/robust-python-10</id><content type="html" xml:base="https://mlops.systems/robustpython/python/books-i-read/2022/02/08/robust-python-10.html">&lt;p&gt;We’ve read &lt;a href=&quot;https://mlops.systems/robustpython/python/books-i-read/2022/01/30/robust-python-8.html&quot;&gt;about enumerations&lt;/a&gt; and we’ve read &lt;a href=&quot;https://mlops.systems/robustpython/python/books-i-read/2022/02/05/robust-python-9.html&quot;&gt;about data classes&lt;/a&gt;. Now it’s the turn of classes. Chapter 10 of Patrick Viafore’s excellent book, ‘&lt;a href=&quot;https://www.amazon.com/Robust-Python-Patrick-Viafore-ebook-dp-B09982C9FX/dp/B09982C9FX/ref=mt_other?qid=&amp;amp;me=&amp;amp;tag=soumet-20&amp;amp;_encoding=UTF8&quot;&gt;Robust Python&lt;/a&gt;’, is the last of the user-defined types to be covered. Early on he makes a good point that classes are often taught really early to those new to Python and/or programming, and that maybe the story is a bit more complicated. As I’ve mentioned before, things like enums and data classes are more or less unmentioned in such educational materials and as such I found this book really helped me fill in the conceptual gaps.&lt;/p&gt;

&lt;p&gt;First off, for someone who has just learned about data classes, how would you explain what is new or distinct when it comes to classes? They’re slightly different syntactically, with classes requiring you to write a bit more boilerplate. Compare the following:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dataclasses&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataclass&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;datetime&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Literal&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# data class definition
&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataclass&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;breed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CatBreed&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;birth_date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;date&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;gender&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Literal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'male'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'female'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# class definition
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Dog&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;breed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CatBreed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;birth_date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gender&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Literal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'male'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'female'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;breed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;breed&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;birth_date&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;birth_date&lt;/span&gt;
		&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gender&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gender&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can note how it seems like the data class version is much more readable and involves less boilerplate to achieve the same effect, and for a simple example like this you’re probably right. The difference, and where classes make sense and shine, is when you have a conceptual grouping or type that includes some notion of invariants.&lt;/p&gt;

&lt;h2 id=&quot;what-is-an-invariant&quot;&gt;What is an invariant?&lt;/h2&gt;

&lt;p&gt;Most of this chapter is about invariants and how they relate to classes, and I’ll admit I had never heard of the concept before reading in this book. An invariant is defined as “a property of an entity that remains unchanged throughout the lifetime of that entity.” You can think of it as some kind of context or a property about that particular type that you need to encode and that won’t change.&lt;/p&gt;

&lt;p&gt;The book gives a pizza example (where a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pizza&lt;/code&gt; object could encode that in its list of toppings, the cheese could only be the final topping (i.e. on top) of the pizza). An alternative might be some kind of rule relating to an ID number, where either it must be unique to some kind of specification, or where the ID must conform to some kind of specification.&lt;/p&gt;

&lt;p&gt;Even with this rudimentary definition, you can see how there might be some advantages to being able to account for these rules and properties of the object type. (With data classes, you don’t have as much flexibility to specify all these nuances.) So what happens when you’re instantiating a class and you hit one of those scenarios where your contextual rules dictate that something can’t happen? (i.e. someone tries to create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pizza&lt;/code&gt; object that has cheese as the bottom-layer topping) The book offers up two options:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Throw an exception — this will break you out of the code flow and prevent the object from being constructed&lt;/li&gt;
  &lt;li&gt;Do something to make the data fit — you can perform some kind of transformation which sees the cheese ingredient as being forced onto the top layer of the pizza toppings (or whatever is the equivalent for your specific scenario)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note that the kinds of restrictions posed by these invariants are things that can’t fully be captured by the typing system. &lt;a href=&quot;https://mlops.systems/categories/#redactionmodel&quot;&gt;We’ve covered type hints&lt;/a&gt; and how they can help make your code more robust, but types don’t help much when it comes to the order of a list, for example.&lt;/p&gt;

&lt;h2 id=&quot;why-code-around-invariants&quot;&gt;Why code around invariants?&lt;/h2&gt;

&lt;p&gt;So why go to all of this trouble in the first place? How does it benefit to code with the invariants in mind? To start with, it’ll probably help you think through edge cases and exceptions that you could do well to be wary of. The invariants alert you to the fact that arguments passed into functions and methods will not always be in the form that you would ideally like. (As a side note, this might also encourage you to add unit tests.)&lt;/p&gt;

&lt;p&gt;It will help you keep the code that handles the invariants together instead of mixing it in with the code that instantiates the objects. In general, it will enhance your ability to reason about the code and the concepts that your code reflects. This is important not only for the implementation in code, but for how you think about any particular part and how it relates to the rest of your code base.&lt;/p&gt;

&lt;p&gt;The goal for all of this: fewer bugs and a more robust system. Yes, it takes a bit more effort to think whether there are implicit or explicit invariants, but doing so makes your code and your system more reliable. In Viafore’s words:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“You’re making an easier API for people to think about, and you reduce the risk of people using your objects incorrectly. […] You never want someone to be surprised when using your code.” (p. 141)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;invariants-and-class-consumers&quot;&gt;Invariants and class consumers&lt;/h2&gt;

&lt;p&gt;The rest of the chapter is about the implementation consequences of thinking about classes in this invariants-first way. For consumers of the class, how should you ensure that the invariants handled are clear? Aside from the implementation itself (in the constructor), docstrings and code comments are suggested as a means to this end. Of course, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;README&lt;/code&gt; files and documentation in general can serve the same purpose, but it’s best if the context and information about invariants is as close to the code as possible.&lt;/p&gt;

&lt;h2 id=&quot;invariants-and-class-maintainers&quot;&gt;Invariants and class maintainers&lt;/h2&gt;

&lt;p&gt;For (future) maintainers of the class, unit tests are the way to go. Make sure that the relevant scenarios and invariants are covered by testing code and you will have extra confidence that your object instantiation really does do what you intend. Your code should already be doing the checking for invariants on the instantiation side, but unit tests are a way of ensuring that this is actually the case (and also that these invariants remain covered as the code base continues to evolve.&lt;/p&gt;

&lt;p&gt;(The book offers one way of doing such tests for invariants with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;contextlib.contextmanager&lt;/code&gt; on page 145.)&lt;/p&gt;

&lt;h2 id=&quot;encapsulation-and-classes&quot;&gt;Encapsulation and classes&lt;/h2&gt;

&lt;p&gt;As the final chunk of the chapter, we learn about private, protected and public access to the properties and methods of a class, and how they relate to the maintenance of invariants.&lt;/p&gt;

&lt;p&gt;This is an important part of the story. As users interface with your class and API, encapsulation is a way to ensure that they update and interact with the these properties in a way that is under your control. For example, even if at instantiation you enforce the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pizza&lt;/code&gt; object having cheese as the top-layer topping, what do we have in place to ensure that the user doesn’t just amend the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;toppings&lt;/code&gt; property such that the cheese is the bottom-layer topping (i.e. AFTER instantiation)? Encapsulation — having an entity hide or restrict access to certain properties and actions — is how you handle that.&lt;/p&gt;

&lt;p&gt;The book goes into a fair amount of detail on the uses of these different levels of access, and introduces the idea of ‘accessors’ and ‘mutators’ as an alternative to the more commonly-used ‘getters’ and ‘setters’.&lt;/p&gt;

&lt;p&gt;Remember, “you use invariants to allow users to reason about your objects and reduce cognitive load.” (p. 151)&lt;/p&gt;

&lt;h2 id=&quot;so-what-am-i-supposed-to-use&quot;&gt;So what am I supposed to use?&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/robust-python-10/which-abstraction.png&quot; alt=&quot;&quot; title=&quot;A super helpful diagram from the book helping you choose which abstraction to pick.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The end of the chapter offers this really helpful flowchart diagram which summarises the choices that we’ve covered during the previous three chapters. I really want to highlight that this chapter helped me think about classes in a way I hadn’t, despite having been through courses, having read numerous articles and of course coded in this class-oriented fashion for several years.&lt;/p&gt;

&lt;p&gt;The next few chapters continue onwards by thinking about how to design your interfaces such that they make sense for your users and allow your code base to grow with as few headaches as possible.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="robustpython" /><category term="python" /><category term="books-i-read" /><summary type="html">We’ve read about enumerations and we’ve read about data classes. Now it’s the turn of classes. Chapter 10 of Patrick Viafore’s excellent book, ‘Robust Python’, is the last of the user-defined types to be covered. Early on he makes a good point that classes are often taught really early to those new to Python and/or programming, and that maybe the story is a bit more complicated. As I’ve mentioned before, things like enums and data classes are more or less unmentioned in such educational materials and as such I found this book really helped me fill in the conceptual gaps.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/robust-python/robust-python-cover.jpeg" /><media:content medium="image" url="https://mlops.systems/images/robust-python/robust-python-cover.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Upgrade your Python dicts with data classes</title><link href="https://mlops.systems/robustpython/python/books-i-read/2022/02/05/robust-python-9.html" rel="alternate" type="text/html" title="Upgrade your Python dicts with data classes" /><published>2022-02-05T00:00:00-06:00</published><updated>2022-02-05T00:00:00-06:00</updated><id>https://mlops.systems/robustpython/python/books-i-read/2022/02/05/robust-python-9</id><content type="html" xml:base="https://mlops.systems/robustpython/python/books-i-read/2022/02/05/robust-python-9.html">&lt;p&gt;I’ve been curious about data classes since more or less my first day at work when someone mentioned to me that Pydantic was built on the shoulders of data classes. I hadn’t taken the opportunity to dive into all the details of what data classes do until now, prompted by their being part of Patrick Viafore’s book, ‘&lt;a href=&quot;https://www.amazon.com/Robust-Python-Patrick-Viafore-ebook-dp-B09982C9FX/dp/B09982C9FX/ref=mt_other?_encoding=UTF8&amp;amp;me=&amp;amp;tag=soumet-20&amp;amp;qid=&quot;&gt;Robust Python&lt;/a&gt;’, specifically chapter nine.&lt;/p&gt;

&lt;p&gt;An example upfront might help ground the conversation. Here is a data class in action:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;datetime&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dataclasses&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataclass&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Literal&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataclass&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;CatPassport&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;breed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CatBreed&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;issue_date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;date&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;expiry_date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;date&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;gender&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Literal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'male'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'female'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;aria&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CatPassport&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Aria&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CatBreed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'bengal'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2022&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2025&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;04&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'female'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aria&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# prints 'Aria'
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From this you can see that it’s an easy way to represent structured data made up of different types. Where it excels over simply using a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dict&lt;/code&gt; or a class you write yourself is the fact that it auto-generates a number of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__&lt;/code&gt; dunder helper methods. You get &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__str__&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__repr__&lt;/code&gt; to handle what this object looks like when you try to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;print()&lt;/code&gt; it. It also creates an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__eq__&lt;/code&gt; method which allows you to check for equality between two objects of the same type with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;==&lt;/code&gt; comparison operator.&lt;/p&gt;

&lt;p&gt;(If you want to add a way to compare between your data class objects, you can add arguments to the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@dataclass&lt;/code&gt; decorator like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@dataclass(eq=True, order=True)&lt;/code&gt; which will handle the creation of the relevant dunder methods.&lt;/p&gt;

&lt;p&gt;The fact that data classes are just classes at heart mean that you can also add behaviours to these collections of values, something that isn’t possible with a plain &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dict&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;You can specify that your data class should be frozen (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@dataclass(frozen=True)&lt;/code&gt;) which effectively makes it an immutable data store, though taking note that objects stored as values on the data class’ properties might themselves not be immutable (think lists and dicts).&lt;/p&gt;

&lt;p&gt;After reading the chapter in ‘Robust Python’, I read around a little to get a sense of this concept. I read &lt;a href=&quot;https://docs.python.org/3/library/dataclasses.html&quot;&gt;the official docs&lt;/a&gt; which were fairly helpful, but in fact it was &lt;a href=&quot;https://www.python.org/dev/peps/pep-0557/&quot;&gt;the PEP document (557)&lt;/a&gt; that I found most interesting. I haven’t previously taken the time to dive into the specifics of PEP specifications before, but I discovered that they are pretty readable and you get a real sense of what problem a particular feature or addition to the language was trying to solve.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.python.org/dev/peps/pep-0557/&quot;&gt;PEP 557&lt;/a&gt; explains some of the alternatives and why it might be useful to include this new feature. I also learned about the &lt;a href=&quot;https://github.com/python-attrs/attrs&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attrs&lt;/code&gt;&lt;/a&gt; package and how data classes are actually just a subset of what &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attrs&lt;/code&gt; offers. (As a side note, I was surprised that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;attrs&lt;/code&gt; seems to have been mentioned nowhere in ‘Robust Python’, even in the context of the upcoming Pydantic chapter. Perhaps it was just too confusing to have all these things alongside one another.)&lt;/p&gt;

&lt;p&gt;Other options to consider alongside data classes when dealing with heterogenous data inside a single object or structure include &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TypedDict&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;namedtuple&lt;/code&gt;, but it seems like the default for this kind of scenario should probably just be a data class, though I should add that it is only part of the standard library for Python 3.7 and above.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="robustpython" /><category term="python" /><category term="books-i-read" /><summary type="html">I’ve been curious about data classes since more or less my first day at work when someone mentioned to me that Pydantic was built on the shoulders of data classes. I hadn’t taken the opportunity to dive into all the details of what data classes do until now, prompted by their being part of Patrick Viafore’s book, ‘Robust Python’, specifically chapter nine.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/robust-python/robust-python-cover.jpeg" /><media:content medium="image" url="https://mlops.systems/images/robust-python/robust-python-cover.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">How and where to use enums in Python</title><link href="https://mlops.systems/robustpython/python/books-i-read/2022/01/30/robust-python-8.html" rel="alternate" type="text/html" title="How and where to use enums in Python" /><published>2022-01-30T00:00:00-06:00</published><updated>2022-01-30T00:00:00-06:00</updated><id>https://mlops.systems/robustpython/python/books-i-read/2022/01/30/robust-python-8</id><content type="html" xml:base="https://mlops.systems/robustpython/python/books-i-read/2022/01/30/robust-python-8.html">&lt;p&gt;The second part of Viafore’s &lt;a href=&quot;https://www.amazon.com/Robust-Python-Patrick-Viafore-ebook-dp-B09982C9FX/dp/B09982C9FX/ref=mt_other?qid=&amp;amp;me=&amp;amp;tag=soumet-20&amp;amp;_encoding=UTF8&quot;&gt;‘Robust Python’&lt;/a&gt; is all about user-created types. We start simple in chapter eight and consider the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Enum&lt;/code&gt; type as a way of defining a particular restricted set of values. An example might help get us started:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;enum&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Enum&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TrafficLightsState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Enum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;RED&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;red&quot;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;YELLOW&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;yellow&quot;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;GREEN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;green&quot;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;OFF&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;off&quot;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;current_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TrafficLightsState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GREEN&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# prints 'green'
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We subclass off &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Enum&lt;/code&gt; and define the pairings of values that belong together. I hope you can see already that this is a readable way to define these values and show that they are part of the same semantic grouping.&lt;/p&gt;

&lt;p&gt;If we’re using these definitions not because we care about the values themselves but because we want to be able to evaluate whether the state of one particular traffic light is the same as a different traffic light, we can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto&lt;/code&gt; to automatically assign values (ascending integers, by default) in the following way:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;enum&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Enum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TrafficLightsState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Enum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;RED&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;YELLOW&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;GREEN&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;OFF&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;current_state&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TrafficLightsState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GREEN&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# prints 3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can iterate through your enums or get their length just as if it was a list, too.&lt;/p&gt;

&lt;p&gt;While writing the above text, I realised that I was getting confused about the difference between types and classes in Python. It turns out that whatever differences once existed, they aren’t much of a thing any more and &lt;a href=&quot;https://stackoverflow.com/questions/4162578/python-terminology-class-vs-type&quot;&gt;to all intents and purposes&lt;/a&gt; they’re practically the same thing.&lt;/p&gt;

&lt;p&gt;A lot of the enum-related definitions at work are defined in &lt;a href=&quot;https://github.com/zenml-io/zenml/blob/0.6.0/src/zenml/enums.py&quot;&gt;this file&lt;/a&gt;. You can see that we tend not to use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;auto&lt;/code&gt;, though I’m not really sure why. (We don’t ever seem to compare against actual values.)&lt;/p&gt;

&lt;p&gt;If you want to make sure that the actual values assigned to these grouped constants are unique, you can add the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;@unique&lt;/code&gt; decorator which will enforce that you aren’t duplicating values.&lt;/p&gt;

&lt;p&gt;Better still for the readability of your code, you can use this collective type in your type annotations. For sure the difference between these two options should be clear:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_status&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;some_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;# code goes here
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_status&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;some_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TrafficLightsState&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;# code goes here
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the first case, it is far less clear what’s going on.&lt;/p&gt;

&lt;p&gt;Note that if you’re purely looking for a way to restrict the assignation to a particular variable, you can also use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Literal&lt;/code&gt; type, introduced in Python 3.8, though remember that it doesn’t help with iteration, runtime checking or map values from name to value. For all that, you’ll want to be using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Enum&lt;/code&gt;.”&lt;/p&gt;

&lt;p&gt;If you want a way to combine Enums together, you can subclass from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;enum.Flag&lt;/code&gt;. Consider the case of when you have a list of enums for days of the week, but you want to represent the weekend as a pairing of Saturday and Sunday (if you were in Europe, e.g.). You could do the following:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;enum&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Weekday&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Flag&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;MONDAY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;TUESDAY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;WEDNESDAY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;THURSDAY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;FRIDAY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;SATURDAY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;SUNDAY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auto&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
	
&lt;span class=&quot;n&quot;&gt;weekend&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Weekday&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SATURDAY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Weekday&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SUNDAY&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can perform bitwise operations on these combined groupings, but note that the values must support bitwise operations. (Strings don’t support them, while integers do.)&lt;/p&gt;

&lt;p&gt;Finally, the chapter covers the special case of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IntEnum&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IntFlag&lt;/code&gt; which allows for the conversion of integer values. This can be confusing and lead to non-robust behaviours, so the book discourages this particular usage.&lt;/p&gt;

&lt;p&gt;Next up is Data Classes, something I’m extremely interested in getting to grips with as it comes up in our codebase at work a decent amount.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="robustpython" /><category term="python" /><category term="books-i-read" /><summary type="html">The second part of Viafore’s ‘Robust Python’ is all about user-created types. We start simple in chapter eight and consider the Enum type as a way of defining a particular restricted set of values. An example might help get us started:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/robust-python/robust-python-cover.jpeg" /><media:content medium="image" url="https://mlops.systems/images/robust-python/robust-python-cover.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Using mypy for Python type checking</title><link href="https://mlops.systems/robustpython/python/books-i-read/2022/01/22/robust-python-6.html" rel="alternate" type="text/html" title="Using mypy for Python type checking" /><published>2022-01-22T00:00:00-06:00</published><updated>2022-01-22T00:00:00-06:00</updated><id>https://mlops.systems/robustpython/python/books-i-read/2022/01/22/robust-python-6</id><content type="html" xml:base="https://mlops.systems/robustpython/python/books-i-read/2022/01/22/robust-python-6.html">&lt;p&gt;The final two chapters of part one of Patrick Viafore’s ‘&lt;a href=&quot;https://www.amazon.com/Robust-Python-Patrick-Viafore-ebook-dp-B09982C9FX/dp/B09982C9FX/ref=mt_other?qid=&amp;amp;me=&amp;amp;tag=soumet-20&amp;amp;_encoding=UTF8&quot;&gt;Robust Python&lt;/a&gt;’ cover more practical advice on how to actually use and implement type checking in either a new project or a legacy codebase.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.mypy-lang.org&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mypy&lt;/code&gt;&lt;/a&gt; is the most commonly used option for type checking in Python and it does most of what you probably need it for. You can run it via the command line, inline as part of your IDE, or as part of a CI/CD pipeline. At work we do all three.&lt;/p&gt;

&lt;p&gt;You can configure &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mypy&lt;/code&gt; to your heart’s desire either with inline comments in your code, or via a configuration file. A configuration file is probably the way to go, particularly if you’re versioning your code and sharing these kinds of settings across a team.&lt;/p&gt;

&lt;p&gt;Chapter 6 goes into detail about some of the specific options or settings you can tweak to make &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mypy&lt;/code&gt; more or less sensitive to certain kinds of errors. For example, in &lt;a href=&quot;https://mlops.systems/robustpython/python/books-i-read/2022/01/08/robust-python-4.html&quot;&gt;a previous post&lt;/a&gt; we mentioned how you can implicitly accept &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;None&lt;/code&gt; as a type with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Optional&lt;/code&gt; type annotation wrapper. But maybe you don’t want to allow this behaviour because it’s generally not a great idea: if so, you can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;—strict-optional&lt;/code&gt; flag to get notified whenever you’re using that particular construction.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mypy&lt;/code&gt; also allows for the export of its results to html and xml, and you can run it in the background as a daemon which (particularly for large code bases) might speed it up.&lt;/p&gt;

&lt;p&gt;We also learn about some alternatives to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mypy&lt;/code&gt;, namely Pyre and Pyright. &lt;a href=&quot;https://pyre-check.org&quot;&gt;Pyre&lt;/a&gt; runs as a daemon in the background and allows you to run queries relating to type usage in your codebase. It also includes a static code analyser called &lt;a href=&quot;https://pyre-check.org/docs/pysa-basics/&quot;&gt;Pysa&lt;/a&gt; that runs a kind of security analysis on your code called ‘taint analysis’. A quick summary of this would be to say that you can specify specific kinds of security flaws that you want to address and/or prevent being part of your codebase.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/microsoft/pyright&quot;&gt;Pyright&lt;/a&gt; is interesting since it has a useful VS Code integration (via the &lt;a href=&quot;https://marketplace.visualstudio.com/items?itemName=ms-python.vscode-pylance&quot;&gt;Pylance extension&lt;/a&gt;). You get all sorts of autocompletion and tooltip goodness by using Pyright/Pylance.&lt;/p&gt;

&lt;p&gt;Finally, chapter 7 thinks through how you might want to approach actually using type checking and type hints in a larger codebase, perhaps one that already exists. It’s useful this was included as I imagine these sorts of practicalities are much more of a blocker to adoption than any technical issues. After a brief discussion of tradeoffs, we learn about some different options for where you might want to start with introducing types to a legacy codebase.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Focusing on the pain points — i.e. where the lack of type hints has already seen bugs emerge in the past&lt;/li&gt;
  &lt;li&gt;or perhaps adding them to new code only&lt;/li&gt;
  &lt;li&gt;or perhaps type annotating the pieces of the codebase that actually drive the product or business’ profits&lt;/li&gt;
  &lt;li&gt;or maybe whatever is complex to understand&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;All of these are options and it will definitely depend on your particular situation.&lt;/p&gt;

&lt;p&gt;We also learn about two tools that might help get you started with type annotation: &lt;a href=&quot;https://github.com/instagram/MonkeyType&quot;&gt;MonkeyType&lt;/a&gt; and &lt;a href=&quot;https://google.github.io/pytype/&quot;&gt;Pytype&lt;/a&gt;. Both auto-generate type hints for your codebase. MonkeyType does so dynamically, so it only generates type hints for parts of your code that it accesses while running the code. Pytype does so by static analysis. Both deliver some kind of output that you can then use (perhaps) as the basis of some annotations of your codebase. My instinct is that these two tools feel like they might lead to some faulty assumptions or errors if you rely on them too much and that in fact it would be better to just methodically go through your code and incrementally add type hints as suggested above.&lt;/p&gt;

&lt;p&gt;This concludes the type hints part of the book. I feel like I really got a solid overview of why type hints are used in large or complex Python codebases as well as how to implement this practically. I will be writing separately about how we use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mypy&lt;/code&gt; and type hinting at ZenML as I think it offers an interesting case study on some of the benefits and tradeoffs that we’ve observed on a day-to-day basis.&lt;/p&gt;

&lt;p&gt;Next up in Robust Python: defining your own types with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Enums&lt;/code&gt;, data classes, classes and how this fits into libraries like Pydantic.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="robustpython" /><category term="python" /><category term="books-i-read" /><summary type="html">The final two chapters of part one of Patrick Viafore’s ‘Robust Python’ cover more practical advice on how to actually use and implement type checking in either a new project or a legacy codebase.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/robust-python/robust-python-cover.jpeg" /><media:content medium="image" url="https://mlops.systems/images/robust-python/robust-python-cover.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Using type annotation with collections in Python</title><link href="https://mlops.systems/robustpython/python/books-i-read/2022/01/18/robust-python-5.html" rel="alternate" type="text/html" title="Using type annotation with collections in Python" /><published>2022-01-18T00:00:00-06:00</published><updated>2022-01-18T00:00:00-06:00</updated><id>https://mlops.systems/robustpython/python/books-i-read/2022/01/18/robust-python-5</id><content type="html" xml:base="https://mlops.systems/robustpython/python/books-i-read/2022/01/18/robust-python-5.html">&lt;p&gt;The fifth chapter of
&lt;a href=&quot;https://www.amazon.com/Robust-Python-Patrick-Viafore-ebook-dp-B09982C9FX/dp/B09982C9FX/ref=mt_other?_encoding=UTF8&amp;amp;me=&amp;amp;tag=soumet-20&amp;amp;qid=&quot;&gt;‘Robust Python’&lt;/a&gt;
continues on from
&lt;a href=&quot;https://mlops.systems/robustpython/python/books-i-read/2022/01/08/robust-python-4.html&quot;&gt;where we left off&lt;/a&gt;
last time. We saw how to apply type annotations when simple things like strings,
integers and floats were involved. This chapter deals with the different ways
you annotate your types when collections get involved.&lt;/p&gt;

&lt;p&gt;We start with the context for why this is even something that requires a
separate chapter to deal with. This involves the difference between homogenous
and heterogeneous types. For a Python list, we could say it had homogenous types
if all the items were of the same type (strings, e.g.). If this list contains
multiple different types (a mix of strings and integers, e.g.) then we’d have to
say it contained heterogenous types. This is of importance given that the
presence of multiple types in a single list is going to require you to handle
the types differently. Even in the most trivial of examples (as with strings and
integers being together), the interfaces for both are different. Try adding a
string to an integer in Python and see what happens.&lt;/p&gt;

&lt;p&gt;So it’s actually not quite true to say that a collection of homogenous types
have to all be exactly the same type, but they must share common interfaces and
ideally be handled using the same logic. If you think about it, in the real
world heterogenous types are pretty common occurrences. There are often
situations where, for example, you have to handle the output of API calls or
data that doesn’t derive from code that’s in yous control and then you’ll
perhaps be dealing with a dictionary that contains all sorts of types.&lt;/p&gt;

&lt;p&gt;In Python we do have the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;typing.Any&lt;/code&gt; annotation, but it’s pretty clear — and
the book emphasises this — that isn’t really useful in the vast majority of
cases. You might as well not bother with type annotations if you’re going to
liberally be using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Any&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-first-of-our-collection-type-helpers-typeddict&quot;&gt;The first of our collection type helpers: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TypedDict&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TypedDict&lt;/code&gt; was introduced in Python 3.8 and allows you to communicate intent
when it comes to the types that are being passed through your code. Note that,
as with a lot of what we’re talking about here, this is all information that’s
useful for a type checker and isn’t something that is dynamically checked.&lt;/p&gt;

&lt;p&gt;You can use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TypedDict&lt;/code&gt; to define structures that specify the types of fields of
your dictionary in a way that is easier to parse as a human reader than just
using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dict&lt;/code&gt;. See this example, adapted from one in the book:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TypedDict&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TypedDict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TypedDict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;unit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;confidenceRange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Range&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;our_stats&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;unit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;some_name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confidenceRange&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;5.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;our_stats&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# returns {'value': 3, 'unit': 'some_name', 'confidenceRange': {'min': 1.3, 'max': 5.5}}
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TypedDict&lt;/code&gt; doesn’t do everything you need it to, we have some other options.&lt;/p&gt;

&lt;h2 id=&quot;custom-collections-with-typevar&quot;&gt;Custom Collections with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TypeVar&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TypeVar&lt;/code&gt; in Python is how you can implement generics. Generics, as I learned
while reading, are ways of representing things that are the same, like when you
don’t care what specific type is being used. Take this example from the book,
where you want to reverse items in a list, but only if the items are all of the
same type. You could write the following:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TypeVar&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TypeVar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'T'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;reverse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;coll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[::&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can use generics in other ways to create new kinds of collections or
groupings. For example, again this one is adapted from the book, if you were
writing a series of methods that returned either something useful or a
particular error message:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_weather_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;location&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Union&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WeatherData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;APIError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;# …
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_financial_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transaction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Union&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FinancialData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;APIError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;# …
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;…and so on, you could use generics as a way of simplifying how this gets
presented:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TypeVar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'T'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;APIResponse&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Union&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;APIError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_weather_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;location&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;APIResponse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WeatherData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;# …
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_financial_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transaction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;APIResponse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FinancialData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;# …
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That looks and feels so much cleaner!&lt;/p&gt;

&lt;h2 id=&quot;tweaking-existing-functionality-with-collections&quot;&gt;Tweaking existing functionality with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;collections&lt;/code&gt;&lt;/h2&gt;

&lt;p&gt;If you’re just making slight changes to the behaviour of collections, instead of
subclassing dictionaries or lists or whatever, it’s better to override the
methods of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;collections.UserDict&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;collections.UserString&lt;/code&gt; and/or
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;collections.UserList&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;You’ll run into fewer problems when you actually implement this. Of course,
there is a slight performance cost to importing these collections, so it’s worth
making sure this cost isn’t too high.&lt;/p&gt;

&lt;p&gt;You’ll maybe have noticed that there isn’t a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;collections.UserSet&lt;/code&gt; in the list
above. For sets we’ll have to use abstract base classes which are found in
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;collections.abc&lt;/code&gt;. The big difference between the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;User*&lt;/code&gt; pattern of classes,
there is no built-in storage for the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;abc&lt;/code&gt; classes. You have to provide your own
storage if you need it. So for sets, we’d use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;collections.abc.Set&lt;/code&gt; and then
implement whatever group of methods are required for that particular class.&lt;/p&gt;

&lt;p&gt;In the set example, we have to implement &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__contains__&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__iter__&lt;/code&gt; and
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__len__&lt;/code&gt;, and then the other set operations will automatically work. There are
currently (as of Python 3.10.2)
&lt;a href=&quot;https://docs.python.org/3/library/collections.abc.html#collections-abstract-base-classes&quot;&gt;25 different ABCs&lt;/a&gt;
available to use. I definitely will be exploring those as they seem really
useful.&lt;/p&gt;

&lt;p&gt;Even though this chapter got into the weeds of collections a little, I learned a
&lt;em&gt;lot&lt;/em&gt; and I’m already finding places in the ZenML codebase where all of this is
being used.&lt;/p&gt;

&lt;h2 id=&quot;typeguard&quot;&gt;Typeguard&lt;/h2&gt;

&lt;p&gt;Before I leave, since we’re still thinking about types, I wanted to share this
little package I discovered the other day:
&lt;a href=&quot;https://typeguard.readthedocs.io/en/latest/index.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;typeguard&lt;/code&gt;&lt;/a&gt;. You can
use it in a bunch of different ways, but a
&lt;a href=&quot;https://calmcode.io/shorts/typeguard.py.html&quot;&gt;useful short video from calmcode.io&lt;/a&gt;
showed how a simple decorator can simplify code and catch type errors.&lt;/p&gt;

&lt;p&gt;Consider the following example code:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;calculate_risk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;risk_factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Calculates how much risk you took&quot;&quot;&quot;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;risk_factor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# arbitrary return value :)
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What if someone passes in a wrong type into this function? It’ll fail. So maybe
we want to handle that particular situation:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;calculate_risk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;risk_factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Calculates how much risk you took&quot;&quot;&quot;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;risk_factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;raise&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Wrong type for risk_factor&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;risk_factor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you have lots of parameters in your function and you have to handle them all,
this could get messy quite quickly. Instead, we can &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install typeguard&lt;/code&gt; and
do the following:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;guard&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typechecked&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;typechecked&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;calculate_risk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;risk_factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;Calculates how much risk you took&quot;&quot;&quot;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;risk_factor&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now that’s a handy little decorator! It’ll handle all the raising of appropriate
errors above based on whether you passed in the right type or not. It works for
classes as well. You’re welcome, and thanks
&lt;a href=&quot;https://twitter.com/fishnets88&quot;&gt;Vincent&lt;/a&gt; for making the introductory video!&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="robustpython" /><category term="python" /><category term="books-i-read" /><summary type="html">The fifth chapter of ‘Robust Python’ continues on from where we left off last time. We saw how to apply type annotations when simple things like strings, integers and floats were involved. This chapter deals with the different ways you annotate your types when collections get involved.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/robust-python/robust-python-cover.jpeg" /><media:content medium="image" url="https://mlops.systems/images/robust-python/robust-python-cover.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A Midway Report on my Computer Vision Project</title><link href="https://mlops.systems/python/fastai/tools/redactionmodel/2022/01/16/midway-report-redaction-project.html" rel="alternate" type="text/html" title="A Midway Report on my Computer Vision Project" /><published>2022-01-16T00:00:00-06:00</published><updated>2022-01-16T00:00:00-06:00</updated><id>https://mlops.systems/python/fastai/tools/redactionmodel/2022/01/16/midway-report-redaction-project</id><content type="html" xml:base="https://mlops.systems/python/fastai/tools/redactionmodel/2022/01/16/midway-report-redaction-project.html">&lt;p&gt;(&lt;em&gt;This post is adapted from
&lt;a href=&quot;https://twitter.com/strickvl/status/1482645800656281604&quot;&gt;a twitter thread&lt;/a&gt;, so
is a bit more terse than usual.&lt;/em&gt;)&lt;/p&gt;

&lt;p&gt;I recently switched what I spend the majority of my professional life doing
(history -&amp;gt; software engineering). I’m currently working as an ML Engineer at
&lt;a href=&quot;https://github.com/zenml-io/zenml&quot;&gt;ZenML&lt;/a&gt; and really enjoying this new world of
MLOps, filled as it is with challenges and opportunities.&lt;/p&gt;

&lt;p&gt;I wanted to get some context for the wider work of a data scientist to help me
appreciate the problem we are trying to address at
&lt;a href=&quot;https://github.com/zenml-io/zenml&quot;&gt;ZenML&lt;/a&gt;, so looked around for a juicy machine
learning problem to work on as a longer project.&lt;/p&gt;

&lt;p&gt;I was also encouraged by
&lt;a href=&quot;https://www.alexstrick.com/blog/fastai-lesson-zero&quot;&gt;Jeremy Howard’s advice&lt;/a&gt; to
“build one project and make it great”. This approach seems like it has really
paid off for those who’ve studied &lt;a href=&quot;https://course.fast.ai&quot;&gt;the fastai course&lt;/a&gt;
and I wanted to really go deep on something myself.&lt;/p&gt;

&lt;p&gt;Following some previous success working with other mentors from
&lt;a href=&quot;https://www.sharpestminds.com&quot;&gt;SharpestMinds&lt;/a&gt; on a previous project, I settled
on Computer Vision and was lucky to find Farid AKA
&lt;a href=&quot;https://twitter.com/ai_fast_track&quot;&gt;@ai_fast_track&lt;/a&gt; to mentor me through the
work.&lt;/p&gt;

&lt;p&gt;In the last 6 weeks, I’ve made what feels like good progress on the problem.
This image offers an overview of the pieces I’ve been working on, to the point
where the ‘solution’ to my original problem feels on the verge of being
practically within reach.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/midway-report-redaction-project/midway-presentation.jpeg&quot; alt=&quot;&quot; title=&quot;Some of the things
I worked on over the past six weeks.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After just a few lessons of the FastAI course, I
&lt;a href=&quot;https://mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html&quot;&gt;trained a classification model&lt;/a&gt;
to ~95% accuracy to help me sort redacted images from unredacted images.&lt;/p&gt;

&lt;p&gt;I used Explosion’s Prodigy to
&lt;a href=&quot;https://mlops.systems/redactionmodel/computervision/datalabelling/2021/11/29/prodigy-object-detection-training.html&quot;&gt;annotate an initial round&lt;/a&gt;
of data to pass into the next step, enjoying how the labelling process brought
me into greater contact with the dataset along the way.&lt;/p&gt;

&lt;p&gt;I switched to using &lt;a href=&quot;https://airctic.com/&quot;&gt;IceVision&lt;/a&gt; to help me with the more
complicated object detection problem, using MMDetection and VFNet
&lt;a href=&quot;https://mlops.systems/redactionmodel/computervision/progressreport/2021/12/11/redaction-progress-week-one.html&quot;&gt;to get pretty good results&lt;/a&gt;
early on.&lt;/p&gt;

&lt;p&gt;I’m currently in the process of creating my own synthetic images to boost the
annotations I’ve manually made. (I’ll be writing about this process soon as
well, as I’m learning a lot about why this is so important for these kinds of
computer vision problems.)&lt;/p&gt;

&lt;p&gt;I’ve also been amazed at the effectiveness of self-training (i.e. using my
initial model in my annotation loop to generate an initial set of annotations
which I can easily amend as appropriate, then feeding those annotations in to
create a better model and so on). More to follow on that step, too.&lt;/p&gt;

&lt;p&gt;I started using &lt;a href=&quot;https://evidentlyai.com&quot;&gt;Evidently&lt;/a&gt; to do some drift detection,
inspired by some work I was doing for ZenML on adding Evidently as an
integration to our own tool. This helped me think about how new data was
affecting the model and the training cycle. I feel like there’s a lot of depth
here to understand, and am looking forward to diving in.&lt;/p&gt;

&lt;p&gt;I made a tiny little demo on &lt;a href=&quot;https://huggingface.co/spaces&quot;&gt;HuggingFace Spaces&lt;/a&gt;
to show off the current inference capabilities and to see the model in a setting
that feels close to reality. This is a simple little
&lt;a href=&quot;https://www.gradio.app&quot;&gt;Gradio&lt;/a&gt; app but I liked how easy this was to put
together (a couple of hours, mainly involving some build issues and a dodgy
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requirements.txt&lt;/code&gt; file)&lt;/p&gt;

&lt;p&gt;Along the way, I found it sometimes quite painful or fiddly to handle the PDF
files that are the main data source for the project, so I built
&lt;a href=&quot;https://pypi.org/project/pdfsplitter/&quot;&gt;my own Python package&lt;/a&gt; to handle the
hard work. I used fastai’s nbdev to
&lt;a href=&quot;https://mlops.systems/python/jupyter/fastai/tools/2022/01/06/nbdev-early-impressions.html&quot;&gt;very quickly get the starters&lt;/a&gt;
of what I’m hoping might be a useful tool for others using PDF data for ML
projects.&lt;/p&gt;

&lt;p&gt;Throughout all this, &lt;a href=&quot;https://www.linkedin.com/in/farid-hassainia-ca/&quot;&gt;Farid&lt;/a&gt;
has been patiently helping guide me forward. He saved me from going down some
dark rabbit holes, from spending too long studying skills and parts of the
problem that needed relatively little mastery in order to get to where I am.&lt;/p&gt;

&lt;p&gt;Farid has been a consistently enthusiastic and kind advocate for my work,
moreover, and this has really helped me stay the course for this project that
takes a decent chunk of my time (especially seeing as I do it completely aside /
separately from my day job).&lt;/p&gt;

&lt;p&gt;I feel like I’m consistently making progress and learning the skills of a data
scientist working in computer vision, even though I have so much left to learn!
My project still has a ways to go before it’s ‘done’, but I’m confident that
I’ll get there with Farid’s support. (Thank you!)&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="python" /><category term="fastai" /><category term="tools" /><category term="redactionmodel" /><summary type="html">(This post is adapted from a twitter thread, so is a bit more terse than usual.)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/midway-report-redaction-project/midway-thumb.png" /><media:content medium="image" url="https://mlops.systems/images/midway-report-redaction-project/midway-thumb.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Different ways to constrain types in Python</title><link href="https://mlops.systems/robustpython/python/books-i-read/2022/01/08/robust-python-4.html" rel="alternate" type="text/html" title="Different ways to constrain types in Python" /><published>2022-01-08T00:00:00-06:00</published><updated>2022-01-08T00:00:00-06:00</updated><id>https://mlops.systems/robustpython/python/books-i-read/2022/01/08/robust-python-4</id><content type="html" xml:base="https://mlops.systems/robustpython/python/books-i-read/2022/01/08/robust-python-4.html">&lt;p&gt;The fourth chapter of &lt;a href=&quot;https://www.amazon.com/Robust-Python-Patrick-Viafore-ebook-dp-B09982C9FX/dp/B09982C9FX/ref=mt_other?_encoding=UTF8&amp;amp;me=&amp;amp;tag=soumet-20&amp;amp;qid=&quot;&gt;‘Robust Python’&lt;/a&gt; continues on from &lt;a href=&quot;https://mlops.systems/robustpython/python/books-i-read/2022/01/03/robust-python-3.html&quot;&gt;where we left off&lt;/a&gt; last time. We had previously learned about the benefits of type annotations in general terms, as well as started to understand how we might apply these annotations to simple code examples. But what if things are a bit more complicated? Then we have a few more options at our disposal.&lt;/p&gt;

&lt;p&gt;Note that you can assign all of these type assignments to variables (‘type aliases’), which might just make your code that much more readable.&lt;/p&gt;

&lt;h2 id=&quot;optional-to-catch-none-references&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Optional&lt;/code&gt; to catch &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;None&lt;/code&gt; references&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Optional&lt;/code&gt; as a type annotation is where you want to allow a specific type or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;None&lt;/code&gt; to be passed in to a particular function:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Optional&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;some_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Optional&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;# your code goes here
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that you’ll probably want (and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mypy&lt;/code&gt; will remind you if you forget) to handle what happens in both those cases inside your function. (You may need to specifically pass in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;—strict-optional&lt;/code&gt; flag to catch this when using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mypy&lt;/code&gt;.)&lt;/p&gt;

&lt;h2 id=&quot;union-to-group-types-together&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Union&lt;/code&gt; to group types together&lt;/h2&gt;

&lt;p&gt;This is used when multiple different types can be used for the same variable:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Union&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;returns_the_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Union&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Union&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This function doesn’t really do anything, but you get the idea. Note, too, that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Optional[int]&lt;/code&gt; is really a version of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Union[int, None]&lt;/code&gt;. (The book gets into exactly why we might care about reducing the number of possible options by way of a little detour into set theory.)&lt;/p&gt;

&lt;h2 id=&quot;literal-to-include-only-specific-values&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Literal&lt;/code&gt; to include only specific values&lt;/h2&gt;

&lt;p&gt;A little like what I believe enumerations do, we also have the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Literal&lt;/code&gt; type. It restricts you to whatever specific values are defined:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Literal&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;some_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Literal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here the function is restricted to inputs that are either 1, 2 or 3. Note that these are a feature that applies to Python 3.8 and above.&lt;/p&gt;

&lt;h2 id=&quot;annotated-for-more-complicated-restrictions&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Annotated&lt;/code&gt; for more complicated restrictions&lt;/h2&gt;

&lt;p&gt;These are available, but not really useful since they only function as a communication method. You can specify specific restrictions such as the following (example is taken from the book, p. 56:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Annotated&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Annotated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ValueRange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Annotated&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatchesRegex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'[abc]{2}'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Read more about it &lt;a href=&quot;https://docs.python.org/3/library/typing.html?highlight=annotated#typing.Annotated&quot;&gt;here&lt;/a&gt;. The book doesn’t spend much time on it and it seems like it’s probably best left alone for the moment.&lt;/p&gt;

&lt;h2 id=&quot;newtype-to-cover-different-contexts-applied-to-the-same-type&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NewType&lt;/code&gt; to cover different contexts applied to the same type&lt;/h2&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;NewType&lt;/code&gt;, on the other hand, is quite useful. You can create new types which are identical to some other type, and those new values made with the new type will have access to all the methods and properties as the original type.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NewType&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;# you implement the class here
&lt;/span&gt;	
&lt;span class=&quot;n&quot;&gt;NewBook&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NewType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;NewBook&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;process_new_book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NewBook&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;# here you handle what happens to the new book
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can achieve something like the same thing with classes and inheritance, I believe, but this is a lightweight version which might be useful to achieve the same end goal.&lt;/p&gt;

&lt;h2 id=&quot;final-to-prevent-reassignment--rebinding&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Final&lt;/code&gt; to prevent reassignment / rebinding&lt;/h2&gt;

&lt;p&gt;You can specify that a particular variable should have a single value and that value only. (Note that mutations of an object etc are all still possible, but reassignment to a new memory address is not possible.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;typing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Final&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;NAME&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Final&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Alex&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you tried to subsequently change this to a different name, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mypy&lt;/code&gt; would catch that you’d tried to do this. This can be valuable across very large codebases, where the potential for someone to reassign a variable might be not insignificant.&lt;/p&gt;

&lt;p&gt;So there you have it: a bunch of different ways to handle combinations of types and/or more complicated annotation scenarios. The next chapter will cover what happens when we throw collections into the mix, and what type annotation challenges are raised.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="robustpython" /><category term="python" /><category term="books-i-read" /><summary type="html">The fourth chapter of ‘Robust Python’ continues on from where we left off last time. We had previously learned about the benefits of type annotations in general terms, as well as started to understand how we might apply these annotations to simple code examples. But what if things are a bit more complicated? Then we have a few more options at our disposal.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/robust-python/robust-python-cover.jpeg" /><media:content medium="image" url="https://mlops.systems/images/robust-python/robust-python-cover.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Learning about ‘nbdev’ while building a Python package for PDF machine learning datasets</title><link href="https://mlops.systems/python/jupyter/fastai/tools/2022/01/06/nbdev-early-impressions.html" rel="alternate" type="text/html" title="Learning about ‘nbdev’ while building a Python package for PDF machine learning datasets" /><published>2022-01-06T00:00:00-06:00</published><updated>2022-01-06T00:00:00-06:00</updated><id>https://mlops.systems/python/jupyter/fastai/tools/2022/01/06/nbdev-early-impressions</id><content type="html" xml:base="https://mlops.systems/python/jupyter/fastai/tools/2022/01/06/nbdev-early-impressions.html">&lt;p&gt;While working to
&lt;a href=&quot;https://mlops.systems/categories/#redactionmodel&quot;&gt;develop a computer vision model&lt;/a&gt;
that detects redactions in documents obtained as a result of
&lt;a href=&quot;https://en.wikipedia.org/wiki/Freedom_of_Information_Act_(United_States)&quot;&gt;FOIA requests&lt;/a&gt;,
I have encountered some tasks that I end up repeating over and over again. Most
of the raw data in the problem domain exists in the form of PDFs. These PDF
files contain scanned images of various government documents. I use these images
as the training data for my model.&lt;/p&gt;

&lt;p&gt;The things I have to do as part of the data acquisition and transformation
process include the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;downloading all the PDF files linked to from a particular website, or series
of web pages&lt;/li&gt;
  &lt;li&gt;converting and splitting all the downloaded PDF files into appropriately sized
individual image files suitable for use in a computer vision model&lt;/li&gt;
  &lt;li&gt;generating statistics on the data being downloaded and processed, as well as
(further down the line) things like detecting data drift for incoming training
data&lt;/li&gt;
  &lt;li&gt;splitting up data as appropriate for train / validation / test data sets&lt;/li&gt;
  &lt;li&gt;extracting text data from the images via an OCR process&lt;/li&gt;
  &lt;li&gt;versioning, syncing and uploading those images to an S3 bucket or some other
cloud equivalent for use in the overall workflow&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s not hard to see that many of these things likely apply to multiple machine
learning data acquisition scenarios. While writing the code to handle these
elements in my specific use case, I realised it might be worth gathering this
functionality together in an agnostic tool that can handle some of these
scenarios.&lt;/p&gt;

&lt;p&gt;I had wanted to try out &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nbdev&lt;/code&gt; ever since
&lt;a href=&quot;https://www.fast.ai/2019/12/02/nbdev/&quot;&gt;it was announced&lt;/a&gt; back in 2019. The
concept was different to what I was used, but there were lots of benefits to be
had. I chose this small project to give it an initial trial run. I didn’t
implement all of the above features. The two notable missing parts are text
extraction and data versioning and/or synchronisation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/strickvl/pdfsplitter/tree/main/&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pdfsplitter&lt;/code&gt;&lt;/a&gt; is the
package I created to scratch that itch. It’s still very much a work in progress,
but I think I did enough with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nbdev&lt;/code&gt; to have an initial opinion.&lt;/p&gt;

&lt;p&gt;I think I had postponed trying it out because I was worried about a steep
learning curve. It turned out that an hour or two was all it took before I was
basically up and running, with an understanding of all the relevant pieces that
you generally use during the development lifecycle.&lt;/p&gt;

&lt;p&gt;Built in to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nbdev&lt;/code&gt; in general is the ability to iterate quickly and driven by
short, small experiments. This is powered by Jupyter notebooks, which are sort
of the core of everything that &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nbdev&lt;/code&gt; is about. If you don’t like notebooks,
you won’t like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nbdev&lt;/code&gt;. It’s a few years since it first saw the light of day as
a tool, and as such it felt like a polished way of working, and most of the
pieces of a typical development workflow were well accounted for. In fact, a lot
of the advantages come from convenience helpers of various kinds. Automatic
parallelised testing, easy submission to
&lt;a href=&quot;https://anaconda.org/anaconda/repo&quot;&gt;Anaconda&lt;/a&gt; and &lt;a href=&quot;https://pypi.org&quot;&gt;PyPi&lt;/a&gt;
package repositories, automatic building of documentation and standardising
locations for making configuration changes. All these parts were great.&lt;/p&gt;

&lt;p&gt;Perhaps the most sneakily pleasant part of using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nbdev&lt;/code&gt; was how it encouraged
best practices. There’s no concept of keeping test and documentation code in
separate silos away from the source notebooks. Following the best traditions of
&lt;a href=&quot;http://literateprogramming.com&quot;&gt;literate programming&lt;/a&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nbdev&lt;/code&gt; encourages you
to do that as you develop. Write a bit of code here, write some narrative
explanation and documentation there, and write some tests over there to confirm
that it’s working in the way you expected. When Jeremy speaks of the significant
boost in productivity, I believe that a lot of it comes from the fact that so
much is happening in one place.&lt;/p&gt;

&lt;p&gt;While working on &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pdfsplitter&lt;/code&gt;, I had the feeling that I could just focus on the
problem at hand, building something to help speed up the process of importing
and generating images from PDF data for machine learning projects.&lt;/p&gt;

&lt;p&gt;Not everything was peaches and roses, however. I ran into a weird mismatch with
the documentation pages generated and my GitHub fork of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nbdev&lt;/code&gt; since I was
using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;main&lt;/code&gt; as the default branch but &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nbdev&lt;/code&gt; still uses &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;master&lt;/code&gt;. I will be
submitting an issue to their repository, and it was an easy fix, but it was
confusing to struggle with that early on in my process. I’m also not sure how
well &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nbdev&lt;/code&gt; will gel with large teams of developers, especially when they’re
working on the same notebooks / modules. I know
&lt;a href=&quot;https://www.reviewnb.com&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;reviewnb&lt;/code&gt;&lt;/a&gt; exists now and even is used within
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fastai&lt;/code&gt; for code reviews, but I would imagine an uphill battle trying to
persuade a team to take a chance with that.&lt;/p&gt;

&lt;p&gt;I’ve been using VSCode at work, supercharged with GitHub Copilot and various
other goodies, so it honestly felt like a bit of a step back to be forced to
develop inside the Jupyter notebook interface, absent all of my tools. I also
found the pre-made CLI functions a little fiddly to use — fiddly in the sense
that I wish I’d set up some aliases for them early on as you end up calling them
all the time. In fact, any time I made a change I would find myself making all
these calls to build the library and then the documentation, not forgetting to
run the tests and so on. That part felt a bit like busy work and I wish some of
those steps could be combined together. Maybe I’m using it wrong.&lt;/p&gt;

&lt;p&gt;All in all, I enjoyed this first few hours of contact with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nbdev&lt;/code&gt; and I will
continue to use it while developing
&lt;a href=&quot;https://github.com/strickvl/pdfsplitter/&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pdfsplitter&lt;/code&gt;&lt;/a&gt;. The experience was
also useful to reflect back into my current development workflow and
environment, especially when it comes to keeping that close relationship between
the code, documentation and tests.&lt;/p&gt;

&lt;p&gt;[Photo by &lt;a href=&quot;https://unsplash.com/@viazavier?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&quot;&gt;Laura
Ockel&lt;/a&gt; on &lt;a href=&quot;https://unsplash.com/s/photos/cogs?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&quot;&gt;Unsplash&lt;/a&gt;]&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="python" /><category term="jupyter" /><category term="fastai" /><category term="tools" /><summary type="html">While working to develop a computer vision model that detects redactions in documents obtained as a result of FOIA requests, I have encountered some tasks that I end up repeating over and over again. Most of the raw data in the problem domain exists in the form of PDFs. These PDF files contain scanned images of various government documents. I use these images as the training data for my model.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/nbdev-early-impressions/laura-ockel-UQ2Fw_9oApU-unsplash.jpg" /><media:content medium="image" url="https://mlops.systems/images/nbdev-early-impressions/laura-ockel-UQ2Fw_9oApU-unsplash.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Getting practical with type annotations and `mypy`</title><link href="https://mlops.systems/robustpython/python/books-i-read/2022/01/03/robust-python-3.html" rel="alternate" type="text/html" title="Getting practical with type annotations and `mypy`" /><published>2022-01-03T00:00:00-06:00</published><updated>2022-01-03T00:00:00-06:00</updated><id>https://mlops.systems/robustpython/python/books-i-read/2022/01/03/robust-python-3</id><content type="html" xml:base="https://mlops.systems/robustpython/python/books-i-read/2022/01/03/robust-python-3.html">&lt;p&gt;The third chapter of &lt;a href=&quot;https://www.amazon.com/Robust-Python-Patrick-Viafore-ebook-dp-B09982C9FX/dp/B09982C9FX/ref=mt_other?_encoding=UTF8&amp;amp;me=&amp;amp;tag=soumet-20&amp;amp;qid=&quot;&gt;‘Robust Python’&lt;/a&gt; offers a quick introduction to the practicalities of type annotations in Python. We also see tools like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mypy&lt;/code&gt; being used to catch places where the reality of your code doesn’t necessarily match the type annotations that you’ve stated.&lt;/p&gt;

&lt;p&gt;For the first, a quick example can suffice:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;alex&quot;&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;some_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;some_number&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;some_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;some text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;# your code goes here
&lt;/span&gt;	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# returns a string
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can see the different places that type annotations might appear. You can annotate variables in your code. I’ve seen this one less often, but it’s possible. Then you can have type annotations for the parameters when defining functions (some even with default values assigned). You can also have type annotations for the return value of those functions.\&lt;/p&gt;

&lt;p&gt;Note that type hints are not used at runtime, so in that sense they are completely optional and don’t affect how your code runs when it’s passed through the Python interpreter. (Type hints were introduced in Python 3.5, though there is a way to achieve the same effect using comments and a standard way of listing type annotations that way if you are stuck with a 2.7 codebase, for example.)&lt;/p&gt;

&lt;p&gt;With some type annotations added to our code, we can use a typechecker like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mypy&lt;/code&gt; to see whether things are really as we imagine. In Viafore’s own words:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“type checkers are what allow the type annotations to transcend from communication method to a safety net. It is a form of static analysis.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If your codebase uses type annotations to communicate intent, and you’re using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mypy&lt;/code&gt; to catch any of those type errors, remember that typecheckers only catch this certain type of errors. You still need to be doing testing and all the other best practices to help catch the rest.&lt;/p&gt;

&lt;p&gt;One forward-looking benefit covered by this chapter was how having code covered with type annotations and type checking could give you the confidence to change things in the codebase that otherwise you would have hesitated to even approach. There  are, of course, also some tradeoffs and disadvantages to adding this in: particularly around speed of iteration and possibly flexibility, but the book makes a strong case for why most large Python codebases could probably use type checking as part of their arsenal.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="robustpython" /><category term="python" /><category term="books-i-read" /><summary type="html">The third chapter of ‘Robust Python’ offers a quick introduction to the practicalities of type annotations in Python. We also see tools like mypy being used to catch places where the reality of your code doesn’t necessarily match the type annotations that you’ve stated.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/robust-python/robust-python-cover.jpeg" /><media:content medium="image" url="https://mlops.systems/images/robust-python/robust-python-cover.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Counter: a shortcut to counting iterables in Python</title><link href="https://mlops.systems/python/2022/01/01/counter.html" rel="alternate" type="text/html" title="Counter: a shortcut to counting iterables in Python" /><published>2022-01-01T00:00:00-06:00</published><updated>2022-01-01T00:00:00-06:00</updated><id>https://mlops.systems/python/2022/01/01/counter</id><content type="html" xml:base="https://mlops.systems/python/2022/01/01/counter.html">&lt;p&gt;I came across this special dictionary type while reading an earlier chapter of ‘Robust Python’ the other day. It’s perhaps best illustrated with an example:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;collections&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# returns Counter({1: 2, 2: 1, 3: 1})
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'The Netherlands'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# returns Counter({'e': 3, 't': 2, 'h': 2, 'n': 2, ' ': 1, 'r': 1, 'l': 1, 'a': 1, 'd': 1, 's': 1})
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I had no idea this existed, and of course usually default to some kind of a cookie-cutter loop when trying get counts of elements and put those counts into a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dict&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To get the inividual elements, just call the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;elements&lt;/code&gt; method on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Counter&lt;/code&gt; object. To get the most common &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n&lt;/code&gt; elements, call the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;most_common(n)&lt;/code&gt; method. To get the total number of counts inside the dictionary, use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;total&lt;/code&gt; method. To reset all the counts, use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;clear&lt;/code&gt; method.&lt;/p&gt;

&lt;p&gt;Just a nice little set of functionality, hiding in plain sight inside the Python standard library.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Photo by &lt;a href=&quot;https://unsplash.com/@photoripey?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&quot;&gt;Ibrahim Rifath&lt;/a&gt; on &lt;a href=&quot;https://unsplash.com/s/photos/count?utm_source=unsplash&amp;amp;utm_medium=referral&amp;amp;utm_content=creditCopyText&quot;&gt;Unsplash&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="python" /><summary type="html">I came across this special dictionary type while reading an earlier chapter of ‘Robust Python’ the other day. It’s perhaps best illustrated with an example:</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/counter/ibrahim-rifath-OApHds2yEGQ-unsplash.jpg" /><media:content medium="image" url="https://mlops.systems/images/counter/ibrahim-rifath-OApHds2yEGQ-unsplash.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">What’s special about types in Python?</title><link href="https://mlops.systems/robustpython/python/books-i-read/2021/12/30/robust-python-2.html" rel="alternate" type="text/html" title="What’s special about types in Python?" /><published>2021-12-30T00:00:00-06:00</published><updated>2021-12-30T00:00:00-06:00</updated><id>https://mlops.systems/robustpython/python/books-i-read/2021/12/30/robust-python-2</id><content type="html" xml:base="https://mlops.systems/robustpython/python/books-i-read/2021/12/30/robust-python-2.html">&lt;p&gt;The first section of
&lt;a href=&quot;https://www.amazon.com/Robust-Python-Patrick-Viafore-ebook-dp-B09982C9FX/dp/B09982C9FX/ref=mt_other?qid=&amp;amp;me=&amp;amp;tag=soumet-20&amp;amp;_encoding=UTF8&quot;&gt;&lt;em&gt;Robust Python&lt;/em&gt;&lt;/a&gt;
dives into types. We begin by taking a step back to think about what exactly
types are being used for, and what they might bring us. Python was not
(&lt;a href=&quot;https://stackoverflow.com/questions/32557920/what-are-type-hints-in-python-3-5&quot;&gt;until v3.5&lt;/a&gt;)
a language with which you could easily use typing. I remember going to
&lt;a href=&quot;http://pylondinium.org/2018/&quot;&gt;the Pylondinium conference&lt;/a&gt; in London in 2018 and
going to
&lt;a href=&quot;https://pylondinium.org/2018/talk.html?talk_id=24&quot;&gt;a talk by Bernat Gabor&lt;/a&gt;
about type hints in Python. Back then I didn’t have much of a sense of how new
they were to many people, but even now I don’t get the feeling that they’ve been
universally adopted. Hence Patrick’s book, I suppose…&lt;/p&gt;

&lt;p&gt;A type is defined in the book as being “a communication method”, both to / for
computers (“mechanical representation”) as well as for humans (“semantic
representation”). For the computer, when a variable is of a certain type this
determines what methods can be called on that particular object. As such, though
I’m straying into territory I don’t fully understand, I believe it also helps
with compilation efficiency. (Python is a dynamically-typed language so any
errors or type mismatches will only become apparent at runtime, however).&lt;/p&gt;

&lt;p&gt;For humans, types can help signal intent. This connects with
&lt;a href=&quot;https://mlops.systems/robustpython/python/books-i-read/2021/12/29/robust-python-1.html&quot;&gt;my previous chapter summary&lt;/a&gt;
from this book where I stated that code should communicate intent well to be
considered ‘robust’. Take the following simple code snippet:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;dates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[...]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;process_date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;date&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;extract_date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;dates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;date&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We have an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;extract_date&lt;/code&gt; function (defined elsewhere in the code), but we have
no real sense of what this &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;input&lt;/code&gt; parameter would be. Are we taking in strings
as input? Are we taking in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;datetime.datetime&lt;/code&gt; objects? Does the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;extract_date&lt;/code&gt;
function accept both, or do we need to ensure that we are only taking a specific
type? All these questions could be cleared up with a simple type hint as part of
the function definition, like so:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;dates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[...]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;process_date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;date&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;extract_date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;dates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;date&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we know what the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;input&lt;/code&gt; should be, and we can also add a type hint to the
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;extract_date&lt;/code&gt; function as well which will help &lt;em&gt;communicate our intent&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;We also learn how Python is more towards the ‘strongly-typed’ side of things on
the language spectrum. If you try to concatenate a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;list&lt;/code&gt; with a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dict&lt;/code&gt; in
Python using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;+&lt;/code&gt; operator, Python will throw a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;TypeError&lt;/code&gt; and fail. If you
try to do the same in Javascript you get two different answers depending on the
order of the two operands:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; [] + {}
&quot;[object Object]&quot;

&amp;gt;&amp;gt;&amp;gt; {} + []
0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For our purposes, using Python, we can use the strong typing to our advantage.&lt;/p&gt;

&lt;p&gt;Python is dynamically typed, though, which takes a bit more caution to handle in
a robust manner. Any type mismatches will only be found at runtime — at least
using just the vanilla install of the language without any extra imports or
modules.&lt;/p&gt;

&lt;p&gt;The chapter ends with a brief discussion of duck typing, defined as “the ability
to use objects and entities in a programming language as long as they adhere to
some interface”. We gain a lot in terms of increased composability, but if you
rely on this feature of the language too much then it can become a hindrance in
terms of communicating intent.&lt;/p&gt;

&lt;p&gt;This chapter didn’t add too many new concepts or skills to my current
understanding of the benefits of types, but it was useful to have this concept
of ‘communicating intent’ to be reiterated. When I think back to how I’ve heard
types mentioned in the past, they often get cast in a technical sense, whereas
thinking about communication between developers I think is a more motivating
framing.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="robustpython" /><category term="python" /><category term="books-i-read" /><summary type="html">The first section of Robust Python dives into types. We begin by taking a step back to think about what exactly types are being used for, and what they might bring us. Python was not (until v3.5) a language with which you could easily use typing. I remember going to the Pylondinium conference in London in 2018 and going to a talk by Bernat Gabor about type hints in Python. Back then I didn’t have much of a sense of how new they were to many people, but even now I don’t get the feeling that they’ve been universally adopted. Hence Patrick’s book, I suppose…</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/robust-python/robust-python-cover.jpeg" /><media:content medium="image" url="https://mlops.systems/images/robust-python/robust-python-cover.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">What makes code robust?</title><link href="https://mlops.systems/robustpython/python/books-i-read/2021/12/29/robust-python-1.html" rel="alternate" type="text/html" title="What makes code robust?" /><published>2021-12-29T00:00:00-06:00</published><updated>2021-12-29T00:00:00-06:00</updated><id>https://mlops.systems/robustpython/python/books-i-read/2021/12/29/robust-python-1</id><content type="html" xml:base="https://mlops.systems/robustpython/python/books-i-read/2021/12/29/robust-python-1.html">&lt;p&gt;We use a lot of modern Python idioms, libraries and patterns at work, so I’ve been wanting to get up to speed on that and maybe even actively contribute to this general direction. A recently-published book, &lt;a href=&quot;https://www.amazon.com/Robust-Python-Patrick-Viafore-ebook-dp-B09982C9FX/dp/B09982C9FX/ref=mt_other?qid=&amp;amp;me=&amp;amp;tag=soumet-20&amp;amp;_encoding=UTF8&quot;&gt;Robust Python: Write Clean and Maintainable Code&lt;/a&gt; by &lt;a href=&quot;https://www.linkedin.com/in/patviafore/&quot;&gt;Patrick Viafore&lt;/a&gt;, seems like it answers many of the questions I have around this topic. It is quite dense in terms of the amount of new things per chapter, so I’ll be working my way through it in the coming months and reflecting on things as I encounter them.&lt;/p&gt;

&lt;p&gt;The first chapter is mainly about setting the scene for all the technical pieces that follow. Patrick asks the core questions: what is robust code and why do we even care? What problems does it solve to think about code in this way.&lt;/p&gt;

&lt;p&gt;What I took away was that a robust codebase emphasises good communication as well as avoiding accidental complexity. A lot has been written about ‘clean code’ and how to achieve this, but it seems that ‘Robust Python’ is arguing for looking a bit further into the future, when you have to come back to refactor your code three months after you wrote it, or when your colleague needs to do the same.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“Writing robust code means deliberately thinking about the future.” (p. 3)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You write robust code, in other words, because you know that the codebase is going to be changing and shifting and that whatever you write today may need to be modified at a later date:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“A robust codebase is resilient and error-free in spite of constant change.” (p. 4)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We’re trying to solve for the way that code is often hard to reason about or understand when you’re outside the original moment when it was written. Accordingly, it pays dividends to take a bit of extra time upfront to write code such that it &lt;em&gt;does&lt;/em&gt; communicate intent well, and that you haven’t made things more complicated than they need to be.&lt;/p&gt;

&lt;p&gt;Moreover, the communication of intent needs to be done in a way that is asynchronous. The book goes into a bit more detail about why communication practices that require minimal cost and minimal proximity are to be preferred. These include: the code itself, in-code comments, tests, version control history, wikis, and in-project documentation.&lt;/p&gt;

&lt;p&gt;The first part of the book is all about type annotation, using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mypy&lt;/code&gt;, and how working with types helps makes your code more robust. We use a lot of this at work so I’m excited to take a deep dive into this.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="robustpython" /><category term="python" /><category term="books-i-read" /><summary type="html">We use a lot of modern Python idioms, libraries and patterns at work, so I’ve been wanting to get up to speed on that and maybe even actively contribute to this general direction. A recently-published book, Robust Python: Write Clean and Maintainable Code by Patrick Viafore, seems like it answers many of the questions I have around this topic. It is quite dense in terms of the amount of new things per chapter, so I’ll be working my way through it in the coming months and reflecting on things as I encounter them.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/robust-python/robust-python-cover.jpeg" /><media:content medium="image" url="https://mlops.systems/images/robust-python/robust-python-cover.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Exploring J, an array programming language</title><link href="https://mlops.systems/j/2021/12/29/j-language.html" rel="alternate" type="text/html" title="Exploring J, an array programming language" /><published>2021-12-29T00:00:00-06:00</published><updated>2021-12-29T00:00:00-06:00</updated><id>https://mlops.systems/j/2021/12/29/j-language</id><content type="html" xml:base="https://mlops.systems/j/2021/12/29/j-language.html">&lt;p&gt;I’ve long wanted to explore &lt;a href=&quot;https://www.jsoftware.com&quot;&gt;the J programming language&lt;/a&gt;. I think I probably first heard about it from Jeremy Howard amidst one of the early iterations of the fastai course. He’s since spoken about it &lt;a href=&quot;https://youtu.be/J6XcP4JOHmk?t=505&quot;&gt;in other places&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It is part of the family of programming languages that includes &lt;a href=&quot;https://en.wikipedia.org/wiki/APL_(programming_language)&quot;&gt;APL&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/K_%28programming_language%29&quot;&gt;K&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Q_(programming_language_from_Kx_Systems)&quot;&gt;Q&lt;/a&gt;. These can broadly be categorised as array-programming languages, where arrays are generally the core data structure and mental model to keep in mind. They used to be extremely popular in the 1970s and 1980s, particularly among institutions or businesses with a requirement for performant calculation / computation. One of these, Q, continues to live on (as a closed-source language) &lt;a href=&quot;https://www.efinancialcareers.se/news/2020/10/kdb-finance-jobs&quot;&gt;in the world of finance and trading&lt;/a&gt;. (Q is popular alongside the proprietary database &lt;a href=&quot;https://en.wikipedia.org/wiki/Kdb%2B&quot;&gt;kdb+&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;You’re probably wondering why someone would want to use this fairly specialised and niche language. When you look at examples of J code — like the ones &lt;a href=&quot;https://code.jsoftware.com/wiki/Studio/IdiosyncraticIntroduction&quot;&gt;here&lt;/a&gt;, for example — it’s easy to simply dismiss it as an unreadable (‘write-only’) language. Indeed, many do dismiss it for this reason. Code is often compact, with single letters or symbols doing all the work. Defenders of J hold this up as a feature, not a problem. The compactness of the language means that you can fit the entirety of the solution (space) of a complex problem on a single screen, whereas in many (most?) other languages you would have to be scrolling up and down through dozens or even hundreds of lines of code.&lt;/p&gt;

&lt;p&gt;The array languages seem to come at solving problems from a particular perspective. The symbols and letters that transform the arrays in J function as a pattern language. For a simple example, think of what you have to do when you want to find the count of a particular element from within an array/list. The array language paradigm argues that you don’t want to waste your time and screen space writing out boilerplate code to carry out this calculation, when it’s a common pattern that you can just use from the language itself. When problem-solving, therefore, spend your time thinking about the problem and not messing around with syntax or repeating yourself.&lt;/p&gt;

&lt;p&gt;J and its cousins are extremely efficient. It is written in C, and I recently heard someone quote one of the early J pioneers as having said that “it is not theoretically possible to write J code that is more performant than C, but it often ends up being so”. For some math- or statistics-heavy domains (think the world of finance), it is extremely helpful to have this highly abstracted language that works performantly on large datasets. Moreover, it seems to be even more helpful when you have a hard problem to work on that isn’t fully understood.&lt;/p&gt;

&lt;p&gt;Kenneth Iverson’s wrote a paper (&lt;a href=&quot;https://www.jsoftware.com/papers/tot.htm&quot;&gt;“Notation as a Tool of Thought”&lt;/a&gt;) that is a classic in computer science and gets into some of the above arguments. (It is written using APL, but &lt;a href=&quot;https://www.hillelwayne.com/post/j-notation/&quot;&gt;it also applies to J&lt;/a&gt;). I will probably return to that at a future date, because it often comes up and is recommended as a particularly rich document worth taking time to explore in depth.&lt;/p&gt;

&lt;p&gt;Very much as a project to indulge my curiosity, I will be exploring J over the coming months. I have been listening to the back catalogue of &lt;a href=&quot;https://www.arraycast.com&quot;&gt;The Array Cast&lt;/a&gt; podcast, and I will be slowly working my way through &lt;a href=&quot;https://code.jsoftware.com/wiki/Main_Page&quot;&gt;some of the resources&lt;/a&gt; listed on the official J site. Let me know if you have experience working with J!&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="j" /><summary type="html">I’ve long wanted to explore the J programming language. I think I probably first heard about it from Jeremy Howard amidst one of the early iterations of the fastai course. He’s since spoken about it in other places.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/j-language/jblue.png" /><media:content medium="image" url="https://mlops.systems/images/j-language/jblue.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">A Taxonomy of Redaction</title><link href="https://mlops.systems/redactionmodel/2021/12/15/redaction-taxonomy.html" rel="alternate" type="text/html" title="A Taxonomy of Redaction" /><published>2021-12-15T00:00:00-06:00</published><updated>2021-12-15T00:00:00-06:00</updated><id>https://mlops.systems/redactionmodel/2021/12/15/redaction-taxonomy</id><content type="html" xml:base="https://mlops.systems/redactionmodel/2021/12/15/redaction-taxonomy.html">&lt;p&gt;One of the things that makes it hard to train a model to detect redactions in documents is the fact that there are lots of kinds of redactions. Not only were different tools or methods used at different times, but even organisations and agencies from the same country or government didn’t always share redaction practices.&lt;/p&gt;

&lt;p&gt;I took a bit of time to try to understand the different kinds of redactions in my (pretty huge) data set. I didn’t have any special process for selecting these images; I randomly sorted the immediate ~70,000 images I have collected and looked through to try to identify some patterns.&lt;/p&gt;

&lt;p&gt;Taking a close look at the actual parts of images that contain redactions gives me a better sense of the challenges involved in detecting those redactions. As I iterate through my collection of images, I can start to build up an intuitive sense of where class imbalances might exist. Among the images that contain redactions, for example, which ones are most represented and which contain fewer examples? In general, where do I need to focus my efforts when it comes to improving my model?&lt;/p&gt;

&lt;p&gt;The first easy distinction to draw is that between digital and hand-applied redactions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/redaction-taxonomy/comparison1.png&quot; alt=&quot;&quot; title=&quot;It's pretty easy to tell the difference.&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It seems that the trend in this is towards digital redactions. Perhaps it is seen as less reliable, or perhaps it’s more time consuming to attach the reasons for redactions having happened. Perhaps, too, there are some legal reasons why each redaction needed to start having a specific reason applied to it.&lt;/p&gt;

&lt;p&gt;At first glance, no pun intended, it would appear that digital redactions are much easier to recognise. They’re often uniform in how they are applied and are usually pretty blunt in their appearance. There are some non-redaction uses for totally black or grey boxes laid on top of text, but they aren’t common and it’s a pretty strong feature to have to predict.&lt;/p&gt;

&lt;p&gt;Handwritten redactions are also easy to recognise, but potentially the borders are harder to make out. Sometimes having a thinner pen with which redactions are applied might make it slightly less accurate.&lt;/p&gt;

&lt;p&gt;It is more practically important to distinguish between redactions that are easy to recognise vs ones that take some time to notice. I can use my own speed at noticing the redaction on a page as a gauge. It’s not a perfect analogy, but Jeremy Howard’s adage that if a human can reliably do some kind of classification or object detection, then probably a computer can as well. I guess the inverse is also true: if a human will find it hard to recognise a particular feature in an image, then a computer will probably also find it hard.&lt;/p&gt;

&lt;p&gt;There isn’t much point spending too long with the ‘easy’ redactions. These are usually whatever is boxy and blunt. It’s the stereotype of a redacted document, one like what was used as the cover art on the (much-censored) &lt;a href=&quot;https://www.amazon.com/gp/product/B00KAEXM1K/ref=dbs_a_def_rwt_hsch_vapi_tkin_p1_i0?tag=soumet-20&quot;&gt;Guantánamo Diary&lt;/a&gt; by Mohamedou Ould Slahi.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/redaction-taxonomy/slahi-diary.png&quot; alt=&quot;&quot; title=&quot;The book was made into the film 'The Mauritanian'&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sometimes you see that the entire page has been redacted with some kind of a coloured box. Other times entire columns of information has been redacted from a table. These definitely feel like they are the more recent types of redactions.&lt;/p&gt;

&lt;p&gt;One thing that makes detecting redactions hard, on the other hand, is if the number of redactions is small. It stands to reason that lots of small redactions can stand out at first glance, whereas a single small redaction on one corner of the page is maybe harder to notice.&lt;/p&gt;

&lt;p&gt;The hardest of redactions seems like it is in examples like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/redaction-taxonomy/hard-pictures.png&quot; alt=&quot;&quot; title=&quot;White boxes on white boxes are hard to recognise!&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A white box on top of other white boxes! I often have to look quite closely at these to distinguish what is normal text and what is a redaction box. Some of them have a faint thin grey boundary box around them, which I guess ends up being pretty useful as a way to make that distinction. Surprisingly, the model that I’ve trained so far is not terrible at making these kinds of distinctions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/redaction-taxonomy/hard-image-prediction.png&quot; alt=&quot;&quot; title=&quot;It even made a decent effort at recognising opaque boxes on a white background&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I have a few hundred annotated images so far, but I now have an intuitive sense of the hard parts of the object detection test. I also have a sense of how represented I feel like those hard parts are — not very.&lt;/p&gt;

&lt;p&gt;As I wrote in &lt;a href=&quot;https://mlops.systems/redactionmodel/computervision/progressreport/2021/12/11/redaction-progress-week-one.html&quot;&gt;my previous update&lt;/a&gt; on my progress in this project, the next step is very much to find ways to increase the volume of good training data that I’m using to train my model. Part of that will involve creating synthetic data, part of that will be using self-training to speed up my annotation, and of course another part will just be doing more manual annotation. I’ve already started work on creating the synthetic data. More on that next time!&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="redactionmodel" /><summary type="html">One of the things that makes it hard to train a model to detect redactions in documents is the fact that there are lots of kinds of redactions. Not only were different tools or methods used at different times, but even organisations and agencies from the same country or government didn’t always share redaction practices.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/redaction-taxonomy/cover-image.jpg" /><media:content medium="image" url="https://mlops.systems/images/redaction-taxonomy/cover-image.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">73% accuracy for redaction object detection</title><link href="https://mlops.systems/redactionmodel/computervision/progressreport/2021/12/11/redaction-progress-week-one.html" rel="alternate" type="text/html" title="73% accuracy for redaction object detection" /><published>2021-12-11T00:00:00-06:00</published><updated>2021-12-11T00:00:00-06:00</updated><id>https://mlops.systems/redactionmodel/computervision/progressreport/2021/12/11/redaction-progress-week-one</id><content type="html" xml:base="https://mlops.systems/redactionmodel/computervision/progressreport/2021/12/11/redaction-progress-week-one.html">&lt;p&gt;Last time I wrote about my redaction model training project, &lt;a href=&quot;https://mlops.systems/redactionmodel/computervision/datalabelling/2021/11/29/prodigy-object-detection-training.html&quot;&gt;I explained how I used Prodigy&lt;/a&gt; to annotate and label a bunch of images. I subsequently spent a long evening going through the process, getting to know my data. I managed to make 663 annotations, though quite a few of those were negative annotations: I was stating that a certain document contained no redactions at all.&lt;/p&gt;

&lt;p&gt;Once I had my redactions, I needed to convert the files from a Prodigy format into a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.coco&lt;/code&gt; annotation format. I am using &lt;a href=&quot;https://airctic.com/&quot;&gt;IceVision&lt;/a&gt;, a really useful computer vision library, for which it is easier if I pass in the annotations in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.coco&lt;/code&gt; format.&lt;/p&gt;

&lt;p&gt;From that point, it was fairly easy to follow the steps of &lt;a href=&quot;https://airctic.com/0.11.0/getting_started_object_detection/&quot;&gt;the object detection tutorial&lt;/a&gt; outlined in the IceVision documentation. I ran into some problems with &lt;a href=&quot;https://gradient.run/&quot;&gt;Paperspace Gradient&lt;/a&gt; not easily installing and importing IceVision. For some reason files don’t get unzipped on Paperspace, but it’s possible to just do this manually:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Do the basic install, including the import of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;icevision.all&lt;/code&gt;. Wait for the error to get raised, then open up a terminal and enter:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; /root/.icevision/mmdetection_configs/
&lt;span class=&quot;nb&quot;&gt;rm &lt;/span&gt;v2.16.0.zip
wget https://github.com/airctic/mmdetection_configs/archive/refs/tags/v2.16.0.zip
unzip v2.16.0.zip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then run it again as normal. Later on, another error will get raised. Fix it with this (again in the terminal):&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jupyter nbextension &lt;span class=&quot;nb&quot;&gt;enable&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--py&lt;/span&gt; widgetsnbextension
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This enables ipywidgets in the notebook, I think.&lt;/p&gt;

&lt;p&gt;Once through all of that, I was able to fine-tune a model based on the annotations which I currently have. I selected &lt;a href=&quot;https://mlops.systems/redactionmodel/computervision/2021/11/30/vfnet-basics.html&quot;&gt;VFNet&lt;/a&gt; as the model I wanted to use as the pertained model. After training for 40 epochs, I reached an accuracy of 73%:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/redaction-progress-week-one/first-training.png&quot; alt=&quot;&quot; title=&quot;Metrics from the last few epochs&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we look at some of the results (using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;model_type.show_results()&lt;/code&gt;) we can get a sense of the parts it found easy and the parts which it found hard. (All the boxes below are what it as predicted, not the ground truth annotations.) Some identification of boxes went as you might expect:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/redaction-progress-week-one/redaction_sample_1.png&quot; alt=&quot;&quot; title=&quot;It was good at identifying solid and clear redactions&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I was surprised that something like this worked as well as it did:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/redaction-progress-week-one/redaction_white_boxes.png&quot; alt=&quot;&quot; title=&quot;It even made a decent effort at recognising opaque boxes on a white background&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It wasn’t perfect, but I don’t remember having annotated too many of this specific redaction type, so I’m fairly happy with how it worked out. You can see it still makes a number of mistakes and isn’t always precise about where the boxes should go. I hope that’ll improve as I add more examples of this type of redaction.&lt;/p&gt;

&lt;p&gt;My next steps for this project include the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;create synthetic data. The redactions are probably easy enough to mimic where we’ll get a lot of value from the use of synthetic data (fake redactions on not-real document backgrounds). It’ll be an easy way to boost my training data set by a good amount, hopefully leading to big improvements in my model accuracy.&lt;/li&gt;
  &lt;li&gt;potentially add in either active learning (to help speed up my annotation process) or self-training (using the model to make annotation suggestions on unlabelled data and using only the suggestions with really high confidence estimates).&lt;/li&gt;
  &lt;li&gt;think through the augmentations that I use as part of my workflow. I basically want augmentations that are similar to however the production use case will be: i.e. the kinds of redacted images that it might see when being given real-world data at inference time post-training.&lt;/li&gt;
  &lt;li&gt;add in experiment tracking. I’ve never used something like Weights &amp;amp; Biases, so I’m excited to try that out and have a real process for tracking my progress throughout this project.&lt;/li&gt;
  &lt;li&gt;cleaning up and refactoring (a bit) my repository where the code lives for processing the input data. It’s starting to get a bit unwieldy and I’m worried I’ll start to forget the order things were done and some of those small details.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="redactionmodel" /><category term="computervision" /><category term="progressreport" /><summary type="html">Last time I wrote about my redaction model training project, I explained how I used Prodigy to annotate and label a bunch of images. I subsequently spent a long evening going through the process, getting to know my data. I managed to make 663 annotations, though quite a few of those were negative annotations: I was stating that a certain document contained no redactions at all.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/redaction-progress-week-one/redaction_sample_3-small.png" /><media:content medium="image" url="https://mlops.systems/images/redaction-progress-week-one/redaction_sample_3-small.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">What is VFNet?</title><link href="https://mlops.systems/redactionmodel/computervision/2021/11/30/vfnet-basics.html" rel="alternate" type="text/html" title="What is VFNet?" /><published>2021-11-30T00:00:00-06:00</published><updated>2021-11-30T00:00:00-06:00</updated><id>https://mlops.systems/redactionmodel/computervision/2021/11/30/vfnet-basics</id><content type="html" xml:base="https://mlops.systems/redactionmodel/computervision/2021/11/30/vfnet-basics.html">&lt;p&gt;VFNet is short for VariFocalNet. This method of object detection was first released in 2008 and it scored 55.1 on the COCO test-dev benchmark, state-of-the-art at the time. There have since been &lt;a href=&quot;https://paperswithcode.com/sota/object-detection-on-coco&quot;&gt;other improvements&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/vfnet-bounding-boxes.png&quot; alt=&quot;&quot; title=&quot;Bounding boxes to detect objects in an image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The original paper is &lt;a href=&quot;https://arxiv.org/abs/2008.13367v2&quot;&gt;here&lt;/a&gt;. The implementation of this model is &lt;a href=&quot;https://github.com/hyz-xmaster/VarifocalNet&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The problem it solves is that when we’re training a model, we have a large number of possible options for objects detected in an image. What we need to do is rank these options in order of likelihood of being a correct bounding of a box.&lt;/p&gt;

&lt;p&gt;It is based on and draws on &lt;a href=&quot;https://github.com/open-mmlab/mmdetection&quot;&gt;the MMDetection model/toolbox&lt;/a&gt;. MMDetection is a Pytorch library for object detection. It is modular, allowing for greater customisability.&lt;/p&gt;

&lt;h2 id=&quot;other-resources&quot;&gt;Other resources&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=wdlrK-D5K_4&quot;&gt;Airctic Presentation on VFNet&lt;/a&gt;&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="redactionmodel" /><category term="computervision" /><summary type="html">VFNet is short for VariFocalNet. This method of object detection was first released in 2008 and it scored 55.1 on the COCO test-dev benchmark, state-of-the-art at the time. There have since been other improvements.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/vfnet.png" /><media:content medium="image" url="https://mlops.systems/images/vfnet.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">How to annotate image data for object detection with Prodigy</title><link href="https://mlops.systems/redactionmodel/computervision/datalabelling/2021/11/29/prodigy-object-detection-training.html" rel="alternate" type="text/html" title="How to annotate image data for object detection with Prodigy" /><published>2021-11-29T00:00:00-06:00</published><updated>2021-11-29T00:00:00-06:00</updated><id>https://mlops.systems/redactionmodel/computervision/datalabelling/2021/11/29/prodigy-object-detection-training</id><content type="html" xml:base="https://mlops.systems/redactionmodel/computervision/datalabelling/2021/11/29/prodigy-object-detection-training.html">&lt;p&gt;I’m back to working on &lt;a href=&quot;https://mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html&quot;&gt;the redaction model&lt;/a&gt;, though this time with a slightly more focused objective: object detection.&lt;/p&gt;

&lt;p&gt;Object detection is when you put bounding boxes around the specific object that you are trying to locate within an image. The end goal for my project is to be able to identify — for an arbitrary image — which parts of the image are redacted, and then to be able to calculate what proportion of the image is redacted.&lt;/p&gt;

&lt;p&gt;For this, I need annotations. Annotations are the data that I will use as the fuel for the model I hope to train. We need a lot of annotations of specific redactions in order for the computer to be able to learn to detect what is a redaction and what is just an empty box, for example.&lt;/p&gt;

&lt;p&gt;I showed in &lt;a href=&quot;https://mlops.systems/fastai/redactionmodel/computervision/datalabelling/2021/09/06/redaction-classification-chapter-2.html&quot;&gt;an earlier post&lt;/a&gt; how I trained a model to detect whether there was any kind of redaction inside an image (to around 95% accuracy). For this next stage, it isn’t enough to offer a binary ‘yes’ or ‘no’ for whether it has been redacted. I need to specify the coordinates of a bounding box which encompasses each redaction.&lt;/p&gt;

&lt;p&gt;In terms of the final output of the annotations, there are two main ways that this could go. I could either:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;get &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;x&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;y&lt;/code&gt; coordinates for the centre of the bounding box, and then a height and a width of the box around this centre point&lt;/li&gt;
  &lt;li&gt;get the four coordinates for each of the corners of the bounding box.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The &lt;a href=&quot;https://www.immersivelimit.com/tutorials/create-coco-annotations-from-scratch&quot;&gt;COCO dataset format&lt;/a&gt; will eventually want datasets in the second format, but &lt;a href=&quot;https://prodi.gy&quot;&gt;Prodigy&lt;/a&gt; has its own way of storing the data which I just left for now. Once I have a better handle on the annotation flow I will write a &lt;a href=&quot;https://prodi.gy/docs/custom-recipes&quot;&gt;custom recipe&lt;/a&gt; which will save the data in exactly the format that I want. For now, it’s good enough.&lt;/p&gt;

&lt;p&gt;Installing &lt;a href=&quot;https://prodi.gy&quot;&gt;Prodigy&lt;/a&gt; into your development environment is a breeze now that you can do it with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;prodigy &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://XXXX-XXXX-XXXX-XXXX@download.prodi.gy &lt;span class=&quot;c&quot;&gt;# where the XXXs are your license code&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Getting going with the image training was as easy as the following CLI command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;prodigy image.manual redaction-object-detection /path/to/image/data &lt;span class=&quot;nt&quot;&gt;--label&lt;/span&gt; CONTENT,REDACTION &lt;span class=&quot;nt&quot;&gt;--remove-base64&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--remove-base64&lt;/code&gt; is to ensure that Prodigy doesn’t store the raw binary image data inside the database alongside the annotations. &lt;a href=&quot;https://prodi.gy&quot;&gt;Prodigy&lt;/a&gt; (and their sister tool &lt;a href=&quot;https://spacy.io&quot;&gt;Spacy&lt;/a&gt;) is a little more focused on textual data, where storing the original data alongside the annotation doesn’t pose too much of an issue, but for image files this probably is a bit of an anti-pattern and could lead to a very large database.&lt;/p&gt;

&lt;p&gt;You get a local URL to go visit and you see an interface where you can make the necessary annotations:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-11-29-prodigy-object-detection-training/prodigy-object-interface.png&quot; alt=&quot;&quot; title=&quot;This is the basic interface for annotating object detection data in Prodigy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;You can see that I am distinguishing between two different classes: redactions and content. Redactions are what we’ve been talking about above. Content, however, is a bounding box for the content on a page. Remember that at the end of all of this we want a percentage of the page that has been redacted. Some images have reduced sized images, where the actual content which could have been redacted only takes up half of the A4 page. If that whole section was redacted, I’d want a final amount closer to 100% for that image rather than the 50% I’d get if I just went with the total percentage of redacted pixels on the whole image file.&lt;/p&gt;

&lt;p&gt;Doing a few annotations, I ran into a couple of issues almost immediately. What do I do with a page like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-11-29-prodigy-object-detection-training/full-square-annotation.png&quot; alt=&quot;&quot; title=&quot;The whole page is redacted... or is it?&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The whole text of the page is annotated, but the text only extended half-way down the page. There was only 50% of the page that could have been redacted, but should the content boundary box encompass more of the page, or just the only full-section redaction?&lt;/p&gt;

&lt;p&gt;And for the following image, what is the right way to think about how to make the annotation?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/2021-11-29-prodigy-object-detection-training/polygon-annotation.png&quot; alt=&quot;&quot; title=&quot;Should this be a single annotation, or two overlapping or adjoining boundary boxes?&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This redaction encompasses multiple lines, so to some extent it doesn’t make a difference whether we have overlapping annotations or two adjoining boundary boxes. But for the purposes of training our model, will this contribute to a less accurate model? Should I be using polygon boundaries (which Prodigy can also use for annotations)?&lt;/p&gt;

&lt;div class=&quot;Toast&quot;&gt;
   &lt;span class=&quot;Toast-icon&quot;&gt;&lt;svg class=&quot;octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;&lt;/span&gt;
   &lt;span class=&quot;Toast-content&quot;&gt;As an aside, this is why annotating your own data is so valuable. You get to see the limits of the annotations, and you get to really own the decisions that are being made. It is a bit early for me to know which approach is the best solution to these two problems, but being aware of them is important.&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;Once we’re done with our annotations, we can easily export our data to a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jsonl&lt;/code&gt; file with the following CLI command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;prodigy db-out redaction-object-detection &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; ./redaction-object-detection-annotations.jsonl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This gives us a file containing all our annotations. A sample for one image gives the idea:&lt;/p&gt;

&lt;div class=&quot;language-json highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;image&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;sample/path/redacted/04-F-0269_Global_Screening_Guidance-03.jpg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;text&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;04-F-0269_Global_Screening_Guidance-03&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;meta&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;file&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;04-F-0269_Global_Screening_Guidance-03.jpg&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;path&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;sample/path/redacted/04-F-0269_Global_Screening_Guidance-03.jpg&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;_is_binary&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;_input_hash&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1413334570&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;_task_hash&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1588323116&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;_view_id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;image_manual&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;width&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;800&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;height&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1035&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;spans&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;0ef6ccd0-4a79-471d-9aa1-9c903c83801e&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;label&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;CONTENT&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;color&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;yellow&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;x&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;76.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;y&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;112.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;height&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;786.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;width&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;587.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;center&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;370.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;505.55&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;rect&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;points&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;76.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;112.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;76.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;898.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;664.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;898.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;664.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;112.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;cd05d521-8efb-416b-87df-4624f16ca7f3&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;label&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;REDACTION&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;color&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;cyan&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;x&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;80.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;y&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;786.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;height&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;20.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;width&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;428.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;center&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;294.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;796.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;rect&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;points&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;80.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;786.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;80.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;806.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;508.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;806.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;508.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;786.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;id&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;3e268e33-4eba-457d-8d17-8271a79ee589&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;label&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;REDACTION&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;color&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;magenta&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;x&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;108.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;y&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;772.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;height&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;15.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;width&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;400.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;center&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;308.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;779.85&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;type&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;rect&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;points&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;108.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;772.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;108.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;787.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;508.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;787.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
        &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;508.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;772.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
      &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;answer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;accept&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;&quot;_timestamp&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1638214078&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Everything we’re interested in is inside the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;spans&lt;/code&gt; attribute, and it actually contains both kinds of the annotation that I mentioned above.&lt;/p&gt;

&lt;p&gt;As you can see, annotating images in this way is fairly painless, and it brings
you in closer contact with your raw data which is an added bonus.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="redactionmodel" /><category term="computervision" /><category term="datalabelling" /><summary type="html">I’m back to working on the redaction model, though this time with a slightly more focused objective: object detection.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/2021-11-29-prodigy-object-detection-training/prodigy-redaction.png" /><media:content medium="image" url="https://mlops.systems/images/2021-11-29-prodigy-object-detection-training/prodigy-redaction.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Launching a podcast about MLOps</title><link href="https://mlops.systems/zenml/podcast/appearances/2021/11/27/pipeline-conversations.html" rel="alternate" type="text/html" title="Launching a podcast about MLOps" /><published>2021-11-27T00:00:00-06:00</published><updated>2021-11-27T00:00:00-06:00</updated><id>https://mlops.systems/zenml/podcast/appearances/2021/11/27/pipeline-conversations</id><content type="html" xml:base="https://mlops.systems/zenml/podcast/appearances/2021/11/27/pipeline-conversations.html">&lt;p&gt;I’ll be co-hosting a new podcast about MLOps, with new episodes out every
fortnight.
&lt;a href=&quot;https://podcast.zenml.io&quot;&gt;Pipeline Conversations: A Machine Learning Podcast by ZenML&lt;/a&gt;
is the new podcast from &lt;a href=&quot;https://zenml.io/&quot;&gt;the company where I work&lt;/a&gt;. (We build
&lt;a href=&quot;https://github.com/zenml-io/zenml&quot;&gt;an open-source tool&lt;/a&gt; for data scientists to
empower them to take control of how their models live in production.)&lt;/p&gt;

&lt;p&gt;Our first episode gets into some of the background for why ZenML exists in the
first place. Upcoming episodes will be discussions with guests from the data
science and MLOps space.&lt;/p&gt;

&lt;p&gt;I’m excited to get the opportunity to talk with so many interesting and smart
people.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="zenml" /><category term="podcast" /><category term="appearances" /><summary type="html">I’ll be co-hosting a new podcast about MLOps, with new episodes out every fortnight. Pipeline Conversations: A Machine Learning Podcast by ZenML is the new podcast from the company where I work. (We build an open-source tool for data scientists to empower them to take control of how their models live in production.)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/pipeline_cover_small.jpeg" /><media:content medium="image" url="https://mlops.systems/images/pipeline_cover_small.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Check your security vulnerabilities with `safety`</title><link href="https://mlops.systems/security/tools/calmcode/2021/11/27/safety-vulnerability-checker.html" rel="alternate" type="text/html" title="Check your security vulnerabilities with `safety`" /><published>2021-11-27T00:00:00-06:00</published><updated>2021-11-27T00:00:00-06:00</updated><id>https://mlops.systems/security/tools/calmcode/2021/11/27/safety-vulnerability-checker</id><content type="html" xml:base="https://mlops.systems/security/tools/calmcode/2021/11/27/safety-vulnerability-checker.html">&lt;p&gt;&lt;a href=&quot;https://github.com/pyupio/safety&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;safety&lt;/code&gt;&lt;/a&gt; is a tiny tool that checks your package’s dependencies for security vulnerabilities. It is free to use for open-source projects, and using it is as a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install safety&lt;/code&gt; followed by &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;safety check&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;It checks a database of known security vulnerabilities. This database is only updated once every month, but if you are not open-source or you need access to the more frequently-updated database, then you can &lt;a href=&quot;https://pyup.io/pricing/&quot;&gt;subscribe via pyup&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;With that caveat, it’s not perfect, but it’s better than nothing. An easy CI win for open-source projects.&lt;/p&gt;

&lt;p&gt;[I first learned of this tool &lt;a href=&quot;https://calmcode.io/shorts/safety.py.html&quot;&gt;here&lt;/a&gt;. Many thanks to &lt;a href=&quot;https://calmcode.io/&quot;&gt;calmcode&lt;/a&gt; for continuing to make these really useful videos.]&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="security" /><category term="tools" /><category term="calmcode" /><summary type="html">safety is a tiny tool that checks your package’s dependencies for security vulnerabilities. It is free to use for open-source projects, and using it is as a pip install safety followed by safety check.</summary></entry><entry><title type="html">How to set and get environment variables using Python</title><link href="https://mlops.systems/python/2021/11/26/environment-variables.html" rel="alternate" type="text/html" title="How to set and get environment variables using Python" /><published>2021-11-26T00:00:00-06:00</published><updated>2021-11-26T00:00:00-06:00</updated><id>https://mlops.systems/python/2021/11/26/environment-variables</id><content type="html" xml:base="https://mlops.systems/python/2021/11/26/environment-variables.html">&lt;p&gt;If you want to get and set environment variables using Python, simply use the relevant methods from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;os&lt;/code&gt;. To set an environment variable, do this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'SOME_ENV_VARIABLE'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;13.5&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And to access an environment variable, there are actually a number of different ways. All these three are essentially the same:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getenv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'SOME_ENV_VARIABLE'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'SOME_ENV_VARIABLE'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;environ&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'SOME_ENV_VARIABLE'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For the final one (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;os.environ('SOME_ENV_VARIABLE')&lt;/code&gt;), if the variable doesn’t exist, it’ll return a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;KeyError&lt;/code&gt;, whereas the first two will just return &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;None&lt;/code&gt; in that case.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="python" /><summary type="html">If you want to get and set environment variables using Python, simply use the relevant methods from os. To set an environment variable, do this:</summary></entry><entry><title type="html">entr: a tool to run commands when files change</title><link href="https://mlops.systems/debugging/testing/tools/calmcode/2021/11/25/entr-reruns-tests.html" rel="alternate" type="text/html" title="entr: a tool to run commands when files change" /><published>2021-11-25T00:00:00-06:00</published><updated>2021-11-25T00:00:00-06:00</updated><id>https://mlops.systems/debugging/testing/tools/calmcode/2021/11/25/entr-reruns-tests</id><content type="html" xml:base="https://mlops.systems/debugging/testing/tools/calmcode/2021/11/25/entr-reruns-tests.html">&lt;p&gt;It’s a fairly common pattern that you have some code that you’re repeatedly running. Perhaps you’re fixing a failing test, and you just have to keep running it every time you make a fix.&lt;/p&gt;

&lt;p&gt;Enter &lt;a href=&quot;http://eradman.com/entrproject/&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;entr&lt;/code&gt;&lt;/a&gt;. This handy little tool reruns a particular command whenever changes are detected in a particular set of files.&lt;/p&gt;

&lt;p&gt;Let’s take the example I mentioned above: you have a failing test that you’re debugging and you need to have it run every time you save a change to the file. Assuming your source code is stored in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;src&lt;/code&gt; and you’re using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pytest&lt;/code&gt;, then you could use something like the following:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;ls &lt;/span&gt;src/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;.py | entr &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; pytest test.py::test_some_feature
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So now, any time you change any Python file inside the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;src&lt;/code&gt; folder, it’ll rerun your test. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;-c&lt;/code&gt; flag will clear the terminal every time the test runs.&lt;/p&gt;

&lt;p&gt;[Many thanks to &lt;a href=&quot;https://calmcode.io/&quot;&gt;calmcode&lt;/a&gt; for continuing to make these really useful videos.]&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="debugging" /><category term="testing" /><category term="tools" /><category term="calmcode" /><summary type="html">It’s a fairly common pattern that you have some code that you’re repeatedly running. Perhaps you’re fixing a failing test, and you just have to keep running it every time you make a fix.</summary></entry><entry><title type="html">On failure</title><link href="https://mlops.systems/debugging/emotions/2021/11/21/on-failure.html" rel="alternate" type="text/html" title="On failure" /><published>2021-11-21T00:00:00-06:00</published><updated>2021-11-21T00:00:00-06:00</updated><id>https://mlops.systems/debugging/emotions/2021/11/21/on-failure</id><content type="html" xml:base="https://mlops.systems/debugging/emotions/2021/11/21/on-failure.html">&lt;p&gt;I’ve been working as a machine learning engineer now for a few months now. If there’s one thing that I have found characterises my experience so far, it’s failure. Software fails; we even have a word for that: bugs. Learning new things might also be characterised as departing from a state of failing to understand.&lt;/p&gt;

&lt;p&gt;There hasn’t been a week that’s gone by since I started where I didn’t encounter some kind of failure, usually my inability to understand why something was behaving in a particular way. &lt;a href=&quot;https://mlops.systems/debugging/2021/10/25/debugging.html&quot;&gt;My last post&lt;/a&gt; was about debugging, and finding ways to move forward in the face of failure is a key aspect of that process.&lt;/p&gt;

&lt;p&gt;Failure isn’t fun. My initial reaction to hitting something I don’t understand is not one of glee and excitement at getting this opportunity to solve some kind of problem. But maybe it should be. It occurred to me this week that actually failure is sort of the name of the game. Solving hard problems is exactly what software engineers get paid to do. If it were just easy, it’d be a different kind of work.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://jvns.ca/blog/debugging-attitude-matters/&quot;&gt;Two&lt;/a&gt; &lt;a href=&quot;https://jvns.ca/blog/2015/11/22/how-i-got-better-at-debugging/&quot;&gt;posts&lt;/a&gt; by &lt;a href=&quot;https://jvns.ca/blog/&quot;&gt;Julia Evans&lt;/a&gt; are pretty great on how a lot of being able to do this kind of work is about mindset. Ellen Ullman covers similar territory in ‘&lt;a href=&quot;https://www.amazon.com/gp/product/B01N4P12XJ/ref=dbs_a_def_rwt_bibl_vppi_i0?tag=soumet-20&quot;&gt;Life in Code&lt;/a&gt;’ and ‘&lt;a href=&quot;https://www.amazon.com/gp/product/B007FU83DY/ref=dbs_a_def_rwt_bibl_vppi_i1?tag=soumet-20&quot;&gt;Close to the Machine&lt;/a&gt;’.&lt;/p&gt;

&lt;p&gt;The point is this: we are paid to confront this failure. This is the work. Thinking that it’s a distraction from the work — some kind of imaginary world where there are no blockers or failing tests — is the real illusion.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="debugging" /><category term="emotions" /><summary type="html">I’ve been working as a machine learning engineer now for a few months now. If there’s one thing that I have found characterises my experience so far, it’s failure. Software fails; we even have a word for that: bugs. Learning new things might also be characterised as departing from a state of failing to understand.</summary></entry><entry><title type="html">Some things I learned about debugging</title><link href="https://mlops.systems/debugging/2021/10/25/debugging.html" rel="alternate" type="text/html" title="Some things I learned about debugging" /><published>2021-10-25T00:00:00-05:00</published><updated>2021-10-25T00:00:00-05:00</updated><id>https://mlops.systems/debugging/2021/10/25/debugging</id><content type="html" xml:base="https://mlops.systems/debugging/2021/10/25/debugging.html">&lt;p&gt;I’ve had to deal with a whole bunch of bugs in the past few days and weeks. I thought it’d be useful to put down some thoughts about things that I’ve learned along the way.&lt;/p&gt;

&lt;h2 id=&quot;logging--printing&quot;&gt;Logging &amp;amp; Printing&lt;/h2&gt;

&lt;p&gt;These are maybe the first things that everyone says you should do when you have a bug you need to fix: log things somewhere where you can see them.&lt;/p&gt;

&lt;p&gt;There are some scenarios where simple &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;print&lt;/code&gt; calls aren’t enough. If you’re running code through a series of tests, then the test harness will often consume all output to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;stdout&lt;/code&gt; so you won’t see any of your print statements. Luckily, test environments can usually be configured to print debug statements of loggers.&lt;/p&gt;

&lt;p&gt;Once you can see what’s happening at a particular moment, you can see if what you expected to happen at that moment is actually happening.&lt;/p&gt;

&lt;h2 id=&quot;breakpoint-your-way-to-infinity&quot;&gt;Breakpoint your way to infinity!&lt;/h2&gt;

&lt;p&gt;The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;breakpoint()&lt;/code&gt; function comes built-in with Python. It’s a convenience wrapper around some &lt;a href=&quot;https://docs.python.org/3/library/pdb.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pdb&lt;/code&gt;&lt;/a&gt; magic, and practically speaking it means you can set a point where you can interrupt the Python execution. Your terminal will halt at that point, and you can inspect the variables or objects available at that particular moment.&lt;/p&gt;

&lt;p&gt;I wish I had known about this earlier on. It’s extremely useful for understanding exactly how a function or piece of code is being executed.&lt;/p&gt;

&lt;h2 id=&quot;come-with-hypotheses&quot;&gt;Come with hypotheses&lt;/h2&gt;

&lt;p&gt;If you don’t have a sense of what you expect to happen, it’s going to be hard to determine if what you’re doing is having any effect or not.&lt;/p&gt;

&lt;p&gt;I’ve been lucky to do some pairing sessions with people as they work through bugs and problems, and I’ve had this ‘come with a hypothesis’ behaviour modelled really well for me.&lt;/p&gt;

&lt;p&gt;It’s not a panacea; there’s still a lot of work to be done around this, but it’s sort of the foundation, particularly for non-trivial bugs.&lt;/p&gt;

&lt;h2 id=&quot;leave-your-assumptions-at-the-door&quot;&gt;Leave your assumptions at the door&lt;/h2&gt;

&lt;p&gt;Don’t assume what’s written is what’s actually working. This applies to the code you’re working on, the documentation, docstrings, everything. This is especially true when your codebase is rapidly changing growing, such as at a startup or a smaller company where not everything has been cemented into place.&lt;/p&gt;

&lt;p&gt;The rapid pace of change means that things can get out of date, or people can make mistakes. This applies to packages or modules you’re importing as well. Of course, it’s probably more likely that you’re misunderstanding something vs the Python standard library has got something wrong, but for many other open-source projects, you should at least be open to the possibility that weird things might show up.&lt;/p&gt;

&lt;h2 id=&quot;follow-the-thread-wherever-it-leads&quot;&gt;Follow the thread wherever it leads&lt;/h2&gt;

&lt;p&gt;This is something about updating your assumptions as you move through the process of testing your assumptions. If you rule out certain pathways, then you should be prepared to go down the remaining ones as far as you need.&lt;/p&gt;

&lt;h2 id=&quot;be-systematic&quot;&gt;Be systematic&lt;/h2&gt;

&lt;p&gt;I’ve found a few times now, that there are certain moments where I notice I’m far &lt;em&gt;far&lt;/em&gt; down the road. I’ll have kept making a bunch of decisions at the various crossroads that I passed. At a certain moment, though, I need to take stock and just note down all the decisions and assumptions I’ve made in order to reach this point.&lt;/p&gt;

&lt;p&gt;I’ll write a short note to myself (mainly), but also for teammates, where I explain all the different assumptions and pathways that I’m travelling down. I’ll specifically write down all the conditions that need to be present for this bug to present (as far as I know them).&lt;/p&gt;

&lt;p&gt;Quite often, just writing these assumptions down will help me solve the problem outright. Even when it doesn’t, it’s extremely useful in re-grounding myself and reminding me of why I’m going down rabbit hole x or y.&lt;/p&gt;

&lt;h2 id=&quot;know-when-to-stop&quot;&gt;Know when to stop&lt;/h2&gt;

&lt;p&gt;In an ideal world you’d get to follow every windy road and to figure out everything that doesn’t make sense. But — and this is again especially true for fast-moving startups — you might not always have time to do that.&lt;/p&gt;

&lt;p&gt;This is somehow connected to the Pareto Principle (also known as the 80/20 rule). At a certain point you should make sure to check in with how much time you’d planned on spending on a particular bug. If you’re finding that it’s taking far longer than expected, and you have other things you’re committed to completing, then you should maybe take an opportunity to connect to your team. Alternatively, you can rescope and find a way to disable or flag a particular bug for the next sprint, or see if someone can help you with it.&lt;/p&gt;

&lt;h2 id=&quot;remember-this-is-the-work&quot;&gt;Remember: this is the work&lt;/h2&gt;

&lt;p&gt;Sometimes when I’m fixing bugs I have the feeling that I’m wasting my time somehow, or that I should be doing something more productive. It’s often the case, though, that this &lt;strong&gt;is&lt;/strong&gt; the work. I’m low on experience, but proxy experience that I’ve gained through reading books tells me that finding, fixing and triaging bugs is a lot of what we do as software engineers.&lt;/p&gt;

&lt;h2 id=&quot;know-when-to-ask-for-help&quot;&gt;Know when to ask for help&lt;/h2&gt;

&lt;p&gt;Sometimes there are bugs which turn out to be bigger than you’re able to handle. It’s certainly worth pushing back against that feeling the first few times you feel it. Early on it’s often going to feel like the bug is unsolvable.&lt;/p&gt;

&lt;p&gt;But some times there are pieces of context you don’t have, which a quick overview of what you’ve done and tried might alert someone more season to the fact that you’re going down the wrong alley. Or it might remind them of something they knew implicitly but had forgotten. The important things is to judge when is the right time to seek outside advice.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="debugging" /><summary type="html">I’ve had to deal with a whole bunch of bugs in the past few days and weeks. I thought it’d be useful to put down some thoughts about things that I’ve learned along the way.</summary></entry><entry><title type="html">Writing Code</title><link href="https://mlops.systems/python/skillbuilding/2021/09/18/writing-code.html" rel="alternate" type="text/html" title="Writing Code" /><published>2021-09-18T00:00:00-05:00</published><updated>2021-09-18T00:00:00-05:00</updated><id>https://mlops.systems/python/skillbuilding/2021/09/18/writing-code</id><content type="html" xml:base="https://mlops.systems/python/skillbuilding/2021/09/18/writing-code.html">&lt;p&gt;I read &lt;a href=&quot;https://daniel.feldroy.com/posts/code-code-code&quot;&gt;Daniel Roy Greenfeld’s post&lt;/a&gt; on how he found that coding &lt;em&gt;a lot&lt;/em&gt; was key to improving his skills. It makes sense. Everything I’ve read so far and my previous experience at the metaskill of learning new things tells me that it is a good investment of time.&lt;/p&gt;

&lt;p&gt;Just like you get good at writing by doing a lot of writing, on some level that is true for coding. (Of course, there are additional pieces to the puzzle: you have to develop some taste alongside the pure production side, you have to do some quality-control and refactor your code, and so on and so on.)&lt;/p&gt;

&lt;p&gt;For me, this looks like the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;coding at work during the week&lt;/li&gt;
  &lt;li&gt;smaller focused exercises from &lt;a href=&quot;https://www.pythonmorsels.com&quot;&gt;PythonMorsels&lt;/a&gt;, &lt;a href=&quot;https://exercism.org&quot;&gt;Exercism&lt;/a&gt;, &lt;a href=&quot;https://leetcode.com&quot;&gt;LeetCode&lt;/a&gt; and &lt;a href=&quot;https://www.algoexpert.io/product&quot;&gt;AlgoExpert&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;code written while working my way through the &lt;a href=&quot;https://course.fast.ai&quot;&gt;fastai course&lt;/a&gt;; this will probably manifest as blog posts here as well, outlining some small project I completed along the way.&lt;/li&gt;
  &lt;li&gt;a bigger project, perhaps a package, that I’ll start building at some point. I have some ideas for things I want to implement. I’ll pick one soon. It’ll probably be related in some way to the fastai coding. I’m thinking right now of making a tool that allows you to download PDFs and use the pages of those PDFs as image files in computer vision problems; a data ingestion tool, in other words.&lt;/li&gt;
  &lt;li&gt;smaller scripts to solve daily problems in my digital life. I’ll store those on &lt;a href=&quot;https://github.com/strickvl&quot;&gt;my GitHub&lt;/a&gt; somewhere and write up the design decisions around the more interesting ones here.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One thing I took note of was how Daniel mentioned that it made sense to specialise and focus on one language at a time, particularly in the early days. Rather than indulging my curiosity and doing 1001 things using Go or lisp or whatever, I will try to stick to Python at least until I feel more confident with it.&lt;/p&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="python" /><category term="skillbuilding" /><summary type="html">I read Daniel Roy Greenfeld’s post on how he found that coding a lot was key to improving his skills. It makes sense. Everything I’ve read so far and my previous experience at the metaskill of learning new things tells me that it is a good investment of time.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/writing-code.jpeg" /><media:content medium="image" url="https://mlops.systems/images/writing-code.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Reading Python Code</title><link href="https://mlops.systems/python/skillbuilding/2021/09/18/reading-python.html" rel="alternate" type="text/html" title="Reading Python Code" /><published>2021-09-18T00:00:00-05:00</published><updated>2021-09-18T00:00:00-05:00</updated><id>https://mlops.systems/python/skillbuilding/2021/09/18/reading-python</id><content type="html" xml:base="https://mlops.systems/python/skillbuilding/2021/09/18/reading-python.html">&lt;p&gt;It’s a truism of sorts that in order to improve your skills, you have to practice them. For coding, the stereotypical image is of someone typing, actually creating new things. But as often as not, you’re going to be &lt;em&gt;reading&lt;/em&gt; code instead. This code might be something you write yesterday or last year, or it might be something that someone else wrote.&lt;/p&gt;

&lt;p&gt;One way or another, reading code is a great way to get increasing familiarity with stylistic, syntactic patterns and to get exposed to some best practices, especially if you get to pick the code you’re reading.&lt;/p&gt;

&lt;p&gt;I’ll be doing the same as I ramp up my Python proficiency. I wanted to gather some lists of codebases and assorted resources in one place for myself, and I hope maybe it’ll be useful for someone else as well.&lt;/p&gt;

&lt;h2 id=&quot;good-quality-python-code&quot;&gt;Good Quality Python Code&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://jinja.palletsprojects.com/en/3.0.x/&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jinja&lt;/code&gt;&lt;/a&gt; — a templating engine written in Python (and see the recommendations for supplemental reading and watching for &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jinja&lt;/code&gt; &lt;a href=&quot;https://death.andgravity.com/aosa&quot;&gt;here&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/gleitz/howdoi&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;howdoi&lt;/code&gt;&lt;/a&gt; — a search tool for coding answers via the command line&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pallets/flask&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flask&lt;/code&gt;&lt;/a&gt; — a micro-web framework for Python&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://fastapi.tiangolo.com&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FastAPI&lt;/code&gt;&lt;/a&gt; — another web framework that’s a bit larger than flask&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/python-diamond/Diamond&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;diamond&lt;/code&gt;&lt;/a&gt; — a Python daemon that collects and publishes system metrics&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/pallets/werkzeug&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;werkzeug&lt;/code&gt;&lt;/a&gt; — a web server gateway library&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.python-requests.org/en/latest/&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;requests&lt;/code&gt;&lt;/a&gt; — an HTTP library, now part of the Python standard library&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://tablib.readthedocs.io/en/stable/&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;tablib&lt;/code&gt;&lt;/a&gt; — library for Pythonic way to work with tabular datasets&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://click.palletsprojects.com/en/8.0.x/&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;click&lt;/code&gt;&lt;/a&gt; — a Python package for creating command line interfaces&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.python.org/3/library/pathlib.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pathlib&lt;/code&gt;&lt;/a&gt; — part of the Python standard library; a module to handle filesystem paths (also the corresponding &lt;a href=&quot;https://www.python.org/dev/peps/pep-0428/&quot;&gt;PEP proposal #428&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.python.org/3/library/dataclasses.html&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;dataclasses&lt;/code&gt;&lt;/a&gt; — a module in the Python standard library; reduces boilerplate of writing classes (also the corresponding &lt;a href=&quot;https://www.python.org/dev/peps/pep-0557/&quot;&gt;PEP proposal #557&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://joblib.readthedocs.io/en/latest/&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;joblib&lt;/code&gt;&lt;/a&gt; — a library to support lightweight pipelining in Python&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;other-resources&quot;&gt;Other Resources&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://aosabook.org/en/index.html&quot;&gt;500 Lines or Less&lt;/a&gt; — a book in which specific small open-source projects are profiled to understand how they approached their particular challenge.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://aosabook.org/en/index.html&quot;&gt;The Architecture of Open Source Applications: Elegance, Evolution and a Few Fearless Hacks&lt;/a&gt; — examination of the structure of the software of some open-source software applications.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://aosabook.org/en/index.html&quot;&gt;The Architecture of Open Source Applications: Volume II: Structure, Scale and a Few More Fearless Hacks&lt;/a&gt; — the second volume in the series.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Alex Strick van Linschoten</name></author><category term="python" /><category term="skillbuilding" /><summary type="html">It’s a truism of sorts that in order to improve your skills, you have to practice them. For coding, the stereotypical image is of someone typing, actually creating new things. But as often as not, you’re going to be reading code instead. This code might be something you write yesterday or last year, or it might be something that someone else wrote.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://mlops.systems/images/python-code.jpeg" /><media:content medium="image" url="https://mlops.systems/images/python-code.jpeg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>